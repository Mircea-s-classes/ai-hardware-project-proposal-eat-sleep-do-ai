# from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def NT_matmul(var_rms_norm325: T.handle, lv806: T.Buffer((T.int64(6144), T.int64(4096)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        rms_norm325 = T.match_buffer(var_rms_norm325, (batch_size, T.int64(1), T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (batch_size, T.int64(1), T.int64(6144)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(6144), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm325[v_i0, v_i1, v_k], lv806[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm325[v_i0, v_i1, v_k] * lv806[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul1(var_reshape643: T.handle, lv808: T.Buffer((T.int64(4096), T.int64(4096)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape643 = T.match_buffer(var_reshape643, (batch_size, T.int64(1), T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (batch_size, T.int64(1), T.int64(4096)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(reshape643[v_i0, v_i1, v_k], lv808[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + reshape643[v_i0, v_i1, v_k] * lv808[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul10(rms_norm65: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), lv164: T.Buffer((T.int64(6144), T.int64(4096)), "float16"), NT_matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(6144)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(6144), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm65[v_i0, v_i1, v_k], lv164[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm65[v_i0, v_i1, v_k] * lv164[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul11(reshape131: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), lv166: T.Buffer((T.int64(4096), T.int64(4096)), "float16"), NT_matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(reshape131[v_i0, v_i1, v_k], lv166[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + reshape131[v_i0, v_i1, v_k] * lv166[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul12(rms_norm66: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), lv167: T.Buffer((T.int64(28672), T.int64(4096)), "float16"), NT_matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(28672)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(28672), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm66[v_i0, v_i1, v_k], lv167[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm66[v_i0, v_i1, v_k] * lv167[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul13(mul32: T.Buffer((T.int64(1), T.int64(1), T.int64(14336)), "float16"), lv168: T.Buffer((T.int64(4096), T.int64(14336)), "float16"), NT_matmul: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(14336)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(mul32[v_i0, v_i1, v_k], lv168[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + mul32[v_i0, v_i1, v_k] * lv168[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul14(rms_norm129: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), var_lv324: T.handle, var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        lv324 = T.match_buffer(var_lv324, (vocab_size, T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (T.int64(1), T.int64(1), vocab_size), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm129[v_i0, v_i1, v_k], lv324[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm129[v_i0, v_i1, v_k] * lv324[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul15(var_hidden_states: T.handle, var_lv1: T.handle, var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        hidden_states = T.match_buffer(var_hidden_states, (seq_len, T.int64(4096)), "float16")
        vocab_size = T.int64()
        lv1 = T.match_buffer(var_lv1, (vocab_size, T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (seq_len, vocab_size), "float16")
        # with T.block("root"):
        for i0, i1, k in T.grid(seq_len, vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(hidden_states[v_i0, v_k], lv1[v_i1, v_k])
                T.writes(NT_matmul[v_i0, v_i1])
                with T.init():
                    NT_matmul[v_i0, v_i1] = T.float16(0.0)
                NT_matmul[v_i0, v_i1] = NT_matmul[v_i0, v_i1] + hidden_states[v_i0, v_k] * lv1[v_i1, v_k]

    @T.prim_func(private=True)
    def NT_matmul2(var_rms_norm326: T.handle, lv809: T.Buffer((T.int64(28672), T.int64(4096)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        rms_norm326 = T.match_buffer(var_rms_norm326, (batch_size, T.int64(1), T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (batch_size, T.int64(1), T.int64(28672)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(28672), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm326[v_i0, v_i1, v_k], lv809[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm326[v_i0, v_i1, v_k] * lv809[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul3(var_mul160: T.handle, lv810: T.Buffer((T.int64(4096), T.int64(14336)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        mul160 = T.match_buffer(var_mul160, (batch_size, T.int64(1), T.int64(14336)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (batch_size, T.int64(1), T.int64(4096)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(4096), T.int64(14336)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(mul160[v_i0, v_i1, v_k], lv810[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + mul160[v_i0, v_i1, v_k] * lv810[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul4(var_rms_norm389: T.handle, var_lv966: T.handle, var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        rms_norm389 = T.match_buffer(var_rms_norm389, (batch_size, T.int64(1), T.int64(4096)), "float16")
        vocab_size = T.int64()
        lv966 = T.match_buffer(var_lv966, (vocab_size, T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (batch_size, T.int64(1), vocab_size), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm389[v_i0, v_i1, v_k], lv966[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm389[v_i0, v_i1, v_k] * lv966[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul5(var_rms_norm260: T.handle, lv645: T.Buffer((T.int64(6144), T.int64(4096)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm260 = T.match_buffer(var_rms_norm260, (T.int64(1), seq_len, T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (T.int64(1), seq_len, T.int64(6144)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(6144), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm260[v_i0, v_i1, v_k], lv645[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm260[v_i0, v_i1, v_k] * lv645[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul6(var_reshape515: T.handle, lv647: T.Buffer((T.int64(4096), T.int64(4096)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        reshape515 = T.match_buffer(var_reshape515, (T.int64(1), seq_len, T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(4096), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(reshape515[v_i0, v_i1, v_k], lv647[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + reshape515[v_i0, v_i1, v_k] * lv647[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul7(var_rms_norm261: T.handle, lv648: T.Buffer((T.int64(28672), T.int64(4096)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm261 = T.match_buffer(var_rms_norm261, (T.int64(1), seq_len, T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (T.int64(1), seq_len, T.int64(28672)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(28672), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm261[v_i0, v_i1, v_k], lv648[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + rms_norm261[v_i0, v_i1, v_k] * lv648[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul8(var_mul128: T.handle, lv649: T.Buffer((T.int64(4096), T.int64(14336)), "float16"), var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        mul128 = T.match_buffer(var_mul128, (T.int64(1), seq_len, T.int64(14336)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(4096), T.int64(14336)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(mul128[v_i0, v_i1, v_k], lv649[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + mul128[v_i0, v_i1, v_k] * lv649[v_i2, v_k]

    @T.prim_func(private=True)
    def NT_matmul9(var_take2: T.handle, var_lv805: T.handle, var_NT_matmul: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        take2 = T.match_buffer(var_take2, (T.int64(1), batch_size, T.int64(4096)), "float16")
        vocab_size = T.int64()
        lv805 = T.match_buffer(var_lv805, (vocab_size, T.int64(4096)), "float16")
        NT_matmul = T.match_buffer(var_NT_matmul, (T.int64(1), batch_size, vocab_size), "float16")
        # with T.block("root"):
        for i0, i1, i2, k in T.grid(T.int64(1), batch_size, vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(take2[v_i0, v_i1, v_k], lv805[v_i2, v_k])
                T.writes(NT_matmul[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul[v_i0, v_i1, v_i2] = NT_matmul[v_i0, v_i1, v_i2] + take2[v_i0, v_i1, v_k] * lv805[v_i2, v_k]

    @T.prim_func
    def _gather_hidden_states(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        m, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (m, n), "float16")
        batch_size = T.int32(is_size_var=True)
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        dst = T.match_buffer(var_dst, (batch_size, n), "float16")
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("gather_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[indices[vb], vj], indices[vb])
                T.writes(dst[vb, vj])
                dst[vb, vj] = src[indices[vb], vj]

    @T.prim_func
    def _scatter_hidden_states(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (batch_size, n), "float16")
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        m = T.int32(is_size_var=True)
        dst = T.match_buffer(var_dst, (m, n), "float16")
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("scatter_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[vb, vj], indices[vb])
                T.writes(dst[indices[vb], vj])
                dst[indices[vb], vj] = src[vb, vj]

    @T.prim_func
    def apply_bitmask_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_bitmask: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        bitmask = T.match_buffer(var_bitmask, (batch_size, (vocab_size + 31) // 32), "int32")
        # with T.block("root"):
        for fused_s_v_0 in T.thread_binding((num_seq * vocab_size + 1023) // 1024, thread="blockIdx.x"):
            for fused_s_v_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vs = T.axis.spatial(num_seq, (fused_s_v_0 * 1024 + fused_s_v_1) // vocab_size)
                    vv = T.axis.spatial(vocab_size, (fused_s_v_0 * 1024 + fused_s_v_1) % vocab_size)
                    T.where(fused_s_v_0 * 1024 + fused_s_v_1 < num_seq * vocab_size)
                    T.reads(bitmask[seq_ids[vs], vv // 32], seq_ids[vs], logits[seq_ids[vs], vv])
                    T.writes(logits[seq_ids[vs], vv])
                    logits[seq_ids[vs], vv] = T.if_then_else(T.bitwise_and(T.shift_right(bitmask[seq_ids[vs], vv // 32], vv % 32), 1) == 1, logits[seq_ids[vs], vv], T.float32(-340282346638528859811704183484516925440.0))

    @T.prim_func
    def apply_logit_bias_inplace(var_logits: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_logit_bias: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        logit_bias = T.match_buffer(var_logit_bias, (num_token,))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[pos2seq_id[vp], token_ids[vp]], pos2seq_id[vp], token_ids[vp], logit_bias[vp])
                    T.writes(logits[pos2seq_id[vp], token_ids[vp]])
                    logits[pos2seq_id[vp], token_ids[vp]] = logits[pos2seq_id[vp], token_ids[vp]] + logit_bias[vp]

    @T.prim_func
    def apply_penalty_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_token_cnt: T.handle, var_penalties: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        token_cnt = T.match_buffer(var_token_cnt, (num_token,), "int32")
        penalties = T.match_buffer(var_penalties, (num_seq, 3))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]], seq_ids[pos2seq_id[vp]], pos2seq_id[vp], token_ids[vp], penalties[pos2seq_id[vp], 0:3], token_cnt[vp])
                    T.writes(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] - (penalties[pos2seq_id[vp], 0] + T.Cast("float32", token_cnt[vp]) * penalties[pos2seq_id[vp], 1])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = T.if_then_else(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] < T.float32(0.0), logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] * penalties[pos2seq_id[vp], 2], logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] / penalties[pos2seq_id[vp], 2])

    @T.prim_func
    def batch_decode_paged_kv(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 128), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 8, 16, 128), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (B,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 128), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.12751743082459868)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(8, thread="blockIdx.y"):
                for ty in T.thread_binding(4, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        for tz in T.thread_binding(4, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[bx], q_rope_position[bx], Q[bx, fused_by_bz // 8 * 4 + fused_by_bz % 8 * 4 + ty, tx * 4 - 64:tx * 4 - 64 + 132])
                                T.writes(output[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((4, 4, 128), scope="shared")
                                md_allreduce = T.alloc_buffer((4, 4, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((4,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 8
                                bz: T.int32 = fused_by_bz // 8
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec + 64] * T.float16(-1.0), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 15) // 16):
                                    tile_start_s: T.int32 = tz * 4 + ty
                                    tile_start_g: T.int32 = (iterator * 4 + tz) * 4 + ty
                                    for j in range(1):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = row_g
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, pages[page_no, 0, by, page_offset, tx * 4 + vec + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 4 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 4 + tz) * 4 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(4):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 4 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(4):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by * 4 + bz * 4 + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by * 4 + bz * 4 + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_decode_paged_kv_sliding_window(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 128), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 8, 16, 128), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, B), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 128), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.12751743082459868)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(8, thread="blockIdx.y"):
                for ty in T.thread_binding(4, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        for tz in T.thread_binding(4, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[0:3, bx], q_rope_position[bx], Q[bx, fused_by_bz // 8 * 4 + fused_by_bz % 8 * 4 + ty, tx * 4 - 64:tx * 4 - 64 + 132])
                                T.writes(output[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((4, 4, 128), scope="shared")
                                md_allreduce = T.alloc_buffer((4, 4, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((4,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 8
                                bz: T.int32 = fused_by_bz // 8
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, batch_idx] - length_info[1, batch_idx] + length_info[2, batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec + 64] * T.float16(-1.0), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 15) // 16):
                                    tile_start_s: T.int32 = tz * 4 + ty
                                    tile_start_g: T.int32 = (iterator * 4 + tz) * 4 + ty
                                    for j in range(1):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = T.if_then_else(row_g < length_info[2, batch_idx], row_g, row_g - length_info[2, batch_idx] + length_info[1, batch_idx])
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, pages[page_no, 0, by, page_offset, tx * 4 + vec + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 4 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 4 + tz) * 4 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(4):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 4 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(4):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by * 4 + bz * 4 + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by * 4 + bz * 4 + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_prefill_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, pages[page_no, 0, by, page_offset, j + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_paged_kv_sliding_window(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, batch_size), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, b_idx] - length_info[1, b_idx] + length_info[2, b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, pages[page_no, 0, by, page_offset, j + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_ragged_kv(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_k_rope_pos_offset: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (kv_len, 8, 128), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[L_kv_base + cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, k[L_kv_base + cur_L, by, j + 64] * T.float16(-1.0), k[L_kv_base + cur_L, by, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), k[L_kv_base + cur_L, by, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                V_smem[i, j] = v[L_kv_base + cur_L, by, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_tree_attn(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_mn_indptr: T.handle, var_mask: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, batch_size: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 128), "float16")
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (kv_len, 8, 128), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        mn_indptr = T.match_buffer(var_mn_indptr, (batch_size + 1,), "int32", offset_factor=1)
        tree_size = T.int32(is_size_var=True)
        mask = T.match_buffer(var_mask, (tree_size, 2), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("KV_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_base + L_kv_start + i
                                                            if L_kv_start + i < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, k[cur_L, by, j + 64] * T.float16(-1.0), k[cur_L, by, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), k[cur_L, by, j])
                                                                V_smem[i, j] = v[cur_L, by, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func(private=True)
    def batch_verify_on_gpu_single_kernel(var_draft_probs: T.handle, var_draft_tokens: T.handle, var_model_probs: T.handle, var_token_tree_first_child: T.handle, var_token_tree_next_sibling: T.handle, var_uniform_samples: T.handle, var_token_tree_parent_ptr: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        num_nodes, vocab_size = T.int32(is_size_var=True), T.int64(is_size_var=True)
        draft_probs = T.match_buffer(var_draft_probs, (num_nodes, vocab_size))
        draft_tokens = T.match_buffer(var_draft_tokens, (num_nodes,), "int32")
        model_probs = T.match_buffer(var_model_probs, (num_nodes, vocab_size))
        token_tree_first_child = T.match_buffer(var_token_tree_first_child, (num_nodes,), "int32")
        token_tree_next_sibling = T.match_buffer(var_token_tree_next_sibling, (num_nodes,), "int32")
        uniform_samples = T.match_buffer(var_uniform_samples, (num_nodes,))
        nbatch = T.int32(is_size_var=True)
        token_tree_parent_ptr = T.match_buffer(var_token_tree_parent_ptr, (nbatch,), "int32")
        # with T.block("root"):
        child_ptr = T.alloc_buffer((1,), "int32", scope="local")
        parent_ptr = T.alloc_buffer((1,), "int32", scope="local")
        child_token = T.alloc_buffer((1,), "int32", scope="local")
        done = T.alloc_buffer((1,), "bool", scope="local")
        psum = T.alloc_buffer((1,), scope="local")
        t0 = T.alloc_buffer((1,), scope="local")
        model_prob_local = T.alloc_buffer((1,), scope="local")
        draft_prob_local = T.alloc_buffer((1,), scope="local")
        p_child = T.alloc_buffer((1,), scope="local")
        q_child = T.alloc_buffer((1,), scope="local")
        uniform_sample = T.alloc_buffer((1,), scope="local")
        pred_shared = T.alloc_buffer((1,), "bool", scope="shared")
        pred_local = T.alloc_buffer((1,), "bool", scope="local")
        for _bx in T.thread_binding(nbatch, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(token_tree_parent_ptr[b], token_tree_first_child[T.min(parent_ptr[0], child_ptr[0]):T.min(parent_ptr[0], child_ptr[0]) + (T.max(parent_ptr[0], child_ptr[0]) + 1 - T.min(parent_ptr[0], child_ptr[0]))], parent_ptr[0], done[0], child_ptr[0], draft_tokens[child_ptr[0]], model_probs[parent_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], child_token[0], draft_probs[child_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], uniform_samples[child_ptr[0]], p_child[0], uniform_sample[0], q_child[0], pred_shared[0], pred_local[0], model_prob_local[0], draft_prob_local[0], psum[0], t0[0], token_tree_next_sibling[child_ptr[0]])
                    T.writes(parent_ptr[0], child_ptr[0], done[0], child_token[0], p_child[0], q_child[0], uniform_sample[0], pred_shared[0], pred_local[0], psum[0], model_prob_local[0], draft_prob_local[0], t0[0], model_probs[parent_ptr[0], T.Cast("int64", tx):T.Cast("int64", tx) + ((vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) - T.int64(1023))], token_tree_parent_ptr[b])
                    parent_ptr[0] = token_tree_parent_ptr[b]
                    child_ptr[0] = token_tree_first_child[parent_ptr[0]]
                    done[0] = T.bool(False)
                    while not done[0]:
                        T.tvm_storage_sync("shared")
                        if child_ptr[0] == -1:
                            done[0] = T.bool(True)
                            T.tvm_storage_sync("shared")
                        else:
                            if tx == 0:
                                child_token[0] = draft_tokens[child_ptr[0]]
                                p_child[0] = model_probs[parent_ptr[0], child_token[0]]
                                q_child[0] = draft_probs[child_ptr[0], child_token[0]]
                                uniform_sample[0] = uniform_samples[child_ptr[0]]
                                pred_shared[0] = p_child[0] >= uniform_sample[0] * q_child[0]
                            T.tvm_storage_sync("shared")
                            pred_local[0] = pred_shared[0]
                            if pred_local[0]:
                                parent_ptr[0] = child_ptr[0]
                                child_ptr[0] = token_tree_first_child[child_ptr[0]]
                            else:
                                psum[0] = T.float32(0.0)
                                for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                    if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                        model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                        psum[0] = psum[0] + model_prob_local[0]
                                with T.block("block_cross_thread"):
                                    T.reads(psum[0])
                                    T.writes(t0[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), psum[0], T.bool(True), t0[0], tx)
                                if t0[0] < T.float32(9.9999999999999995e-08):
                                    parent_ptr[0] = child_ptr[0]
                                    child_ptr[0] = token_tree_first_child[child_ptr[0]]
                                else:
                                    for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                        if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                            model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                            model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)] = model_prob_local[0] / t0[0]
                                    child_ptr[0] = token_tree_next_sibling[child_ptr[0]]
                    if tx == 0:
                        token_tree_parent_ptr[b] = parent_ptr[0]

    @T.prim_func
    def chunk_lse(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        # with T.block("root"):
        A_pad = T.alloc_buffer((batch_size, num_chunks, T.int64(4096)))
        temp_max = T.alloc_buffer((batch_size, num_chunks))
        temp_sum = T.alloc_buffer((batch_size, num_chunks))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("pad"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2])
                T.writes(A_pad[v0, v1, v2])
                A_pad[v0, v1, v2] = T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), A[v0, v1 * T.int64(4096) + v2] / temperature[v0], A[v0, v1 * T.int64(4096) + v2]), T.float32(-340282346638528859811704183484516925440.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("max"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(A_pad[v0, v1, v2])
                T.writes(temp_max[v0, v1])
                with T.init():
                    temp_max[v0, v1] = T.float32(-340282346638528859811704183484516925440.0)
                temp_max[v0, v1] = T.max(temp_max[v0, v1], A_pad[v0, v1, v2])
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("sum_exp"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(temperature[v0], A_pad[v0, v1, v2], temp_max[v0, v1])
                T.writes(temp_sum[v0, v1])
                with T.init():
                    temp_sum[v0, v1] = T.float32(0.0)
                temp_sum[v0, v1] = temp_sum[v0, v1] + T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A_pad[v0, v1, v2] - temp_max[v0, v1]), T.Cast("float32", A_pad[v0, v1, v2] == temp_max[v0, v1])), T.float32(0.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(1)):
            with T.block("log"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], temp_sum[v0, v1], temp_max[v0, v1])
                T.writes(chunked_sum[v0, v1], chunked_max[v0, v1])
                chunked_sum[v0, v1] = T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.log(temp_sum[v0, v1]), temp_sum[v0, v1])
                chunked_max[v0, v1] = temp_max[v0, v1]

    @T.prim_func
    def compact_kv_copy(var_pages: T.handle, var_copy_length_indptr: T.handle, var_copy_src_dst_pos: T.handle, batch_size: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        num_pages = T.int32()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, 16, 128), "float16")
        copy_length_indptr = T.match_buffer(var_copy_length_indptr, (batch_size + 1,), "int32", offset_factor=1)
        total_copy_length = T.int32()
        copy_src_dst_pos = T.match_buffer(var_copy_src_dst_pos, (2, total_copy_length), "int32", offset_factor=1)
        with T.block("root"):
            T.reads()
            T.writes()
            for bhd_o in T.thread_binding(batch_size, thread="blockIdx.x"):
                for bhd_i in T.thread_binding(1024, thread="threadIdx.x"):
                    b: T.int32 = (bhd_o * 1024 + bhd_i) // 1024
                    h: T.int32 = (bhd_o * 1024 + bhd_i) // 128 % 8
                    d: T.int32 = (bhd_o * 1024 + bhd_i) % 128
                    if bhd_o * 1024 + bhd_i < batch_size * 8 * 128:
                        for i in range(copy_length_indptr[b + 1] - copy_length_indptr[b]):
                            src_pos: T.int32 = copy_src_dst_pos[0, copy_length_indptr[b] + i]
                            dst_pos: T.int32 = copy_src_dst_pos[1, copy_length_indptr[b] + i]
                            pages[dst_pos // 16, 0, h, dst_pos % 16, d] = pages[src_pos // 16, 0, h, src_pos % 16, d]
                            pages[dst_pos // 16, 1, h, dst_pos % 16, d] = pages[src_pos // 16, 1, h, src_pos % 16, d]

    @T.prim_func
    def copy_single_page(var_pages: T.handle, src_page_id: T.int64, tgt_page_id: T.int64, copy_length: T.int64):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        num_pages, page_size = T.int32(), T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, page_size, 128), "float16")
        # with T.block("root"):
        for b in T.thread_binding(copy_length, thread="blockIdx.x"):
            for t in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("copy"):
                    vh = T.axis.spatial(8, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) // (copy_length * T.int64(128))))
                    vp = T.axis.spatial(copy_length, (b * T.int64(1024) + T.Cast("int64", t)) % (copy_length * T.int64(128)) // T.int64(128))
                    vd = T.axis.spatial(128, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) % T.int64(128)))
                    T.where(b * T.int64(1024) + T.Cast("int64", t) < copy_length * T.int64(8) * T.int64(128))
                    T.reads(pages[src_page_id, 0:2, vh, vp, vd])
                    T.writes(pages[tgt_page_id, 0:2, vh, vp, vd])
                    pages[tgt_page_id, 0, vh, vp, vd] = pages[src_page_id, 0, vh, vp, vd]
                    pages[tgt_page_id, 1, vh, vp, vd] = pages[src_page_id, 1, vh, vp, vd]

    @T.prim_func(private=True)
    def dequantize(var_model_embed_tokens_q_weight: T.handle, var_model_embed_tokens_q_scale: T.handle, var_dequantize: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        model_embed_tokens_q_weight = T.match_buffer(var_model_embed_tokens_q_weight, (vocab_size, T.int64(412)), "uint32")
        model_embed_tokens_q_scale = T.match_buffer(var_model_embed_tokens_q_scale, (vocab_size, T.int64(103)), "float16")
        dequantize = T.match_buffer(var_dequantize, (vocab_size, T.int64(4096)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_embed_tokens_q_weight[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_embed_tokens_q_weight[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_embed_tokens_q_scale[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_embed_tokens_q_scale[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize1(model_layers_0_self_attn_qkv_proj_q_weight2: T.Buffer((T.int64(6144), T.int64(412)), "uint32"), model_layers_0_self_attn_qkv_proj_q_scale2: T.Buffer((T.int64(6144), T.int64(103)), "float16"), dequantize: T.Buffer((T.int64(6144), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_qkv_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_qkv_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_qkv_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_qkv_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize2(model_layers_0_self_attn_o_proj_q_weight2: T.Buffer((T.int64(4096), T.int64(412)), "uint32"), model_layers_0_self_attn_o_proj_q_scale2: T.Buffer((T.int64(4096), T.int64(103)), "float16"), dequantize: T.Buffer((T.int64(4096), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_o_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_o_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_o_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_o_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize3(model_layers_0_mlp_gate_up_proj_q_weight2: T.Buffer((T.int64(28672), T.int64(412)), "uint32"), model_layers_0_mlp_gate_up_proj_q_scale2: T.Buffer((T.int64(28672), T.int64(103)), "float16"), dequantize: T.Buffer((T.int64(28672), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_gate_up_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_gate_up_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_gate_up_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_gate_up_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize4(model_layers_0_mlp_down_proj_q_weight2: T.Buffer((T.int64(4096), T.int64(1436)), "uint32"), model_layers_0_mlp_down_proj_q_scale2: T.Buffer((T.int64(4096), T.int64(359)), "float16"), dequantize: T.Buffer((T.int64(4096), T.int64(14336)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_down_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_down_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_down_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_down_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func
    def full(var_result: T.handle, value: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch_size = T.int32(is_size_var=True)
        result = T.match_buffer(var_result, (batch_size, 1), "int32")
        # with T.block("root"):
        for i in range(batch_size):
            with T.block("block"):
                vi = T.axis.spatial(batch_size, i)
                T.reads()
                T.writes(result[vi, 0])
                result[vi, 0] = value

    @T.prim_func(private=True)
    def fuse_add_norm_decode(pA: T.handle, pB: T.handle, C: T.Buffer((4096,), "float16"), pO: T.handle, pAdd: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int32()
        A = T.match_buffer(pA, (batch_size, 1, 4096), "float16")
        B = T.match_buffer(pB, (batch_size, 1, 4096), "float16")
        O = T.match_buffer(pO, (batch_size, 1, 4096), "float16")
        add = T.match_buffer(pAdd, (batch_size, 1, 4096), "float16")
        # with T.block("root"):
        add_local = T.alloc_buffer((4,), "float16", scope="local")
        sum_shared = T.alloc_buffer((batch_size, 1), scope="shared")
        sum_local = T.alloc_buffer((1024, batch_size, 1), scope="local")
        for v_bx in T.thread_binding(batch_size, thread="blockIdx.x"):
            for v_tx in T.thread_binding(1024, thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                for i in range(4):
                    with T.block("T_add"):
                        bx = T.axis.spatial(batch_size, v_bx)
                        h = T.axis.spatial(4096, i * 1024 + v_tx)
                        T.reads(A[bx, 0, h], B[bx, 0, h])
                        T.writes(add_local[h // 1024])
                        add_local[h // 1024] = A[bx, 0, h] + B[bx, 0, h]
                    with T.block("T_write_back"):
                        bx = T.axis.spatial(batch_size, v_bx)
                        v_ax1 = T.axis.spatial(1, 0)
                        h = T.axis.spatial(4096, i * 1024 + v_tx)
                        T.reads(add_local[h // 1024])
                        T.writes(add[bx, v_ax1, h])
                        add[bx, v_ax1, h] = add_local[h // 1024]
                with T.block("T_multiply_red_rf_init"):
                    tx, bx = T.axis.remap("SS", [v_tx, v_bx])
                    T.reads()
                    T.writes(sum_local[tx, bx, 0])
                    sum_local[tx, bx, 0] = T.float32(0.0)
                for v_i, _j in T.grid(4, 1):
                    with T.block("T_multiply_red_rf_update"):
                        tx, bx, i = T.axis.remap("SSR", [v_tx, v_bx, v_i])
                        T.reads(sum_local[tx, bx, 0], add_local[i])
                        T.writes(sum_local[tx, bx, 0])
                        sum_local[tx, bx, 0] = sum_local[tx, bx, 0] + T.Cast("float32", add_local[i]) * T.Cast("float32", add_local[i])
            for _j in range(1):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_multiply_red"):
                        tx, bx = T.axis.remap("RS", [v_tx_2, v_bx])
                        T.reads(sum_local[tx, bx, 0])
                        T.writes(sum_shared[bx, 0])
                        with T.init():
                            sum_shared[bx, 0] = T.float32(0.0)
                        sum_shared[bx, 0] = sum_shared[bx, 0] + sum_local[tx, bx, 0]
            for i in range(4):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_cast_2"):
                        bx = T.axis.spatial(batch_size, v_bx)
                        h = T.axis.spatial(4096, i * 1024 + v_tx_2)
                        T.reads(sum_shared[bx, 0], add_local[h // 1024], C[h])
                        T.writes(O[bx, 0, h])
                        O[bx, 0, h] = T.Cast("float16", T.rsqrt(sum_shared[bx, 0] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05)) * T.Cast("float32", add_local[h // 1024]) * T.Cast("float32", C[h]))

    @T.prim_func(private=True)
    def fuse_add_norm_prefill(pA: T.handle, pB: T.handle, C: T.Buffer((4096,), "float16"), pO: T.handle, pAdd: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        seq_len = T.int32()
        A = T.match_buffer(pA, (1, seq_len, 4096), "float16")
        B = T.match_buffer(pB, (1, seq_len, 4096), "float16")
        O = T.match_buffer(pO, (1, seq_len, 4096), "float16")
        add = T.match_buffer(pAdd, (1, seq_len, 4096), "float16")
        # with T.block("root"):
        add_local = T.alloc_buffer((4,), "float16", scope="local")
        sum_shared = T.alloc_buffer((1, seq_len), scope="shared")
        sum_local = T.alloc_buffer((1024, 1, seq_len), scope="local")
        for v_bx in T.thread_binding(seq_len, thread="blockIdx.x"):
            for v_tx in T.thread_binding(1024, thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                for v_i in range(4):
                    with T.block("T_add"):
                        bx = T.axis.spatial(seq_len, v_bx)
                        h = T.axis.spatial(4096, v_i * 1024 + v_tx)
                        T.reads(A[0, bx, h], B[0, bx, h])
                        T.writes(add_local[h // 1024])
                        add_local[h // 1024] = A[0, bx, h] + B[0, bx, h]
                    with T.block("T_write_back"):
                        bx = T.axis.spatial(seq_len, v_bx)
                        h = T.axis.spatial(4096, v_i * 1024 + v_tx)
                        T.reads(add_local[h // 1024])
                        T.writes(add[0, bx, h])
                        add[0, bx, h] = add_local[h // 1024]
                with T.block("T_multiply_red_rf_init"):
                    tx, bx = T.axis.remap("SS", [v_tx, v_bx])
                    T.reads()
                    T.writes(sum_local[tx, 0, bx])
                    sum_local[tx, 0, bx] = T.float32(0.0)
                for v_i, _j in T.grid(4, 1):
                    with T.block("T_multiply_red_rf_update"):
                        tx, bx, i = T.axis.remap("SSR", [v_tx, v_bx, v_i])
                        T.reads(sum_local[tx, 0, bx], add_local[i])
                        T.writes(sum_local[tx, 0, bx])
                        sum_local[tx, 0, bx] = sum_local[tx, 0, bx] + T.Cast("float32", add_local[i]) * T.Cast("float32", add_local[i])
            for _j in range(1):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_multiply_red"):
                        tx, bx = T.axis.remap("RS", [v_tx_2, v_bx])
                        T.reads(sum_local[tx, 0, bx])
                        T.writes(sum_shared[0, bx])
                        with T.init():
                            sum_shared[0, bx] = T.float32(0.0)
                        sum_shared[0, bx] = sum_shared[0, bx] + sum_local[tx, 0, bx]
            for v_i in range(4):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_cast_2"):
                        bx = T.axis.spatial(seq_len, v_bx)
                        v1 = T.axis.spatial(4096, v_i * 1024 + v_tx_2)
                        T.reads(sum_shared[0, bx], add_local[v1 // 1024], C[v1])
                        T.writes(O[0, bx, v1])
                        O[0, bx, v1] = T.Cast("float16", T.rsqrt(sum_shared[0, bx] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05)) * T.Cast("float32", add_local[v1 // 1024]) * T.Cast("float32", C[v1]))

    @T.prim_func
    def fused_rope(var_qkv: T.handle, var_position_map: T.handle, var_q: T.handle, var_k: T.handle, var_v: T.handle, apply_rope: T.int32):
        T.func_attr({"op_pattern": 8, "target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        seq_len = T.int32()
        qkv = T.match_buffer(var_qkv, (seq_len, 48, 128), "float16")
        position_map = T.match_buffer(var_position_map, (seq_len,), "int32", offset_factor=1)
        q = T.match_buffer(var_q, (seq_len, 32, 128), "float16")
        k = T.match_buffer(var_k, (seq_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (seq_len, 8, 128), "float16")
        # with T.block("root"):
        for iters_0, iters_1, iters_2 in T.grid(seq_len, 48, 128):
            with T.block("llama_fused_rope"):
                s, h, d = T.axis.remap("SSS", [iters_0, iters_1, iters_2])
                T.reads(position_map[s], qkv[s, h, d - 64:d - 64 + 129])
                T.writes(q[s, h, d], k[s, h - 32, d], v[s, h - 40, d])
                if h < 32:
                    freq = T.float32()
                    q[s, h, d] = T.if_then_else(apply_rope > 0 and d < 128, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 64, qkv[s, h, d + 64] * T.float16(-1.0), qkv[s, h, d - 64]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(500000.0), T.Cast("float32", d * 2 % 128) / T.float32(128.0))}), qkv[s, h, d])
                else:
                    if h < 40:
                        freq = T.float32()
                        k[s, h - 32, d] = T.if_then_else(apply_rope > 0 and d < 128, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 64, qkv[s, h, d + 64] * T.float16(-1.0), qkv[s, h, d - 64]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(500000.0), T.Cast("float32", d * 2 % 128) / T.float32(128.0))}), qkv[s, h, d])
                    else:
                        v[s, h - 40, d] = qkv[s, h, d]

    @T.prim_func
    def gather_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        m, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (m, n))
        batch_size = T.int32(is_size_var=True)
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        dst = T.match_buffer(var_dst, (batch_size, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("gather_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[indices[vb], vj], indices[vb])
                T.writes(dst[vb, vj])
                dst[vb, vj] = src[indices[vb], vj]

    @T.prim_func(private=True)
    def get_index_from_sorted(A: T.handle, B: T.handle, C: T.handle, D: T.handle, E: T.handle, F: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        indices = T.match_buffer(B, (batch, vocab_size), "int32")
        renorm_prob = T.match_buffer(C, (batch, 1))
        out_batch = T.int64(is_size_var=True)
        usample = T.match_buffer(D, (out_batch, 1))
        sample_indices = T.match_buffer(E, (out_batch, 1), "int32")
        output_index = T.match_buffer(F, (out_batch, 1), "int32")
        # with T.block("root"):
        for ax0, ax1 in T.grid(out_batch, vocab_size):
            with T.block("T_get_index_from_sorted"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(usample[v_ax0, T.int64(0)], cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1):v_ax1 - T.int64(1) + T.int64(2)], sample_indices[v_ax0, T.int64(0)], renorm_prob[sample_indices[v_ax0, T.int64(0)], 0], indices[sample_indices[v_ax0, T.int64(0)], T.min(T.int64(0), v_ax1):T.min(T.int64(0), v_ax1) + (T.max(T.int64(0), v_ax1) + T.int64(1) - T.min(T.int64(0), v_ax1))])
                T.writes(output_index[v_ax0, 0])
                if usample[v_ax0, T.int64(0)] < cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0] or v_ax1 + T.int64(1) == vocab_size:
                    if v_ax1 == T.int64(0):
                        output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], 0]
                    else:
                        if usample[v_ax0, T.int64(0)] >= cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1)] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0]:
                            output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], v_ax1]

    @T.prim_func(private=True)
    def get_renorm_prob(A: T.handle, B: T.handle, C: T.handle, D: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        top_p = T.match_buffer(B, (batch, 1))
        top_k = T.match_buffer(C, (batch, 1), "int32")
        renorm_prob = T.match_buffer(D, (batch, 1))
        # with T.block("root"):
        for ax0, ax1 in T.grid(batch, vocab_size):
            with T.block("T_get_renorm_prob"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(cumsum_sorted[v_ax0, T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)):T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + (T.max(T.max(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + T.int64(1) - T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)))], top_p[v_ax0, 0], top_k[v_ax0, 0])
                T.writes(renorm_prob[v_ax0, 0])
                if not (cumsum_sorted[v_ax0, 0] < top_p[v_ax0, 0] and top_k[v_ax0, 0] > 1):
                    renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, 0]
                else:
                    if cumsum_sorted[v_ax0, v_ax1] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0]):
                        if v_ax1 + T.int64(1) == vocab_size:
                            renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1]
                        else:
                            if not (cumsum_sorted[v_ax0, v_ax1 + T.int64(1)] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0])):
                                renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1 + T.int64(1)]

    @T.prim_func(private=True)
    def index(var_rms_norm64: T.handle, index: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm64 = T.match_buffer(var_rms_norm64, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        for i, _, k in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("index"):
                v_i, v__, v_k = T.axis.remap("SSS", [i, _, k])
                T.reads(rms_norm64[v_i, seq_len - T.int64(1), v_k])
                T.writes(index[v_i, v__, v_k])
                index[v_i, v__, v_k] = rms_norm64[v_i, seq_len - T.int64(1), v_k]

    @T.prim_func
    def merge_state_inplace(v: T.handle, s: T.handle, v_other: T.handle, s_other: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        N, H, D = T.int32(is_size_var=True), T.int32(is_size_var=True), T.int32(is_size_var=True)
        V = T.match_buffer(v, (N, H, D), "float16")
        S = T.match_buffer(s, (N, H))
        V_other = T.match_buffer(v_other, (N, H, D), "float16")
        S_other = T.match_buffer(s_other, (N, H))
        # with T.block("root"):
        for bx in T.thread_binding(N, thread="blockIdx.x"):
            for by in T.thread_binding(1, thread="blockIdx.y"):
                for ty in T.thread_binding(32, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("merge"):
                            T.reads(S[bx, ty + by * 32], S_other[bx, ty + by * 32], V[bx, ty + by * 32, tx * 4:tx * 4 + 4], V_other[bx, ty + by * 32, tx * 4:tx * 4 + 4])
                            T.writes(V[bx, ty + by * 32, tx * 4:tx * 4 + 4], S[bx, ty + by * 32])
                            s_val = T.alloc_buffer((1,), scope="local")
                            s_other_val = T.alloc_buffer((1,), scope="local")
                            s_max = T.alloc_buffer((1,), scope="local")
                            scale = T.alloc_buffer((1,), scope="local")
                            other_scale = T.alloc_buffer((1,), scope="local")
                            v_vec = T.alloc_buffer((4,), "float16", scope="local")
                            v_other_vec = T.alloc_buffer((4,), "float16", scope="local")
                            s_val[0] = S[bx, ty + by * 32]
                            s_other_val[0] = S_other[bx, ty + by * 32]
                            s_max[0] = T.max(s_val[0], s_other_val[0])
                            s_val[0] = T.exp2(s_val[0] - s_max[0])
                            s_other_val[0] = T.exp2(s_other_val[0] - s_max[0])
                            scale[0] = s_val[0] / (s_val[0] + s_other_val[0])
                            other_scale[0] = s_other_val[0] / (s_val[0] + s_other_val[0])
                            for vec in T.vectorized(4):
                                v_vec[vec] = V[bx, ty + by * 32, tx * 4 + vec]
                            for vec in T.vectorized(4):
                                v_other_vec[vec] = V_other[bx, ty + by * 32, tx * 4 + vec]
                            for vec in range(4):
                                v_vec[vec] = T.Cast("float16", T.Cast("float32", v_vec[vec]) * scale[0] + T.Cast("float32", v_other_vec[vec]) * other_scale[0])
                            for vec in T.vectorized(4):
                                V[bx, ty + by * 32, tx * 4 + vec] = v_vec[vec]
                            S[bx, ty + by * 32] = T.log2(s_val[0] + s_other_val[0]) + s_max[0]

    @T.prim_func
    def sampler_take_probs_tir(var_unsorted_probs: T.handle, var_sorted_indices: T.handle, var_sample_indices: T.handle, var_sampling_results: T.handle, var_top_prob_offsets: T.handle, var_sampled_values: T.handle, var_top_prob_probs: T.handle, var_top_prob_indices: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        unsorted_probs = T.match_buffer(var_unsorted_probs, (batch_size, vocab_size))
        sorted_indices = T.match_buffer(var_sorted_indices, (batch_size, vocab_size), "int32")
        num_samples = T.int32(is_size_var=True)
        sample_indices = T.match_buffer(var_sample_indices, (num_samples,), "int32")
        sampling_results = T.match_buffer(var_sampling_results, (num_samples,), "int32")
        num_positions = T.int32(is_size_var=True)
        top_prob_offsets = T.match_buffer(var_top_prob_offsets, (num_positions,), "int32")
        sampled_values = T.match_buffer(var_sampled_values, (num_samples,))
        top_prob_probs = T.match_buffer(var_top_prob_probs, (num_positions,))
        top_prob_indices = T.match_buffer(var_top_prob_indices, (num_positions,), "int32")
        # with T.block("root"):
        for i in range(num_positions + num_samples):
            with T.block("block"):
                vi = T.axis.spatial(num_positions + num_samples, i)
                T.reads(top_prob_offsets[vi], sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], unsorted_probs[T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]):T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + (T.max(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + 1 - T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions])), T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]):T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + (T.max(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + 1 - T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]))], sample_indices[vi - num_positions], sampling_results[vi - num_positions])
                T.writes(top_prob_indices[vi], top_prob_probs[vi], sampled_values[vi - num_positions])
                if vi < num_positions:
                    row: T.int32 = top_prob_offsets[vi] // vocab_size
                    col: T.int32 = top_prob_offsets[vi] % vocab_size
                    top_prob_indices[vi] = sorted_indices[row, col]
                    top_prob_probs[vi] = unsorted_probs[row, sorted_indices[row, col]]
                else:
                    vj: T.int32 = vi - num_positions
                    sampled_values[vj] = unsorted_probs[sample_indices[vj], sampling_results[vj]]

    @T.prim_func
    def scatter_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (batch_size, n))
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        m = T.int32(is_size_var=True)
        dst = T.match_buffer(var_dst, (m, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("scatter_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[vb, vj], indices[vb])
                T.writes(dst[indices[vb], vj])
                dst[indices[vb], vj] = src[vb, vj]

    @T.prim_func
    def softmax_with_chunked_sum(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle, var_softmax: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        softmax = T.match_buffer(var_softmax, (batch_size, vocab_size))
        # with T.block("root"):
        temp_max_shared = T.alloc_buffer((batch_size,), scope="shared")
        temp_sum_shared = T.alloc_buffer((batch_size,), scope="shared")
        for l0_l1_fused in T.thread_binding(batch_size * num_chunks, thread="blockIdx.x"):
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("max"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(chunked_max[v0, v1])
                        T.writes(temp_max_shared[v0])
                        with T.init():
                            temp_max_shared[v0] = T.float32(-340282346638528859811704183484516925440.0)
                        temp_max_shared[v0] = T.max(temp_max_shared[v0], chunked_max[v0, v1])
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("sum_exp"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(temperature[v0], chunked_sum[v0, v1], chunked_max[v0, v1], temp_max_shared[v0])
                        T.writes(temp_sum_shared[v0])
                        with T.init():
                            temp_sum_shared[v0] = T.float32(0.0)
                        temp_sum_shared[v0] = temp_sum_shared[v0] + T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(chunked_sum[v0, v1] + chunked_max[v0, v1] - temp_max_shared[v0]), T.Cast("float32", chunked_max[v0, v1] == temp_max_shared[v0]) * chunked_sum[v0, v1])
            for l2_0 in T.serial(T.int64(4), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                for l2_1 in T.thread_binding(T.int64(32), thread="threadIdx.y"):
                    for l2_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        with T.block("log_pad"):
                            v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                            v1 = T.axis.spatial(num_chunks, l0_l1_fused % num_chunks)
                            v2 = T.axis.spatial(T.int64(4096), l2_0 * T.int64(1024) + l2_1 * T.int64(32) + l2_2)
                            T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2], temp_sum_shared[v0], temp_max_shared[v0])
                            T.writes(softmax[v0, v1 * T.int64(4096) + v2])
                            if v1 * T.int64(4096) + v2 < vocab_size:
                                softmax[v0, v1 * T.int64(4096) + v2] = T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A[v0, v1 * T.int64(4096) + v2] / temperature[v0] - (T.log(temp_sum_shared[v0]) + temp_max_shared[v0])), T.Cast("float32", A[v0, v1 * T.int64(4096) + v2] == temp_max_shared[v0]) / temp_sum_shared[v0])

    @T.prim_func(private=True)
    def take_sorted_probs(var_probs: T.handle, var_lv1: T.handle, var_take_sorted_probs: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(), T.int64()
        probs = T.match_buffer(var_probs, (batch_size, vocab_size))
        lv1 = T.match_buffer(var_lv1, (batch_size, vocab_size), "int32")
        batch_size_1, vocab_size_1 = T.int64(is_size_var=True), T.int64(is_size_var=True)
        take_sorted_probs = T.match_buffer(var_take_sorted_probs, (batch_size_1, vocab_size_1))
        # with T.block("root"):
        for i, j in T.grid(batch_size_1, vocab_size_1):
            with T.block("take_sorted_probs"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads(probs[v_i, lv1[v_i, v_j]], lv1[v_i, v_j])
                T.writes(take_sorted_probs[v_i, v_j])
                take_sorted_probs[v_i, v_j] = probs[v_i, lv1[v_i, v_j]]

    @T.prim_func
    def tir_kv_cache_debug_get_kv(var_pages: T.handle, var_position_map: T.handle, var_k_data: T.handle, var_v_data: T.handle, layer_id: T.int64):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        num_pages, page_size = T.int64(), T.int64(is_size_var=True)
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, page_size, 128), "float16")
        seqlen = T.int64(is_size_var=True)
        position_map = T.match_buffer(var_position_map, (seqlen,), "int32", offset_factor=1)
        k_data = T.match_buffer(var_k_data, (32, seqlen, 8, 128), "float16")
        v_data = T.match_buffer(var_v_data, (32, seqlen, 8, 128), "float16")
        # with T.block("root"):
        for p, h, d in T.grid(seqlen, 8, 128):
            with T.block("copy0"):
                vp, vh, vd = T.axis.remap("SSS", [p, h, d])
                T.reads(position_map[vp], pages[T.Cast("int64", position_map[vp]) // page_size, 0:2, vh, T.Cast("int64", position_map[vp]) % page_size, vd])
                T.writes(k_data[layer_id, vp, vh, vd], v_data[layer_id, vp, vh, vd])
                position: T.int32 = position_map[vp]
                k_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 0, vh, T.Cast("int64", position) % page_size, vd]
                v_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 1, vh, T.Cast("int64", position) % page_size, vd]

    @T.prim_func
    def tir_kv_cache_transpose_append(var_pages: T.handle, var_k_data: T.handle, var_v_data: T.handle, var_position_map: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        num_pages = T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, 16, 128), "float16")
        ntoken = T.int64(is_size_var=True)
        k_data = T.match_buffer(var_k_data, (ntoken, 8, 128), "float16")
        v_data = T.match_buffer(var_v_data, (ntoken, 8, 128), "float16")
        position_map = T.match_buffer(var_position_map, (ntoken,), "int32", offset_factor=1)
        # with T.block("root"):
        for global_pos, h, f in T.grid(ntoken, 8, 128):
            if position_map[global_pos] != -1:
                with T.block("k_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], k_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 0, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 0, vh, position % 16, vf] = k_data[vgpos, vh, vf]
                with T.block("v_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], v_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 1, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 1, vh, position % 16, vf] = v_data[vgpos, vh, vf]

    @T.prim_func(private=True)
    def top_p_pivot_cutoff(var_prob: T.handle, var_top_p_arr: T.handle, var_init_pivots: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        top_p_arr = T.match_buffer(var_top_p_arr, (B,))
        init_pivots = T.match_buffer(var_init_pivots, (B, 3))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        # with T.block("root"):
        pivot = T.alloc_buffer((3,), scope="local")
        top_p = T.alloc_buffer((1,), scope="local")
        L = T.alloc_buffer((1,), scope="shared")
        R_1 = T.alloc_buffer((1,), scope="shared")
        L_local = T.alloc_buffer((1,), scope="local")
        R_local = T.alloc_buffer((1,), scope="local")
        q = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((3,), scope="local")
        lmin_broadcast = T.alloc_buffer((1,), scope="shared")
        lmin_broadcast_local = T.alloc_buffer((1,), scope="local")
        lmin = T.alloc_buffer((3,), scope="local")
        cmin = T.alloc_buffer((3,), "int32", scope="local")
        total_sum = T.alloc_buffer((1,), scope="local")
        it = T.alloc_buffer((1,), "int32", scope="local")
        es_local = T.alloc_buffer((1,), "bool", scope="local")
        es = T.alloc_buffer((1,), "bool", scope="shared")
        find_pivot_local = T.alloc_buffer((1,), "bool", scope="local")
        find_pivot = T.alloc_buffer((1,), "bool", scope="shared")
        total_sum_reduce = T.alloc_buffer((1,), scope="local")
        lsum_reduce = T.alloc_buffer((1,), scope="local")
        lmin_reduce = T.alloc_buffer((1,), scope="local")
        cmin_reduce = T.alloc_buffer((1,), "int32", scope="local")
        for _bx in T.thread_binding(B, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(top_p_arr[b], top_p[0], L[0], R_1[0], init_pivots[b, 0:3], L_local[0], R_local[0], find_pivot_local[0], it[0], es_local[0], prob[b, it[0] * 1024 + tx], total_sum[0], q[0], pivot[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lsum[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], cmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], total_sum_reduce[0], es[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], lsum_reduce[0], cmin_reduce[0], find_pivot[0])
                    T.writes(top_p[0], L[0], R_1[0], find_pivot[0], L_local[0], R_local[0], pivot[0:3], find_pivot_local[0], final_lsum[b], final_pivot[b], lsum[0:3], lmin[0:3], cmin[0:3], total_sum[0], it[0], es_local[0], q[0], total_sum_reduce[0], es[0], lsum_reduce[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], cmin_reduce[0])
                    top_p[0] = top_p_arr[b]
                    if tx == 0:
                        L[0] = T.float32(1.0) - top_p[0]
                        R_1[0] = T.float32(9.9999999999999995e-08)
                        find_pivot[0] = T.bool(False)
                    T.tvm_storage_sync("shared")
                    L_local[0] = L[0]
                    R_local[0] = R_1[0]
                    for i in T.unroll(3):
                        pivot[i] = init_pivots[b, i]
                    find_pivot_local[0] = T.bool(False)
                    if L_local[0] - R_local[0] <= T.float32(9.9999999999999995e-08):
                        if tx == 0:
                            final_lsum[b] = T.float32(1.0)
                            final_pivot[b] = T.float32(0.0)
                        find_pivot_local[0] = T.bool(True)
                    while T.tvm_thread_invariant(L_local[0] - R_local[0] > T.float32(9.9999999999999995e-08) and not find_pivot_local[0]):
                        T.tvm_storage_sync("shared")
                        for pidx in T.unroll(3):
                            lsum[pidx] = T.float32(0.0)
                            lmin[pidx] = T.float32(340282346638528859811704183484516925440.0)
                            cmin[pidx] = 0
                        total_sum[0] = T.float32(0.0)
                        it[0] = 0
                        es_local[0] = T.bool(False)
                        while it[0] < (N + 1024 - 1) // 1024 and not es_local[0]:
                            q[0] = T.if_then_else(it[0] * 1024 + tx < N, prob[b, it[0] * 1024 + tx], T.float32(0.0))
                            total_sum[0] = total_sum[0] + q[0]
                            for pidx in T.unroll(3):
                                if q[0] >= pivot[pidx]:
                                    lsum[pidx] = lsum[pidx] + q[0]
                                    if lmin[pidx] > q[0]:
                                        lmin[pidx] = q[0]
                                        cmin[pidx] = 1
                                    else:
                                        if lmin[pidx] == q[0]:
                                            cmin[pidx] = cmin[pidx] + 1
                            it[0] = it[0] + 1
                            if it[0] % 32 == 0:
                                with T.block("block_cross_thread"):
                                    T.reads(total_sum[0])
                                    T.writes(total_sum_reduce[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), total_sum[0], T.bool(True), total_sum_reduce[0], tx)
                                if tx == 0:
                                    es[0] = T.float32(1.0) - total_sum_reduce[0] < pivot[2]
                                T.tvm_storage_sync("shared")
                                es_local[0] = es[0]
                        T.tvm_storage_sync("shared")
                        for pidx in range(3):
                            with T.block("block_cross_thread"):
                                T.reads(lsum[pidx])
                                T.writes(lsum_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lsum[pidx], T.bool(True), lsum_reduce[0], tx)
                            with T.block("block_cross_thread"):
                                T.reads(lmin[pidx])
                                T.writes(lmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: T.min(x0, y0), [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lmin[pidx], T.bool(True), lmin_reduce[0], tx)
                            if tx == 0:
                                lmin_broadcast[0] = lmin_reduce[0]
                            T.tvm_storage_sync("shared")
                            lmin_broadcast_local[0] = lmin_broadcast[0]
                            if lmin[pidx] > lmin_broadcast_local[0]:
                                cmin[pidx] = 0
                            if tx == 0:
                                lsum[pidx] = lsum_reduce[0]
                                lmin[pidx] = lmin_reduce[0]
                            with T.block("block_cross_thread"):
                                T.reads(cmin[pidx])
                                T.writes(cmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [0]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), cmin[pidx], T.bool(True), cmin_reduce[0], tx)
                            if tx == 0:
                                cmin[pidx] = cmin_reduce[0]
                        T.tvm_storage_sync("shared")
                        if tx == 0:
                            it[0] = 0
                            while it[0] < 3 and not find_pivot_local[0]:
                                if lsum[it[0]] >= top_p[0] and top_p[0] > lsum[it[0]] - T.Cast("float32", cmin[it[0]]) * lmin[it[0]]:
                                    find_pivot[0] = T.bool(True)
                                    find_pivot_local[0] = T.bool(True)
                                    final_pivot[b] = pivot[it[0]]
                                    final_lsum[b] = lsum[it[0]]
                                else:
                                    if lsum[it[0]] - lmin[it[0]] * T.Cast("float32", cmin[it[0]]) >= top_p[0]:
                                        R_1[0] = pivot[it[0]]
                                        final_lsum[b] = lsum[it[0]]
                                    else:
                                        if lsum[it[0]] < top_p[0]:
                                            L[0] = pivot[it[0]]
                                it[0] = it[0] + 1
                        T.tvm_storage_sync("shared")
                        L_local[0] = L[0]
                        R_local[0] = R_1[0]
                        find_pivot_local[0] = find_pivot[0]
                        for pidx in T.unroll(3):
                            pivot[pidx] = L[0] - T.Cast("float32", pidx + 1) * (L_local[0] - R_local[0]) / T.float32(4.0)
                    if tx == 0:
                        if not find_pivot_local[0]:
                            final_pivot[b] = R_local[0]
                            if R_local[0] == T.float32(9.9999999999999995e-08):
                                final_lsum[b] = lsum[2]

    @T.prim_func(private=True)
    def top_p_renorm_after_cutoff(var_prob: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle, var_renorm_prob: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        renorm_prob = T.match_buffer(var_renorm_prob, (B, N))
        # with T.block("root"):
        pivot = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((1,), scope="local")
        for _by in T.thread_binding(B, thread="blockIdx.y"):
            for _bx in T.thread_binding(511 // B + 1, thread="blockIdx.x"):
                for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("CTA"):
                        by, bx, tx = T.axis.remap("SSS", [_by, _bx, _tx])
                        T.reads(final_pivot[by], final_lsum[by], prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)], pivot[0], lsum[0])
                        T.writes(pivot[0], lsum[0], renorm_prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)])
                        pivot[0] = final_pivot[by]
                        lsum[0] = final_lsum[by]
                        for i in range((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024)):
                            if i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx < N:
                                renorm_prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] = T.if_then_else(prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] >= pivot[0], prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] / lsum[0], T.float32(0.0))

    @T.prim_func
    def tree_attn_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, tree_order_indptr_handle: T.handle, tree_order_handle: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        tree_order_indptr = T.match_buffer(tree_order_indptr_handle, (batch_size + 1,), "int32", offset_factor=1)
        total_tree_order_len = T.int32(is_size_var=True)
        tree_order = T.match_buffer(tree_order_handle, (total_tree_order_len, 2), "int32", offset_factor=1)
        # with T.block("root"):
        assert rotary_mode == 0, "Inline rotary mode is not supported in tree attention."
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                K_smem[i, j] = pages[page_no, 0, by, page_offset, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @R.function
    def alloc_embedding_tensor() -> R.Tensor((8192, 4096), dtype="float16"):
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        gv: R.Tensor((8192, 4096), dtype="float16") = R.builtin.alloc_tensor(R.shape([8192, 4096]), R.dtype("float16"), R.prim_value(0), R.str("global"))
        return gv

    @R.function
    def argsort_probs(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32")) -> R.Tuple(R.Tensor(("batch_size", "vocab_size"), dtype="float32"), R.Tensor(("batch_size", "vocab_size"), dtype="int32")):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv1: R.Tensor((batch_size, vocab_size), dtype="int32") = R.argsort(probs, axis=-1, descending=True, dtype="int32")
            lv2 = R.call_tir(cls.take_sorted_probs, (probs, lv1), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv1: R.Tuple(R.Tensor((batch_size, vocab_size), dtype="float32"), R.Tensor((batch_size, vocab_size), dtype="int32")) = lv2, lv1
            R.output(gv1)
        return gv1

    @R.function
    def batch_decode(input_embeds: R.Tensor(("batch_size", 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight7: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale7: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm325: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv806 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight7, model_layers_0_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv = R.call_tir(cls.NT_matmul, (rms_norm325, lv806), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape640: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv, R.shape([batch_size, 1, 48, 128]))
            reshape641: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape640, R.shape([batch_size, 48, 128]))
            lv807 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape641), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape642: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv807, R.shape([batch_size, 1, 32, 128]))
            reshape643: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape642, R.shape([batch_size, 1, 4096]))
            lv808 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight7, model_layers_0_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1 = R.call_tir(cls.NT_matmul1, (reshape643, lv808), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv_1 = R.call_tir(cls.fuse_add_norm_decode, (lv1, input_embeds, model_layers_0_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv1_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv_1[1]
            rms_norm326: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv_1[0]
            lv809 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight7, model_layers_0_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv2 = R.call_tir(cls.NT_matmul2, (rms_norm326, lv809), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split160: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv2, indices_or_sections=2, axis=-1)
            split_0160: R.Tensor((batch_size, 1, 14336), dtype="float16") = split160[0]
            split_1160: R.Tensor((batch_size, 1, 14336), dtype="float16") = split160[1]
            silu160: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0160)
            mul160: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu160, split_1160)
            lv810 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight7, model_layers_0_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv3 = R.call_tir(cls.NT_matmul3, (mul160, lv810), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv2_1 = R.call_tir(cls.fuse_add_norm_decode, (lv3, lv1_1, model_layers_1_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv3_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv2_1[1]
            rms_norm327: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv2_1[0]
            lv811 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight7, model_layers_1_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv4 = R.call_tir(cls.NT_matmul, (rms_norm327, lv811), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape644: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv4, R.shape([batch_size, 1, 48, 128]))
            reshape645: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape644, R.shape([batch_size, 48, 128]))
            lv812 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape645), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape646: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv812, R.shape([batch_size, 1, 32, 128]))
            reshape647: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape646, R.shape([batch_size, 1, 4096]))
            lv813 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight7, model_layers_1_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv5 = R.call_tir(cls.NT_matmul1, (reshape647, lv813), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv4_1 = R.call_tir(cls.fuse_add_norm_decode, (lv5, lv3_1, model_layers_1_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv5_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv4_1[1]
            rms_norm328: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv4_1[0]
            lv814 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight7, model_layers_1_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv6 = R.call_tir(cls.NT_matmul2, (rms_norm328, lv814), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split161: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv6, indices_or_sections=2, axis=-1)
            split_0161: R.Tensor((batch_size, 1, 14336), dtype="float16") = split161[0]
            split_1161: R.Tensor((batch_size, 1, 14336), dtype="float16") = split161[1]
            silu161: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0161)
            mul161: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu161, split_1161)
            lv815 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight7, model_layers_1_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv7 = R.call_tir(cls.NT_matmul3, (mul161, lv815), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv6_1 = R.call_tir(cls.fuse_add_norm_decode, (lv7, lv5_1, model_layers_2_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv7_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv6_1[1]
            rms_norm329: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv6_1[0]
            lv816 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight7, model_layers_2_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv8 = R.call_tir(cls.NT_matmul, (rms_norm329, lv816), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape648: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv8, R.shape([batch_size, 1, 48, 128]))
            reshape649: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape648, R.shape([batch_size, 48, 128]))
            lv817 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape649), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape650: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv817, R.shape([batch_size, 1, 32, 128]))
            reshape651: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape650, R.shape([batch_size, 1, 4096]))
            lv818 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight7, model_layers_2_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv9 = R.call_tir(cls.NT_matmul1, (reshape651, lv818), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv8_1 = R.call_tir(cls.fuse_add_norm_decode, (lv9, lv7_1, model_layers_2_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv9_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv8_1[1]
            rms_norm330: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv8_1[0]
            lv819 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight7, model_layers_2_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv10 = R.call_tir(cls.NT_matmul2, (rms_norm330, lv819), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split162: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv10, indices_or_sections=2, axis=-1)
            split_0162: R.Tensor((batch_size, 1, 14336), dtype="float16") = split162[0]
            split_1162: R.Tensor((batch_size, 1, 14336), dtype="float16") = split162[1]
            silu162: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0162)
            mul162: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu162, split_1162)
            lv820 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight7, model_layers_2_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv11 = R.call_tir(cls.NT_matmul3, (mul162, lv820), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv10_1 = R.call_tir(cls.fuse_add_norm_decode, (lv11, lv9_1, model_layers_3_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv11_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv10_1[1]
            rms_norm331: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv10_1[0]
            lv821 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight7, model_layers_3_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv12 = R.call_tir(cls.NT_matmul, (rms_norm331, lv821), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape652: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv12, R.shape([batch_size, 1, 48, 128]))
            reshape653: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape652, R.shape([batch_size, 48, 128]))
            lv822 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape653), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape654: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv822, R.shape([batch_size, 1, 32, 128]))
            reshape655: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape654, R.shape([batch_size, 1, 4096]))
            lv823 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight7, model_layers_3_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv13 = R.call_tir(cls.NT_matmul1, (reshape655, lv823), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv12_1 = R.call_tir(cls.fuse_add_norm_decode, (lv13, lv11_1, model_layers_3_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv13_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv12_1[1]
            rms_norm332: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv12_1[0]
            lv824 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight7, model_layers_3_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv14 = R.call_tir(cls.NT_matmul2, (rms_norm332, lv824), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split163: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv14, indices_or_sections=2, axis=-1)
            split_0163: R.Tensor((batch_size, 1, 14336), dtype="float16") = split163[0]
            split_1163: R.Tensor((batch_size, 1, 14336), dtype="float16") = split163[1]
            silu163: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0163)
            mul163: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu163, split_1163)
            lv825 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight7, model_layers_3_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv15 = R.call_tir(cls.NT_matmul3, (mul163, lv825), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv14_1 = R.call_tir(cls.fuse_add_norm_decode, (lv15, lv13_1, model_layers_4_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv15_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv14_1[1]
            rms_norm333: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv14_1[0]
            lv826 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight7, model_layers_4_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv16 = R.call_tir(cls.NT_matmul, (rms_norm333, lv826), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape656: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv16, R.shape([batch_size, 1, 48, 128]))
            reshape657: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape656, R.shape([batch_size, 48, 128]))
            lv827 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape657), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape658: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv827, R.shape([batch_size, 1, 32, 128]))
            reshape659: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape658, R.shape([batch_size, 1, 4096]))
            lv828 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight7, model_layers_4_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv17 = R.call_tir(cls.NT_matmul1, (reshape659, lv828), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv16_1 = R.call_tir(cls.fuse_add_norm_decode, (lv17, lv15_1, model_layers_4_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv17_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv16_1[1]
            rms_norm334: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv16_1[0]
            lv829 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight7, model_layers_4_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv18 = R.call_tir(cls.NT_matmul2, (rms_norm334, lv829), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split164: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv18, indices_or_sections=2, axis=-1)
            split_0164: R.Tensor((batch_size, 1, 14336), dtype="float16") = split164[0]
            split_1164: R.Tensor((batch_size, 1, 14336), dtype="float16") = split164[1]
            silu164: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0164)
            mul164: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu164, split_1164)
            lv830 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight7, model_layers_4_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv19 = R.call_tir(cls.NT_matmul3, (mul164, lv830), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv18_1 = R.call_tir(cls.fuse_add_norm_decode, (lv19, lv17_1, model_layers_5_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv19_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv18_1[1]
            rms_norm335: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv18_1[0]
            lv831 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight7, model_layers_5_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv20 = R.call_tir(cls.NT_matmul, (rms_norm335, lv831), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape660: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv20, R.shape([batch_size, 1, 48, 128]))
            reshape661: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape660, R.shape([batch_size, 48, 128]))
            lv832 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape661), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape662: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv832, R.shape([batch_size, 1, 32, 128]))
            reshape663: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape662, R.shape([batch_size, 1, 4096]))
            lv833 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight7, model_layers_5_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv21 = R.call_tir(cls.NT_matmul1, (reshape663, lv833), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv20_1 = R.call_tir(cls.fuse_add_norm_decode, (lv21, lv19_1, model_layers_5_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv21_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv20_1[1]
            rms_norm336: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv20_1[0]
            lv834 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight7, model_layers_5_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv22 = R.call_tir(cls.NT_matmul2, (rms_norm336, lv834), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split165: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv22, indices_or_sections=2, axis=-1)
            split_0165: R.Tensor((batch_size, 1, 14336), dtype="float16") = split165[0]
            split_1165: R.Tensor((batch_size, 1, 14336), dtype="float16") = split165[1]
            silu165: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0165)
            mul165: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu165, split_1165)
            lv835 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight7, model_layers_5_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv23 = R.call_tir(cls.NT_matmul3, (mul165, lv835), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv22_1 = R.call_tir(cls.fuse_add_norm_decode, (lv23, lv21_1, model_layers_6_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv23_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv22_1[1]
            rms_norm337: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv22_1[0]
            lv836 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight7, model_layers_6_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv24 = R.call_tir(cls.NT_matmul, (rms_norm337, lv836), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape664: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv24, R.shape([batch_size, 1, 48, 128]))
            reshape665: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape664, R.shape([batch_size, 48, 128]))
            lv837 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape665), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape666: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv837, R.shape([batch_size, 1, 32, 128]))
            reshape667: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape666, R.shape([batch_size, 1, 4096]))
            lv838 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight7, model_layers_6_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv25 = R.call_tir(cls.NT_matmul1, (reshape667, lv838), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv24_1 = R.call_tir(cls.fuse_add_norm_decode, (lv25, lv23_1, model_layers_6_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv25_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv24_1[1]
            rms_norm338: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv24_1[0]
            lv839 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight7, model_layers_6_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv26 = R.call_tir(cls.NT_matmul2, (rms_norm338, lv839), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split166: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv26, indices_or_sections=2, axis=-1)
            split_0166: R.Tensor((batch_size, 1, 14336), dtype="float16") = split166[0]
            split_1166: R.Tensor((batch_size, 1, 14336), dtype="float16") = split166[1]
            silu166: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0166)
            mul166: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu166, split_1166)
            lv840 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight7, model_layers_6_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv27 = R.call_tir(cls.NT_matmul3, (mul166, lv840), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv26_1 = R.call_tir(cls.fuse_add_norm_decode, (lv27, lv25_1, model_layers_7_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv27_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv26_1[1]
            rms_norm339: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv26_1[0]
            lv841 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight7, model_layers_7_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv28 = R.call_tir(cls.NT_matmul, (rms_norm339, lv841), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape668: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv28, R.shape([batch_size, 1, 48, 128]))
            reshape669: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape668, R.shape([batch_size, 48, 128]))
            lv842 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape669), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape670: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv842, R.shape([batch_size, 1, 32, 128]))
            reshape671: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape670, R.shape([batch_size, 1, 4096]))
            lv843 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight7, model_layers_7_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv29 = R.call_tir(cls.NT_matmul1, (reshape671, lv843), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv28_1 = R.call_tir(cls.fuse_add_norm_decode, (lv29, lv27_1, model_layers_7_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv29_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv28_1[1]
            rms_norm340: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv28_1[0]
            lv844 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight7, model_layers_7_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv30 = R.call_tir(cls.NT_matmul2, (rms_norm340, lv844), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split167: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv30, indices_or_sections=2, axis=-1)
            split_0167: R.Tensor((batch_size, 1, 14336), dtype="float16") = split167[0]
            split_1167: R.Tensor((batch_size, 1, 14336), dtype="float16") = split167[1]
            silu167: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0167)
            mul167: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu167, split_1167)
            lv845 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight7, model_layers_7_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv31 = R.call_tir(cls.NT_matmul3, (mul167, lv845), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv30_1 = R.call_tir(cls.fuse_add_norm_decode, (lv31, lv29_1, model_layers_8_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv31_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv30_1[1]
            rms_norm341: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv30_1[0]
            lv846 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight7, model_layers_8_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv32 = R.call_tir(cls.NT_matmul, (rms_norm341, lv846), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape672: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv32, R.shape([batch_size, 1, 48, 128]))
            reshape673: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape672, R.shape([batch_size, 48, 128]))
            lv847 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape673), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape674: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv847, R.shape([batch_size, 1, 32, 128]))
            reshape675: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape674, R.shape([batch_size, 1, 4096]))
            lv848 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight7, model_layers_8_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv33 = R.call_tir(cls.NT_matmul1, (reshape675, lv848), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv32_1 = R.call_tir(cls.fuse_add_norm_decode, (lv33, lv31_1, model_layers_8_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv33_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv32_1[1]
            rms_norm342: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv32_1[0]
            lv849 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight7, model_layers_8_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv34 = R.call_tir(cls.NT_matmul2, (rms_norm342, lv849), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split168: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv34, indices_or_sections=2, axis=-1)
            split_0168: R.Tensor((batch_size, 1, 14336), dtype="float16") = split168[0]
            split_1168: R.Tensor((batch_size, 1, 14336), dtype="float16") = split168[1]
            silu168: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0168)
            mul168: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu168, split_1168)
            lv850 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight7, model_layers_8_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv35 = R.call_tir(cls.NT_matmul3, (mul168, lv850), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv34_1 = R.call_tir(cls.fuse_add_norm_decode, (lv35, lv33_1, model_layers_9_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv35_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv34_1[1]
            rms_norm343: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv34_1[0]
            lv851 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight7, model_layers_9_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv36 = R.call_tir(cls.NT_matmul, (rms_norm343, lv851), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape676: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv36, R.shape([batch_size, 1, 48, 128]))
            reshape677: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape676, R.shape([batch_size, 48, 128]))
            lv852 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape677), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape678: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv852, R.shape([batch_size, 1, 32, 128]))
            reshape679: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape678, R.shape([batch_size, 1, 4096]))
            lv853 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight7, model_layers_9_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv37 = R.call_tir(cls.NT_matmul1, (reshape679, lv853), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv36_1 = R.call_tir(cls.fuse_add_norm_decode, (lv37, lv35_1, model_layers_9_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv37_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv36_1[1]
            rms_norm344: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv36_1[0]
            lv854 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight7, model_layers_9_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv38 = R.call_tir(cls.NT_matmul2, (rms_norm344, lv854), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split169: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv38, indices_or_sections=2, axis=-1)
            split_0169: R.Tensor((batch_size, 1, 14336), dtype="float16") = split169[0]
            split_1169: R.Tensor((batch_size, 1, 14336), dtype="float16") = split169[1]
            silu169: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0169)
            mul169: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu169, split_1169)
            lv855 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight7, model_layers_9_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv39 = R.call_tir(cls.NT_matmul3, (mul169, lv855), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv38_1 = R.call_tir(cls.fuse_add_norm_decode, (lv39, lv37_1, model_layers_10_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv39_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv38_1[1]
            rms_norm345: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv38_1[0]
            lv856 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight7, model_layers_10_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv40 = R.call_tir(cls.NT_matmul, (rms_norm345, lv856), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape680: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv40, R.shape([batch_size, 1, 48, 128]))
            reshape681: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape680, R.shape([batch_size, 48, 128]))
            lv857 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape681), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape682: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv857, R.shape([batch_size, 1, 32, 128]))
            reshape683: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape682, R.shape([batch_size, 1, 4096]))
            lv858 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight7, model_layers_10_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv41 = R.call_tir(cls.NT_matmul1, (reshape683, lv858), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv40_1 = R.call_tir(cls.fuse_add_norm_decode, (lv41, lv39_1, model_layers_10_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv41_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv40_1[1]
            rms_norm346: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv40_1[0]
            lv859 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight7, model_layers_10_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv42 = R.call_tir(cls.NT_matmul2, (rms_norm346, lv859), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split170: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv42, indices_or_sections=2, axis=-1)
            split_0170: R.Tensor((batch_size, 1, 14336), dtype="float16") = split170[0]
            split_1170: R.Tensor((batch_size, 1, 14336), dtype="float16") = split170[1]
            silu170: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0170)
            mul170: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu170, split_1170)
            lv860 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight7, model_layers_10_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv43 = R.call_tir(cls.NT_matmul3, (mul170, lv860), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv42_1 = R.call_tir(cls.fuse_add_norm_decode, (lv43, lv41_1, model_layers_11_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv43_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv42_1[1]
            rms_norm347: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv42_1[0]
            lv861 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight7, model_layers_11_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv44 = R.call_tir(cls.NT_matmul, (rms_norm347, lv861), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape684: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv44, R.shape([batch_size, 1, 48, 128]))
            reshape685: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape684, R.shape([batch_size, 48, 128]))
            lv862 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape685), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape686: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv862, R.shape([batch_size, 1, 32, 128]))
            reshape687: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape686, R.shape([batch_size, 1, 4096]))
            lv863 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight7, model_layers_11_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv45 = R.call_tir(cls.NT_matmul1, (reshape687, lv863), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv44_1 = R.call_tir(cls.fuse_add_norm_decode, (lv45, lv43_1, model_layers_11_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv45_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv44_1[1]
            rms_norm348: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv44_1[0]
            lv864 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight7, model_layers_11_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv46 = R.call_tir(cls.NT_matmul2, (rms_norm348, lv864), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split171: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv46, indices_or_sections=2, axis=-1)
            split_0171: R.Tensor((batch_size, 1, 14336), dtype="float16") = split171[0]
            split_1171: R.Tensor((batch_size, 1, 14336), dtype="float16") = split171[1]
            silu171: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0171)
            mul171: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu171, split_1171)
            lv865 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight7, model_layers_11_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv47 = R.call_tir(cls.NT_matmul3, (mul171, lv865), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv46_1 = R.call_tir(cls.fuse_add_norm_decode, (lv47, lv45_1, model_layers_12_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv47_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv46_1[1]
            rms_norm349: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv46_1[0]
            lv866 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight7, model_layers_12_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv48 = R.call_tir(cls.NT_matmul, (rms_norm349, lv866), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape688: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv48, R.shape([batch_size, 1, 48, 128]))
            reshape689: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape688, R.shape([batch_size, 48, 128]))
            lv867 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape689), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape690: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv867, R.shape([batch_size, 1, 32, 128]))
            reshape691: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape690, R.shape([batch_size, 1, 4096]))
            lv868 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight7, model_layers_12_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv49 = R.call_tir(cls.NT_matmul1, (reshape691, lv868), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv48_1 = R.call_tir(cls.fuse_add_norm_decode, (lv49, lv47_1, model_layers_12_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv49_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv48_1[1]
            rms_norm350: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv48_1[0]
            lv869 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight7, model_layers_12_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv50 = R.call_tir(cls.NT_matmul2, (rms_norm350, lv869), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split172: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv50, indices_or_sections=2, axis=-1)
            split_0172: R.Tensor((batch_size, 1, 14336), dtype="float16") = split172[0]
            split_1172: R.Tensor((batch_size, 1, 14336), dtype="float16") = split172[1]
            silu172: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0172)
            mul172: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu172, split_1172)
            lv870 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight7, model_layers_12_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv51 = R.call_tir(cls.NT_matmul3, (mul172, lv870), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv50_1 = R.call_tir(cls.fuse_add_norm_decode, (lv51, lv49_1, model_layers_13_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv51_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv50_1[1]
            rms_norm351: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv50_1[0]
            lv871 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight7, model_layers_13_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv52 = R.call_tir(cls.NT_matmul, (rms_norm351, lv871), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape692: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv52, R.shape([batch_size, 1, 48, 128]))
            reshape693: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape692, R.shape([batch_size, 48, 128]))
            lv872 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape693), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape694: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv872, R.shape([batch_size, 1, 32, 128]))
            reshape695: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape694, R.shape([batch_size, 1, 4096]))
            lv873 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight7, model_layers_13_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv53 = R.call_tir(cls.NT_matmul1, (reshape695, lv873), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv52_1 = R.call_tir(cls.fuse_add_norm_decode, (lv53, lv51_1, model_layers_13_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv53_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv52_1[1]
            rms_norm352: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv52_1[0]
            lv874 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight7, model_layers_13_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv54 = R.call_tir(cls.NT_matmul2, (rms_norm352, lv874), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split173: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv54, indices_or_sections=2, axis=-1)
            split_0173: R.Tensor((batch_size, 1, 14336), dtype="float16") = split173[0]
            split_1173: R.Tensor((batch_size, 1, 14336), dtype="float16") = split173[1]
            silu173: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0173)
            mul173: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu173, split_1173)
            lv875 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight7, model_layers_13_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv55 = R.call_tir(cls.NT_matmul3, (mul173, lv875), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv54_1 = R.call_tir(cls.fuse_add_norm_decode, (lv55, lv53_1, model_layers_14_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv55_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv54_1[1]
            rms_norm353: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv54_1[0]
            lv876 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight7, model_layers_14_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv56 = R.call_tir(cls.NT_matmul, (rms_norm353, lv876), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape696: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv56, R.shape([batch_size, 1, 48, 128]))
            reshape697: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape696, R.shape([batch_size, 48, 128]))
            lv877 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape697), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape698: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv877, R.shape([batch_size, 1, 32, 128]))
            reshape699: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape698, R.shape([batch_size, 1, 4096]))
            lv878 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight7, model_layers_14_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv57 = R.call_tir(cls.NT_matmul1, (reshape699, lv878), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv56_1 = R.call_tir(cls.fuse_add_norm_decode, (lv57, lv55_1, model_layers_14_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv57_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv56_1[1]
            rms_norm354: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv56_1[0]
            lv879 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight7, model_layers_14_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv58 = R.call_tir(cls.NT_matmul2, (rms_norm354, lv879), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split174: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv58, indices_or_sections=2, axis=-1)
            split_0174: R.Tensor((batch_size, 1, 14336), dtype="float16") = split174[0]
            split_1174: R.Tensor((batch_size, 1, 14336), dtype="float16") = split174[1]
            silu174: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0174)
            mul174: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu174, split_1174)
            lv880 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight7, model_layers_14_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv59 = R.call_tir(cls.NT_matmul3, (mul174, lv880), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv58_1 = R.call_tir(cls.fuse_add_norm_decode, (lv59, lv57_1, model_layers_15_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv59_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv58_1[1]
            rms_norm355: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv58_1[0]
            lv881 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight7, model_layers_15_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv60 = R.call_tir(cls.NT_matmul, (rms_norm355, lv881), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape700: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv60, R.shape([batch_size, 1, 48, 128]))
            reshape701: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape700, R.shape([batch_size, 48, 128]))
            lv882 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape701), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape702: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv882, R.shape([batch_size, 1, 32, 128]))
            reshape703: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape702, R.shape([batch_size, 1, 4096]))
            lv883 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight7, model_layers_15_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv61 = R.call_tir(cls.NT_matmul1, (reshape703, lv883), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv60_1 = R.call_tir(cls.fuse_add_norm_decode, (lv61, lv59_1, model_layers_15_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv61_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv60_1[1]
            rms_norm356: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv60_1[0]
            lv884 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight7, model_layers_15_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv62 = R.call_tir(cls.NT_matmul2, (rms_norm356, lv884), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split175: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv62, indices_or_sections=2, axis=-1)
            split_0175: R.Tensor((batch_size, 1, 14336), dtype="float16") = split175[0]
            split_1175: R.Tensor((batch_size, 1, 14336), dtype="float16") = split175[1]
            silu175: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0175)
            mul175: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu175, split_1175)
            lv885 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight7, model_layers_15_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv63 = R.call_tir(cls.NT_matmul3, (mul175, lv885), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv62_1 = R.call_tir(cls.fuse_add_norm_decode, (lv63, lv61_1, model_layers_16_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv63_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv62_1[1]
            rms_norm357: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv62_1[0]
            lv886 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight7, model_layers_16_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv64 = R.call_tir(cls.NT_matmul, (rms_norm357, lv886), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape704: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv64, R.shape([batch_size, 1, 48, 128]))
            reshape705: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape704, R.shape([batch_size, 48, 128]))
            lv887 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape705), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape706: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv887, R.shape([batch_size, 1, 32, 128]))
            reshape707: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape706, R.shape([batch_size, 1, 4096]))
            lv888 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight7, model_layers_16_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv65 = R.call_tir(cls.NT_matmul1, (reshape707, lv888), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv64_1 = R.call_tir(cls.fuse_add_norm_decode, (lv65, lv63_1, model_layers_16_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv65_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv64_1[1]
            rms_norm358: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv64_1[0]
            lv889 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight7, model_layers_16_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv66 = R.call_tir(cls.NT_matmul2, (rms_norm358, lv889), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split176: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv66, indices_or_sections=2, axis=-1)
            split_0176: R.Tensor((batch_size, 1, 14336), dtype="float16") = split176[0]
            split_1176: R.Tensor((batch_size, 1, 14336), dtype="float16") = split176[1]
            silu176: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0176)
            mul176: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu176, split_1176)
            lv890 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight7, model_layers_16_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv67 = R.call_tir(cls.NT_matmul3, (mul176, lv890), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv66_1 = R.call_tir(cls.fuse_add_norm_decode, (lv67, lv65_1, model_layers_17_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv67_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv66_1[1]
            rms_norm359: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv66_1[0]
            lv891 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight7, model_layers_17_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv68 = R.call_tir(cls.NT_matmul, (rms_norm359, lv891), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape708: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv68, R.shape([batch_size, 1, 48, 128]))
            reshape709: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape708, R.shape([batch_size, 48, 128]))
            lv892 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape709), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape710: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv892, R.shape([batch_size, 1, 32, 128]))
            reshape711: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape710, R.shape([batch_size, 1, 4096]))
            lv893 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight7, model_layers_17_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv69 = R.call_tir(cls.NT_matmul1, (reshape711, lv893), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv68_1 = R.call_tir(cls.fuse_add_norm_decode, (lv69, lv67_1, model_layers_17_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv69_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv68_1[1]
            rms_norm360: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv68_1[0]
            lv894 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight7, model_layers_17_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv70 = R.call_tir(cls.NT_matmul2, (rms_norm360, lv894), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split177: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv70, indices_or_sections=2, axis=-1)
            split_0177: R.Tensor((batch_size, 1, 14336), dtype="float16") = split177[0]
            split_1177: R.Tensor((batch_size, 1, 14336), dtype="float16") = split177[1]
            silu177: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0177)
            mul177: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu177, split_1177)
            lv895 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight7, model_layers_17_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv71 = R.call_tir(cls.NT_matmul3, (mul177, lv895), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv70_1 = R.call_tir(cls.fuse_add_norm_decode, (lv71, lv69_1, model_layers_18_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv71_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv70_1[1]
            rms_norm361: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv70_1[0]
            lv896 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight7, model_layers_18_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv72 = R.call_tir(cls.NT_matmul, (rms_norm361, lv896), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape712: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv72, R.shape([batch_size, 1, 48, 128]))
            reshape713: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape712, R.shape([batch_size, 48, 128]))
            lv897 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape713), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape714: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv897, R.shape([batch_size, 1, 32, 128]))
            reshape715: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape714, R.shape([batch_size, 1, 4096]))
            lv898 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight7, model_layers_18_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv73 = R.call_tir(cls.NT_matmul1, (reshape715, lv898), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv72_1 = R.call_tir(cls.fuse_add_norm_decode, (lv73, lv71_1, model_layers_18_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv73_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv72_1[1]
            rms_norm362: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv72_1[0]
            lv899 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight7, model_layers_18_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv74 = R.call_tir(cls.NT_matmul2, (rms_norm362, lv899), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split178: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv74, indices_or_sections=2, axis=-1)
            split_0178: R.Tensor((batch_size, 1, 14336), dtype="float16") = split178[0]
            split_1178: R.Tensor((batch_size, 1, 14336), dtype="float16") = split178[1]
            silu178: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0178)
            mul178: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu178, split_1178)
            lv900 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight7, model_layers_18_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv75 = R.call_tir(cls.NT_matmul3, (mul178, lv900), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv74_1 = R.call_tir(cls.fuse_add_norm_decode, (lv75, lv73_1, model_layers_19_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv75_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv74_1[1]
            rms_norm363: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv74_1[0]
            lv901 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight7, model_layers_19_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv76 = R.call_tir(cls.NT_matmul, (rms_norm363, lv901), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape716: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv76, R.shape([batch_size, 1, 48, 128]))
            reshape717: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape716, R.shape([batch_size, 48, 128]))
            lv902 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape717), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape718: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv902, R.shape([batch_size, 1, 32, 128]))
            reshape719: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape718, R.shape([batch_size, 1, 4096]))
            lv903 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight7, model_layers_19_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv77 = R.call_tir(cls.NT_matmul1, (reshape719, lv903), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv76_1 = R.call_tir(cls.fuse_add_norm_decode, (lv77, lv75_1, model_layers_19_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv77_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv76_1[1]
            rms_norm364: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv76_1[0]
            lv904 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight7, model_layers_19_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv78 = R.call_tir(cls.NT_matmul2, (rms_norm364, lv904), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split179: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv78, indices_or_sections=2, axis=-1)
            split_0179: R.Tensor((batch_size, 1, 14336), dtype="float16") = split179[0]
            split_1179: R.Tensor((batch_size, 1, 14336), dtype="float16") = split179[1]
            silu179: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0179)
            mul179: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu179, split_1179)
            lv905 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight7, model_layers_19_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv79 = R.call_tir(cls.NT_matmul3, (mul179, lv905), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv78_1 = R.call_tir(cls.fuse_add_norm_decode, (lv79, lv77_1, model_layers_20_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv79_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv78_1[1]
            rms_norm365: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv78_1[0]
            lv906 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight7, model_layers_20_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv80 = R.call_tir(cls.NT_matmul, (rms_norm365, lv906), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape720: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv80, R.shape([batch_size, 1, 48, 128]))
            reshape721: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape720, R.shape([batch_size, 48, 128]))
            lv907 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape721), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape722: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv907, R.shape([batch_size, 1, 32, 128]))
            reshape723: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape722, R.shape([batch_size, 1, 4096]))
            lv908 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight7, model_layers_20_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv81 = R.call_tir(cls.NT_matmul1, (reshape723, lv908), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv80_1 = R.call_tir(cls.fuse_add_norm_decode, (lv81, lv79_1, model_layers_20_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv81_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv80_1[1]
            rms_norm366: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv80_1[0]
            lv909 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight7, model_layers_20_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv82 = R.call_tir(cls.NT_matmul2, (rms_norm366, lv909), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split180: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv82, indices_or_sections=2, axis=-1)
            split_0180: R.Tensor((batch_size, 1, 14336), dtype="float16") = split180[0]
            split_1180: R.Tensor((batch_size, 1, 14336), dtype="float16") = split180[1]
            silu180: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0180)
            mul180: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu180, split_1180)
            lv910 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight7, model_layers_20_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv83 = R.call_tir(cls.NT_matmul3, (mul180, lv910), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv82_1 = R.call_tir(cls.fuse_add_norm_decode, (lv83, lv81_1, model_layers_21_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv83_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv82_1[1]
            rms_norm367: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv82_1[0]
            lv911 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight7, model_layers_21_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv84 = R.call_tir(cls.NT_matmul, (rms_norm367, lv911), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape724: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv84, R.shape([batch_size, 1, 48, 128]))
            reshape725: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape724, R.shape([batch_size, 48, 128]))
            lv912 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape725), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape726: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv912, R.shape([batch_size, 1, 32, 128]))
            reshape727: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape726, R.shape([batch_size, 1, 4096]))
            lv913 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight7, model_layers_21_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv85 = R.call_tir(cls.NT_matmul1, (reshape727, lv913), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv84_1 = R.call_tir(cls.fuse_add_norm_decode, (lv85, lv83_1, model_layers_21_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv85_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv84_1[1]
            rms_norm368: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv84_1[0]
            lv914 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight7, model_layers_21_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv86 = R.call_tir(cls.NT_matmul2, (rms_norm368, lv914), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split181: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv86, indices_or_sections=2, axis=-1)
            split_0181: R.Tensor((batch_size, 1, 14336), dtype="float16") = split181[0]
            split_1181: R.Tensor((batch_size, 1, 14336), dtype="float16") = split181[1]
            silu181: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0181)
            mul181: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu181, split_1181)
            lv915 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight7, model_layers_21_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv87 = R.call_tir(cls.NT_matmul3, (mul181, lv915), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv86_1 = R.call_tir(cls.fuse_add_norm_decode, (lv87, lv85_1, model_layers_22_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv87_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv86_1[1]
            rms_norm369: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv86_1[0]
            lv916 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight7, model_layers_22_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv88 = R.call_tir(cls.NT_matmul, (rms_norm369, lv916), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape728: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv88, R.shape([batch_size, 1, 48, 128]))
            reshape729: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape728, R.shape([batch_size, 48, 128]))
            lv917 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape729), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape730: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv917, R.shape([batch_size, 1, 32, 128]))
            reshape731: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape730, R.shape([batch_size, 1, 4096]))
            lv918 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight7, model_layers_22_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv89 = R.call_tir(cls.NT_matmul1, (reshape731, lv918), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv88_1 = R.call_tir(cls.fuse_add_norm_decode, (lv89, lv87_1, model_layers_22_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv89_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv88_1[1]
            rms_norm370: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv88_1[0]
            lv919 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight7, model_layers_22_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv90 = R.call_tir(cls.NT_matmul2, (rms_norm370, lv919), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split182: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv90, indices_or_sections=2, axis=-1)
            split_0182: R.Tensor((batch_size, 1, 14336), dtype="float16") = split182[0]
            split_1182: R.Tensor((batch_size, 1, 14336), dtype="float16") = split182[1]
            silu182: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0182)
            mul182: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu182, split_1182)
            lv920 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight7, model_layers_22_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv91 = R.call_tir(cls.NT_matmul3, (mul182, lv920), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv90_1 = R.call_tir(cls.fuse_add_norm_decode, (lv91, lv89_1, model_layers_23_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv91_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv90_1[1]
            rms_norm371: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv90_1[0]
            lv921 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight7, model_layers_23_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv92 = R.call_tir(cls.NT_matmul, (rms_norm371, lv921), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape732: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv92, R.shape([batch_size, 1, 48, 128]))
            reshape733: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape732, R.shape([batch_size, 48, 128]))
            lv922 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape733), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape734: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv922, R.shape([batch_size, 1, 32, 128]))
            reshape735: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape734, R.shape([batch_size, 1, 4096]))
            lv923 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight7, model_layers_23_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv93 = R.call_tir(cls.NT_matmul1, (reshape735, lv923), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv92_1 = R.call_tir(cls.fuse_add_norm_decode, (lv93, lv91_1, model_layers_23_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv93_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv92_1[1]
            rms_norm372: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv92_1[0]
            lv924 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight7, model_layers_23_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv94 = R.call_tir(cls.NT_matmul2, (rms_norm372, lv924), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split183: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv94, indices_or_sections=2, axis=-1)
            split_0183: R.Tensor((batch_size, 1, 14336), dtype="float16") = split183[0]
            split_1183: R.Tensor((batch_size, 1, 14336), dtype="float16") = split183[1]
            silu183: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0183)
            mul183: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu183, split_1183)
            lv925 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight7, model_layers_23_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv95 = R.call_tir(cls.NT_matmul3, (mul183, lv925), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv94_1 = R.call_tir(cls.fuse_add_norm_decode, (lv95, lv93_1, model_layers_24_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv95_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv94_1[1]
            rms_norm373: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv94_1[0]
            lv926 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight7, model_layers_24_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv96 = R.call_tir(cls.NT_matmul, (rms_norm373, lv926), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape736: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv96, R.shape([batch_size, 1, 48, 128]))
            reshape737: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape736, R.shape([batch_size, 48, 128]))
            lv927 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape737), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape738: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv927, R.shape([batch_size, 1, 32, 128]))
            reshape739: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape738, R.shape([batch_size, 1, 4096]))
            lv928 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight7, model_layers_24_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv97 = R.call_tir(cls.NT_matmul1, (reshape739, lv928), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv96_1 = R.call_tir(cls.fuse_add_norm_decode, (lv97, lv95_1, model_layers_24_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv97_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv96_1[1]
            rms_norm374: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv96_1[0]
            lv929 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight7, model_layers_24_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv98 = R.call_tir(cls.NT_matmul2, (rms_norm374, lv929), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split184: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv98, indices_or_sections=2, axis=-1)
            split_0184: R.Tensor((batch_size, 1, 14336), dtype="float16") = split184[0]
            split_1184: R.Tensor((batch_size, 1, 14336), dtype="float16") = split184[1]
            silu184: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0184)
            mul184: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu184, split_1184)
            lv930 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight7, model_layers_24_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv99 = R.call_tir(cls.NT_matmul3, (mul184, lv930), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv98_1 = R.call_tir(cls.fuse_add_norm_decode, (lv99, lv97_1, model_layers_25_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv99_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv98_1[1]
            rms_norm375: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv98_1[0]
            lv931 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight7, model_layers_25_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv100 = R.call_tir(cls.NT_matmul, (rms_norm375, lv931), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape740: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv100, R.shape([batch_size, 1, 48, 128]))
            reshape741: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape740, R.shape([batch_size, 48, 128]))
            lv932 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape741), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape742: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv932, R.shape([batch_size, 1, 32, 128]))
            reshape743: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape742, R.shape([batch_size, 1, 4096]))
            lv933 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight7, model_layers_25_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv101 = R.call_tir(cls.NT_matmul1, (reshape743, lv933), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv100_1 = R.call_tir(cls.fuse_add_norm_decode, (lv101, lv99_1, model_layers_25_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv101_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv100_1[1]
            rms_norm376: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv100_1[0]
            lv934 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight7, model_layers_25_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv102 = R.call_tir(cls.NT_matmul2, (rms_norm376, lv934), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split185: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv102, indices_or_sections=2, axis=-1)
            split_0185: R.Tensor((batch_size, 1, 14336), dtype="float16") = split185[0]
            split_1185: R.Tensor((batch_size, 1, 14336), dtype="float16") = split185[1]
            silu185: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0185)
            mul185: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu185, split_1185)
            lv935 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight7, model_layers_25_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv103 = R.call_tir(cls.NT_matmul3, (mul185, lv935), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv102_1 = R.call_tir(cls.fuse_add_norm_decode, (lv103, lv101_1, model_layers_26_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv103_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv102_1[1]
            rms_norm377: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv102_1[0]
            lv936 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight7, model_layers_26_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv104 = R.call_tir(cls.NT_matmul, (rms_norm377, lv936), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape744: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv104, R.shape([batch_size, 1, 48, 128]))
            reshape745: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape744, R.shape([batch_size, 48, 128]))
            lv937 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape745), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape746: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv937, R.shape([batch_size, 1, 32, 128]))
            reshape747: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape746, R.shape([batch_size, 1, 4096]))
            lv938 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight7, model_layers_26_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv105 = R.call_tir(cls.NT_matmul1, (reshape747, lv938), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv104_1 = R.call_tir(cls.fuse_add_norm_decode, (lv105, lv103_1, model_layers_26_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv105_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv104_1[1]
            rms_norm378: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv104_1[0]
            lv939 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight7, model_layers_26_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv106 = R.call_tir(cls.NT_matmul2, (rms_norm378, lv939), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split186: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv106, indices_or_sections=2, axis=-1)
            split_0186: R.Tensor((batch_size, 1, 14336), dtype="float16") = split186[0]
            split_1186: R.Tensor((batch_size, 1, 14336), dtype="float16") = split186[1]
            silu186: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0186)
            mul186: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu186, split_1186)
            lv940 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight7, model_layers_26_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv107 = R.call_tir(cls.NT_matmul3, (mul186, lv940), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv106_1 = R.call_tir(cls.fuse_add_norm_decode, (lv107, lv105_1, model_layers_27_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv107_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv106_1[1]
            rms_norm379: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv106_1[0]
            lv941 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight7, model_layers_27_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv108 = R.call_tir(cls.NT_matmul, (rms_norm379, lv941), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape748: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv108, R.shape([batch_size, 1, 48, 128]))
            reshape749: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape748, R.shape([batch_size, 48, 128]))
            lv942 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape749), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape750: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv942, R.shape([batch_size, 1, 32, 128]))
            reshape751: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape750, R.shape([batch_size, 1, 4096]))
            lv943 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight7, model_layers_27_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv109 = R.call_tir(cls.NT_matmul1, (reshape751, lv943), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv108_1 = R.call_tir(cls.fuse_add_norm_decode, (lv109, lv107_1, model_layers_27_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv109_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv108_1[1]
            rms_norm380: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv108_1[0]
            lv944 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight7, model_layers_27_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv110 = R.call_tir(cls.NT_matmul2, (rms_norm380, lv944), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split187: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv110, indices_or_sections=2, axis=-1)
            split_0187: R.Tensor((batch_size, 1, 14336), dtype="float16") = split187[0]
            split_1187: R.Tensor((batch_size, 1, 14336), dtype="float16") = split187[1]
            silu187: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0187)
            mul187: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu187, split_1187)
            lv945 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight7, model_layers_27_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv111 = R.call_tir(cls.NT_matmul3, (mul187, lv945), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv110_1 = R.call_tir(cls.fuse_add_norm_decode, (lv111, lv109_1, model_layers_28_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv111_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv110_1[1]
            rms_norm381: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv110_1[0]
            lv946 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight7, model_layers_28_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv112 = R.call_tir(cls.NT_matmul, (rms_norm381, lv946), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape752: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv112, R.shape([batch_size, 1, 48, 128]))
            reshape753: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape752, R.shape([batch_size, 48, 128]))
            lv947 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape753), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape754: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv947, R.shape([batch_size, 1, 32, 128]))
            reshape755: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape754, R.shape([batch_size, 1, 4096]))
            lv948 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight7, model_layers_28_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv113 = R.call_tir(cls.NT_matmul1, (reshape755, lv948), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv112_1 = R.call_tir(cls.fuse_add_norm_decode, (lv113, lv111_1, model_layers_28_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv113_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv112_1[1]
            rms_norm382: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv112_1[0]
            lv949 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight7, model_layers_28_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv114 = R.call_tir(cls.NT_matmul2, (rms_norm382, lv949), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split188: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv114, indices_or_sections=2, axis=-1)
            split_0188: R.Tensor((batch_size, 1, 14336), dtype="float16") = split188[0]
            split_1188: R.Tensor((batch_size, 1, 14336), dtype="float16") = split188[1]
            silu188: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0188)
            mul188: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu188, split_1188)
            lv950 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight7, model_layers_28_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv115 = R.call_tir(cls.NT_matmul3, (mul188, lv950), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv114_1 = R.call_tir(cls.fuse_add_norm_decode, (lv115, lv113_1, model_layers_29_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv115_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv114_1[1]
            rms_norm383: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv114_1[0]
            lv951 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight7, model_layers_29_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv116 = R.call_tir(cls.NT_matmul, (rms_norm383, lv951), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape756: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv116, R.shape([batch_size, 1, 48, 128]))
            reshape757: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape756, R.shape([batch_size, 48, 128]))
            lv952 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape757), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape758: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv952, R.shape([batch_size, 1, 32, 128]))
            reshape759: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape758, R.shape([batch_size, 1, 4096]))
            lv953 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight7, model_layers_29_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv117 = R.call_tir(cls.NT_matmul1, (reshape759, lv953), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv116_1 = R.call_tir(cls.fuse_add_norm_decode, (lv117, lv115_1, model_layers_29_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv117_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv116_1[1]
            rms_norm384: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv116_1[0]
            lv954 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight7, model_layers_29_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv118 = R.call_tir(cls.NT_matmul2, (rms_norm384, lv954), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split189: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv118, indices_or_sections=2, axis=-1)
            split_0189: R.Tensor((batch_size, 1, 14336), dtype="float16") = split189[0]
            split_1189: R.Tensor((batch_size, 1, 14336), dtype="float16") = split189[1]
            silu189: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0189)
            mul189: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu189, split_1189)
            lv955 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight7, model_layers_29_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv119 = R.call_tir(cls.NT_matmul3, (mul189, lv955), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv118_1 = R.call_tir(cls.fuse_add_norm_decode, (lv119, lv117_1, model_layers_30_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv119_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv118_1[1]
            rms_norm385: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv118_1[0]
            lv956 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight7, model_layers_30_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv120 = R.call_tir(cls.NT_matmul, (rms_norm385, lv956), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape760: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv120, R.shape([batch_size, 1, 48, 128]))
            reshape761: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape760, R.shape([batch_size, 48, 128]))
            lv957 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape761), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape762: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv957, R.shape([batch_size, 1, 32, 128]))
            reshape763: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape762, R.shape([batch_size, 1, 4096]))
            lv958 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight7, model_layers_30_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv121 = R.call_tir(cls.NT_matmul1, (reshape763, lv958), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv120_1 = R.call_tir(cls.fuse_add_norm_decode, (lv121, lv119_1, model_layers_30_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv121_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv120_1[1]
            rms_norm386: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv120_1[0]
            lv959 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight7, model_layers_30_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv122 = R.call_tir(cls.NT_matmul2, (rms_norm386, lv959), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split190: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv122, indices_or_sections=2, axis=-1)
            split_0190: R.Tensor((batch_size, 1, 14336), dtype="float16") = split190[0]
            split_1190: R.Tensor((batch_size, 1, 14336), dtype="float16") = split190[1]
            silu190: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0190)
            mul190: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu190, split_1190)
            lv960 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight7, model_layers_30_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv123 = R.call_tir(cls.NT_matmul3, (mul190, lv960), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv122_1 = R.call_tir(cls.fuse_add_norm_decode, (lv123, lv121_1, model_layers_31_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv123_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv122_1[1]
            rms_norm387: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv122_1[0]
            lv961 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight7, model_layers_31_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv124 = R.call_tir(cls.NT_matmul, (rms_norm387, lv961), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape764: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv124, R.shape([batch_size, 1, 48, 128]))
            reshape765: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape764, R.shape([batch_size, 48, 128]))
            lv962 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape765), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape766: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv962, R.shape([batch_size, 1, 32, 128]))
            reshape767: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape766, R.shape([batch_size, 1, 4096]))
            lv963 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight7, model_layers_31_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv125 = R.call_tir(cls.NT_matmul1, (reshape767, lv963), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv124_1 = R.call_tir(cls.fuse_add_norm_decode, (lv125, lv123_1, model_layers_31_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv125_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv124_1[1]
            rms_norm388: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv124_1[0]
            lv964 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight7, model_layers_31_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv126 = R.call_tir(cls.NT_matmul2, (rms_norm388, lv964), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split191: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv126, indices_or_sections=2, axis=-1)
            split_0191: R.Tensor((batch_size, 1, 14336), dtype="float16") = split191[0]
            split_1191: R.Tensor((batch_size, 1, 14336), dtype="float16") = split191[1]
            silu191: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0191)
            mul191: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu191, split_1191)
            lv965 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight7, model_layers_31_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv127 = R.call_tir(cls.NT_matmul3, (mul191, lv965), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv126_1 = R.call_tir(cls.fuse_add_norm_decode, (lv127, lv125_1, model_norm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            rms_norm389: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv126_1[0]
            lv966 = R.call_tir(cls.dequantize, (lm_head_q_weight7, lm_head_q_scale7), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            lv128 = R.call_tir(cls.NT_matmul4, (rms_norm389, lv966), out_sinfo=R.Tensor((batch_size, 1, vocab_size), dtype="float16"))
            astype4: R.Tensor((batch_size, 1, vocab_size), dtype="float32") = R.astype(lv128, dtype="float32")
            gv8: R.Tuple(R.Tensor((batch_size, 1, vocab_size), dtype="float32"), R.Object) = astype4, paged_kv_cache
            R.output(gv8)
        return gv8

    @R.function
    def batch_decode_to_last_hidden_states(input_embeds: R.Tensor(("batch_size", 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor(("batch_size", 1, 4096), dtype="float16"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm520: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1288 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight10, model_layers_0_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv129 = R.call_tir(cls.NT_matmul, (rms_norm520, lv1288), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1024: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv129, R.shape([batch_size, 1, 48, 128]))
            reshape1025: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1024, R.shape([batch_size, 48, 128]))
            lv1289 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1025), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1026: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1289, R.shape([batch_size, 1, 32, 128]))
            reshape1027: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1026, R.shape([batch_size, 1, 4096]))
            lv1290 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight10, model_layers_0_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv130 = R.call_tir(cls.NT_matmul1, (reshape1027, lv1290), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv128 = R.call_tir(cls.fuse_add_norm_decode, (lv130, input_embeds, model_layers_0_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv129_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv128[1]
            rms_norm521: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv128[0]
            lv1291 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight10, model_layers_0_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv131 = R.call_tir(cls.NT_matmul2, (rms_norm521, lv1291), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split256: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv131, indices_or_sections=2, axis=-1)
            split_0256: R.Tensor((batch_size, 1, 14336), dtype="float16") = split256[0]
            split_1256: R.Tensor((batch_size, 1, 14336), dtype="float16") = split256[1]
            silu256: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0256)
            mul256: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu256, split_1256)
            lv1292 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight10, model_layers_0_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv132 = R.call_tir(cls.NT_matmul3, (mul256, lv1292), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv130_1 = R.call_tir(cls.fuse_add_norm_decode, (lv132, lv129_1, model_layers_1_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv131_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv130_1[1]
            rms_norm522: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv130_1[0]
            lv1293 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight10, model_layers_1_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv133 = R.call_tir(cls.NT_matmul, (rms_norm522, lv1293), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1028: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv133, R.shape([batch_size, 1, 48, 128]))
            reshape1029: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1028, R.shape([batch_size, 48, 128]))
            lv1294 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape1029), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1030: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1294, R.shape([batch_size, 1, 32, 128]))
            reshape1031: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1030, R.shape([batch_size, 1, 4096]))
            lv1295 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight10, model_layers_1_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv134 = R.call_tir(cls.NT_matmul1, (reshape1031, lv1295), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv132_1 = R.call_tir(cls.fuse_add_norm_decode, (lv134, lv131_1, model_layers_1_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv133_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv132_1[1]
            rms_norm523: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv132_1[0]
            lv1296 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight10, model_layers_1_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv135 = R.call_tir(cls.NT_matmul2, (rms_norm523, lv1296), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split257: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv135, indices_or_sections=2, axis=-1)
            split_0257: R.Tensor((batch_size, 1, 14336), dtype="float16") = split257[0]
            split_1257: R.Tensor((batch_size, 1, 14336), dtype="float16") = split257[1]
            silu257: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0257)
            mul257: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu257, split_1257)
            lv1297 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight10, model_layers_1_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv136 = R.call_tir(cls.NT_matmul3, (mul257, lv1297), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv134_1 = R.call_tir(cls.fuse_add_norm_decode, (lv136, lv133_1, model_layers_2_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv135_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv134_1[1]
            rms_norm524: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv134_1[0]
            lv1298 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight10, model_layers_2_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv137 = R.call_tir(cls.NT_matmul, (rms_norm524, lv1298), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1032: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv137, R.shape([batch_size, 1, 48, 128]))
            reshape1033: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1032, R.shape([batch_size, 48, 128]))
            lv1299 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape1033), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1034: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1299, R.shape([batch_size, 1, 32, 128]))
            reshape1035: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1034, R.shape([batch_size, 1, 4096]))
            lv1300 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight10, model_layers_2_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv138 = R.call_tir(cls.NT_matmul1, (reshape1035, lv1300), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv136_1 = R.call_tir(cls.fuse_add_norm_decode, (lv138, lv135_1, model_layers_2_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv137_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv136_1[1]
            rms_norm525: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv136_1[0]
            lv1301 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight10, model_layers_2_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv139 = R.call_tir(cls.NT_matmul2, (rms_norm525, lv1301), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split258: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv139, indices_or_sections=2, axis=-1)
            split_0258: R.Tensor((batch_size, 1, 14336), dtype="float16") = split258[0]
            split_1258: R.Tensor((batch_size, 1, 14336), dtype="float16") = split258[1]
            silu258: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0258)
            mul258: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu258, split_1258)
            lv1302 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight10, model_layers_2_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv140 = R.call_tir(cls.NT_matmul3, (mul258, lv1302), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv138_1 = R.call_tir(cls.fuse_add_norm_decode, (lv140, lv137_1, model_layers_3_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv139_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv138_1[1]
            rms_norm526: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv138_1[0]
            lv1303 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight10, model_layers_3_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv141 = R.call_tir(cls.NT_matmul, (rms_norm526, lv1303), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1036: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv141, R.shape([batch_size, 1, 48, 128]))
            reshape1037: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1036, R.shape([batch_size, 48, 128]))
            lv1304 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape1037), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1038: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1304, R.shape([batch_size, 1, 32, 128]))
            reshape1039: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1038, R.shape([batch_size, 1, 4096]))
            lv1305 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight10, model_layers_3_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv142 = R.call_tir(cls.NT_matmul1, (reshape1039, lv1305), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv140_1 = R.call_tir(cls.fuse_add_norm_decode, (lv142, lv139_1, model_layers_3_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv141_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv140_1[1]
            rms_norm527: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv140_1[0]
            lv1306 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight10, model_layers_3_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv143 = R.call_tir(cls.NT_matmul2, (rms_norm527, lv1306), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split259: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv143, indices_or_sections=2, axis=-1)
            split_0259: R.Tensor((batch_size, 1, 14336), dtype="float16") = split259[0]
            split_1259: R.Tensor((batch_size, 1, 14336), dtype="float16") = split259[1]
            silu259: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0259)
            mul259: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu259, split_1259)
            lv1307 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight10, model_layers_3_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv144 = R.call_tir(cls.NT_matmul3, (mul259, lv1307), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv142_1 = R.call_tir(cls.fuse_add_norm_decode, (lv144, lv141_1, model_layers_4_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv143_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv142_1[1]
            rms_norm528: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv142_1[0]
            lv1308 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight10, model_layers_4_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv145 = R.call_tir(cls.NT_matmul, (rms_norm528, lv1308), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1040: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv145, R.shape([batch_size, 1, 48, 128]))
            reshape1041: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1040, R.shape([batch_size, 48, 128]))
            lv1309 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape1041), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1042: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1309, R.shape([batch_size, 1, 32, 128]))
            reshape1043: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1042, R.shape([batch_size, 1, 4096]))
            lv1310 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight10, model_layers_4_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv146 = R.call_tir(cls.NT_matmul1, (reshape1043, lv1310), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv144_1 = R.call_tir(cls.fuse_add_norm_decode, (lv146, lv143_1, model_layers_4_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv145_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv144_1[1]
            rms_norm529: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv144_1[0]
            lv1311 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight10, model_layers_4_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv147 = R.call_tir(cls.NT_matmul2, (rms_norm529, lv1311), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split260: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv147, indices_or_sections=2, axis=-1)
            split_0260: R.Tensor((batch_size, 1, 14336), dtype="float16") = split260[0]
            split_1260: R.Tensor((batch_size, 1, 14336), dtype="float16") = split260[1]
            silu260: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0260)
            mul260: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu260, split_1260)
            lv1312 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight10, model_layers_4_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv148 = R.call_tir(cls.NT_matmul3, (mul260, lv1312), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv146_1 = R.call_tir(cls.fuse_add_norm_decode, (lv148, lv145_1, model_layers_5_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv147_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv146_1[1]
            rms_norm530: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv146_1[0]
            lv1313 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight10, model_layers_5_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv149 = R.call_tir(cls.NT_matmul, (rms_norm530, lv1313), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1044: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv149, R.shape([batch_size, 1, 48, 128]))
            reshape1045: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1044, R.shape([batch_size, 48, 128]))
            lv1314 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape1045), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1046: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1314, R.shape([batch_size, 1, 32, 128]))
            reshape1047: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1046, R.shape([batch_size, 1, 4096]))
            lv1315 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight10, model_layers_5_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv150 = R.call_tir(cls.NT_matmul1, (reshape1047, lv1315), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv148_1 = R.call_tir(cls.fuse_add_norm_decode, (lv150, lv147_1, model_layers_5_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv149_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv148_1[1]
            rms_norm531: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv148_1[0]
            lv1316 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight10, model_layers_5_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv151 = R.call_tir(cls.NT_matmul2, (rms_norm531, lv1316), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split261: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv151, indices_or_sections=2, axis=-1)
            split_0261: R.Tensor((batch_size, 1, 14336), dtype="float16") = split261[0]
            split_1261: R.Tensor((batch_size, 1, 14336), dtype="float16") = split261[1]
            silu261: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0261)
            mul261: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu261, split_1261)
            lv1317 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight10, model_layers_5_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv152 = R.call_tir(cls.NT_matmul3, (mul261, lv1317), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv150_1 = R.call_tir(cls.fuse_add_norm_decode, (lv152, lv149_1, model_layers_6_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv151_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv150_1[1]
            rms_norm532: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv150_1[0]
            lv1318 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight10, model_layers_6_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv153 = R.call_tir(cls.NT_matmul, (rms_norm532, lv1318), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1048: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv153, R.shape([batch_size, 1, 48, 128]))
            reshape1049: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1048, R.shape([batch_size, 48, 128]))
            lv1319 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape1049), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1050: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1319, R.shape([batch_size, 1, 32, 128]))
            reshape1051: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1050, R.shape([batch_size, 1, 4096]))
            lv1320 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight10, model_layers_6_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv154 = R.call_tir(cls.NT_matmul1, (reshape1051, lv1320), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv152_1 = R.call_tir(cls.fuse_add_norm_decode, (lv154, lv151_1, model_layers_6_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv153_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv152_1[1]
            rms_norm533: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv152_1[0]
            lv1321 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight10, model_layers_6_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv155 = R.call_tir(cls.NT_matmul2, (rms_norm533, lv1321), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split262: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv155, indices_or_sections=2, axis=-1)
            split_0262: R.Tensor((batch_size, 1, 14336), dtype="float16") = split262[0]
            split_1262: R.Tensor((batch_size, 1, 14336), dtype="float16") = split262[1]
            silu262: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0262)
            mul262: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu262, split_1262)
            lv1322 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight10, model_layers_6_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv156 = R.call_tir(cls.NT_matmul3, (mul262, lv1322), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv154_1 = R.call_tir(cls.fuse_add_norm_decode, (lv156, lv153_1, model_layers_7_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv155_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv154_1[1]
            rms_norm534: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv154_1[0]
            lv1323 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight10, model_layers_7_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv157 = R.call_tir(cls.NT_matmul, (rms_norm534, lv1323), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1052: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv157, R.shape([batch_size, 1, 48, 128]))
            reshape1053: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1052, R.shape([batch_size, 48, 128]))
            lv1324 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape1053), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1054: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1324, R.shape([batch_size, 1, 32, 128]))
            reshape1055: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1054, R.shape([batch_size, 1, 4096]))
            lv1325 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight10, model_layers_7_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv158 = R.call_tir(cls.NT_matmul1, (reshape1055, lv1325), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv156_1 = R.call_tir(cls.fuse_add_norm_decode, (lv158, lv155_1, model_layers_7_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv157_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv156_1[1]
            rms_norm535: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv156_1[0]
            lv1326 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight10, model_layers_7_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv159 = R.call_tir(cls.NT_matmul2, (rms_norm535, lv1326), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split263: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv159, indices_or_sections=2, axis=-1)
            split_0263: R.Tensor((batch_size, 1, 14336), dtype="float16") = split263[0]
            split_1263: R.Tensor((batch_size, 1, 14336), dtype="float16") = split263[1]
            silu263: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0263)
            mul263: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu263, split_1263)
            lv1327 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight10, model_layers_7_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv160 = R.call_tir(cls.NT_matmul3, (mul263, lv1327), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv158_1 = R.call_tir(cls.fuse_add_norm_decode, (lv160, lv157_1, model_layers_8_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv159_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv158_1[1]
            rms_norm536: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv158_1[0]
            lv1328 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight10, model_layers_8_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv161 = R.call_tir(cls.NT_matmul, (rms_norm536, lv1328), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1056: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv161, R.shape([batch_size, 1, 48, 128]))
            reshape1057: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1056, R.shape([batch_size, 48, 128]))
            lv1329 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape1057), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1058: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1329, R.shape([batch_size, 1, 32, 128]))
            reshape1059: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1058, R.shape([batch_size, 1, 4096]))
            lv1330 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight10, model_layers_8_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv162 = R.call_tir(cls.NT_matmul1, (reshape1059, lv1330), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv160_1 = R.call_tir(cls.fuse_add_norm_decode, (lv162, lv159_1, model_layers_8_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv161_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv160_1[1]
            rms_norm537: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv160_1[0]
            lv1331 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight10, model_layers_8_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv163 = R.call_tir(cls.NT_matmul2, (rms_norm537, lv1331), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split264: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv163, indices_or_sections=2, axis=-1)
            split_0264: R.Tensor((batch_size, 1, 14336), dtype="float16") = split264[0]
            split_1264: R.Tensor((batch_size, 1, 14336), dtype="float16") = split264[1]
            silu264: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0264)
            mul264: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu264, split_1264)
            lv1332 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight10, model_layers_8_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv164 = R.call_tir(cls.NT_matmul3, (mul264, lv1332), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv162_1 = R.call_tir(cls.fuse_add_norm_decode, (lv164, lv161_1, model_layers_9_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv163_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv162_1[1]
            rms_norm538: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv162_1[0]
            lv1333 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight10, model_layers_9_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv165 = R.call_tir(cls.NT_matmul, (rms_norm538, lv1333), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1060: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv165, R.shape([batch_size, 1, 48, 128]))
            reshape1061: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1060, R.shape([batch_size, 48, 128]))
            lv1334 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape1061), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1062: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1334, R.shape([batch_size, 1, 32, 128]))
            reshape1063: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1062, R.shape([batch_size, 1, 4096]))
            lv1335 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight10, model_layers_9_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv166 = R.call_tir(cls.NT_matmul1, (reshape1063, lv1335), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv164_1 = R.call_tir(cls.fuse_add_norm_decode, (lv166, lv163_1, model_layers_9_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv165_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv164_1[1]
            rms_norm539: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv164_1[0]
            lv1336 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight10, model_layers_9_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv167 = R.call_tir(cls.NT_matmul2, (rms_norm539, lv1336), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split265: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv167, indices_or_sections=2, axis=-1)
            split_0265: R.Tensor((batch_size, 1, 14336), dtype="float16") = split265[0]
            split_1265: R.Tensor((batch_size, 1, 14336), dtype="float16") = split265[1]
            silu265: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0265)
            mul265: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu265, split_1265)
            lv1337 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight10, model_layers_9_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv168 = R.call_tir(cls.NT_matmul3, (mul265, lv1337), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv166_1 = R.call_tir(cls.fuse_add_norm_decode, (lv168, lv165_1, model_layers_10_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv167_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv166_1[1]
            rms_norm540: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv166_1[0]
            lv1338 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight10, model_layers_10_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv169 = R.call_tir(cls.NT_matmul, (rms_norm540, lv1338), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1064: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv169, R.shape([batch_size, 1, 48, 128]))
            reshape1065: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1064, R.shape([batch_size, 48, 128]))
            lv1339 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape1065), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1066: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1339, R.shape([batch_size, 1, 32, 128]))
            reshape1067: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1066, R.shape([batch_size, 1, 4096]))
            lv1340 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight10, model_layers_10_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv170 = R.call_tir(cls.NT_matmul1, (reshape1067, lv1340), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv168_1 = R.call_tir(cls.fuse_add_norm_decode, (lv170, lv167_1, model_layers_10_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv169_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv168_1[1]
            rms_norm541: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv168_1[0]
            lv1341 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight10, model_layers_10_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv171 = R.call_tir(cls.NT_matmul2, (rms_norm541, lv1341), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split266: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv171, indices_or_sections=2, axis=-1)
            split_0266: R.Tensor((batch_size, 1, 14336), dtype="float16") = split266[0]
            split_1266: R.Tensor((batch_size, 1, 14336), dtype="float16") = split266[1]
            silu266: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0266)
            mul266: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu266, split_1266)
            lv1342 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight10, model_layers_10_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv172 = R.call_tir(cls.NT_matmul3, (mul266, lv1342), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv170_1 = R.call_tir(cls.fuse_add_norm_decode, (lv172, lv169_1, model_layers_11_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv171_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv170_1[1]
            rms_norm542: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv170_1[0]
            lv1343 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight10, model_layers_11_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv173 = R.call_tir(cls.NT_matmul, (rms_norm542, lv1343), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1068: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv173, R.shape([batch_size, 1, 48, 128]))
            reshape1069: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1068, R.shape([batch_size, 48, 128]))
            lv1344 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape1069), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1070: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1344, R.shape([batch_size, 1, 32, 128]))
            reshape1071: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1070, R.shape([batch_size, 1, 4096]))
            lv1345 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight10, model_layers_11_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv174 = R.call_tir(cls.NT_matmul1, (reshape1071, lv1345), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv172_1 = R.call_tir(cls.fuse_add_norm_decode, (lv174, lv171_1, model_layers_11_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv173_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv172_1[1]
            rms_norm543: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv172_1[0]
            lv1346 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight10, model_layers_11_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv175 = R.call_tir(cls.NT_matmul2, (rms_norm543, lv1346), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split267: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv175, indices_or_sections=2, axis=-1)
            split_0267: R.Tensor((batch_size, 1, 14336), dtype="float16") = split267[0]
            split_1267: R.Tensor((batch_size, 1, 14336), dtype="float16") = split267[1]
            silu267: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0267)
            mul267: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu267, split_1267)
            lv1347 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight10, model_layers_11_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv176 = R.call_tir(cls.NT_matmul3, (mul267, lv1347), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv174_1 = R.call_tir(cls.fuse_add_norm_decode, (lv176, lv173_1, model_layers_12_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv175_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv174_1[1]
            rms_norm544: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv174_1[0]
            lv1348 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight10, model_layers_12_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv177 = R.call_tir(cls.NT_matmul, (rms_norm544, lv1348), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1072: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv177, R.shape([batch_size, 1, 48, 128]))
            reshape1073: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1072, R.shape([batch_size, 48, 128]))
            lv1349 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape1073), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1074: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1349, R.shape([batch_size, 1, 32, 128]))
            reshape1075: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1074, R.shape([batch_size, 1, 4096]))
            lv1350 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight10, model_layers_12_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv178 = R.call_tir(cls.NT_matmul1, (reshape1075, lv1350), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv176_1 = R.call_tir(cls.fuse_add_norm_decode, (lv178, lv175_1, model_layers_12_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv177_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv176_1[1]
            rms_norm545: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv176_1[0]
            lv1351 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight10, model_layers_12_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv179 = R.call_tir(cls.NT_matmul2, (rms_norm545, lv1351), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split268: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv179, indices_or_sections=2, axis=-1)
            split_0268: R.Tensor((batch_size, 1, 14336), dtype="float16") = split268[0]
            split_1268: R.Tensor((batch_size, 1, 14336), dtype="float16") = split268[1]
            silu268: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0268)
            mul268: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu268, split_1268)
            lv1352 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight10, model_layers_12_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv180 = R.call_tir(cls.NT_matmul3, (mul268, lv1352), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv178_1 = R.call_tir(cls.fuse_add_norm_decode, (lv180, lv177_1, model_layers_13_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv179_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv178_1[1]
            rms_norm546: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv178_1[0]
            lv1353 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight10, model_layers_13_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv181 = R.call_tir(cls.NT_matmul, (rms_norm546, lv1353), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1076: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv181, R.shape([batch_size, 1, 48, 128]))
            reshape1077: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1076, R.shape([batch_size, 48, 128]))
            lv1354 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape1077), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1078: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1354, R.shape([batch_size, 1, 32, 128]))
            reshape1079: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1078, R.shape([batch_size, 1, 4096]))
            lv1355 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight10, model_layers_13_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv182 = R.call_tir(cls.NT_matmul1, (reshape1079, lv1355), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv180_1 = R.call_tir(cls.fuse_add_norm_decode, (lv182, lv179_1, model_layers_13_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv181_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv180_1[1]
            rms_norm547: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv180_1[0]
            lv1356 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight10, model_layers_13_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv183 = R.call_tir(cls.NT_matmul2, (rms_norm547, lv1356), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split269: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv183, indices_or_sections=2, axis=-1)
            split_0269: R.Tensor((batch_size, 1, 14336), dtype="float16") = split269[0]
            split_1269: R.Tensor((batch_size, 1, 14336), dtype="float16") = split269[1]
            silu269: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0269)
            mul269: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu269, split_1269)
            lv1357 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight10, model_layers_13_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv184 = R.call_tir(cls.NT_matmul3, (mul269, lv1357), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv182_1 = R.call_tir(cls.fuse_add_norm_decode, (lv184, lv181_1, model_layers_14_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv183_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv182_1[1]
            rms_norm548: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv182_1[0]
            lv1358 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight10, model_layers_14_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv185 = R.call_tir(cls.NT_matmul, (rms_norm548, lv1358), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1080: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv185, R.shape([batch_size, 1, 48, 128]))
            reshape1081: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1080, R.shape([batch_size, 48, 128]))
            lv1359 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape1081), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1082: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1359, R.shape([batch_size, 1, 32, 128]))
            reshape1083: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1082, R.shape([batch_size, 1, 4096]))
            lv1360 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight10, model_layers_14_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv186 = R.call_tir(cls.NT_matmul1, (reshape1083, lv1360), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv184_1 = R.call_tir(cls.fuse_add_norm_decode, (lv186, lv183_1, model_layers_14_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv185_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv184_1[1]
            rms_norm549: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv184_1[0]
            lv1361 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight10, model_layers_14_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv187 = R.call_tir(cls.NT_matmul2, (rms_norm549, lv1361), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split270: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv187, indices_or_sections=2, axis=-1)
            split_0270: R.Tensor((batch_size, 1, 14336), dtype="float16") = split270[0]
            split_1270: R.Tensor((batch_size, 1, 14336), dtype="float16") = split270[1]
            silu270: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0270)
            mul270: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu270, split_1270)
            lv1362 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight10, model_layers_14_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv188 = R.call_tir(cls.NT_matmul3, (mul270, lv1362), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv186_1 = R.call_tir(cls.fuse_add_norm_decode, (lv188, lv185_1, model_layers_15_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv187_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv186_1[1]
            rms_norm550: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv186_1[0]
            lv1363 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight10, model_layers_15_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv189 = R.call_tir(cls.NT_matmul, (rms_norm550, lv1363), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1084: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv189, R.shape([batch_size, 1, 48, 128]))
            reshape1085: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1084, R.shape([batch_size, 48, 128]))
            lv1364 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape1085), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1086: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1364, R.shape([batch_size, 1, 32, 128]))
            reshape1087: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1086, R.shape([batch_size, 1, 4096]))
            lv1365 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight10, model_layers_15_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv190 = R.call_tir(cls.NT_matmul1, (reshape1087, lv1365), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv188_1 = R.call_tir(cls.fuse_add_norm_decode, (lv190, lv187_1, model_layers_15_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv189_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv188_1[1]
            rms_norm551: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv188_1[0]
            lv1366 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight10, model_layers_15_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv191 = R.call_tir(cls.NT_matmul2, (rms_norm551, lv1366), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split271: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv191, indices_or_sections=2, axis=-1)
            split_0271: R.Tensor((batch_size, 1, 14336), dtype="float16") = split271[0]
            split_1271: R.Tensor((batch_size, 1, 14336), dtype="float16") = split271[1]
            silu271: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0271)
            mul271: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu271, split_1271)
            lv1367 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight10, model_layers_15_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv192 = R.call_tir(cls.NT_matmul3, (mul271, lv1367), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv190_1 = R.call_tir(cls.fuse_add_norm_decode, (lv192, lv189_1, model_layers_16_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv191_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv190_1[1]
            rms_norm552: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv190_1[0]
            lv1368 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight10, model_layers_16_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv193 = R.call_tir(cls.NT_matmul, (rms_norm552, lv1368), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1088: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv193, R.shape([batch_size, 1, 48, 128]))
            reshape1089: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1088, R.shape([batch_size, 48, 128]))
            lv1369 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape1089), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1090: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1369, R.shape([batch_size, 1, 32, 128]))
            reshape1091: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1090, R.shape([batch_size, 1, 4096]))
            lv1370 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight10, model_layers_16_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv194 = R.call_tir(cls.NT_matmul1, (reshape1091, lv1370), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv192_1 = R.call_tir(cls.fuse_add_norm_decode, (lv194, lv191_1, model_layers_16_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv193_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv192_1[1]
            rms_norm553: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv192_1[0]
            lv1371 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight10, model_layers_16_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv195 = R.call_tir(cls.NT_matmul2, (rms_norm553, lv1371), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split272: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv195, indices_or_sections=2, axis=-1)
            split_0272: R.Tensor((batch_size, 1, 14336), dtype="float16") = split272[0]
            split_1272: R.Tensor((batch_size, 1, 14336), dtype="float16") = split272[1]
            silu272: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0272)
            mul272: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu272, split_1272)
            lv1372 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight10, model_layers_16_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv196 = R.call_tir(cls.NT_matmul3, (mul272, lv1372), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv194_1 = R.call_tir(cls.fuse_add_norm_decode, (lv196, lv193_1, model_layers_17_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv195_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv194_1[1]
            rms_norm554: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv194_1[0]
            lv1373 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight10, model_layers_17_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv197 = R.call_tir(cls.NT_matmul, (rms_norm554, lv1373), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1092: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv197, R.shape([batch_size, 1, 48, 128]))
            reshape1093: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1092, R.shape([batch_size, 48, 128]))
            lv1374 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape1093), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1094: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1374, R.shape([batch_size, 1, 32, 128]))
            reshape1095: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1094, R.shape([batch_size, 1, 4096]))
            lv1375 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight10, model_layers_17_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv198 = R.call_tir(cls.NT_matmul1, (reshape1095, lv1375), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv196_1 = R.call_tir(cls.fuse_add_norm_decode, (lv198, lv195_1, model_layers_17_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv197_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv196_1[1]
            rms_norm555: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv196_1[0]
            lv1376 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight10, model_layers_17_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv199 = R.call_tir(cls.NT_matmul2, (rms_norm555, lv1376), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split273: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv199, indices_or_sections=2, axis=-1)
            split_0273: R.Tensor((batch_size, 1, 14336), dtype="float16") = split273[0]
            split_1273: R.Tensor((batch_size, 1, 14336), dtype="float16") = split273[1]
            silu273: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0273)
            mul273: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu273, split_1273)
            lv1377 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight10, model_layers_17_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv200 = R.call_tir(cls.NT_matmul3, (mul273, lv1377), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv198_1 = R.call_tir(cls.fuse_add_norm_decode, (lv200, lv197_1, model_layers_18_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv199_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv198_1[1]
            rms_norm556: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv198_1[0]
            lv1378 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight10, model_layers_18_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv201 = R.call_tir(cls.NT_matmul, (rms_norm556, lv1378), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1096: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv201, R.shape([batch_size, 1, 48, 128]))
            reshape1097: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1096, R.shape([batch_size, 48, 128]))
            lv1379 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape1097), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1098: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1379, R.shape([batch_size, 1, 32, 128]))
            reshape1099: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1098, R.shape([batch_size, 1, 4096]))
            lv1380 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight10, model_layers_18_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv202 = R.call_tir(cls.NT_matmul1, (reshape1099, lv1380), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv200_1 = R.call_tir(cls.fuse_add_norm_decode, (lv202, lv199_1, model_layers_18_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv201_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv200_1[1]
            rms_norm557: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv200_1[0]
            lv1381 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight10, model_layers_18_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv203 = R.call_tir(cls.NT_matmul2, (rms_norm557, lv1381), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split274: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv203, indices_or_sections=2, axis=-1)
            split_0274: R.Tensor((batch_size, 1, 14336), dtype="float16") = split274[0]
            split_1274: R.Tensor((batch_size, 1, 14336), dtype="float16") = split274[1]
            silu274: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0274)
            mul274: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu274, split_1274)
            lv1382 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight10, model_layers_18_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv204 = R.call_tir(cls.NT_matmul3, (mul274, lv1382), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv202_1 = R.call_tir(cls.fuse_add_norm_decode, (lv204, lv201_1, model_layers_19_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv203_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv202_1[1]
            rms_norm558: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv202_1[0]
            lv1383 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight10, model_layers_19_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv205 = R.call_tir(cls.NT_matmul, (rms_norm558, lv1383), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1100: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv205, R.shape([batch_size, 1, 48, 128]))
            reshape1101: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1100, R.shape([batch_size, 48, 128]))
            lv1384 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape1101), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1102: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1384, R.shape([batch_size, 1, 32, 128]))
            reshape1103: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1102, R.shape([batch_size, 1, 4096]))
            lv1385 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight10, model_layers_19_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv206 = R.call_tir(cls.NT_matmul1, (reshape1103, lv1385), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv204_1 = R.call_tir(cls.fuse_add_norm_decode, (lv206, lv203_1, model_layers_19_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv205_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv204_1[1]
            rms_norm559: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv204_1[0]
            lv1386 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight10, model_layers_19_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv207 = R.call_tir(cls.NT_matmul2, (rms_norm559, lv1386), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split275: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv207, indices_or_sections=2, axis=-1)
            split_0275: R.Tensor((batch_size, 1, 14336), dtype="float16") = split275[0]
            split_1275: R.Tensor((batch_size, 1, 14336), dtype="float16") = split275[1]
            silu275: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0275)
            mul275: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu275, split_1275)
            lv1387 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight10, model_layers_19_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv208 = R.call_tir(cls.NT_matmul3, (mul275, lv1387), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv206_1 = R.call_tir(cls.fuse_add_norm_decode, (lv208, lv205_1, model_layers_20_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv207_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv206_1[1]
            rms_norm560: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv206_1[0]
            lv1388 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight10, model_layers_20_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv209 = R.call_tir(cls.NT_matmul, (rms_norm560, lv1388), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1104: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv209, R.shape([batch_size, 1, 48, 128]))
            reshape1105: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1104, R.shape([batch_size, 48, 128]))
            lv1389 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape1105), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1106: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1389, R.shape([batch_size, 1, 32, 128]))
            reshape1107: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1106, R.shape([batch_size, 1, 4096]))
            lv1390 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight10, model_layers_20_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv210 = R.call_tir(cls.NT_matmul1, (reshape1107, lv1390), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv208_1 = R.call_tir(cls.fuse_add_norm_decode, (lv210, lv207_1, model_layers_20_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv209_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv208_1[1]
            rms_norm561: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv208_1[0]
            lv1391 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight10, model_layers_20_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv211 = R.call_tir(cls.NT_matmul2, (rms_norm561, lv1391), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split276: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv211, indices_or_sections=2, axis=-1)
            split_0276: R.Tensor((batch_size, 1, 14336), dtype="float16") = split276[0]
            split_1276: R.Tensor((batch_size, 1, 14336), dtype="float16") = split276[1]
            silu276: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0276)
            mul276: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu276, split_1276)
            lv1392 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight10, model_layers_20_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv212 = R.call_tir(cls.NT_matmul3, (mul276, lv1392), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv210_1 = R.call_tir(cls.fuse_add_norm_decode, (lv212, lv209_1, model_layers_21_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv211_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv210_1[1]
            rms_norm562: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv210_1[0]
            lv1393 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight10, model_layers_21_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv213 = R.call_tir(cls.NT_matmul, (rms_norm562, lv1393), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1108: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv213, R.shape([batch_size, 1, 48, 128]))
            reshape1109: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1108, R.shape([batch_size, 48, 128]))
            lv1394 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape1109), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1110: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1394, R.shape([batch_size, 1, 32, 128]))
            reshape1111: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1110, R.shape([batch_size, 1, 4096]))
            lv1395 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight10, model_layers_21_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv214 = R.call_tir(cls.NT_matmul1, (reshape1111, lv1395), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv212_1 = R.call_tir(cls.fuse_add_norm_decode, (lv214, lv211_1, model_layers_21_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv213_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv212_1[1]
            rms_norm563: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv212_1[0]
            lv1396 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight10, model_layers_21_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv215 = R.call_tir(cls.NT_matmul2, (rms_norm563, lv1396), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split277: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv215, indices_or_sections=2, axis=-1)
            split_0277: R.Tensor((batch_size, 1, 14336), dtype="float16") = split277[0]
            split_1277: R.Tensor((batch_size, 1, 14336), dtype="float16") = split277[1]
            silu277: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0277)
            mul277: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu277, split_1277)
            lv1397 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight10, model_layers_21_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv216 = R.call_tir(cls.NT_matmul3, (mul277, lv1397), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv214_1 = R.call_tir(cls.fuse_add_norm_decode, (lv216, lv213_1, model_layers_22_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv215_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv214_1[1]
            rms_norm564: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv214_1[0]
            lv1398 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight10, model_layers_22_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv217 = R.call_tir(cls.NT_matmul, (rms_norm564, lv1398), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1112: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv217, R.shape([batch_size, 1, 48, 128]))
            reshape1113: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1112, R.shape([batch_size, 48, 128]))
            lv1399 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape1113), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1114: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1399, R.shape([batch_size, 1, 32, 128]))
            reshape1115: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1114, R.shape([batch_size, 1, 4096]))
            lv1400 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight10, model_layers_22_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv218 = R.call_tir(cls.NT_matmul1, (reshape1115, lv1400), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv216_1 = R.call_tir(cls.fuse_add_norm_decode, (lv218, lv215_1, model_layers_22_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv217_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv216_1[1]
            rms_norm565: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv216_1[0]
            lv1401 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight10, model_layers_22_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv219 = R.call_tir(cls.NT_matmul2, (rms_norm565, lv1401), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split278: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv219, indices_or_sections=2, axis=-1)
            split_0278: R.Tensor((batch_size, 1, 14336), dtype="float16") = split278[0]
            split_1278: R.Tensor((batch_size, 1, 14336), dtype="float16") = split278[1]
            silu278: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0278)
            mul278: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu278, split_1278)
            lv1402 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight10, model_layers_22_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv220 = R.call_tir(cls.NT_matmul3, (mul278, lv1402), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv218_1 = R.call_tir(cls.fuse_add_norm_decode, (lv220, lv217_1, model_layers_23_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv219_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv218_1[1]
            rms_norm566: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv218_1[0]
            lv1403 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight10, model_layers_23_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv221 = R.call_tir(cls.NT_matmul, (rms_norm566, lv1403), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1116: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv221, R.shape([batch_size, 1, 48, 128]))
            reshape1117: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1116, R.shape([batch_size, 48, 128]))
            lv1404 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape1117), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1118: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1404, R.shape([batch_size, 1, 32, 128]))
            reshape1119: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1118, R.shape([batch_size, 1, 4096]))
            lv1405 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight10, model_layers_23_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv222 = R.call_tir(cls.NT_matmul1, (reshape1119, lv1405), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv220_1 = R.call_tir(cls.fuse_add_norm_decode, (lv222, lv219_1, model_layers_23_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv221_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv220_1[1]
            rms_norm567: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv220_1[0]
            lv1406 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight10, model_layers_23_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv223 = R.call_tir(cls.NT_matmul2, (rms_norm567, lv1406), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split279: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv223, indices_or_sections=2, axis=-1)
            split_0279: R.Tensor((batch_size, 1, 14336), dtype="float16") = split279[0]
            split_1279: R.Tensor((batch_size, 1, 14336), dtype="float16") = split279[1]
            silu279: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0279)
            mul279: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu279, split_1279)
            lv1407 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight10, model_layers_23_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv224 = R.call_tir(cls.NT_matmul3, (mul279, lv1407), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv222_1 = R.call_tir(cls.fuse_add_norm_decode, (lv224, lv221_1, model_layers_24_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv223_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv222_1[1]
            rms_norm568: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv222_1[0]
            lv1408 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight10, model_layers_24_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv225 = R.call_tir(cls.NT_matmul, (rms_norm568, lv1408), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1120: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv225, R.shape([batch_size, 1, 48, 128]))
            reshape1121: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1120, R.shape([batch_size, 48, 128]))
            lv1409 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape1121), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1122: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1409, R.shape([batch_size, 1, 32, 128]))
            reshape1123: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1122, R.shape([batch_size, 1, 4096]))
            lv1410 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight10, model_layers_24_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv226 = R.call_tir(cls.NT_matmul1, (reshape1123, lv1410), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv224_1 = R.call_tir(cls.fuse_add_norm_decode, (lv226, lv223_1, model_layers_24_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv225_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv224_1[1]
            rms_norm569: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv224_1[0]
            lv1411 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight10, model_layers_24_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv227 = R.call_tir(cls.NT_matmul2, (rms_norm569, lv1411), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split280: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv227, indices_or_sections=2, axis=-1)
            split_0280: R.Tensor((batch_size, 1, 14336), dtype="float16") = split280[0]
            split_1280: R.Tensor((batch_size, 1, 14336), dtype="float16") = split280[1]
            silu280: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0280)
            mul280: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu280, split_1280)
            lv1412 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight10, model_layers_24_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv228 = R.call_tir(cls.NT_matmul3, (mul280, lv1412), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv226_1 = R.call_tir(cls.fuse_add_norm_decode, (lv228, lv225_1, model_layers_25_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv227_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv226_1[1]
            rms_norm570: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv226_1[0]
            lv1413 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight10, model_layers_25_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv229 = R.call_tir(cls.NT_matmul, (rms_norm570, lv1413), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1124: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv229, R.shape([batch_size, 1, 48, 128]))
            reshape1125: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1124, R.shape([batch_size, 48, 128]))
            lv1414 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape1125), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1126: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1414, R.shape([batch_size, 1, 32, 128]))
            reshape1127: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1126, R.shape([batch_size, 1, 4096]))
            lv1415 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight10, model_layers_25_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv230 = R.call_tir(cls.NT_matmul1, (reshape1127, lv1415), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv228_1 = R.call_tir(cls.fuse_add_norm_decode, (lv230, lv227_1, model_layers_25_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv229_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv228_1[1]
            rms_norm571: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv228_1[0]
            lv1416 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight10, model_layers_25_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv231 = R.call_tir(cls.NT_matmul2, (rms_norm571, lv1416), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split281: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv231, indices_or_sections=2, axis=-1)
            split_0281: R.Tensor((batch_size, 1, 14336), dtype="float16") = split281[0]
            split_1281: R.Tensor((batch_size, 1, 14336), dtype="float16") = split281[1]
            silu281: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0281)
            mul281: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu281, split_1281)
            lv1417 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight10, model_layers_25_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv232 = R.call_tir(cls.NT_matmul3, (mul281, lv1417), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv230_1 = R.call_tir(cls.fuse_add_norm_decode, (lv232, lv229_1, model_layers_26_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv231_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv230_1[1]
            rms_norm572: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv230_1[0]
            lv1418 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight10, model_layers_26_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv233 = R.call_tir(cls.NT_matmul, (rms_norm572, lv1418), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1128: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv233, R.shape([batch_size, 1, 48, 128]))
            reshape1129: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1128, R.shape([batch_size, 48, 128]))
            lv1419 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1129), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1130: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1419, R.shape([batch_size, 1, 32, 128]))
            reshape1131: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1130, R.shape([batch_size, 1, 4096]))
            lv1420 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight10, model_layers_26_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv234 = R.call_tir(cls.NT_matmul1, (reshape1131, lv1420), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv232_1 = R.call_tir(cls.fuse_add_norm_decode, (lv234, lv231_1, model_layers_26_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv233_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv232_1[1]
            rms_norm573: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv232_1[0]
            lv1421 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight10, model_layers_26_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv235 = R.call_tir(cls.NT_matmul2, (rms_norm573, lv1421), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split282: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv235, indices_or_sections=2, axis=-1)
            split_0282: R.Tensor((batch_size, 1, 14336), dtype="float16") = split282[0]
            split_1282: R.Tensor((batch_size, 1, 14336), dtype="float16") = split282[1]
            silu282: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0282)
            mul282: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu282, split_1282)
            lv1422 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight10, model_layers_26_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv236 = R.call_tir(cls.NT_matmul3, (mul282, lv1422), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv234_1 = R.call_tir(cls.fuse_add_norm_decode, (lv236, lv233_1, model_layers_27_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv235_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv234_1[1]
            rms_norm574: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv234_1[0]
            lv1423 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight10, model_layers_27_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv237 = R.call_tir(cls.NT_matmul, (rms_norm574, lv1423), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1132: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv237, R.shape([batch_size, 1, 48, 128]))
            reshape1133: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1132, R.shape([batch_size, 48, 128]))
            lv1424 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1133), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1134: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1424, R.shape([batch_size, 1, 32, 128]))
            reshape1135: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1134, R.shape([batch_size, 1, 4096]))
            lv1425 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight10, model_layers_27_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv238 = R.call_tir(cls.NT_matmul1, (reshape1135, lv1425), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv236_1 = R.call_tir(cls.fuse_add_norm_decode, (lv238, lv235_1, model_layers_27_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv237_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv236_1[1]
            rms_norm575: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv236_1[0]
            lv1426 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight10, model_layers_27_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv239 = R.call_tir(cls.NT_matmul2, (rms_norm575, lv1426), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split283: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv239, indices_or_sections=2, axis=-1)
            split_0283: R.Tensor((batch_size, 1, 14336), dtype="float16") = split283[0]
            split_1283: R.Tensor((batch_size, 1, 14336), dtype="float16") = split283[1]
            silu283: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0283)
            mul283: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu283, split_1283)
            lv1427 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight10, model_layers_27_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv240 = R.call_tir(cls.NT_matmul3, (mul283, lv1427), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv238_1 = R.call_tir(cls.fuse_add_norm_decode, (lv240, lv237_1, model_layers_28_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv239_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv238_1[1]
            rms_norm576: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv238_1[0]
            lv1428 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight10, model_layers_28_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv241 = R.call_tir(cls.NT_matmul, (rms_norm576, lv1428), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1136: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv241, R.shape([batch_size, 1, 48, 128]))
            reshape1137: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1136, R.shape([batch_size, 48, 128]))
            lv1429 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1137), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1138: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1429, R.shape([batch_size, 1, 32, 128]))
            reshape1139: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1138, R.shape([batch_size, 1, 4096]))
            lv1430 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight10, model_layers_28_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv242 = R.call_tir(cls.NT_matmul1, (reshape1139, lv1430), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv240_1 = R.call_tir(cls.fuse_add_norm_decode, (lv242, lv239_1, model_layers_28_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv241_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv240_1[1]
            rms_norm577: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv240_1[0]
            lv1431 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight10, model_layers_28_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv243 = R.call_tir(cls.NT_matmul2, (rms_norm577, lv1431), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split284: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv243, indices_or_sections=2, axis=-1)
            split_0284: R.Tensor((batch_size, 1, 14336), dtype="float16") = split284[0]
            split_1284: R.Tensor((batch_size, 1, 14336), dtype="float16") = split284[1]
            silu284: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0284)
            mul284: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu284, split_1284)
            lv1432 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight10, model_layers_28_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv244 = R.call_tir(cls.NT_matmul3, (mul284, lv1432), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv242_1 = R.call_tir(cls.fuse_add_norm_decode, (lv244, lv241_1, model_layers_29_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv243_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv242_1[1]
            rms_norm578: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv242_1[0]
            lv1433 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight10, model_layers_29_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv245 = R.call_tir(cls.NT_matmul, (rms_norm578, lv1433), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1140: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv245, R.shape([batch_size, 1, 48, 128]))
            reshape1141: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1140, R.shape([batch_size, 48, 128]))
            lv1434 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1141), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1142: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1434, R.shape([batch_size, 1, 32, 128]))
            reshape1143: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1142, R.shape([batch_size, 1, 4096]))
            lv1435 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight10, model_layers_29_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv246 = R.call_tir(cls.NT_matmul1, (reshape1143, lv1435), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv244_1 = R.call_tir(cls.fuse_add_norm_decode, (lv246, lv243_1, model_layers_29_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv245_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv244_1[1]
            rms_norm579: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv244_1[0]
            lv1436 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight10, model_layers_29_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv247 = R.call_tir(cls.NT_matmul2, (rms_norm579, lv1436), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split285: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv247, indices_or_sections=2, axis=-1)
            split_0285: R.Tensor((batch_size, 1, 14336), dtype="float16") = split285[0]
            split_1285: R.Tensor((batch_size, 1, 14336), dtype="float16") = split285[1]
            silu285: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0285)
            mul285: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu285, split_1285)
            lv1437 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight10, model_layers_29_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv248 = R.call_tir(cls.NT_matmul3, (mul285, lv1437), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv246_1 = R.call_tir(cls.fuse_add_norm_decode, (lv248, lv245_1, model_layers_30_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv247_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv246_1[1]
            rms_norm580: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv246_1[0]
            lv1438 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight10, model_layers_30_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv249 = R.call_tir(cls.NT_matmul, (rms_norm580, lv1438), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1144: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv249, R.shape([batch_size, 1, 48, 128]))
            reshape1145: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1144, R.shape([batch_size, 48, 128]))
            lv1439 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1145), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1146: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1439, R.shape([batch_size, 1, 32, 128]))
            reshape1147: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1146, R.shape([batch_size, 1, 4096]))
            lv1440 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight10, model_layers_30_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv250 = R.call_tir(cls.NT_matmul1, (reshape1147, lv1440), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv248_1 = R.call_tir(cls.fuse_add_norm_decode, (lv250, lv247_1, model_layers_30_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv249_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv248_1[1]
            rms_norm581: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv248_1[0]
            lv1441 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight10, model_layers_30_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv251 = R.call_tir(cls.NT_matmul2, (rms_norm581, lv1441), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split286: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv251, indices_or_sections=2, axis=-1)
            split_0286: R.Tensor((batch_size, 1, 14336), dtype="float16") = split286[0]
            split_1286: R.Tensor((batch_size, 1, 14336), dtype="float16") = split286[1]
            silu286: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0286)
            mul286: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu286, split_1286)
            lv1442 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight10, model_layers_30_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv252 = R.call_tir(cls.NT_matmul3, (mul286, lv1442), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv250_1 = R.call_tir(cls.fuse_add_norm_decode, (lv252, lv249_1, model_layers_31_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv251_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv250_1[1]
            rms_norm582: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv250_1[0]
            lv1443 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight10, model_layers_31_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv253 = R.call_tir(cls.NT_matmul, (rms_norm582, lv1443), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1148: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(lv253, R.shape([batch_size, 1, 48, 128]))
            reshape1149: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1148, R.shape([batch_size, 48, 128]))
            lv1444 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1149), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1150: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1444, R.shape([batch_size, 1, 32, 128]))
            reshape1151: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1150, R.shape([batch_size, 1, 4096]))
            lv1445 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight10, model_layers_31_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv254 = R.call_tir(cls.NT_matmul1, (reshape1151, lv1445), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv252_1 = R.call_tir(cls.fuse_add_norm_decode, (lv254, lv251_1, model_layers_31_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv253_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv252_1[1]
            rms_norm583: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv252_1[0]
            lv1446 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight10, model_layers_31_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv255 = R.call_tir(cls.NT_matmul2, (rms_norm583, lv1446), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            split287: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(lv255, indices_or_sections=2, axis=-1)
            split_0287: R.Tensor((batch_size, 1, 14336), dtype="float16") = split287[0]
            split_1287: R.Tensor((batch_size, 1, 14336), dtype="float16") = split287[1]
            silu287: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0287)
            mul287: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu287, split_1287)
            lv1447 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight10, model_layers_31_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv256 = R.call_tir(cls.NT_matmul3, (mul287, lv1447), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv254_1 = R.call_tir(cls.fuse_add_norm_decode, (lv256, lv253_1, model_norm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            rms_norm584: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv254_1[0]
            gv11: R.Tuple(R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Object) = rms_norm584, paged_kv_cache
            R.output(gv11)
        return gv11

    @R.function
    def batch_prefill(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), logit_positions: R.Tensor(("batch_size",), dtype="int32"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "batch_size", "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 3, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight6: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale6: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm260: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv645 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight6, model_layers_0_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv257 = R.call_tir(cls.NT_matmul5, (rms_norm260, lv645), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape512: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv257, R.shape([1, seq_len, 48, 128]))
            reshape513: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape512, R.shape([seq_len, 48, 128]))
            lv646 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape513), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape514: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv646, R.shape([1, seq_len, 32, 128]))
            reshape515: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape514, R.shape([1, seq_len, 4096]))
            lv647 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight6, model_layers_0_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv258 = R.call_tir(cls.NT_matmul6, (reshape515, lv647), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv256 = R.call_tir(cls.fuse_add_norm_prefill, (lv258, input_embeds, model_layers_0_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv257_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv256[1]
            rms_norm261: R.Tensor((1, seq_len, 4096), dtype="float16") = lv256[0]
            lv648 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight6, model_layers_0_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv259 = R.call_tir(cls.NT_matmul7, (rms_norm261, lv648), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split128: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv259, indices_or_sections=2, axis=-1)
            split_0128: R.Tensor((1, seq_len, 14336), dtype="float16") = split128[0]
            split_1128: R.Tensor((1, seq_len, 14336), dtype="float16") = split128[1]
            silu128: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0128)
            mul128: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu128, split_1128)
            lv649 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight6, model_layers_0_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv260 = R.call_tir(cls.NT_matmul8, (mul128, lv649), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv258_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv260, lv257_1, model_layers_1_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv259_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv258_1[1]
            rms_norm262: R.Tensor((1, seq_len, 4096), dtype="float16") = lv258_1[0]
            lv650 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight6, model_layers_1_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv261 = R.call_tir(cls.NT_matmul5, (rms_norm262, lv650), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape516: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv261, R.shape([1, seq_len, 48, 128]))
            reshape517: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape516, R.shape([seq_len, 48, 128]))
            lv651 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape517), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape518: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv651, R.shape([1, seq_len, 32, 128]))
            reshape519: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape518, R.shape([1, seq_len, 4096]))
            lv652 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight6, model_layers_1_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv262 = R.call_tir(cls.NT_matmul6, (reshape519, lv652), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv260_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv262, lv259_1, model_layers_1_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv261_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv260_1[1]
            rms_norm263: R.Tensor((1, seq_len, 4096), dtype="float16") = lv260_1[0]
            lv653 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight6, model_layers_1_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv263 = R.call_tir(cls.NT_matmul7, (rms_norm263, lv653), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split129: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv263, indices_or_sections=2, axis=-1)
            split_0129: R.Tensor((1, seq_len, 14336), dtype="float16") = split129[0]
            split_1129: R.Tensor((1, seq_len, 14336), dtype="float16") = split129[1]
            silu129: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0129)
            mul129: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu129, split_1129)
            lv654 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight6, model_layers_1_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv264 = R.call_tir(cls.NT_matmul8, (mul129, lv654), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv262_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv264, lv261_1, model_layers_2_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv263_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv262_1[1]
            rms_norm264: R.Tensor((1, seq_len, 4096), dtype="float16") = lv262_1[0]
            lv655 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight6, model_layers_2_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv265 = R.call_tir(cls.NT_matmul5, (rms_norm264, lv655), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape520: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv265, R.shape([1, seq_len, 48, 128]))
            reshape521: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape520, R.shape([seq_len, 48, 128]))
            lv656 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape521), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape522: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv656, R.shape([1, seq_len, 32, 128]))
            reshape523: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape522, R.shape([1, seq_len, 4096]))
            lv657 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight6, model_layers_2_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv266 = R.call_tir(cls.NT_matmul6, (reshape523, lv657), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv264_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv266, lv263_1, model_layers_2_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv265_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv264_1[1]
            rms_norm265: R.Tensor((1, seq_len, 4096), dtype="float16") = lv264_1[0]
            lv658 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight6, model_layers_2_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv267 = R.call_tir(cls.NT_matmul7, (rms_norm265, lv658), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split130: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv267, indices_or_sections=2, axis=-1)
            split_0130: R.Tensor((1, seq_len, 14336), dtype="float16") = split130[0]
            split_1130: R.Tensor((1, seq_len, 14336), dtype="float16") = split130[1]
            silu130: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0130)
            mul130: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu130, split_1130)
            lv659 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight6, model_layers_2_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv268 = R.call_tir(cls.NT_matmul8, (mul130, lv659), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv266_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv268, lv265_1, model_layers_3_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv267_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv266_1[1]
            rms_norm266: R.Tensor((1, seq_len, 4096), dtype="float16") = lv266_1[0]
            lv660 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight6, model_layers_3_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv269 = R.call_tir(cls.NT_matmul5, (rms_norm266, lv660), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape524: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv269, R.shape([1, seq_len, 48, 128]))
            reshape525: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape524, R.shape([seq_len, 48, 128]))
            lv661 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape525), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape526: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv661, R.shape([1, seq_len, 32, 128]))
            reshape527: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape526, R.shape([1, seq_len, 4096]))
            lv662 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight6, model_layers_3_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv270 = R.call_tir(cls.NT_matmul6, (reshape527, lv662), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv268_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv270, lv267_1, model_layers_3_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv269_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv268_1[1]
            rms_norm267: R.Tensor((1, seq_len, 4096), dtype="float16") = lv268_1[0]
            lv663 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight6, model_layers_3_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv271 = R.call_tir(cls.NT_matmul7, (rms_norm267, lv663), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split131: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv271, indices_or_sections=2, axis=-1)
            split_0131: R.Tensor((1, seq_len, 14336), dtype="float16") = split131[0]
            split_1131: R.Tensor((1, seq_len, 14336), dtype="float16") = split131[1]
            silu131: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0131)
            mul131: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu131, split_1131)
            lv664 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight6, model_layers_3_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv272 = R.call_tir(cls.NT_matmul8, (mul131, lv664), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv270_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv272, lv269_1, model_layers_4_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv271_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv270_1[1]
            rms_norm268: R.Tensor((1, seq_len, 4096), dtype="float16") = lv270_1[0]
            lv665 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight6, model_layers_4_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv273 = R.call_tir(cls.NT_matmul5, (rms_norm268, lv665), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape528: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv273, R.shape([1, seq_len, 48, 128]))
            reshape529: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape528, R.shape([seq_len, 48, 128]))
            lv666 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape529), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape530: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv666, R.shape([1, seq_len, 32, 128]))
            reshape531: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape530, R.shape([1, seq_len, 4096]))
            lv667 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight6, model_layers_4_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv274 = R.call_tir(cls.NT_matmul6, (reshape531, lv667), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv272_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv274, lv271_1, model_layers_4_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv273_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv272_1[1]
            rms_norm269: R.Tensor((1, seq_len, 4096), dtype="float16") = lv272_1[0]
            lv668 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight6, model_layers_4_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv275 = R.call_tir(cls.NT_matmul7, (rms_norm269, lv668), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split132: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv275, indices_or_sections=2, axis=-1)
            split_0132: R.Tensor((1, seq_len, 14336), dtype="float16") = split132[0]
            split_1132: R.Tensor((1, seq_len, 14336), dtype="float16") = split132[1]
            silu132: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0132)
            mul132: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu132, split_1132)
            lv669 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight6, model_layers_4_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv276 = R.call_tir(cls.NT_matmul8, (mul132, lv669), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv274_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv276, lv273_1, model_layers_5_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv275_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv274_1[1]
            rms_norm270: R.Tensor((1, seq_len, 4096), dtype="float16") = lv274_1[0]
            lv670 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight6, model_layers_5_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv277 = R.call_tir(cls.NT_matmul5, (rms_norm270, lv670), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape532: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv277, R.shape([1, seq_len, 48, 128]))
            reshape533: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape532, R.shape([seq_len, 48, 128]))
            lv671 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape533), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape534: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv671, R.shape([1, seq_len, 32, 128]))
            reshape535: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape534, R.shape([1, seq_len, 4096]))
            lv672 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight6, model_layers_5_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv278 = R.call_tir(cls.NT_matmul6, (reshape535, lv672), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv276_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv278, lv275_1, model_layers_5_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv277_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv276_1[1]
            rms_norm271: R.Tensor((1, seq_len, 4096), dtype="float16") = lv276_1[0]
            lv673 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight6, model_layers_5_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv279 = R.call_tir(cls.NT_matmul7, (rms_norm271, lv673), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split133: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv279, indices_or_sections=2, axis=-1)
            split_0133: R.Tensor((1, seq_len, 14336), dtype="float16") = split133[0]
            split_1133: R.Tensor((1, seq_len, 14336), dtype="float16") = split133[1]
            silu133: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0133)
            mul133: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu133, split_1133)
            lv674 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight6, model_layers_5_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv280 = R.call_tir(cls.NT_matmul8, (mul133, lv674), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv278_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv280, lv277_1, model_layers_6_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv279_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv278_1[1]
            rms_norm272: R.Tensor((1, seq_len, 4096), dtype="float16") = lv278_1[0]
            lv675 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight6, model_layers_6_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv281 = R.call_tir(cls.NT_matmul5, (rms_norm272, lv675), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape536: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv281, R.shape([1, seq_len, 48, 128]))
            reshape537: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape536, R.shape([seq_len, 48, 128]))
            lv676 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape537), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape538: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv676, R.shape([1, seq_len, 32, 128]))
            reshape539: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape538, R.shape([1, seq_len, 4096]))
            lv677 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight6, model_layers_6_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv282 = R.call_tir(cls.NT_matmul6, (reshape539, lv677), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv280_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv282, lv279_1, model_layers_6_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv281_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv280_1[1]
            rms_norm273: R.Tensor((1, seq_len, 4096), dtype="float16") = lv280_1[0]
            lv678 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight6, model_layers_6_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv283 = R.call_tir(cls.NT_matmul7, (rms_norm273, lv678), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split134: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv283, indices_or_sections=2, axis=-1)
            split_0134: R.Tensor((1, seq_len, 14336), dtype="float16") = split134[0]
            split_1134: R.Tensor((1, seq_len, 14336), dtype="float16") = split134[1]
            silu134: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0134)
            mul134: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu134, split_1134)
            lv679 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight6, model_layers_6_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv284 = R.call_tir(cls.NT_matmul8, (mul134, lv679), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv282_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv284, lv281_1, model_layers_7_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv283_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv282_1[1]
            rms_norm274: R.Tensor((1, seq_len, 4096), dtype="float16") = lv282_1[0]
            lv680 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight6, model_layers_7_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv285 = R.call_tir(cls.NT_matmul5, (rms_norm274, lv680), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape540: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv285, R.shape([1, seq_len, 48, 128]))
            reshape541: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape540, R.shape([seq_len, 48, 128]))
            lv681 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape541), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape542: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv681, R.shape([1, seq_len, 32, 128]))
            reshape543: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape542, R.shape([1, seq_len, 4096]))
            lv682 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight6, model_layers_7_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv286 = R.call_tir(cls.NT_matmul6, (reshape543, lv682), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv284_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv286, lv283_1, model_layers_7_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv285_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv284_1[1]
            rms_norm275: R.Tensor((1, seq_len, 4096), dtype="float16") = lv284_1[0]
            lv683 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight6, model_layers_7_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv287 = R.call_tir(cls.NT_matmul7, (rms_norm275, lv683), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split135: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv287, indices_or_sections=2, axis=-1)
            split_0135: R.Tensor((1, seq_len, 14336), dtype="float16") = split135[0]
            split_1135: R.Tensor((1, seq_len, 14336), dtype="float16") = split135[1]
            silu135: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0135)
            mul135: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu135, split_1135)
            lv684 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight6, model_layers_7_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv288 = R.call_tir(cls.NT_matmul8, (mul135, lv684), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv286_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv288, lv285_1, model_layers_8_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv287_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv286_1[1]
            rms_norm276: R.Tensor((1, seq_len, 4096), dtype="float16") = lv286_1[0]
            lv685 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight6, model_layers_8_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv289 = R.call_tir(cls.NT_matmul5, (rms_norm276, lv685), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape544: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv289, R.shape([1, seq_len, 48, 128]))
            reshape545: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape544, R.shape([seq_len, 48, 128]))
            lv686 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape545), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape546: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv686, R.shape([1, seq_len, 32, 128]))
            reshape547: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape546, R.shape([1, seq_len, 4096]))
            lv687 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight6, model_layers_8_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv290 = R.call_tir(cls.NT_matmul6, (reshape547, lv687), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv288_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv290, lv287_1, model_layers_8_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv289_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv288_1[1]
            rms_norm277: R.Tensor((1, seq_len, 4096), dtype="float16") = lv288_1[0]
            lv688 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight6, model_layers_8_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv291 = R.call_tir(cls.NT_matmul7, (rms_norm277, lv688), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split136: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv291, indices_or_sections=2, axis=-1)
            split_0136: R.Tensor((1, seq_len, 14336), dtype="float16") = split136[0]
            split_1136: R.Tensor((1, seq_len, 14336), dtype="float16") = split136[1]
            silu136: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0136)
            mul136: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu136, split_1136)
            lv689 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight6, model_layers_8_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv292 = R.call_tir(cls.NT_matmul8, (mul136, lv689), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv290_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv292, lv289_1, model_layers_9_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv291_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv290_1[1]
            rms_norm278: R.Tensor((1, seq_len, 4096), dtype="float16") = lv290_1[0]
            lv690 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight6, model_layers_9_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv293 = R.call_tir(cls.NT_matmul5, (rms_norm278, lv690), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape548: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv293, R.shape([1, seq_len, 48, 128]))
            reshape549: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape548, R.shape([seq_len, 48, 128]))
            lv691 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape549), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape550: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv691, R.shape([1, seq_len, 32, 128]))
            reshape551: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape550, R.shape([1, seq_len, 4096]))
            lv692 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight6, model_layers_9_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv294 = R.call_tir(cls.NT_matmul6, (reshape551, lv692), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv292_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv294, lv291_1, model_layers_9_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv293_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv292_1[1]
            rms_norm279: R.Tensor((1, seq_len, 4096), dtype="float16") = lv292_1[0]
            lv693 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight6, model_layers_9_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv295 = R.call_tir(cls.NT_matmul7, (rms_norm279, lv693), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split137: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv295, indices_or_sections=2, axis=-1)
            split_0137: R.Tensor((1, seq_len, 14336), dtype="float16") = split137[0]
            split_1137: R.Tensor((1, seq_len, 14336), dtype="float16") = split137[1]
            silu137: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0137)
            mul137: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu137, split_1137)
            lv694 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight6, model_layers_9_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv296 = R.call_tir(cls.NT_matmul8, (mul137, lv694), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv294_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv296, lv293_1, model_layers_10_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv295_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv294_1[1]
            rms_norm280: R.Tensor((1, seq_len, 4096), dtype="float16") = lv294_1[0]
            lv695 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight6, model_layers_10_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv297 = R.call_tir(cls.NT_matmul5, (rms_norm280, lv695), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape552: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv297, R.shape([1, seq_len, 48, 128]))
            reshape553: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape552, R.shape([seq_len, 48, 128]))
            lv696 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape553), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape554: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv696, R.shape([1, seq_len, 32, 128]))
            reshape555: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape554, R.shape([1, seq_len, 4096]))
            lv697 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight6, model_layers_10_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv298 = R.call_tir(cls.NT_matmul6, (reshape555, lv697), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv296_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv298, lv295_1, model_layers_10_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv297_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv296_1[1]
            rms_norm281: R.Tensor((1, seq_len, 4096), dtype="float16") = lv296_1[0]
            lv698 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight6, model_layers_10_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv299 = R.call_tir(cls.NT_matmul7, (rms_norm281, lv698), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split138: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv299, indices_or_sections=2, axis=-1)
            split_0138: R.Tensor((1, seq_len, 14336), dtype="float16") = split138[0]
            split_1138: R.Tensor((1, seq_len, 14336), dtype="float16") = split138[1]
            silu138: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0138)
            mul138: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu138, split_1138)
            lv699 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight6, model_layers_10_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv300 = R.call_tir(cls.NT_matmul8, (mul138, lv699), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv298_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv300, lv297_1, model_layers_11_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv299_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv298_1[1]
            rms_norm282: R.Tensor((1, seq_len, 4096), dtype="float16") = lv298_1[0]
            lv700 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight6, model_layers_11_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv301 = R.call_tir(cls.NT_matmul5, (rms_norm282, lv700), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape556: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv301, R.shape([1, seq_len, 48, 128]))
            reshape557: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape556, R.shape([seq_len, 48, 128]))
            lv701 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape557), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape558: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv701, R.shape([1, seq_len, 32, 128]))
            reshape559: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape558, R.shape([1, seq_len, 4096]))
            lv702 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight6, model_layers_11_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv302 = R.call_tir(cls.NT_matmul6, (reshape559, lv702), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv300_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv302, lv299_1, model_layers_11_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv301_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv300_1[1]
            rms_norm283: R.Tensor((1, seq_len, 4096), dtype="float16") = lv300_1[0]
            lv703 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight6, model_layers_11_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv303 = R.call_tir(cls.NT_matmul7, (rms_norm283, lv703), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split139: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv303, indices_or_sections=2, axis=-1)
            split_0139: R.Tensor((1, seq_len, 14336), dtype="float16") = split139[0]
            split_1139: R.Tensor((1, seq_len, 14336), dtype="float16") = split139[1]
            silu139: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0139)
            mul139: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu139, split_1139)
            lv704 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight6, model_layers_11_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv304 = R.call_tir(cls.NT_matmul8, (mul139, lv704), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv302_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv304, lv301_1, model_layers_12_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv303_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv302_1[1]
            rms_norm284: R.Tensor((1, seq_len, 4096), dtype="float16") = lv302_1[0]
            lv705 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight6, model_layers_12_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv305 = R.call_tir(cls.NT_matmul5, (rms_norm284, lv705), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape560: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv305, R.shape([1, seq_len, 48, 128]))
            reshape561: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape560, R.shape([seq_len, 48, 128]))
            lv706 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape561), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape562: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv706, R.shape([1, seq_len, 32, 128]))
            reshape563: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape562, R.shape([1, seq_len, 4096]))
            lv707 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight6, model_layers_12_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv306 = R.call_tir(cls.NT_matmul6, (reshape563, lv707), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv304_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv306, lv303_1, model_layers_12_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv305_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv304_1[1]
            rms_norm285: R.Tensor((1, seq_len, 4096), dtype="float16") = lv304_1[0]
            lv708 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight6, model_layers_12_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv307 = R.call_tir(cls.NT_matmul7, (rms_norm285, lv708), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split140: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv307, indices_or_sections=2, axis=-1)
            split_0140: R.Tensor((1, seq_len, 14336), dtype="float16") = split140[0]
            split_1140: R.Tensor((1, seq_len, 14336), dtype="float16") = split140[1]
            silu140: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0140)
            mul140: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu140, split_1140)
            lv709 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight6, model_layers_12_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv308 = R.call_tir(cls.NT_matmul8, (mul140, lv709), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv306_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv308, lv305_1, model_layers_13_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv307_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv306_1[1]
            rms_norm286: R.Tensor((1, seq_len, 4096), dtype="float16") = lv306_1[0]
            lv710 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight6, model_layers_13_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv309 = R.call_tir(cls.NT_matmul5, (rms_norm286, lv710), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape564: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv309, R.shape([1, seq_len, 48, 128]))
            reshape565: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape564, R.shape([seq_len, 48, 128]))
            lv711 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape565), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape566: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv711, R.shape([1, seq_len, 32, 128]))
            reshape567: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape566, R.shape([1, seq_len, 4096]))
            lv712 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight6, model_layers_13_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv310 = R.call_tir(cls.NT_matmul6, (reshape567, lv712), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv308_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv310, lv307_1, model_layers_13_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv309_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv308_1[1]
            rms_norm287: R.Tensor((1, seq_len, 4096), dtype="float16") = lv308_1[0]
            lv713 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight6, model_layers_13_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv311 = R.call_tir(cls.NT_matmul7, (rms_norm287, lv713), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split141: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv311, indices_or_sections=2, axis=-1)
            split_0141: R.Tensor((1, seq_len, 14336), dtype="float16") = split141[0]
            split_1141: R.Tensor((1, seq_len, 14336), dtype="float16") = split141[1]
            silu141: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0141)
            mul141: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu141, split_1141)
            lv714 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight6, model_layers_13_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv312 = R.call_tir(cls.NT_matmul8, (mul141, lv714), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv310_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv312, lv309_1, model_layers_14_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv311_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv310_1[1]
            rms_norm288: R.Tensor((1, seq_len, 4096), dtype="float16") = lv310_1[0]
            lv715 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight6, model_layers_14_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv313 = R.call_tir(cls.NT_matmul5, (rms_norm288, lv715), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape568: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv313, R.shape([1, seq_len, 48, 128]))
            reshape569: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape568, R.shape([seq_len, 48, 128]))
            lv716 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape569), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape570: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv716, R.shape([1, seq_len, 32, 128]))
            reshape571: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape570, R.shape([1, seq_len, 4096]))
            lv717 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight6, model_layers_14_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv314 = R.call_tir(cls.NT_matmul6, (reshape571, lv717), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv312_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv314, lv311_1, model_layers_14_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv313_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv312_1[1]
            rms_norm289: R.Tensor((1, seq_len, 4096), dtype="float16") = lv312_1[0]
            lv718 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight6, model_layers_14_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv315 = R.call_tir(cls.NT_matmul7, (rms_norm289, lv718), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split142: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv315, indices_or_sections=2, axis=-1)
            split_0142: R.Tensor((1, seq_len, 14336), dtype="float16") = split142[0]
            split_1142: R.Tensor((1, seq_len, 14336), dtype="float16") = split142[1]
            silu142: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0142)
            mul142: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu142, split_1142)
            lv719 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight6, model_layers_14_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv316 = R.call_tir(cls.NT_matmul8, (mul142, lv719), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv314_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv316, lv313_1, model_layers_15_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv315_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv314_1[1]
            rms_norm290: R.Tensor((1, seq_len, 4096), dtype="float16") = lv314_1[0]
            lv720 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight6, model_layers_15_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv317 = R.call_tir(cls.NT_matmul5, (rms_norm290, lv720), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape572: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv317, R.shape([1, seq_len, 48, 128]))
            reshape573: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape572, R.shape([seq_len, 48, 128]))
            lv721 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape573), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape574: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv721, R.shape([1, seq_len, 32, 128]))
            reshape575: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape574, R.shape([1, seq_len, 4096]))
            lv722 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight6, model_layers_15_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv318 = R.call_tir(cls.NT_matmul6, (reshape575, lv722), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv316_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv318, lv315_1, model_layers_15_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv317_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv316_1[1]
            rms_norm291: R.Tensor((1, seq_len, 4096), dtype="float16") = lv316_1[0]
            lv723 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight6, model_layers_15_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv319 = R.call_tir(cls.NT_matmul7, (rms_norm291, lv723), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split143: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv319, indices_or_sections=2, axis=-1)
            split_0143: R.Tensor((1, seq_len, 14336), dtype="float16") = split143[0]
            split_1143: R.Tensor((1, seq_len, 14336), dtype="float16") = split143[1]
            silu143: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0143)
            mul143: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu143, split_1143)
            lv724 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight6, model_layers_15_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv320 = R.call_tir(cls.NT_matmul8, (mul143, lv724), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv318_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv320, lv317_1, model_layers_16_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv319_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv318_1[1]
            rms_norm292: R.Tensor((1, seq_len, 4096), dtype="float16") = lv318_1[0]
            lv725 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight6, model_layers_16_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv321 = R.call_tir(cls.NT_matmul5, (rms_norm292, lv725), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape576: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv321, R.shape([1, seq_len, 48, 128]))
            reshape577: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape576, R.shape([seq_len, 48, 128]))
            lv726 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape577), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape578: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv726, R.shape([1, seq_len, 32, 128]))
            reshape579: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape578, R.shape([1, seq_len, 4096]))
            lv727 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight6, model_layers_16_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv322 = R.call_tir(cls.NT_matmul6, (reshape579, lv727), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv320_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv322, lv319_1, model_layers_16_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv321_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv320_1[1]
            rms_norm293: R.Tensor((1, seq_len, 4096), dtype="float16") = lv320_1[0]
            lv728 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight6, model_layers_16_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv323 = R.call_tir(cls.NT_matmul7, (rms_norm293, lv728), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split144: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv323, indices_or_sections=2, axis=-1)
            split_0144: R.Tensor((1, seq_len, 14336), dtype="float16") = split144[0]
            split_1144: R.Tensor((1, seq_len, 14336), dtype="float16") = split144[1]
            silu144: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0144)
            mul144: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu144, split_1144)
            lv729 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight6, model_layers_16_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv324 = R.call_tir(cls.NT_matmul8, (mul144, lv729), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv322_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv324, lv321_1, model_layers_17_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv323_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv322_1[1]
            rms_norm294: R.Tensor((1, seq_len, 4096), dtype="float16") = lv322_1[0]
            lv730 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight6, model_layers_17_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv325 = R.call_tir(cls.NT_matmul5, (rms_norm294, lv730), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape580: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv325, R.shape([1, seq_len, 48, 128]))
            reshape581: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape580, R.shape([seq_len, 48, 128]))
            lv731 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape581), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape582: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv731, R.shape([1, seq_len, 32, 128]))
            reshape583: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape582, R.shape([1, seq_len, 4096]))
            lv732 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight6, model_layers_17_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv326 = R.call_tir(cls.NT_matmul6, (reshape583, lv732), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv324_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv326, lv323_1, model_layers_17_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv325_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv324_1[1]
            rms_norm295: R.Tensor((1, seq_len, 4096), dtype="float16") = lv324_1[0]
            lv733 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight6, model_layers_17_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv327 = R.call_tir(cls.NT_matmul7, (rms_norm295, lv733), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split145: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv327, indices_or_sections=2, axis=-1)
            split_0145: R.Tensor((1, seq_len, 14336), dtype="float16") = split145[0]
            split_1145: R.Tensor((1, seq_len, 14336), dtype="float16") = split145[1]
            silu145: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0145)
            mul145: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu145, split_1145)
            lv734 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight6, model_layers_17_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv328 = R.call_tir(cls.NT_matmul8, (mul145, lv734), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv326_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv328, lv325_1, model_layers_18_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv327_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv326_1[1]
            rms_norm296: R.Tensor((1, seq_len, 4096), dtype="float16") = lv326_1[0]
            lv735 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight6, model_layers_18_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv329 = R.call_tir(cls.NT_matmul5, (rms_norm296, lv735), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape584: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv329, R.shape([1, seq_len, 48, 128]))
            reshape585: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape584, R.shape([seq_len, 48, 128]))
            lv736 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape585), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape586: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv736, R.shape([1, seq_len, 32, 128]))
            reshape587: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape586, R.shape([1, seq_len, 4096]))
            lv737 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight6, model_layers_18_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv330 = R.call_tir(cls.NT_matmul6, (reshape587, lv737), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv328_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv330, lv327_1, model_layers_18_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv329_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv328_1[1]
            rms_norm297: R.Tensor((1, seq_len, 4096), dtype="float16") = lv328_1[0]
            lv738 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight6, model_layers_18_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv331 = R.call_tir(cls.NT_matmul7, (rms_norm297, lv738), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split146: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv331, indices_or_sections=2, axis=-1)
            split_0146: R.Tensor((1, seq_len, 14336), dtype="float16") = split146[0]
            split_1146: R.Tensor((1, seq_len, 14336), dtype="float16") = split146[1]
            silu146: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0146)
            mul146: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu146, split_1146)
            lv739 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight6, model_layers_18_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv332 = R.call_tir(cls.NT_matmul8, (mul146, lv739), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv330_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv332, lv329_1, model_layers_19_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv331_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv330_1[1]
            rms_norm298: R.Tensor((1, seq_len, 4096), dtype="float16") = lv330_1[0]
            lv740 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight6, model_layers_19_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv333 = R.call_tir(cls.NT_matmul5, (rms_norm298, lv740), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape588: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv333, R.shape([1, seq_len, 48, 128]))
            reshape589: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape588, R.shape([seq_len, 48, 128]))
            lv741 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape589), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape590: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv741, R.shape([1, seq_len, 32, 128]))
            reshape591: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape590, R.shape([1, seq_len, 4096]))
            lv742 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight6, model_layers_19_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv334 = R.call_tir(cls.NT_matmul6, (reshape591, lv742), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv332_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv334, lv331_1, model_layers_19_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv333_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv332_1[1]
            rms_norm299: R.Tensor((1, seq_len, 4096), dtype="float16") = lv332_1[0]
            lv743 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight6, model_layers_19_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv335 = R.call_tir(cls.NT_matmul7, (rms_norm299, lv743), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split147: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv335, indices_or_sections=2, axis=-1)
            split_0147: R.Tensor((1, seq_len, 14336), dtype="float16") = split147[0]
            split_1147: R.Tensor((1, seq_len, 14336), dtype="float16") = split147[1]
            silu147: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0147)
            mul147: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu147, split_1147)
            lv744 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight6, model_layers_19_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv336 = R.call_tir(cls.NT_matmul8, (mul147, lv744), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv334_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv336, lv333_1, model_layers_20_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv335_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv334_1[1]
            rms_norm300: R.Tensor((1, seq_len, 4096), dtype="float16") = lv334_1[0]
            lv745 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight6, model_layers_20_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv337 = R.call_tir(cls.NT_matmul5, (rms_norm300, lv745), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape592: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv337, R.shape([1, seq_len, 48, 128]))
            reshape593: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape592, R.shape([seq_len, 48, 128]))
            lv746 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape593), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape594: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv746, R.shape([1, seq_len, 32, 128]))
            reshape595: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape594, R.shape([1, seq_len, 4096]))
            lv747 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight6, model_layers_20_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv338 = R.call_tir(cls.NT_matmul6, (reshape595, lv747), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv336_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv338, lv335_1, model_layers_20_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv337_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv336_1[1]
            rms_norm301: R.Tensor((1, seq_len, 4096), dtype="float16") = lv336_1[0]
            lv748 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight6, model_layers_20_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv339 = R.call_tir(cls.NT_matmul7, (rms_norm301, lv748), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split148: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv339, indices_or_sections=2, axis=-1)
            split_0148: R.Tensor((1, seq_len, 14336), dtype="float16") = split148[0]
            split_1148: R.Tensor((1, seq_len, 14336), dtype="float16") = split148[1]
            silu148: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0148)
            mul148: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu148, split_1148)
            lv749 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight6, model_layers_20_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv340 = R.call_tir(cls.NT_matmul8, (mul148, lv749), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv338_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv340, lv337_1, model_layers_21_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv339_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv338_1[1]
            rms_norm302: R.Tensor((1, seq_len, 4096), dtype="float16") = lv338_1[0]
            lv750 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight6, model_layers_21_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv341 = R.call_tir(cls.NT_matmul5, (rms_norm302, lv750), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape596: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv341, R.shape([1, seq_len, 48, 128]))
            reshape597: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape596, R.shape([seq_len, 48, 128]))
            lv751 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape597), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape598: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv751, R.shape([1, seq_len, 32, 128]))
            reshape599: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape598, R.shape([1, seq_len, 4096]))
            lv752 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight6, model_layers_21_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv342 = R.call_tir(cls.NT_matmul6, (reshape599, lv752), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv340_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv342, lv339_1, model_layers_21_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv341_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv340_1[1]
            rms_norm303: R.Tensor((1, seq_len, 4096), dtype="float16") = lv340_1[0]
            lv753 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight6, model_layers_21_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv343 = R.call_tir(cls.NT_matmul7, (rms_norm303, lv753), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split149: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv343, indices_or_sections=2, axis=-1)
            split_0149: R.Tensor((1, seq_len, 14336), dtype="float16") = split149[0]
            split_1149: R.Tensor((1, seq_len, 14336), dtype="float16") = split149[1]
            silu149: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0149)
            mul149: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu149, split_1149)
            lv754 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight6, model_layers_21_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv344 = R.call_tir(cls.NT_matmul8, (mul149, lv754), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv342_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv344, lv341_1, model_layers_22_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv343_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv342_1[1]
            rms_norm304: R.Tensor((1, seq_len, 4096), dtype="float16") = lv342_1[0]
            lv755 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight6, model_layers_22_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv345 = R.call_tir(cls.NT_matmul5, (rms_norm304, lv755), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape600: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv345, R.shape([1, seq_len, 48, 128]))
            reshape601: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape600, R.shape([seq_len, 48, 128]))
            lv756 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape601), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape602: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv756, R.shape([1, seq_len, 32, 128]))
            reshape603: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape602, R.shape([1, seq_len, 4096]))
            lv757 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight6, model_layers_22_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv346 = R.call_tir(cls.NT_matmul6, (reshape603, lv757), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv344_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv346, lv343_1, model_layers_22_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv345_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv344_1[1]
            rms_norm305: R.Tensor((1, seq_len, 4096), dtype="float16") = lv344_1[0]
            lv758 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight6, model_layers_22_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv347 = R.call_tir(cls.NT_matmul7, (rms_norm305, lv758), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split150: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv347, indices_or_sections=2, axis=-1)
            split_0150: R.Tensor((1, seq_len, 14336), dtype="float16") = split150[0]
            split_1150: R.Tensor((1, seq_len, 14336), dtype="float16") = split150[1]
            silu150: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0150)
            mul150: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu150, split_1150)
            lv759 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight6, model_layers_22_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv348 = R.call_tir(cls.NT_matmul8, (mul150, lv759), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv346_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv348, lv345_1, model_layers_23_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv347_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv346_1[1]
            rms_norm306: R.Tensor((1, seq_len, 4096), dtype="float16") = lv346_1[0]
            lv760 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight6, model_layers_23_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv349 = R.call_tir(cls.NT_matmul5, (rms_norm306, lv760), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape604: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv349, R.shape([1, seq_len, 48, 128]))
            reshape605: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape604, R.shape([seq_len, 48, 128]))
            lv761 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape605), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape606: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv761, R.shape([1, seq_len, 32, 128]))
            reshape607: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape606, R.shape([1, seq_len, 4096]))
            lv762 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight6, model_layers_23_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv350 = R.call_tir(cls.NT_matmul6, (reshape607, lv762), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv348_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv350, lv347_1, model_layers_23_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv349_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv348_1[1]
            rms_norm307: R.Tensor((1, seq_len, 4096), dtype="float16") = lv348_1[0]
            lv763 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight6, model_layers_23_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv351 = R.call_tir(cls.NT_matmul7, (rms_norm307, lv763), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split151: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv351, indices_or_sections=2, axis=-1)
            split_0151: R.Tensor((1, seq_len, 14336), dtype="float16") = split151[0]
            split_1151: R.Tensor((1, seq_len, 14336), dtype="float16") = split151[1]
            silu151: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0151)
            mul151: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu151, split_1151)
            lv764 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight6, model_layers_23_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv352 = R.call_tir(cls.NT_matmul8, (mul151, lv764), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv350_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv352, lv349_1, model_layers_24_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv351_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv350_1[1]
            rms_norm308: R.Tensor((1, seq_len, 4096), dtype="float16") = lv350_1[0]
            lv765 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight6, model_layers_24_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv353 = R.call_tir(cls.NT_matmul5, (rms_norm308, lv765), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape608: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv353, R.shape([1, seq_len, 48, 128]))
            reshape609: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape608, R.shape([seq_len, 48, 128]))
            lv766 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape609), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape610: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv766, R.shape([1, seq_len, 32, 128]))
            reshape611: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape610, R.shape([1, seq_len, 4096]))
            lv767 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight6, model_layers_24_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv354 = R.call_tir(cls.NT_matmul6, (reshape611, lv767), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv352_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv354, lv351_1, model_layers_24_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv353_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv352_1[1]
            rms_norm309: R.Tensor((1, seq_len, 4096), dtype="float16") = lv352_1[0]
            lv768 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight6, model_layers_24_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv355 = R.call_tir(cls.NT_matmul7, (rms_norm309, lv768), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split152: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv355, indices_or_sections=2, axis=-1)
            split_0152: R.Tensor((1, seq_len, 14336), dtype="float16") = split152[0]
            split_1152: R.Tensor((1, seq_len, 14336), dtype="float16") = split152[1]
            silu152: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0152)
            mul152: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu152, split_1152)
            lv769 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight6, model_layers_24_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv356 = R.call_tir(cls.NT_matmul8, (mul152, lv769), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv354_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv356, lv353_1, model_layers_25_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv355_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv354_1[1]
            rms_norm310: R.Tensor((1, seq_len, 4096), dtype="float16") = lv354_1[0]
            lv770 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight6, model_layers_25_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv357 = R.call_tir(cls.NT_matmul5, (rms_norm310, lv770), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape612: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv357, R.shape([1, seq_len, 48, 128]))
            reshape613: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape612, R.shape([seq_len, 48, 128]))
            lv771 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape613), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape614: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv771, R.shape([1, seq_len, 32, 128]))
            reshape615: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape614, R.shape([1, seq_len, 4096]))
            lv772 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight6, model_layers_25_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv358 = R.call_tir(cls.NT_matmul6, (reshape615, lv772), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv356_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv358, lv355_1, model_layers_25_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv357_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv356_1[1]
            rms_norm311: R.Tensor((1, seq_len, 4096), dtype="float16") = lv356_1[0]
            lv773 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight6, model_layers_25_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv359 = R.call_tir(cls.NT_matmul7, (rms_norm311, lv773), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split153: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv359, indices_or_sections=2, axis=-1)
            split_0153: R.Tensor((1, seq_len, 14336), dtype="float16") = split153[0]
            split_1153: R.Tensor((1, seq_len, 14336), dtype="float16") = split153[1]
            silu153: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0153)
            mul153: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu153, split_1153)
            lv774 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight6, model_layers_25_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv360 = R.call_tir(cls.NT_matmul8, (mul153, lv774), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv358_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv360, lv357_1, model_layers_26_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv359_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv358_1[1]
            rms_norm312: R.Tensor((1, seq_len, 4096), dtype="float16") = lv358_1[0]
            lv775 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight6, model_layers_26_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv361 = R.call_tir(cls.NT_matmul5, (rms_norm312, lv775), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape616: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv361, R.shape([1, seq_len, 48, 128]))
            reshape617: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape616, R.shape([seq_len, 48, 128]))
            lv776 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape617), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape618: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv776, R.shape([1, seq_len, 32, 128]))
            reshape619: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape618, R.shape([1, seq_len, 4096]))
            lv777 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight6, model_layers_26_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv362 = R.call_tir(cls.NT_matmul6, (reshape619, lv777), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv360_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv362, lv359_1, model_layers_26_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv361_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv360_1[1]
            rms_norm313: R.Tensor((1, seq_len, 4096), dtype="float16") = lv360_1[0]
            lv778 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight6, model_layers_26_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv363 = R.call_tir(cls.NT_matmul7, (rms_norm313, lv778), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split154: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv363, indices_or_sections=2, axis=-1)
            split_0154: R.Tensor((1, seq_len, 14336), dtype="float16") = split154[0]
            split_1154: R.Tensor((1, seq_len, 14336), dtype="float16") = split154[1]
            silu154: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0154)
            mul154: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu154, split_1154)
            lv779 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight6, model_layers_26_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv364 = R.call_tir(cls.NT_matmul8, (mul154, lv779), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv362_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv364, lv361_1, model_layers_27_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv363_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv362_1[1]
            rms_norm314: R.Tensor((1, seq_len, 4096), dtype="float16") = lv362_1[0]
            lv780 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight6, model_layers_27_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv365 = R.call_tir(cls.NT_matmul5, (rms_norm314, lv780), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape620: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv365, R.shape([1, seq_len, 48, 128]))
            reshape621: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape620, R.shape([seq_len, 48, 128]))
            lv781 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape621), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape622: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv781, R.shape([1, seq_len, 32, 128]))
            reshape623: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape622, R.shape([1, seq_len, 4096]))
            lv782 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight6, model_layers_27_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv366 = R.call_tir(cls.NT_matmul6, (reshape623, lv782), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv364_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv366, lv363_1, model_layers_27_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv365_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv364_1[1]
            rms_norm315: R.Tensor((1, seq_len, 4096), dtype="float16") = lv364_1[0]
            lv783 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight6, model_layers_27_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv367 = R.call_tir(cls.NT_matmul7, (rms_norm315, lv783), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split155: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv367, indices_or_sections=2, axis=-1)
            split_0155: R.Tensor((1, seq_len, 14336), dtype="float16") = split155[0]
            split_1155: R.Tensor((1, seq_len, 14336), dtype="float16") = split155[1]
            silu155: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0155)
            mul155: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu155, split_1155)
            lv784 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight6, model_layers_27_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv368 = R.call_tir(cls.NT_matmul8, (mul155, lv784), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv366_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv368, lv365_1, model_layers_28_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv367_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv366_1[1]
            rms_norm316: R.Tensor((1, seq_len, 4096), dtype="float16") = lv366_1[0]
            lv785 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight6, model_layers_28_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv369 = R.call_tir(cls.NT_matmul5, (rms_norm316, lv785), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape624: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv369, R.shape([1, seq_len, 48, 128]))
            reshape625: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape624, R.shape([seq_len, 48, 128]))
            lv786 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape625), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape626: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv786, R.shape([1, seq_len, 32, 128]))
            reshape627: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape626, R.shape([1, seq_len, 4096]))
            lv787 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight6, model_layers_28_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv370 = R.call_tir(cls.NT_matmul6, (reshape627, lv787), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv368_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv370, lv367_1, model_layers_28_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv369_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv368_1[1]
            rms_norm317: R.Tensor((1, seq_len, 4096), dtype="float16") = lv368_1[0]
            lv788 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight6, model_layers_28_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv371 = R.call_tir(cls.NT_matmul7, (rms_norm317, lv788), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split156: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv371, indices_or_sections=2, axis=-1)
            split_0156: R.Tensor((1, seq_len, 14336), dtype="float16") = split156[0]
            split_1156: R.Tensor((1, seq_len, 14336), dtype="float16") = split156[1]
            silu156: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0156)
            mul156: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu156, split_1156)
            lv789 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight6, model_layers_28_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv372 = R.call_tir(cls.NT_matmul8, (mul156, lv789), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv370_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv372, lv369_1, model_layers_29_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv371_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv370_1[1]
            rms_norm318: R.Tensor((1, seq_len, 4096), dtype="float16") = lv370_1[0]
            lv790 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight6, model_layers_29_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv373 = R.call_tir(cls.NT_matmul5, (rms_norm318, lv790), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape628: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv373, R.shape([1, seq_len, 48, 128]))
            reshape629: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape628, R.shape([seq_len, 48, 128]))
            lv791 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape629), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape630: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv791, R.shape([1, seq_len, 32, 128]))
            reshape631: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape630, R.shape([1, seq_len, 4096]))
            lv792 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight6, model_layers_29_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv374 = R.call_tir(cls.NT_matmul6, (reshape631, lv792), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv372_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv374, lv371_1, model_layers_29_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv373_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv372_1[1]
            rms_norm319: R.Tensor((1, seq_len, 4096), dtype="float16") = lv372_1[0]
            lv793 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight6, model_layers_29_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv375 = R.call_tir(cls.NT_matmul7, (rms_norm319, lv793), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split157: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv375, indices_or_sections=2, axis=-1)
            split_0157: R.Tensor((1, seq_len, 14336), dtype="float16") = split157[0]
            split_1157: R.Tensor((1, seq_len, 14336), dtype="float16") = split157[1]
            silu157: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0157)
            mul157: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu157, split_1157)
            lv794 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight6, model_layers_29_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv376 = R.call_tir(cls.NT_matmul8, (mul157, lv794), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv374_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv376, lv373_1, model_layers_30_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv375_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv374_1[1]
            rms_norm320: R.Tensor((1, seq_len, 4096), dtype="float16") = lv374_1[0]
            lv795 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight6, model_layers_30_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv377 = R.call_tir(cls.NT_matmul5, (rms_norm320, lv795), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape632: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv377, R.shape([1, seq_len, 48, 128]))
            reshape633: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape632, R.shape([seq_len, 48, 128]))
            lv796 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape633), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape634: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv796, R.shape([1, seq_len, 32, 128]))
            reshape635: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape634, R.shape([1, seq_len, 4096]))
            lv797 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight6, model_layers_30_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv378 = R.call_tir(cls.NT_matmul6, (reshape635, lv797), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv376_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv378, lv375_1, model_layers_30_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv377_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv376_1[1]
            rms_norm321: R.Tensor((1, seq_len, 4096), dtype="float16") = lv376_1[0]
            lv798 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight6, model_layers_30_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv379 = R.call_tir(cls.NT_matmul7, (rms_norm321, lv798), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split158: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv379, indices_or_sections=2, axis=-1)
            split_0158: R.Tensor((1, seq_len, 14336), dtype="float16") = split158[0]
            split_1158: R.Tensor((1, seq_len, 14336), dtype="float16") = split158[1]
            silu158: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0158)
            mul158: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu158, split_1158)
            lv799 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight6, model_layers_30_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv380 = R.call_tir(cls.NT_matmul8, (mul158, lv799), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv378_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv380, lv377_1, model_layers_31_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv379_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv378_1[1]
            rms_norm322: R.Tensor((1, seq_len, 4096), dtype="float16") = lv378_1[0]
            lv800 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight6, model_layers_31_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv381 = R.call_tir(cls.NT_matmul5, (rms_norm322, lv800), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape636: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv381, R.shape([1, seq_len, 48, 128]))
            reshape637: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape636, R.shape([seq_len, 48, 128]))
            lv801 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape637), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape638: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv801, R.shape([1, seq_len, 32, 128]))
            reshape639: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape638, R.shape([1, seq_len, 4096]))
            lv802 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight6, model_layers_31_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv382 = R.call_tir(cls.NT_matmul6, (reshape639, lv802), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv380_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv382, lv379_1, model_layers_31_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv381_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv380_1[1]
            rms_norm323: R.Tensor((1, seq_len, 4096), dtype="float16") = lv380_1[0]
            lv803 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight6, model_layers_31_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv383 = R.call_tir(cls.NT_matmul7, (rms_norm323, lv803), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split159: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv383, indices_or_sections=2, axis=-1)
            split_0159: R.Tensor((1, seq_len, 14336), dtype="float16") = split159[0]
            split_1159: R.Tensor((1, seq_len, 14336), dtype="float16") = split159[1]
            silu159: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0159)
            mul159: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu159, split_1159)
            lv804 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight6, model_layers_31_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv384 = R.call_tir(cls.NT_matmul8, (mul159, lv804), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv382_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv384, lv381_1, model_norm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm324: R.Tensor((1, seq_len, 4096), dtype="float16") = lv382_1[0]
            take2: R.Tensor((1, batch_size, 4096), dtype="float16") = R.take(rms_norm324, logit_positions, axis=1)
            lv805 = R.call_tir(cls.dequantize, (lm_head_q_weight6, lm_head_q_scale6), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            lv385 = R.call_tir(cls.NT_matmul9, (take2, lv805), out_sinfo=R.Tensor((1, batch_size, vocab_size), dtype="float16"))
            astype3: R.Tensor((1, batch_size, vocab_size), dtype="float32") = R.astype(lv385, dtype="float32")
            gv7: R.Tuple(R.Tensor((1, batch_size, vocab_size), dtype="float32"), R.Object) = astype3, paged_kv_cache
            R.output(gv7)
        return gv7

    @R.function
    def batch_prefill_to_last_hidden_states(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm455: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1128 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight9, model_layers_0_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv386 = R.call_tir(cls.NT_matmul5, (rms_norm455, lv1128), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape896: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv386, R.shape([1, seq_len, 48, 128]))
            reshape897: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape896, R.shape([seq_len, 48, 128]))
            lv1129 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape897), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape898: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1129, R.shape([1, seq_len, 32, 128]))
            reshape899: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape898, R.shape([1, seq_len, 4096]))
            lv1130 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight9, model_layers_0_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv387 = R.call_tir(cls.NT_matmul6, (reshape899, lv1130), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv384 = R.call_tir(cls.fuse_add_norm_prefill, (lv387, input_embeds, model_layers_0_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv385: R.Tensor((1, seq_len, 4096), dtype="float16") = lv384[1]
            rms_norm456: R.Tensor((1, seq_len, 4096), dtype="float16") = lv384[0]
            lv1131 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight9, model_layers_0_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv388 = R.call_tir(cls.NT_matmul7, (rms_norm456, lv1131), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split224: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv388, indices_or_sections=2, axis=-1)
            split_0224: R.Tensor((1, seq_len, 14336), dtype="float16") = split224[0]
            split_1224: R.Tensor((1, seq_len, 14336), dtype="float16") = split224[1]
            silu224: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0224)
            mul224: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu224, split_1224)
            lv1132 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight9, model_layers_0_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv389 = R.call_tir(cls.NT_matmul8, (mul224, lv1132), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv386_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv389, lv385, model_layers_1_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv387_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv386_1[1]
            rms_norm457: R.Tensor((1, seq_len, 4096), dtype="float16") = lv386_1[0]
            lv1133 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight9, model_layers_1_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv390 = R.call_tir(cls.NT_matmul5, (rms_norm457, lv1133), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape900: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv390, R.shape([1, seq_len, 48, 128]))
            reshape901: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape900, R.shape([seq_len, 48, 128]))
            lv1134 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape901), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape902: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1134, R.shape([1, seq_len, 32, 128]))
            reshape903: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape902, R.shape([1, seq_len, 4096]))
            lv1135 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight9, model_layers_1_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv391 = R.call_tir(cls.NT_matmul6, (reshape903, lv1135), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv388_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv391, lv387_1, model_layers_1_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv389_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv388_1[1]
            rms_norm458: R.Tensor((1, seq_len, 4096), dtype="float16") = lv388_1[0]
            lv1136 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight9, model_layers_1_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv392 = R.call_tir(cls.NT_matmul7, (rms_norm458, lv1136), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split225: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv392, indices_or_sections=2, axis=-1)
            split_0225: R.Tensor((1, seq_len, 14336), dtype="float16") = split225[0]
            split_1225: R.Tensor((1, seq_len, 14336), dtype="float16") = split225[1]
            silu225: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0225)
            mul225: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu225, split_1225)
            lv1137 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight9, model_layers_1_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv393 = R.call_tir(cls.NT_matmul8, (mul225, lv1137), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv390_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv393, lv389_1, model_layers_2_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv391_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv390_1[1]
            rms_norm459: R.Tensor((1, seq_len, 4096), dtype="float16") = lv390_1[0]
            lv1138 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight9, model_layers_2_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv394 = R.call_tir(cls.NT_matmul5, (rms_norm459, lv1138), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape904: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv394, R.shape([1, seq_len, 48, 128]))
            reshape905: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape904, R.shape([seq_len, 48, 128]))
            lv1139 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape905), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape906: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1139, R.shape([1, seq_len, 32, 128]))
            reshape907: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape906, R.shape([1, seq_len, 4096]))
            lv1140 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight9, model_layers_2_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv395 = R.call_tir(cls.NT_matmul6, (reshape907, lv1140), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv392_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv395, lv391_1, model_layers_2_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv393_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv392_1[1]
            rms_norm460: R.Tensor((1, seq_len, 4096), dtype="float16") = lv392_1[0]
            lv1141 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight9, model_layers_2_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv396 = R.call_tir(cls.NT_matmul7, (rms_norm460, lv1141), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split226: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv396, indices_or_sections=2, axis=-1)
            split_0226: R.Tensor((1, seq_len, 14336), dtype="float16") = split226[0]
            split_1226: R.Tensor((1, seq_len, 14336), dtype="float16") = split226[1]
            silu226: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0226)
            mul226: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu226, split_1226)
            lv1142 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight9, model_layers_2_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv397 = R.call_tir(cls.NT_matmul8, (mul226, lv1142), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv394_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv397, lv393_1, model_layers_3_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv395_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv394_1[1]
            rms_norm461: R.Tensor((1, seq_len, 4096), dtype="float16") = lv394_1[0]
            lv1143 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight9, model_layers_3_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv398 = R.call_tir(cls.NT_matmul5, (rms_norm461, lv1143), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape908: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv398, R.shape([1, seq_len, 48, 128]))
            reshape909: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape908, R.shape([seq_len, 48, 128]))
            lv1144 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape909), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape910: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1144, R.shape([1, seq_len, 32, 128]))
            reshape911: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape910, R.shape([1, seq_len, 4096]))
            lv1145 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight9, model_layers_3_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv399 = R.call_tir(cls.NT_matmul6, (reshape911, lv1145), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv396_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv399, lv395_1, model_layers_3_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv397_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv396_1[1]
            rms_norm462: R.Tensor((1, seq_len, 4096), dtype="float16") = lv396_1[0]
            lv1146 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight9, model_layers_3_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv400 = R.call_tir(cls.NT_matmul7, (rms_norm462, lv1146), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split227: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv400, indices_or_sections=2, axis=-1)
            split_0227: R.Tensor((1, seq_len, 14336), dtype="float16") = split227[0]
            split_1227: R.Tensor((1, seq_len, 14336), dtype="float16") = split227[1]
            silu227: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0227)
            mul227: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu227, split_1227)
            lv1147 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight9, model_layers_3_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv401 = R.call_tir(cls.NT_matmul8, (mul227, lv1147), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv398_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv401, lv397_1, model_layers_4_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv399_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv398_1[1]
            rms_norm463: R.Tensor((1, seq_len, 4096), dtype="float16") = lv398_1[0]
            lv1148 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight9, model_layers_4_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv402 = R.call_tir(cls.NT_matmul5, (rms_norm463, lv1148), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape912: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv402, R.shape([1, seq_len, 48, 128]))
            reshape913: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape912, R.shape([seq_len, 48, 128]))
            lv1149 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape913), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape914: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1149, R.shape([1, seq_len, 32, 128]))
            reshape915: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape914, R.shape([1, seq_len, 4096]))
            lv1150 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight9, model_layers_4_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv403 = R.call_tir(cls.NT_matmul6, (reshape915, lv1150), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv400_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv403, lv399_1, model_layers_4_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv401_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv400_1[1]
            rms_norm464: R.Tensor((1, seq_len, 4096), dtype="float16") = lv400_1[0]
            lv1151 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight9, model_layers_4_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv404 = R.call_tir(cls.NT_matmul7, (rms_norm464, lv1151), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split228: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv404, indices_or_sections=2, axis=-1)
            split_0228: R.Tensor((1, seq_len, 14336), dtype="float16") = split228[0]
            split_1228: R.Tensor((1, seq_len, 14336), dtype="float16") = split228[1]
            silu228: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0228)
            mul228: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu228, split_1228)
            lv1152 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight9, model_layers_4_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv405 = R.call_tir(cls.NT_matmul8, (mul228, lv1152), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv402_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv405, lv401_1, model_layers_5_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv403_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv402_1[1]
            rms_norm465: R.Tensor((1, seq_len, 4096), dtype="float16") = lv402_1[0]
            lv1153 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight9, model_layers_5_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv406 = R.call_tir(cls.NT_matmul5, (rms_norm465, lv1153), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape916: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv406, R.shape([1, seq_len, 48, 128]))
            reshape917: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape916, R.shape([seq_len, 48, 128]))
            lv1154 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape917), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape918: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1154, R.shape([1, seq_len, 32, 128]))
            reshape919: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape918, R.shape([1, seq_len, 4096]))
            lv1155 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight9, model_layers_5_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv407 = R.call_tir(cls.NT_matmul6, (reshape919, lv1155), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv404_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv407, lv403_1, model_layers_5_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv405_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv404_1[1]
            rms_norm466: R.Tensor((1, seq_len, 4096), dtype="float16") = lv404_1[0]
            lv1156 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight9, model_layers_5_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv408 = R.call_tir(cls.NT_matmul7, (rms_norm466, lv1156), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split229: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv408, indices_or_sections=2, axis=-1)
            split_0229: R.Tensor((1, seq_len, 14336), dtype="float16") = split229[0]
            split_1229: R.Tensor((1, seq_len, 14336), dtype="float16") = split229[1]
            silu229: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0229)
            mul229: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu229, split_1229)
            lv1157 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight9, model_layers_5_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv409 = R.call_tir(cls.NT_matmul8, (mul229, lv1157), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv406_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv409, lv405_1, model_layers_6_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv407_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv406_1[1]
            rms_norm467: R.Tensor((1, seq_len, 4096), dtype="float16") = lv406_1[0]
            lv1158 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight9, model_layers_6_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv410 = R.call_tir(cls.NT_matmul5, (rms_norm467, lv1158), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape920: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv410, R.shape([1, seq_len, 48, 128]))
            reshape921: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape920, R.shape([seq_len, 48, 128]))
            lv1159 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape921), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape922: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1159, R.shape([1, seq_len, 32, 128]))
            reshape923: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape922, R.shape([1, seq_len, 4096]))
            lv1160 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight9, model_layers_6_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv411 = R.call_tir(cls.NT_matmul6, (reshape923, lv1160), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv408_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv411, lv407_1, model_layers_6_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv409_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv408_1[1]
            rms_norm468: R.Tensor((1, seq_len, 4096), dtype="float16") = lv408_1[0]
            lv1161 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight9, model_layers_6_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv412 = R.call_tir(cls.NT_matmul7, (rms_norm468, lv1161), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split230: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv412, indices_or_sections=2, axis=-1)
            split_0230: R.Tensor((1, seq_len, 14336), dtype="float16") = split230[0]
            split_1230: R.Tensor((1, seq_len, 14336), dtype="float16") = split230[1]
            silu230: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0230)
            mul230: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu230, split_1230)
            lv1162 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight9, model_layers_6_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv413 = R.call_tir(cls.NT_matmul8, (mul230, lv1162), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv410_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv413, lv409_1, model_layers_7_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv411_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv410_1[1]
            rms_norm469: R.Tensor((1, seq_len, 4096), dtype="float16") = lv410_1[0]
            lv1163 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight9, model_layers_7_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv414 = R.call_tir(cls.NT_matmul5, (rms_norm469, lv1163), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape924: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv414, R.shape([1, seq_len, 48, 128]))
            reshape925: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape924, R.shape([seq_len, 48, 128]))
            lv1164 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape925), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape926: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1164, R.shape([1, seq_len, 32, 128]))
            reshape927: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape926, R.shape([1, seq_len, 4096]))
            lv1165 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight9, model_layers_7_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv415 = R.call_tir(cls.NT_matmul6, (reshape927, lv1165), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv412_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv415, lv411_1, model_layers_7_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv413_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv412_1[1]
            rms_norm470: R.Tensor((1, seq_len, 4096), dtype="float16") = lv412_1[0]
            lv1166 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight9, model_layers_7_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv416 = R.call_tir(cls.NT_matmul7, (rms_norm470, lv1166), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split231: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv416, indices_or_sections=2, axis=-1)
            split_0231: R.Tensor((1, seq_len, 14336), dtype="float16") = split231[0]
            split_1231: R.Tensor((1, seq_len, 14336), dtype="float16") = split231[1]
            silu231: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0231)
            mul231: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu231, split_1231)
            lv1167 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight9, model_layers_7_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv417 = R.call_tir(cls.NT_matmul8, (mul231, lv1167), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv414_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv417, lv413_1, model_layers_8_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv415_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv414_1[1]
            rms_norm471: R.Tensor((1, seq_len, 4096), dtype="float16") = lv414_1[0]
            lv1168 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight9, model_layers_8_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv418 = R.call_tir(cls.NT_matmul5, (rms_norm471, lv1168), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape928: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv418, R.shape([1, seq_len, 48, 128]))
            reshape929: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape928, R.shape([seq_len, 48, 128]))
            lv1169 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape929), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape930: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1169, R.shape([1, seq_len, 32, 128]))
            reshape931: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape930, R.shape([1, seq_len, 4096]))
            lv1170 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight9, model_layers_8_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv419 = R.call_tir(cls.NT_matmul6, (reshape931, lv1170), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv416_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv419, lv415_1, model_layers_8_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv417_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv416_1[1]
            rms_norm472: R.Tensor((1, seq_len, 4096), dtype="float16") = lv416_1[0]
            lv1171 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight9, model_layers_8_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv420 = R.call_tir(cls.NT_matmul7, (rms_norm472, lv1171), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split232: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv420, indices_or_sections=2, axis=-1)
            split_0232: R.Tensor((1, seq_len, 14336), dtype="float16") = split232[0]
            split_1232: R.Tensor((1, seq_len, 14336), dtype="float16") = split232[1]
            silu232: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0232)
            mul232: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu232, split_1232)
            lv1172 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight9, model_layers_8_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv421 = R.call_tir(cls.NT_matmul8, (mul232, lv1172), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv418_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv421, lv417_1, model_layers_9_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv419_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv418_1[1]
            rms_norm473: R.Tensor((1, seq_len, 4096), dtype="float16") = lv418_1[0]
            lv1173 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight9, model_layers_9_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv422 = R.call_tir(cls.NT_matmul5, (rms_norm473, lv1173), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape932: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv422, R.shape([1, seq_len, 48, 128]))
            reshape933: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape932, R.shape([seq_len, 48, 128]))
            lv1174 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape933), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape934: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1174, R.shape([1, seq_len, 32, 128]))
            reshape935: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape934, R.shape([1, seq_len, 4096]))
            lv1175 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight9, model_layers_9_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv423 = R.call_tir(cls.NT_matmul6, (reshape935, lv1175), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv420_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv423, lv419_1, model_layers_9_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv421_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv420_1[1]
            rms_norm474: R.Tensor((1, seq_len, 4096), dtype="float16") = lv420_1[0]
            lv1176 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight9, model_layers_9_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv424 = R.call_tir(cls.NT_matmul7, (rms_norm474, lv1176), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split233: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv424, indices_or_sections=2, axis=-1)
            split_0233: R.Tensor((1, seq_len, 14336), dtype="float16") = split233[0]
            split_1233: R.Tensor((1, seq_len, 14336), dtype="float16") = split233[1]
            silu233: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0233)
            mul233: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu233, split_1233)
            lv1177 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight9, model_layers_9_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv425 = R.call_tir(cls.NT_matmul8, (mul233, lv1177), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv422_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv425, lv421_1, model_layers_10_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv423_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv422_1[1]
            rms_norm475: R.Tensor((1, seq_len, 4096), dtype="float16") = lv422_1[0]
            lv1178 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight9, model_layers_10_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv426 = R.call_tir(cls.NT_matmul5, (rms_norm475, lv1178), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape936: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv426, R.shape([1, seq_len, 48, 128]))
            reshape937: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape936, R.shape([seq_len, 48, 128]))
            lv1179 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape937), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape938: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1179, R.shape([1, seq_len, 32, 128]))
            reshape939: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape938, R.shape([1, seq_len, 4096]))
            lv1180 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight9, model_layers_10_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv427 = R.call_tir(cls.NT_matmul6, (reshape939, lv1180), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv424_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv427, lv423_1, model_layers_10_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv425_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv424_1[1]
            rms_norm476: R.Tensor((1, seq_len, 4096), dtype="float16") = lv424_1[0]
            lv1181 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight9, model_layers_10_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv428 = R.call_tir(cls.NT_matmul7, (rms_norm476, lv1181), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split234: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv428, indices_or_sections=2, axis=-1)
            split_0234: R.Tensor((1, seq_len, 14336), dtype="float16") = split234[0]
            split_1234: R.Tensor((1, seq_len, 14336), dtype="float16") = split234[1]
            silu234: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0234)
            mul234: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu234, split_1234)
            lv1182 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight9, model_layers_10_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv429 = R.call_tir(cls.NT_matmul8, (mul234, lv1182), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv426_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv429, lv425_1, model_layers_11_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv427_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv426_1[1]
            rms_norm477: R.Tensor((1, seq_len, 4096), dtype="float16") = lv426_1[0]
            lv1183 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight9, model_layers_11_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv430 = R.call_tir(cls.NT_matmul5, (rms_norm477, lv1183), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape940: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv430, R.shape([1, seq_len, 48, 128]))
            reshape941: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape940, R.shape([seq_len, 48, 128]))
            lv1184 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape941), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape942: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1184, R.shape([1, seq_len, 32, 128]))
            reshape943: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape942, R.shape([1, seq_len, 4096]))
            lv1185 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight9, model_layers_11_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv431 = R.call_tir(cls.NT_matmul6, (reshape943, lv1185), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv428_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv431, lv427_1, model_layers_11_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv429_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv428_1[1]
            rms_norm478: R.Tensor((1, seq_len, 4096), dtype="float16") = lv428_1[0]
            lv1186 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight9, model_layers_11_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv432 = R.call_tir(cls.NT_matmul7, (rms_norm478, lv1186), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split235: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv432, indices_or_sections=2, axis=-1)
            split_0235: R.Tensor((1, seq_len, 14336), dtype="float16") = split235[0]
            split_1235: R.Tensor((1, seq_len, 14336), dtype="float16") = split235[1]
            silu235: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0235)
            mul235: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu235, split_1235)
            lv1187 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight9, model_layers_11_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv433 = R.call_tir(cls.NT_matmul8, (mul235, lv1187), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv430_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv433, lv429_1, model_layers_12_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv431_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv430_1[1]
            rms_norm479: R.Tensor((1, seq_len, 4096), dtype="float16") = lv430_1[0]
            lv1188 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight9, model_layers_12_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv434 = R.call_tir(cls.NT_matmul5, (rms_norm479, lv1188), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape944: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv434, R.shape([1, seq_len, 48, 128]))
            reshape945: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape944, R.shape([seq_len, 48, 128]))
            lv1189 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape945), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape946: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1189, R.shape([1, seq_len, 32, 128]))
            reshape947: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape946, R.shape([1, seq_len, 4096]))
            lv1190 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight9, model_layers_12_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv435 = R.call_tir(cls.NT_matmul6, (reshape947, lv1190), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv432_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv435, lv431_1, model_layers_12_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv433_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv432_1[1]
            rms_norm480: R.Tensor((1, seq_len, 4096), dtype="float16") = lv432_1[0]
            lv1191 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight9, model_layers_12_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv436 = R.call_tir(cls.NT_matmul7, (rms_norm480, lv1191), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split236: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv436, indices_or_sections=2, axis=-1)
            split_0236: R.Tensor((1, seq_len, 14336), dtype="float16") = split236[0]
            split_1236: R.Tensor((1, seq_len, 14336), dtype="float16") = split236[1]
            silu236: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0236)
            mul236: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu236, split_1236)
            lv1192 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight9, model_layers_12_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv437 = R.call_tir(cls.NT_matmul8, (mul236, lv1192), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv434_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv437, lv433_1, model_layers_13_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv435_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv434_1[1]
            rms_norm481: R.Tensor((1, seq_len, 4096), dtype="float16") = lv434_1[0]
            lv1193 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight9, model_layers_13_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv438 = R.call_tir(cls.NT_matmul5, (rms_norm481, lv1193), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape948: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv438, R.shape([1, seq_len, 48, 128]))
            reshape949: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape948, R.shape([seq_len, 48, 128]))
            lv1194 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape949), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape950: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1194, R.shape([1, seq_len, 32, 128]))
            reshape951: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape950, R.shape([1, seq_len, 4096]))
            lv1195 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight9, model_layers_13_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv439 = R.call_tir(cls.NT_matmul6, (reshape951, lv1195), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv436_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv439, lv435_1, model_layers_13_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv437_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv436_1[1]
            rms_norm482: R.Tensor((1, seq_len, 4096), dtype="float16") = lv436_1[0]
            lv1196 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight9, model_layers_13_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv440 = R.call_tir(cls.NT_matmul7, (rms_norm482, lv1196), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split237: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv440, indices_or_sections=2, axis=-1)
            split_0237: R.Tensor((1, seq_len, 14336), dtype="float16") = split237[0]
            split_1237: R.Tensor((1, seq_len, 14336), dtype="float16") = split237[1]
            silu237: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0237)
            mul237: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu237, split_1237)
            lv1197 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight9, model_layers_13_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv441 = R.call_tir(cls.NT_matmul8, (mul237, lv1197), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv438_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv441, lv437_1, model_layers_14_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv439_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv438_1[1]
            rms_norm483: R.Tensor((1, seq_len, 4096), dtype="float16") = lv438_1[0]
            lv1198 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight9, model_layers_14_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv442 = R.call_tir(cls.NT_matmul5, (rms_norm483, lv1198), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape952: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv442, R.shape([1, seq_len, 48, 128]))
            reshape953: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape952, R.shape([seq_len, 48, 128]))
            lv1199 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape953), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape954: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1199, R.shape([1, seq_len, 32, 128]))
            reshape955: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape954, R.shape([1, seq_len, 4096]))
            lv1200 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight9, model_layers_14_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv443 = R.call_tir(cls.NT_matmul6, (reshape955, lv1200), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv440_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv443, lv439_1, model_layers_14_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv441_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv440_1[1]
            rms_norm484: R.Tensor((1, seq_len, 4096), dtype="float16") = lv440_1[0]
            lv1201 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight9, model_layers_14_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv444 = R.call_tir(cls.NT_matmul7, (rms_norm484, lv1201), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split238: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv444, indices_or_sections=2, axis=-1)
            split_0238: R.Tensor((1, seq_len, 14336), dtype="float16") = split238[0]
            split_1238: R.Tensor((1, seq_len, 14336), dtype="float16") = split238[1]
            silu238: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0238)
            mul238: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu238, split_1238)
            lv1202 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight9, model_layers_14_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv445 = R.call_tir(cls.NT_matmul8, (mul238, lv1202), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv442_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv445, lv441_1, model_layers_15_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv443_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv442_1[1]
            rms_norm485: R.Tensor((1, seq_len, 4096), dtype="float16") = lv442_1[0]
            lv1203 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight9, model_layers_15_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv446 = R.call_tir(cls.NT_matmul5, (rms_norm485, lv1203), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape956: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv446, R.shape([1, seq_len, 48, 128]))
            reshape957: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape956, R.shape([seq_len, 48, 128]))
            lv1204 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape957), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape958: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1204, R.shape([1, seq_len, 32, 128]))
            reshape959: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape958, R.shape([1, seq_len, 4096]))
            lv1205 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight9, model_layers_15_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv447 = R.call_tir(cls.NT_matmul6, (reshape959, lv1205), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv444_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv447, lv443_1, model_layers_15_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv445_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv444_1[1]
            rms_norm486: R.Tensor((1, seq_len, 4096), dtype="float16") = lv444_1[0]
            lv1206 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight9, model_layers_15_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv448 = R.call_tir(cls.NT_matmul7, (rms_norm486, lv1206), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split239: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv448, indices_or_sections=2, axis=-1)
            split_0239: R.Tensor((1, seq_len, 14336), dtype="float16") = split239[0]
            split_1239: R.Tensor((1, seq_len, 14336), dtype="float16") = split239[1]
            silu239: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0239)
            mul239: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu239, split_1239)
            lv1207 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight9, model_layers_15_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv449 = R.call_tir(cls.NT_matmul8, (mul239, lv1207), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv446_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv449, lv445_1, model_layers_16_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv447_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv446_1[1]
            rms_norm487: R.Tensor((1, seq_len, 4096), dtype="float16") = lv446_1[0]
            lv1208 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight9, model_layers_16_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv450 = R.call_tir(cls.NT_matmul5, (rms_norm487, lv1208), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape960: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv450, R.shape([1, seq_len, 48, 128]))
            reshape961: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape960, R.shape([seq_len, 48, 128]))
            lv1209 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape961), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape962: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1209, R.shape([1, seq_len, 32, 128]))
            reshape963: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape962, R.shape([1, seq_len, 4096]))
            lv1210 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight9, model_layers_16_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv451 = R.call_tir(cls.NT_matmul6, (reshape963, lv1210), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv448_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv451, lv447_1, model_layers_16_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv449_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv448_1[1]
            rms_norm488: R.Tensor((1, seq_len, 4096), dtype="float16") = lv448_1[0]
            lv1211 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight9, model_layers_16_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv452 = R.call_tir(cls.NT_matmul7, (rms_norm488, lv1211), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split240: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv452, indices_or_sections=2, axis=-1)
            split_0240: R.Tensor((1, seq_len, 14336), dtype="float16") = split240[0]
            split_1240: R.Tensor((1, seq_len, 14336), dtype="float16") = split240[1]
            silu240: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0240)
            mul240: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu240, split_1240)
            lv1212 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight9, model_layers_16_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv453 = R.call_tir(cls.NT_matmul8, (mul240, lv1212), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv450_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv453, lv449_1, model_layers_17_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv451_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv450_1[1]
            rms_norm489: R.Tensor((1, seq_len, 4096), dtype="float16") = lv450_1[0]
            lv1213 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight9, model_layers_17_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv454 = R.call_tir(cls.NT_matmul5, (rms_norm489, lv1213), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape964: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv454, R.shape([1, seq_len, 48, 128]))
            reshape965: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape964, R.shape([seq_len, 48, 128]))
            lv1214 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape965), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape966: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1214, R.shape([1, seq_len, 32, 128]))
            reshape967: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape966, R.shape([1, seq_len, 4096]))
            lv1215 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight9, model_layers_17_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv455 = R.call_tir(cls.NT_matmul6, (reshape967, lv1215), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv452_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv455, lv451_1, model_layers_17_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv453_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv452_1[1]
            rms_norm490: R.Tensor((1, seq_len, 4096), dtype="float16") = lv452_1[0]
            lv1216 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight9, model_layers_17_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv456 = R.call_tir(cls.NT_matmul7, (rms_norm490, lv1216), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split241: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv456, indices_or_sections=2, axis=-1)
            split_0241: R.Tensor((1, seq_len, 14336), dtype="float16") = split241[0]
            split_1241: R.Tensor((1, seq_len, 14336), dtype="float16") = split241[1]
            silu241: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0241)
            mul241: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu241, split_1241)
            lv1217 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight9, model_layers_17_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv457 = R.call_tir(cls.NT_matmul8, (mul241, lv1217), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv454_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv457, lv453_1, model_layers_18_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv455_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv454_1[1]
            rms_norm491: R.Tensor((1, seq_len, 4096), dtype="float16") = lv454_1[0]
            lv1218 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight9, model_layers_18_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv458 = R.call_tir(cls.NT_matmul5, (rms_norm491, lv1218), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape968: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv458, R.shape([1, seq_len, 48, 128]))
            reshape969: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape968, R.shape([seq_len, 48, 128]))
            lv1219 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape969), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape970: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1219, R.shape([1, seq_len, 32, 128]))
            reshape971: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape970, R.shape([1, seq_len, 4096]))
            lv1220 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight9, model_layers_18_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv459 = R.call_tir(cls.NT_matmul6, (reshape971, lv1220), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv456_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv459, lv455_1, model_layers_18_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv457_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv456_1[1]
            rms_norm492: R.Tensor((1, seq_len, 4096), dtype="float16") = lv456_1[0]
            lv1221 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight9, model_layers_18_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv460 = R.call_tir(cls.NT_matmul7, (rms_norm492, lv1221), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split242: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv460, indices_or_sections=2, axis=-1)
            split_0242: R.Tensor((1, seq_len, 14336), dtype="float16") = split242[0]
            split_1242: R.Tensor((1, seq_len, 14336), dtype="float16") = split242[1]
            silu242: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0242)
            mul242: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu242, split_1242)
            lv1222 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight9, model_layers_18_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv461 = R.call_tir(cls.NT_matmul8, (mul242, lv1222), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv458_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv461, lv457_1, model_layers_19_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv459_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv458_1[1]
            rms_norm493: R.Tensor((1, seq_len, 4096), dtype="float16") = lv458_1[0]
            lv1223 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight9, model_layers_19_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv462 = R.call_tir(cls.NT_matmul5, (rms_norm493, lv1223), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape972: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv462, R.shape([1, seq_len, 48, 128]))
            reshape973: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape972, R.shape([seq_len, 48, 128]))
            lv1224 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape973), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape974: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1224, R.shape([1, seq_len, 32, 128]))
            reshape975: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape974, R.shape([1, seq_len, 4096]))
            lv1225 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight9, model_layers_19_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv463 = R.call_tir(cls.NT_matmul6, (reshape975, lv1225), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv460_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv463, lv459_1, model_layers_19_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv461_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv460_1[1]
            rms_norm494: R.Tensor((1, seq_len, 4096), dtype="float16") = lv460_1[0]
            lv1226 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight9, model_layers_19_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv464 = R.call_tir(cls.NT_matmul7, (rms_norm494, lv1226), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split243: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv464, indices_or_sections=2, axis=-1)
            split_0243: R.Tensor((1, seq_len, 14336), dtype="float16") = split243[0]
            split_1243: R.Tensor((1, seq_len, 14336), dtype="float16") = split243[1]
            silu243: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0243)
            mul243: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu243, split_1243)
            lv1227 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight9, model_layers_19_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv465 = R.call_tir(cls.NT_matmul8, (mul243, lv1227), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv462_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv465, lv461_1, model_layers_20_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv463_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv462_1[1]
            rms_norm495: R.Tensor((1, seq_len, 4096), dtype="float16") = lv462_1[0]
            lv1228 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight9, model_layers_20_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv466 = R.call_tir(cls.NT_matmul5, (rms_norm495, lv1228), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape976: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv466, R.shape([1, seq_len, 48, 128]))
            reshape977: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape976, R.shape([seq_len, 48, 128]))
            lv1229 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape977), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape978: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1229, R.shape([1, seq_len, 32, 128]))
            reshape979: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape978, R.shape([1, seq_len, 4096]))
            lv1230 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight9, model_layers_20_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv467 = R.call_tir(cls.NT_matmul6, (reshape979, lv1230), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv464_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv467, lv463_1, model_layers_20_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv465_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv464_1[1]
            rms_norm496: R.Tensor((1, seq_len, 4096), dtype="float16") = lv464_1[0]
            lv1231 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight9, model_layers_20_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv468 = R.call_tir(cls.NT_matmul7, (rms_norm496, lv1231), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split244: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv468, indices_or_sections=2, axis=-1)
            split_0244: R.Tensor((1, seq_len, 14336), dtype="float16") = split244[0]
            split_1244: R.Tensor((1, seq_len, 14336), dtype="float16") = split244[1]
            silu244: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0244)
            mul244: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu244, split_1244)
            lv1232 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight9, model_layers_20_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv469 = R.call_tir(cls.NT_matmul8, (mul244, lv1232), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv466_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv469, lv465_1, model_layers_21_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv467_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv466_1[1]
            rms_norm497: R.Tensor((1, seq_len, 4096), dtype="float16") = lv466_1[0]
            lv1233 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight9, model_layers_21_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv470 = R.call_tir(cls.NT_matmul5, (rms_norm497, lv1233), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape980: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv470, R.shape([1, seq_len, 48, 128]))
            reshape981: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape980, R.shape([seq_len, 48, 128]))
            lv1234 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape981), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape982: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1234, R.shape([1, seq_len, 32, 128]))
            reshape983: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape982, R.shape([1, seq_len, 4096]))
            lv1235 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight9, model_layers_21_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv471 = R.call_tir(cls.NT_matmul6, (reshape983, lv1235), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv468_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv471, lv467_1, model_layers_21_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv469_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv468_1[1]
            rms_norm498: R.Tensor((1, seq_len, 4096), dtype="float16") = lv468_1[0]
            lv1236 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight9, model_layers_21_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv472 = R.call_tir(cls.NT_matmul7, (rms_norm498, lv1236), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split245: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv472, indices_or_sections=2, axis=-1)
            split_0245: R.Tensor((1, seq_len, 14336), dtype="float16") = split245[0]
            split_1245: R.Tensor((1, seq_len, 14336), dtype="float16") = split245[1]
            silu245: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0245)
            mul245: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu245, split_1245)
            lv1237 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight9, model_layers_21_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv473 = R.call_tir(cls.NT_matmul8, (mul245, lv1237), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv470_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv473, lv469_1, model_layers_22_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv471_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv470_1[1]
            rms_norm499: R.Tensor((1, seq_len, 4096), dtype="float16") = lv470_1[0]
            lv1238 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight9, model_layers_22_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv474 = R.call_tir(cls.NT_matmul5, (rms_norm499, lv1238), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape984: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv474, R.shape([1, seq_len, 48, 128]))
            reshape985: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape984, R.shape([seq_len, 48, 128]))
            lv1239 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape985), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape986: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1239, R.shape([1, seq_len, 32, 128]))
            reshape987: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape986, R.shape([1, seq_len, 4096]))
            lv1240 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight9, model_layers_22_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv475 = R.call_tir(cls.NT_matmul6, (reshape987, lv1240), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv472_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv475, lv471_1, model_layers_22_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv473_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv472_1[1]
            rms_norm500: R.Tensor((1, seq_len, 4096), dtype="float16") = lv472_1[0]
            lv1241 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight9, model_layers_22_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv476 = R.call_tir(cls.NT_matmul7, (rms_norm500, lv1241), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split246: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv476, indices_or_sections=2, axis=-1)
            split_0246: R.Tensor((1, seq_len, 14336), dtype="float16") = split246[0]
            split_1246: R.Tensor((1, seq_len, 14336), dtype="float16") = split246[1]
            silu246: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0246)
            mul246: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu246, split_1246)
            lv1242 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight9, model_layers_22_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv477 = R.call_tir(cls.NT_matmul8, (mul246, lv1242), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv474_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv477, lv473_1, model_layers_23_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv475_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv474_1[1]
            rms_norm501: R.Tensor((1, seq_len, 4096), dtype="float16") = lv474_1[0]
            lv1243 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight9, model_layers_23_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv478 = R.call_tir(cls.NT_matmul5, (rms_norm501, lv1243), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape988: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv478, R.shape([1, seq_len, 48, 128]))
            reshape989: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape988, R.shape([seq_len, 48, 128]))
            lv1244 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape989), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape990: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1244, R.shape([1, seq_len, 32, 128]))
            reshape991: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape990, R.shape([1, seq_len, 4096]))
            lv1245 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight9, model_layers_23_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv479 = R.call_tir(cls.NT_matmul6, (reshape991, lv1245), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv476_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv479, lv475_1, model_layers_23_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv477_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv476_1[1]
            rms_norm502: R.Tensor((1, seq_len, 4096), dtype="float16") = lv476_1[0]
            lv1246 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight9, model_layers_23_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv480 = R.call_tir(cls.NT_matmul7, (rms_norm502, lv1246), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split247: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv480, indices_or_sections=2, axis=-1)
            split_0247: R.Tensor((1, seq_len, 14336), dtype="float16") = split247[0]
            split_1247: R.Tensor((1, seq_len, 14336), dtype="float16") = split247[1]
            silu247: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0247)
            mul247: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu247, split_1247)
            lv1247 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight9, model_layers_23_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv481 = R.call_tir(cls.NT_matmul8, (mul247, lv1247), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv478_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv481, lv477_1, model_layers_24_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv479_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv478_1[1]
            rms_norm503: R.Tensor((1, seq_len, 4096), dtype="float16") = lv478_1[0]
            lv1248 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight9, model_layers_24_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv482 = R.call_tir(cls.NT_matmul5, (rms_norm503, lv1248), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape992: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv482, R.shape([1, seq_len, 48, 128]))
            reshape993: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape992, R.shape([seq_len, 48, 128]))
            lv1249 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape993), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape994: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1249, R.shape([1, seq_len, 32, 128]))
            reshape995: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape994, R.shape([1, seq_len, 4096]))
            lv1250 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight9, model_layers_24_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv483 = R.call_tir(cls.NT_matmul6, (reshape995, lv1250), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv480_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv483, lv479_1, model_layers_24_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv481_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv480_1[1]
            rms_norm504: R.Tensor((1, seq_len, 4096), dtype="float16") = lv480_1[0]
            lv1251 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight9, model_layers_24_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv484 = R.call_tir(cls.NT_matmul7, (rms_norm504, lv1251), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split248: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv484, indices_or_sections=2, axis=-1)
            split_0248: R.Tensor((1, seq_len, 14336), dtype="float16") = split248[0]
            split_1248: R.Tensor((1, seq_len, 14336), dtype="float16") = split248[1]
            silu248: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0248)
            mul248: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu248, split_1248)
            lv1252 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight9, model_layers_24_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv485 = R.call_tir(cls.NT_matmul8, (mul248, lv1252), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv482_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv485, lv481_1, model_layers_25_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv483_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv482_1[1]
            rms_norm505: R.Tensor((1, seq_len, 4096), dtype="float16") = lv482_1[0]
            lv1253 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight9, model_layers_25_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv486 = R.call_tir(cls.NT_matmul5, (rms_norm505, lv1253), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape996: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv486, R.shape([1, seq_len, 48, 128]))
            reshape997: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape996, R.shape([seq_len, 48, 128]))
            lv1254 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape997), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape998: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1254, R.shape([1, seq_len, 32, 128]))
            reshape999: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape998, R.shape([1, seq_len, 4096]))
            lv1255 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight9, model_layers_25_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv487 = R.call_tir(cls.NT_matmul6, (reshape999, lv1255), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv484_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv487, lv483_1, model_layers_25_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv485_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv484_1[1]
            rms_norm506: R.Tensor((1, seq_len, 4096), dtype="float16") = lv484_1[0]
            lv1256 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight9, model_layers_25_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv488 = R.call_tir(cls.NT_matmul7, (rms_norm506, lv1256), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split249: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv488, indices_or_sections=2, axis=-1)
            split_0249: R.Tensor((1, seq_len, 14336), dtype="float16") = split249[0]
            split_1249: R.Tensor((1, seq_len, 14336), dtype="float16") = split249[1]
            silu249: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0249)
            mul249: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu249, split_1249)
            lv1257 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight9, model_layers_25_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv489 = R.call_tir(cls.NT_matmul8, (mul249, lv1257), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv486_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv489, lv485_1, model_layers_26_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv487_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv486_1[1]
            rms_norm507: R.Tensor((1, seq_len, 4096), dtype="float16") = lv486_1[0]
            lv1258 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight9, model_layers_26_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv490 = R.call_tir(cls.NT_matmul5, (rms_norm507, lv1258), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1000: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv490, R.shape([1, seq_len, 48, 128]))
            reshape1001: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1000, R.shape([seq_len, 48, 128]))
            lv1259 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1001), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1002: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1259, R.shape([1, seq_len, 32, 128]))
            reshape1003: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1002, R.shape([1, seq_len, 4096]))
            lv1260 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight9, model_layers_26_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv491 = R.call_tir(cls.NT_matmul6, (reshape1003, lv1260), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv488_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv491, lv487_1, model_layers_26_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv489_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv488_1[1]
            rms_norm508: R.Tensor((1, seq_len, 4096), dtype="float16") = lv488_1[0]
            lv1261 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight9, model_layers_26_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv492 = R.call_tir(cls.NT_matmul7, (rms_norm508, lv1261), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split250: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv492, indices_or_sections=2, axis=-1)
            split_0250: R.Tensor((1, seq_len, 14336), dtype="float16") = split250[0]
            split_1250: R.Tensor((1, seq_len, 14336), dtype="float16") = split250[1]
            silu250: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0250)
            mul250: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu250, split_1250)
            lv1262 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight9, model_layers_26_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv493 = R.call_tir(cls.NT_matmul8, (mul250, lv1262), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv490_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv493, lv489_1, model_layers_27_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv491_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv490_1[1]
            rms_norm509: R.Tensor((1, seq_len, 4096), dtype="float16") = lv490_1[0]
            lv1263 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight9, model_layers_27_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv494 = R.call_tir(cls.NT_matmul5, (rms_norm509, lv1263), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1004: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv494, R.shape([1, seq_len, 48, 128]))
            reshape1005: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1004, R.shape([seq_len, 48, 128]))
            lv1264 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1005), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1006: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1264, R.shape([1, seq_len, 32, 128]))
            reshape1007: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1006, R.shape([1, seq_len, 4096]))
            lv1265 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight9, model_layers_27_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv495 = R.call_tir(cls.NT_matmul6, (reshape1007, lv1265), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv492_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv495, lv491_1, model_layers_27_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv493_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv492_1[1]
            rms_norm510: R.Tensor((1, seq_len, 4096), dtype="float16") = lv492_1[0]
            lv1266 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight9, model_layers_27_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv496 = R.call_tir(cls.NT_matmul7, (rms_norm510, lv1266), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split251: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv496, indices_or_sections=2, axis=-1)
            split_0251: R.Tensor((1, seq_len, 14336), dtype="float16") = split251[0]
            split_1251: R.Tensor((1, seq_len, 14336), dtype="float16") = split251[1]
            silu251: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0251)
            mul251: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu251, split_1251)
            lv1267 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight9, model_layers_27_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv497 = R.call_tir(cls.NT_matmul8, (mul251, lv1267), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv494_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv497, lv493_1, model_layers_28_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv495_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv494_1[1]
            rms_norm511: R.Tensor((1, seq_len, 4096), dtype="float16") = lv494_1[0]
            lv1268 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight9, model_layers_28_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv498 = R.call_tir(cls.NT_matmul5, (rms_norm511, lv1268), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1008: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv498, R.shape([1, seq_len, 48, 128]))
            reshape1009: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1008, R.shape([seq_len, 48, 128]))
            lv1269 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1009), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1010: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1269, R.shape([1, seq_len, 32, 128]))
            reshape1011: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1010, R.shape([1, seq_len, 4096]))
            lv1270 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight9, model_layers_28_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv499 = R.call_tir(cls.NT_matmul6, (reshape1011, lv1270), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv496_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv499, lv495_1, model_layers_28_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv497_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv496_1[1]
            rms_norm512: R.Tensor((1, seq_len, 4096), dtype="float16") = lv496_1[0]
            lv1271 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight9, model_layers_28_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv500 = R.call_tir(cls.NT_matmul7, (rms_norm512, lv1271), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split252: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv500, indices_or_sections=2, axis=-1)
            split_0252: R.Tensor((1, seq_len, 14336), dtype="float16") = split252[0]
            split_1252: R.Tensor((1, seq_len, 14336), dtype="float16") = split252[1]
            silu252: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0252)
            mul252: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu252, split_1252)
            lv1272 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight9, model_layers_28_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv501 = R.call_tir(cls.NT_matmul8, (mul252, lv1272), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv498_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv501, lv497_1, model_layers_29_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv499_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv498_1[1]
            rms_norm513: R.Tensor((1, seq_len, 4096), dtype="float16") = lv498_1[0]
            lv1273 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight9, model_layers_29_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv502 = R.call_tir(cls.NT_matmul5, (rms_norm513, lv1273), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1012: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv502, R.shape([1, seq_len, 48, 128]))
            reshape1013: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1012, R.shape([seq_len, 48, 128]))
            lv1274 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1013), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1014: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1274, R.shape([1, seq_len, 32, 128]))
            reshape1015: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1014, R.shape([1, seq_len, 4096]))
            lv1275 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight9, model_layers_29_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv503 = R.call_tir(cls.NT_matmul6, (reshape1015, lv1275), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv500_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv503, lv499_1, model_layers_29_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv501_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv500_1[1]
            rms_norm514: R.Tensor((1, seq_len, 4096), dtype="float16") = lv500_1[0]
            lv1276 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight9, model_layers_29_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv504 = R.call_tir(cls.NT_matmul7, (rms_norm514, lv1276), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split253: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv504, indices_or_sections=2, axis=-1)
            split_0253: R.Tensor((1, seq_len, 14336), dtype="float16") = split253[0]
            split_1253: R.Tensor((1, seq_len, 14336), dtype="float16") = split253[1]
            silu253: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0253)
            mul253: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu253, split_1253)
            lv1277 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight9, model_layers_29_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv505 = R.call_tir(cls.NT_matmul8, (mul253, lv1277), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv502_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv505, lv501_1, model_layers_30_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv503_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv502_1[1]
            rms_norm515: R.Tensor((1, seq_len, 4096), dtype="float16") = lv502_1[0]
            lv1278 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight9, model_layers_30_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv506 = R.call_tir(cls.NT_matmul5, (rms_norm515, lv1278), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1016: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv506, R.shape([1, seq_len, 48, 128]))
            reshape1017: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1016, R.shape([seq_len, 48, 128]))
            lv1279 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1017), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1018: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1279, R.shape([1, seq_len, 32, 128]))
            reshape1019: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1018, R.shape([1, seq_len, 4096]))
            lv1280 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight9, model_layers_30_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv507 = R.call_tir(cls.NT_matmul6, (reshape1019, lv1280), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv504_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv507, lv503_1, model_layers_30_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv505_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv504_1[1]
            rms_norm516: R.Tensor((1, seq_len, 4096), dtype="float16") = lv504_1[0]
            lv1281 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight9, model_layers_30_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv508 = R.call_tir(cls.NT_matmul7, (rms_norm516, lv1281), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split254: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv508, indices_or_sections=2, axis=-1)
            split_0254: R.Tensor((1, seq_len, 14336), dtype="float16") = split254[0]
            split_1254: R.Tensor((1, seq_len, 14336), dtype="float16") = split254[1]
            silu254: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0254)
            mul254: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu254, split_1254)
            lv1282 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight9, model_layers_30_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv509 = R.call_tir(cls.NT_matmul8, (mul254, lv1282), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv506_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv509, lv505_1, model_layers_31_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv507_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv506_1[1]
            rms_norm517: R.Tensor((1, seq_len, 4096), dtype="float16") = lv506_1[0]
            lv1283 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight9, model_layers_31_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv510 = R.call_tir(cls.NT_matmul5, (rms_norm517, lv1283), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1020: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv510, R.shape([1, seq_len, 48, 128]))
            reshape1021: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1020, R.shape([seq_len, 48, 128]))
            lv1284 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1021), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1022: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1284, R.shape([1, seq_len, 32, 128]))
            reshape1023: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1022, R.shape([1, seq_len, 4096]))
            lv1285 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight9, model_layers_31_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv511 = R.call_tir(cls.NT_matmul6, (reshape1023, lv1285), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv508_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv511, lv507_1, model_layers_31_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv509_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv508_1[1]
            rms_norm518: R.Tensor((1, seq_len, 4096), dtype="float16") = lv508_1[0]
            lv1286 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight9, model_layers_31_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv512 = R.call_tir(cls.NT_matmul7, (rms_norm518, lv1286), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split255: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv512, indices_or_sections=2, axis=-1)
            split_0255: R.Tensor((1, seq_len, 14336), dtype="float16") = split255[0]
            split_1255: R.Tensor((1, seq_len, 14336), dtype="float16") = split255[1]
            silu255: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0255)
            mul255: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu255, split_1255)
            lv1287 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight9, model_layers_31_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv513 = R.call_tir(cls.NT_matmul8, (mul255, lv1287), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv510_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv513, lv509_1, model_norm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm519: R.Tensor((1, seq_len, 4096), dtype="float16") = lv510_1[0]
            gv10: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm519, paged_kv_cache
            R.output(gv10)
        return gv10

    @R.function
    def batch_select_last_hidden_states(hidden_states: R.Tensor(("seq_len", 4096), dtype="float16"), logit_positions: R.Tensor(("batch_size",), dtype="int32")) -> R.Tensor(("batch_size", 4096), dtype="float16"):
        batch_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 2, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        with R.dataflow():
            take1: R.Tensor((batch_size, 4096), dtype="float16") = R.take(hidden_states, logit_positions, axis=0)
            gv2: R.Tensor((batch_size, 4096), dtype="float16") = take1
            R.output(gv2)
        return gv2

    @R.function
    def batch_verify(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", "vocab_size"), dtype="float32"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size", "seq_len"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight8: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale8: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm390: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv967 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight8, model_layers_0_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv514 = R.call_tir(cls.NT_matmul5, (rms_norm390, lv967), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape768: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv514, R.shape([1, seq_len, 48, 128]))
            reshape769: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape768, R.shape([seq_len, 48, 128]))
            lv968 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape769), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape770: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv968, R.shape([1, seq_len, 32, 128]))
            reshape771: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape770, R.shape([1, seq_len, 4096]))
            lv969 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight8, model_layers_0_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv515 = R.call_tir(cls.NT_matmul6, (reshape771, lv969), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv512 = R.call_tir(cls.fuse_add_norm_prefill, (lv515, input_embeds, model_layers_0_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv513: R.Tensor((1, seq_len, 4096), dtype="float16") = lv512[1]
            rms_norm391: R.Tensor((1, seq_len, 4096), dtype="float16") = lv512[0]
            lv970 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight8, model_layers_0_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv516 = R.call_tir(cls.NT_matmul7, (rms_norm391, lv970), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split192: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv516, indices_or_sections=2, axis=-1)
            split_0192: R.Tensor((1, seq_len, 14336), dtype="float16") = split192[0]
            split_1192: R.Tensor((1, seq_len, 14336), dtype="float16") = split192[1]
            silu192: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0192)
            mul192: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu192, split_1192)
            lv971 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight8, model_layers_0_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv517 = R.call_tir(cls.NT_matmul8, (mul192, lv971), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv514_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv517, lv513, model_layers_1_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv515_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv514_1[1]
            rms_norm392: R.Tensor((1, seq_len, 4096), dtype="float16") = lv514_1[0]
            lv972 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight8, model_layers_1_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv518 = R.call_tir(cls.NT_matmul5, (rms_norm392, lv972), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape772: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv518, R.shape([1, seq_len, 48, 128]))
            reshape773: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape772, R.shape([seq_len, 48, 128]))
            lv973 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape773), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape774: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv973, R.shape([1, seq_len, 32, 128]))
            reshape775: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape774, R.shape([1, seq_len, 4096]))
            lv974 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight8, model_layers_1_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv519 = R.call_tir(cls.NT_matmul6, (reshape775, lv974), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv516_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv519, lv515_1, model_layers_1_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv517_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv516_1[1]
            rms_norm393: R.Tensor((1, seq_len, 4096), dtype="float16") = lv516_1[0]
            lv975 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight8, model_layers_1_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv520 = R.call_tir(cls.NT_matmul7, (rms_norm393, lv975), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split193: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv520, indices_or_sections=2, axis=-1)
            split_0193: R.Tensor((1, seq_len, 14336), dtype="float16") = split193[0]
            split_1193: R.Tensor((1, seq_len, 14336), dtype="float16") = split193[1]
            silu193: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0193)
            mul193: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu193, split_1193)
            lv976 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight8, model_layers_1_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv521 = R.call_tir(cls.NT_matmul8, (mul193, lv976), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv518_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv521, lv517_1, model_layers_2_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv519_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv518_1[1]
            rms_norm394: R.Tensor((1, seq_len, 4096), dtype="float16") = lv518_1[0]
            lv977 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight8, model_layers_2_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv522 = R.call_tir(cls.NT_matmul5, (rms_norm394, lv977), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape776: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv522, R.shape([1, seq_len, 48, 128]))
            reshape777: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape776, R.shape([seq_len, 48, 128]))
            lv978 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape777), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape778: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv978, R.shape([1, seq_len, 32, 128]))
            reshape779: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape778, R.shape([1, seq_len, 4096]))
            lv979 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight8, model_layers_2_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv523 = R.call_tir(cls.NT_matmul6, (reshape779, lv979), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv520_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv523, lv519_1, model_layers_2_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv521_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv520_1[1]
            rms_norm395: R.Tensor((1, seq_len, 4096), dtype="float16") = lv520_1[0]
            lv980 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight8, model_layers_2_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv524 = R.call_tir(cls.NT_matmul7, (rms_norm395, lv980), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split194: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv524, indices_or_sections=2, axis=-1)
            split_0194: R.Tensor((1, seq_len, 14336), dtype="float16") = split194[0]
            split_1194: R.Tensor((1, seq_len, 14336), dtype="float16") = split194[1]
            silu194: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0194)
            mul194: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu194, split_1194)
            lv981 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight8, model_layers_2_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv525 = R.call_tir(cls.NT_matmul8, (mul194, lv981), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv522_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv525, lv521_1, model_layers_3_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv523_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv522_1[1]
            rms_norm396: R.Tensor((1, seq_len, 4096), dtype="float16") = lv522_1[0]
            lv982 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight8, model_layers_3_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv526 = R.call_tir(cls.NT_matmul5, (rms_norm396, lv982), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape780: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv526, R.shape([1, seq_len, 48, 128]))
            reshape781: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape780, R.shape([seq_len, 48, 128]))
            lv983 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape781), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape782: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv983, R.shape([1, seq_len, 32, 128]))
            reshape783: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape782, R.shape([1, seq_len, 4096]))
            lv984 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight8, model_layers_3_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv527 = R.call_tir(cls.NT_matmul6, (reshape783, lv984), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv524_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv527, lv523_1, model_layers_3_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv525_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv524_1[1]
            rms_norm397: R.Tensor((1, seq_len, 4096), dtype="float16") = lv524_1[0]
            lv985 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight8, model_layers_3_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv528 = R.call_tir(cls.NT_matmul7, (rms_norm397, lv985), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split195: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv528, indices_or_sections=2, axis=-1)
            split_0195: R.Tensor((1, seq_len, 14336), dtype="float16") = split195[0]
            split_1195: R.Tensor((1, seq_len, 14336), dtype="float16") = split195[1]
            silu195: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0195)
            mul195: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu195, split_1195)
            lv986 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight8, model_layers_3_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv529 = R.call_tir(cls.NT_matmul8, (mul195, lv986), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv526_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv529, lv525_1, model_layers_4_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv527_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv526_1[1]
            rms_norm398: R.Tensor((1, seq_len, 4096), dtype="float16") = lv526_1[0]
            lv987 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight8, model_layers_4_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv530 = R.call_tir(cls.NT_matmul5, (rms_norm398, lv987), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape784: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv530, R.shape([1, seq_len, 48, 128]))
            reshape785: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape784, R.shape([seq_len, 48, 128]))
            lv988 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape785), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape786: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv988, R.shape([1, seq_len, 32, 128]))
            reshape787: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape786, R.shape([1, seq_len, 4096]))
            lv989 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight8, model_layers_4_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv531 = R.call_tir(cls.NT_matmul6, (reshape787, lv989), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv528_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv531, lv527_1, model_layers_4_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv529_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv528_1[1]
            rms_norm399: R.Tensor((1, seq_len, 4096), dtype="float16") = lv528_1[0]
            lv990 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight8, model_layers_4_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv532 = R.call_tir(cls.NT_matmul7, (rms_norm399, lv990), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split196: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv532, indices_or_sections=2, axis=-1)
            split_0196: R.Tensor((1, seq_len, 14336), dtype="float16") = split196[0]
            split_1196: R.Tensor((1, seq_len, 14336), dtype="float16") = split196[1]
            silu196: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0196)
            mul196: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu196, split_1196)
            lv991 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight8, model_layers_4_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv533 = R.call_tir(cls.NT_matmul8, (mul196, lv991), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv530_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv533, lv529_1, model_layers_5_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv531_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv530_1[1]
            rms_norm400: R.Tensor((1, seq_len, 4096), dtype="float16") = lv530_1[0]
            lv992 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight8, model_layers_5_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv534 = R.call_tir(cls.NT_matmul5, (rms_norm400, lv992), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape788: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv534, R.shape([1, seq_len, 48, 128]))
            reshape789: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape788, R.shape([seq_len, 48, 128]))
            lv993 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape789), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape790: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv993, R.shape([1, seq_len, 32, 128]))
            reshape791: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape790, R.shape([1, seq_len, 4096]))
            lv994 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight8, model_layers_5_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv535 = R.call_tir(cls.NT_matmul6, (reshape791, lv994), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv532_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv535, lv531_1, model_layers_5_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv533_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv532_1[1]
            rms_norm401: R.Tensor((1, seq_len, 4096), dtype="float16") = lv532_1[0]
            lv995 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight8, model_layers_5_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv536 = R.call_tir(cls.NT_matmul7, (rms_norm401, lv995), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split197: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv536, indices_or_sections=2, axis=-1)
            split_0197: R.Tensor((1, seq_len, 14336), dtype="float16") = split197[0]
            split_1197: R.Tensor((1, seq_len, 14336), dtype="float16") = split197[1]
            silu197: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0197)
            mul197: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu197, split_1197)
            lv996 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight8, model_layers_5_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv537 = R.call_tir(cls.NT_matmul8, (mul197, lv996), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv534_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv537, lv533_1, model_layers_6_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv535_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv534_1[1]
            rms_norm402: R.Tensor((1, seq_len, 4096), dtype="float16") = lv534_1[0]
            lv997 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight8, model_layers_6_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv538 = R.call_tir(cls.NT_matmul5, (rms_norm402, lv997), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape792: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv538, R.shape([1, seq_len, 48, 128]))
            reshape793: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape792, R.shape([seq_len, 48, 128]))
            lv998 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape793), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape794: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv998, R.shape([1, seq_len, 32, 128]))
            reshape795: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape794, R.shape([1, seq_len, 4096]))
            lv999 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight8, model_layers_6_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv539 = R.call_tir(cls.NT_matmul6, (reshape795, lv999), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv536_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv539, lv535_1, model_layers_6_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv537_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv536_1[1]
            rms_norm403: R.Tensor((1, seq_len, 4096), dtype="float16") = lv536_1[0]
            lv1000 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight8, model_layers_6_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv540 = R.call_tir(cls.NT_matmul7, (rms_norm403, lv1000), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split198: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv540, indices_or_sections=2, axis=-1)
            split_0198: R.Tensor((1, seq_len, 14336), dtype="float16") = split198[0]
            split_1198: R.Tensor((1, seq_len, 14336), dtype="float16") = split198[1]
            silu198: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0198)
            mul198: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu198, split_1198)
            lv1001 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight8, model_layers_6_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv541 = R.call_tir(cls.NT_matmul8, (mul198, lv1001), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv538_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv541, lv537_1, model_layers_7_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv539_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv538_1[1]
            rms_norm404: R.Tensor((1, seq_len, 4096), dtype="float16") = lv538_1[0]
            lv1002 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight8, model_layers_7_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv542 = R.call_tir(cls.NT_matmul5, (rms_norm404, lv1002), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape796: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv542, R.shape([1, seq_len, 48, 128]))
            reshape797: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape796, R.shape([seq_len, 48, 128]))
            lv1003 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape797), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape798: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1003, R.shape([1, seq_len, 32, 128]))
            reshape799: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape798, R.shape([1, seq_len, 4096]))
            lv1004 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight8, model_layers_7_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv543 = R.call_tir(cls.NT_matmul6, (reshape799, lv1004), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv540_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv543, lv539_1, model_layers_7_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv541_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv540_1[1]
            rms_norm405: R.Tensor((1, seq_len, 4096), dtype="float16") = lv540_1[0]
            lv1005 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight8, model_layers_7_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv544 = R.call_tir(cls.NT_matmul7, (rms_norm405, lv1005), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split199: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv544, indices_or_sections=2, axis=-1)
            split_0199: R.Tensor((1, seq_len, 14336), dtype="float16") = split199[0]
            split_1199: R.Tensor((1, seq_len, 14336), dtype="float16") = split199[1]
            silu199: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0199)
            mul199: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu199, split_1199)
            lv1006 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight8, model_layers_7_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv545 = R.call_tir(cls.NT_matmul8, (mul199, lv1006), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv542_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv545, lv541_1, model_layers_8_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv543_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv542_1[1]
            rms_norm406: R.Tensor((1, seq_len, 4096), dtype="float16") = lv542_1[0]
            lv1007 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight8, model_layers_8_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv546 = R.call_tir(cls.NT_matmul5, (rms_norm406, lv1007), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape800: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv546, R.shape([1, seq_len, 48, 128]))
            reshape801: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape800, R.shape([seq_len, 48, 128]))
            lv1008 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape801), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape802: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1008, R.shape([1, seq_len, 32, 128]))
            reshape803: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape802, R.shape([1, seq_len, 4096]))
            lv1009 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight8, model_layers_8_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv547 = R.call_tir(cls.NT_matmul6, (reshape803, lv1009), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv544_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv547, lv543_1, model_layers_8_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv545_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv544_1[1]
            rms_norm407: R.Tensor((1, seq_len, 4096), dtype="float16") = lv544_1[0]
            lv1010 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight8, model_layers_8_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv548 = R.call_tir(cls.NT_matmul7, (rms_norm407, lv1010), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split200: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv548, indices_or_sections=2, axis=-1)
            split_0200: R.Tensor((1, seq_len, 14336), dtype="float16") = split200[0]
            split_1200: R.Tensor((1, seq_len, 14336), dtype="float16") = split200[1]
            silu200: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0200)
            mul200: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu200, split_1200)
            lv1011 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight8, model_layers_8_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv549 = R.call_tir(cls.NT_matmul8, (mul200, lv1011), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv546_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv549, lv545_1, model_layers_9_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv547_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv546_1[1]
            rms_norm408: R.Tensor((1, seq_len, 4096), dtype="float16") = lv546_1[0]
            lv1012 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight8, model_layers_9_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv550 = R.call_tir(cls.NT_matmul5, (rms_norm408, lv1012), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape804: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv550, R.shape([1, seq_len, 48, 128]))
            reshape805: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape804, R.shape([seq_len, 48, 128]))
            lv1013 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape805), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape806: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1013, R.shape([1, seq_len, 32, 128]))
            reshape807: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape806, R.shape([1, seq_len, 4096]))
            lv1014 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight8, model_layers_9_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv551 = R.call_tir(cls.NT_matmul6, (reshape807, lv1014), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv548_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv551, lv547_1, model_layers_9_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv549_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv548_1[1]
            rms_norm409: R.Tensor((1, seq_len, 4096), dtype="float16") = lv548_1[0]
            lv1015 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight8, model_layers_9_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv552 = R.call_tir(cls.NT_matmul7, (rms_norm409, lv1015), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split201: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv552, indices_or_sections=2, axis=-1)
            split_0201: R.Tensor((1, seq_len, 14336), dtype="float16") = split201[0]
            split_1201: R.Tensor((1, seq_len, 14336), dtype="float16") = split201[1]
            silu201: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0201)
            mul201: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu201, split_1201)
            lv1016 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight8, model_layers_9_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv553 = R.call_tir(cls.NT_matmul8, (mul201, lv1016), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv550_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv553, lv549_1, model_layers_10_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv551_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv550_1[1]
            rms_norm410: R.Tensor((1, seq_len, 4096), dtype="float16") = lv550_1[0]
            lv1017 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight8, model_layers_10_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv554 = R.call_tir(cls.NT_matmul5, (rms_norm410, lv1017), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape808: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv554, R.shape([1, seq_len, 48, 128]))
            reshape809: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape808, R.shape([seq_len, 48, 128]))
            lv1018 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape809), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape810: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1018, R.shape([1, seq_len, 32, 128]))
            reshape811: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape810, R.shape([1, seq_len, 4096]))
            lv1019 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight8, model_layers_10_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv555 = R.call_tir(cls.NT_matmul6, (reshape811, lv1019), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv552_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv555, lv551_1, model_layers_10_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv553_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv552_1[1]
            rms_norm411: R.Tensor((1, seq_len, 4096), dtype="float16") = lv552_1[0]
            lv1020 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight8, model_layers_10_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv556 = R.call_tir(cls.NT_matmul7, (rms_norm411, lv1020), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split202: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv556, indices_or_sections=2, axis=-1)
            split_0202: R.Tensor((1, seq_len, 14336), dtype="float16") = split202[0]
            split_1202: R.Tensor((1, seq_len, 14336), dtype="float16") = split202[1]
            silu202: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0202)
            mul202: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu202, split_1202)
            lv1021 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight8, model_layers_10_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv557 = R.call_tir(cls.NT_matmul8, (mul202, lv1021), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv554_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv557, lv553_1, model_layers_11_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv555_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv554_1[1]
            rms_norm412: R.Tensor((1, seq_len, 4096), dtype="float16") = lv554_1[0]
            lv1022 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight8, model_layers_11_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv558 = R.call_tir(cls.NT_matmul5, (rms_norm412, lv1022), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape812: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv558, R.shape([1, seq_len, 48, 128]))
            reshape813: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape812, R.shape([seq_len, 48, 128]))
            lv1023 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape813), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape814: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1023, R.shape([1, seq_len, 32, 128]))
            reshape815: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape814, R.shape([1, seq_len, 4096]))
            lv1024 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight8, model_layers_11_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv559 = R.call_tir(cls.NT_matmul6, (reshape815, lv1024), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv556_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv559, lv555_1, model_layers_11_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv557_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv556_1[1]
            rms_norm413: R.Tensor((1, seq_len, 4096), dtype="float16") = lv556_1[0]
            lv1025 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight8, model_layers_11_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv560 = R.call_tir(cls.NT_matmul7, (rms_norm413, lv1025), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split203: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv560, indices_or_sections=2, axis=-1)
            split_0203: R.Tensor((1, seq_len, 14336), dtype="float16") = split203[0]
            split_1203: R.Tensor((1, seq_len, 14336), dtype="float16") = split203[1]
            silu203: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0203)
            mul203: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu203, split_1203)
            lv1026 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight8, model_layers_11_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv561 = R.call_tir(cls.NT_matmul8, (mul203, lv1026), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv558_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv561, lv557_1, model_layers_12_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv559_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv558_1[1]
            rms_norm414: R.Tensor((1, seq_len, 4096), dtype="float16") = lv558_1[0]
            lv1027 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight8, model_layers_12_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv562 = R.call_tir(cls.NT_matmul5, (rms_norm414, lv1027), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape816: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv562, R.shape([1, seq_len, 48, 128]))
            reshape817: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape816, R.shape([seq_len, 48, 128]))
            lv1028 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape817), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape818: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1028, R.shape([1, seq_len, 32, 128]))
            reshape819: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape818, R.shape([1, seq_len, 4096]))
            lv1029 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight8, model_layers_12_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv563 = R.call_tir(cls.NT_matmul6, (reshape819, lv1029), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv560_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv563, lv559_1, model_layers_12_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv561_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv560_1[1]
            rms_norm415: R.Tensor((1, seq_len, 4096), dtype="float16") = lv560_1[0]
            lv1030 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight8, model_layers_12_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv564 = R.call_tir(cls.NT_matmul7, (rms_norm415, lv1030), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split204: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv564, indices_or_sections=2, axis=-1)
            split_0204: R.Tensor((1, seq_len, 14336), dtype="float16") = split204[0]
            split_1204: R.Tensor((1, seq_len, 14336), dtype="float16") = split204[1]
            silu204: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0204)
            mul204: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu204, split_1204)
            lv1031 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight8, model_layers_12_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv565 = R.call_tir(cls.NT_matmul8, (mul204, lv1031), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv562_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv565, lv561_1, model_layers_13_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv563_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv562_1[1]
            rms_norm416: R.Tensor((1, seq_len, 4096), dtype="float16") = lv562_1[0]
            lv1032 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight8, model_layers_13_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv566 = R.call_tir(cls.NT_matmul5, (rms_norm416, lv1032), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape820: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv566, R.shape([1, seq_len, 48, 128]))
            reshape821: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape820, R.shape([seq_len, 48, 128]))
            lv1033 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape821), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape822: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1033, R.shape([1, seq_len, 32, 128]))
            reshape823: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape822, R.shape([1, seq_len, 4096]))
            lv1034 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight8, model_layers_13_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv567 = R.call_tir(cls.NT_matmul6, (reshape823, lv1034), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv564_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv567, lv563_1, model_layers_13_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv565_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv564_1[1]
            rms_norm417: R.Tensor((1, seq_len, 4096), dtype="float16") = lv564_1[0]
            lv1035 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight8, model_layers_13_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv568 = R.call_tir(cls.NT_matmul7, (rms_norm417, lv1035), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split205: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv568, indices_or_sections=2, axis=-1)
            split_0205: R.Tensor((1, seq_len, 14336), dtype="float16") = split205[0]
            split_1205: R.Tensor((1, seq_len, 14336), dtype="float16") = split205[1]
            silu205: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0205)
            mul205: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu205, split_1205)
            lv1036 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight8, model_layers_13_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv569 = R.call_tir(cls.NT_matmul8, (mul205, lv1036), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv566_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv569, lv565_1, model_layers_14_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv567_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv566_1[1]
            rms_norm418: R.Tensor((1, seq_len, 4096), dtype="float16") = lv566_1[0]
            lv1037 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight8, model_layers_14_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv570 = R.call_tir(cls.NT_matmul5, (rms_norm418, lv1037), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape824: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv570, R.shape([1, seq_len, 48, 128]))
            reshape825: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape824, R.shape([seq_len, 48, 128]))
            lv1038 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape825), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape826: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1038, R.shape([1, seq_len, 32, 128]))
            reshape827: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape826, R.shape([1, seq_len, 4096]))
            lv1039 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight8, model_layers_14_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv571 = R.call_tir(cls.NT_matmul6, (reshape827, lv1039), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv568_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv571, lv567_1, model_layers_14_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv569_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv568_1[1]
            rms_norm419: R.Tensor((1, seq_len, 4096), dtype="float16") = lv568_1[0]
            lv1040 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight8, model_layers_14_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv572 = R.call_tir(cls.NT_matmul7, (rms_norm419, lv1040), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split206: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv572, indices_or_sections=2, axis=-1)
            split_0206: R.Tensor((1, seq_len, 14336), dtype="float16") = split206[0]
            split_1206: R.Tensor((1, seq_len, 14336), dtype="float16") = split206[1]
            silu206: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0206)
            mul206: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu206, split_1206)
            lv1041 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight8, model_layers_14_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv573 = R.call_tir(cls.NT_matmul8, (mul206, lv1041), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv570_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv573, lv569_1, model_layers_15_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv571_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv570_1[1]
            rms_norm420: R.Tensor((1, seq_len, 4096), dtype="float16") = lv570_1[0]
            lv1042 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight8, model_layers_15_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv574 = R.call_tir(cls.NT_matmul5, (rms_norm420, lv1042), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape828: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv574, R.shape([1, seq_len, 48, 128]))
            reshape829: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape828, R.shape([seq_len, 48, 128]))
            lv1043 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape829), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape830: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1043, R.shape([1, seq_len, 32, 128]))
            reshape831: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape830, R.shape([1, seq_len, 4096]))
            lv1044 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight8, model_layers_15_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv575 = R.call_tir(cls.NT_matmul6, (reshape831, lv1044), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv572_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv575, lv571_1, model_layers_15_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv573_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv572_1[1]
            rms_norm421: R.Tensor((1, seq_len, 4096), dtype="float16") = lv572_1[0]
            lv1045 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight8, model_layers_15_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv576 = R.call_tir(cls.NT_matmul7, (rms_norm421, lv1045), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split207: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv576, indices_or_sections=2, axis=-1)
            split_0207: R.Tensor((1, seq_len, 14336), dtype="float16") = split207[0]
            split_1207: R.Tensor((1, seq_len, 14336), dtype="float16") = split207[1]
            silu207: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0207)
            mul207: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu207, split_1207)
            lv1046 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight8, model_layers_15_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv577 = R.call_tir(cls.NT_matmul8, (mul207, lv1046), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv574_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv577, lv573_1, model_layers_16_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv575_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv574_1[1]
            rms_norm422: R.Tensor((1, seq_len, 4096), dtype="float16") = lv574_1[0]
            lv1047 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight8, model_layers_16_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv578 = R.call_tir(cls.NT_matmul5, (rms_norm422, lv1047), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape832: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv578, R.shape([1, seq_len, 48, 128]))
            reshape833: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape832, R.shape([seq_len, 48, 128]))
            lv1048 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape833), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape834: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1048, R.shape([1, seq_len, 32, 128]))
            reshape835: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape834, R.shape([1, seq_len, 4096]))
            lv1049 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight8, model_layers_16_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv579 = R.call_tir(cls.NT_matmul6, (reshape835, lv1049), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv576_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv579, lv575_1, model_layers_16_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv577_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv576_1[1]
            rms_norm423: R.Tensor((1, seq_len, 4096), dtype="float16") = lv576_1[0]
            lv1050 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight8, model_layers_16_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv580 = R.call_tir(cls.NT_matmul7, (rms_norm423, lv1050), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split208: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv580, indices_or_sections=2, axis=-1)
            split_0208: R.Tensor((1, seq_len, 14336), dtype="float16") = split208[0]
            split_1208: R.Tensor((1, seq_len, 14336), dtype="float16") = split208[1]
            silu208: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0208)
            mul208: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu208, split_1208)
            lv1051 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight8, model_layers_16_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv581 = R.call_tir(cls.NT_matmul8, (mul208, lv1051), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv578_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv581, lv577_1, model_layers_17_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv579_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv578_1[1]
            rms_norm424: R.Tensor((1, seq_len, 4096), dtype="float16") = lv578_1[0]
            lv1052 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight8, model_layers_17_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv582 = R.call_tir(cls.NT_matmul5, (rms_norm424, lv1052), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape836: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv582, R.shape([1, seq_len, 48, 128]))
            reshape837: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape836, R.shape([seq_len, 48, 128]))
            lv1053 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape837), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape838: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1053, R.shape([1, seq_len, 32, 128]))
            reshape839: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape838, R.shape([1, seq_len, 4096]))
            lv1054 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight8, model_layers_17_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv583 = R.call_tir(cls.NT_matmul6, (reshape839, lv1054), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv580_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv583, lv579_1, model_layers_17_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv581_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv580_1[1]
            rms_norm425: R.Tensor((1, seq_len, 4096), dtype="float16") = lv580_1[0]
            lv1055 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight8, model_layers_17_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv584 = R.call_tir(cls.NT_matmul7, (rms_norm425, lv1055), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split209: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv584, indices_or_sections=2, axis=-1)
            split_0209: R.Tensor((1, seq_len, 14336), dtype="float16") = split209[0]
            split_1209: R.Tensor((1, seq_len, 14336), dtype="float16") = split209[1]
            silu209: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0209)
            mul209: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu209, split_1209)
            lv1056 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight8, model_layers_17_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv585 = R.call_tir(cls.NT_matmul8, (mul209, lv1056), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv582_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv585, lv581_1, model_layers_18_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv583_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv582_1[1]
            rms_norm426: R.Tensor((1, seq_len, 4096), dtype="float16") = lv582_1[0]
            lv1057 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight8, model_layers_18_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv586 = R.call_tir(cls.NT_matmul5, (rms_norm426, lv1057), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape840: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv586, R.shape([1, seq_len, 48, 128]))
            reshape841: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape840, R.shape([seq_len, 48, 128]))
            lv1058 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape841), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape842: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1058, R.shape([1, seq_len, 32, 128]))
            reshape843: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape842, R.shape([1, seq_len, 4096]))
            lv1059 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight8, model_layers_18_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv587 = R.call_tir(cls.NT_matmul6, (reshape843, lv1059), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv584_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv587, lv583_1, model_layers_18_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv585_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv584_1[1]
            rms_norm427: R.Tensor((1, seq_len, 4096), dtype="float16") = lv584_1[0]
            lv1060 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight8, model_layers_18_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv588 = R.call_tir(cls.NT_matmul7, (rms_norm427, lv1060), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split210: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv588, indices_or_sections=2, axis=-1)
            split_0210: R.Tensor((1, seq_len, 14336), dtype="float16") = split210[0]
            split_1210: R.Tensor((1, seq_len, 14336), dtype="float16") = split210[1]
            silu210: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0210)
            mul210: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu210, split_1210)
            lv1061 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight8, model_layers_18_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv589 = R.call_tir(cls.NT_matmul8, (mul210, lv1061), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv586_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv589, lv585_1, model_layers_19_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv587_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv586_1[1]
            rms_norm428: R.Tensor((1, seq_len, 4096), dtype="float16") = lv586_1[0]
            lv1062 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight8, model_layers_19_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv590 = R.call_tir(cls.NT_matmul5, (rms_norm428, lv1062), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape844: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv590, R.shape([1, seq_len, 48, 128]))
            reshape845: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape844, R.shape([seq_len, 48, 128]))
            lv1063 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape845), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape846: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1063, R.shape([1, seq_len, 32, 128]))
            reshape847: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape846, R.shape([1, seq_len, 4096]))
            lv1064 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight8, model_layers_19_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv591 = R.call_tir(cls.NT_matmul6, (reshape847, lv1064), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv588_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv591, lv587_1, model_layers_19_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv589_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv588_1[1]
            rms_norm429: R.Tensor((1, seq_len, 4096), dtype="float16") = lv588_1[0]
            lv1065 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight8, model_layers_19_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv592 = R.call_tir(cls.NT_matmul7, (rms_norm429, lv1065), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split211: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv592, indices_or_sections=2, axis=-1)
            split_0211: R.Tensor((1, seq_len, 14336), dtype="float16") = split211[0]
            split_1211: R.Tensor((1, seq_len, 14336), dtype="float16") = split211[1]
            silu211: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0211)
            mul211: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu211, split_1211)
            lv1066 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight8, model_layers_19_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv593 = R.call_tir(cls.NT_matmul8, (mul211, lv1066), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv590_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv593, lv589_1, model_layers_20_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv591_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv590_1[1]
            rms_norm430: R.Tensor((1, seq_len, 4096), dtype="float16") = lv590_1[0]
            lv1067 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight8, model_layers_20_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv594 = R.call_tir(cls.NT_matmul5, (rms_norm430, lv1067), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape848: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv594, R.shape([1, seq_len, 48, 128]))
            reshape849: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape848, R.shape([seq_len, 48, 128]))
            lv1068 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape849), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape850: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1068, R.shape([1, seq_len, 32, 128]))
            reshape851: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape850, R.shape([1, seq_len, 4096]))
            lv1069 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight8, model_layers_20_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv595 = R.call_tir(cls.NT_matmul6, (reshape851, lv1069), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv592_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv595, lv591_1, model_layers_20_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv593_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv592_1[1]
            rms_norm431: R.Tensor((1, seq_len, 4096), dtype="float16") = lv592_1[0]
            lv1070 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight8, model_layers_20_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv596 = R.call_tir(cls.NT_matmul7, (rms_norm431, lv1070), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split212: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv596, indices_or_sections=2, axis=-1)
            split_0212: R.Tensor((1, seq_len, 14336), dtype="float16") = split212[0]
            split_1212: R.Tensor((1, seq_len, 14336), dtype="float16") = split212[1]
            silu212: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0212)
            mul212: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu212, split_1212)
            lv1071 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight8, model_layers_20_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv597 = R.call_tir(cls.NT_matmul8, (mul212, lv1071), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv594_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv597, lv593_1, model_layers_21_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv595_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv594_1[1]
            rms_norm432: R.Tensor((1, seq_len, 4096), dtype="float16") = lv594_1[0]
            lv1072 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight8, model_layers_21_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv598 = R.call_tir(cls.NT_matmul5, (rms_norm432, lv1072), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape852: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv598, R.shape([1, seq_len, 48, 128]))
            reshape853: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape852, R.shape([seq_len, 48, 128]))
            lv1073 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape853), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape854: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1073, R.shape([1, seq_len, 32, 128]))
            reshape855: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape854, R.shape([1, seq_len, 4096]))
            lv1074 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight8, model_layers_21_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv599 = R.call_tir(cls.NT_matmul6, (reshape855, lv1074), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv596_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv599, lv595_1, model_layers_21_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv597_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv596_1[1]
            rms_norm433: R.Tensor((1, seq_len, 4096), dtype="float16") = lv596_1[0]
            lv1075 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight8, model_layers_21_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv600 = R.call_tir(cls.NT_matmul7, (rms_norm433, lv1075), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split213: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv600, indices_or_sections=2, axis=-1)
            split_0213: R.Tensor((1, seq_len, 14336), dtype="float16") = split213[0]
            split_1213: R.Tensor((1, seq_len, 14336), dtype="float16") = split213[1]
            silu213: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0213)
            mul213: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu213, split_1213)
            lv1076 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight8, model_layers_21_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv601 = R.call_tir(cls.NT_matmul8, (mul213, lv1076), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv598_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv601, lv597_1, model_layers_22_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv599_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv598_1[1]
            rms_norm434: R.Tensor((1, seq_len, 4096), dtype="float16") = lv598_1[0]
            lv1077 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight8, model_layers_22_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv602 = R.call_tir(cls.NT_matmul5, (rms_norm434, lv1077), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape856: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv602, R.shape([1, seq_len, 48, 128]))
            reshape857: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape856, R.shape([seq_len, 48, 128]))
            lv1078 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape857), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape858: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1078, R.shape([1, seq_len, 32, 128]))
            reshape859: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape858, R.shape([1, seq_len, 4096]))
            lv1079 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight8, model_layers_22_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv603 = R.call_tir(cls.NT_matmul6, (reshape859, lv1079), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv600_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv603, lv599_1, model_layers_22_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv601_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv600_1[1]
            rms_norm435: R.Tensor((1, seq_len, 4096), dtype="float16") = lv600_1[0]
            lv1080 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight8, model_layers_22_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv604 = R.call_tir(cls.NT_matmul7, (rms_norm435, lv1080), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split214: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv604, indices_or_sections=2, axis=-1)
            split_0214: R.Tensor((1, seq_len, 14336), dtype="float16") = split214[0]
            split_1214: R.Tensor((1, seq_len, 14336), dtype="float16") = split214[1]
            silu214: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0214)
            mul214: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu214, split_1214)
            lv1081 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight8, model_layers_22_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv605 = R.call_tir(cls.NT_matmul8, (mul214, lv1081), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv602_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv605, lv601_1, model_layers_23_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv603_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv602_1[1]
            rms_norm436: R.Tensor((1, seq_len, 4096), dtype="float16") = lv602_1[0]
            lv1082 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight8, model_layers_23_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv606 = R.call_tir(cls.NT_matmul5, (rms_norm436, lv1082), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape860: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv606, R.shape([1, seq_len, 48, 128]))
            reshape861: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape860, R.shape([seq_len, 48, 128]))
            lv1083 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape861), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape862: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1083, R.shape([1, seq_len, 32, 128]))
            reshape863: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape862, R.shape([1, seq_len, 4096]))
            lv1084 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight8, model_layers_23_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv607 = R.call_tir(cls.NT_matmul6, (reshape863, lv1084), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv604_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv607, lv603_1, model_layers_23_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv605_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv604_1[1]
            rms_norm437: R.Tensor((1, seq_len, 4096), dtype="float16") = lv604_1[0]
            lv1085 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight8, model_layers_23_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv608 = R.call_tir(cls.NT_matmul7, (rms_norm437, lv1085), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split215: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv608, indices_or_sections=2, axis=-1)
            split_0215: R.Tensor((1, seq_len, 14336), dtype="float16") = split215[0]
            split_1215: R.Tensor((1, seq_len, 14336), dtype="float16") = split215[1]
            silu215: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0215)
            mul215: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu215, split_1215)
            lv1086 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight8, model_layers_23_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv609 = R.call_tir(cls.NT_matmul8, (mul215, lv1086), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv606_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv609, lv605_1, model_layers_24_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv607_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv606_1[1]
            rms_norm438: R.Tensor((1, seq_len, 4096), dtype="float16") = lv606_1[0]
            lv1087 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight8, model_layers_24_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv610 = R.call_tir(cls.NT_matmul5, (rms_norm438, lv1087), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape864: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv610, R.shape([1, seq_len, 48, 128]))
            reshape865: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape864, R.shape([seq_len, 48, 128]))
            lv1088 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape865), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape866: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1088, R.shape([1, seq_len, 32, 128]))
            reshape867: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape866, R.shape([1, seq_len, 4096]))
            lv1089 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight8, model_layers_24_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv611 = R.call_tir(cls.NT_matmul6, (reshape867, lv1089), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv608_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv611, lv607_1, model_layers_24_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv609_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv608_1[1]
            rms_norm439: R.Tensor((1, seq_len, 4096), dtype="float16") = lv608_1[0]
            lv1090 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight8, model_layers_24_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv612 = R.call_tir(cls.NT_matmul7, (rms_norm439, lv1090), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split216: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv612, indices_or_sections=2, axis=-1)
            split_0216: R.Tensor((1, seq_len, 14336), dtype="float16") = split216[0]
            split_1216: R.Tensor((1, seq_len, 14336), dtype="float16") = split216[1]
            silu216: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0216)
            mul216: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu216, split_1216)
            lv1091 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight8, model_layers_24_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv613 = R.call_tir(cls.NT_matmul8, (mul216, lv1091), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv610_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv613, lv609_1, model_layers_25_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv611_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv610_1[1]
            rms_norm440: R.Tensor((1, seq_len, 4096), dtype="float16") = lv610_1[0]
            lv1092 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight8, model_layers_25_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv614 = R.call_tir(cls.NT_matmul5, (rms_norm440, lv1092), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape868: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv614, R.shape([1, seq_len, 48, 128]))
            reshape869: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape868, R.shape([seq_len, 48, 128]))
            lv1093 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape869), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape870: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1093, R.shape([1, seq_len, 32, 128]))
            reshape871: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape870, R.shape([1, seq_len, 4096]))
            lv1094 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight8, model_layers_25_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv615 = R.call_tir(cls.NT_matmul6, (reshape871, lv1094), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv612_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv615, lv611_1, model_layers_25_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv613_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv612_1[1]
            rms_norm441: R.Tensor((1, seq_len, 4096), dtype="float16") = lv612_1[0]
            lv1095 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight8, model_layers_25_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv616 = R.call_tir(cls.NT_matmul7, (rms_norm441, lv1095), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split217: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv616, indices_or_sections=2, axis=-1)
            split_0217: R.Tensor((1, seq_len, 14336), dtype="float16") = split217[0]
            split_1217: R.Tensor((1, seq_len, 14336), dtype="float16") = split217[1]
            silu217: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0217)
            mul217: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu217, split_1217)
            lv1096 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight8, model_layers_25_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv617 = R.call_tir(cls.NT_matmul8, (mul217, lv1096), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv614_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv617, lv613_1, model_layers_26_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv615_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv614_1[1]
            rms_norm442: R.Tensor((1, seq_len, 4096), dtype="float16") = lv614_1[0]
            lv1097 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight8, model_layers_26_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv618 = R.call_tir(cls.NT_matmul5, (rms_norm442, lv1097), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape872: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv618, R.shape([1, seq_len, 48, 128]))
            reshape873: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape872, R.shape([seq_len, 48, 128]))
            lv1098 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape873), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape874: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1098, R.shape([1, seq_len, 32, 128]))
            reshape875: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape874, R.shape([1, seq_len, 4096]))
            lv1099 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight8, model_layers_26_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv619 = R.call_tir(cls.NT_matmul6, (reshape875, lv1099), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv616_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv619, lv615_1, model_layers_26_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv617_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv616_1[1]
            rms_norm443: R.Tensor((1, seq_len, 4096), dtype="float16") = lv616_1[0]
            lv1100 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight8, model_layers_26_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv620 = R.call_tir(cls.NT_matmul7, (rms_norm443, lv1100), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split218: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv620, indices_or_sections=2, axis=-1)
            split_0218: R.Tensor((1, seq_len, 14336), dtype="float16") = split218[0]
            split_1218: R.Tensor((1, seq_len, 14336), dtype="float16") = split218[1]
            silu218: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0218)
            mul218: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu218, split_1218)
            lv1101 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight8, model_layers_26_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv621 = R.call_tir(cls.NT_matmul8, (mul218, lv1101), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv618_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv621, lv617_1, model_layers_27_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv619_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv618_1[1]
            rms_norm444: R.Tensor((1, seq_len, 4096), dtype="float16") = lv618_1[0]
            lv1102 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight8, model_layers_27_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv622 = R.call_tir(cls.NT_matmul5, (rms_norm444, lv1102), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape876: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv622, R.shape([1, seq_len, 48, 128]))
            reshape877: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape876, R.shape([seq_len, 48, 128]))
            lv1103 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape877), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape878: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1103, R.shape([1, seq_len, 32, 128]))
            reshape879: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape878, R.shape([1, seq_len, 4096]))
            lv1104 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight8, model_layers_27_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv623 = R.call_tir(cls.NT_matmul6, (reshape879, lv1104), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv620_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv623, lv619_1, model_layers_27_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv621_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv620_1[1]
            rms_norm445: R.Tensor((1, seq_len, 4096), dtype="float16") = lv620_1[0]
            lv1105 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight8, model_layers_27_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv624 = R.call_tir(cls.NT_matmul7, (rms_norm445, lv1105), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split219: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv624, indices_or_sections=2, axis=-1)
            split_0219: R.Tensor((1, seq_len, 14336), dtype="float16") = split219[0]
            split_1219: R.Tensor((1, seq_len, 14336), dtype="float16") = split219[1]
            silu219: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0219)
            mul219: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu219, split_1219)
            lv1106 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight8, model_layers_27_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv625 = R.call_tir(cls.NT_matmul8, (mul219, lv1106), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv622_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv625, lv621_1, model_layers_28_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv623_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv622_1[1]
            rms_norm446: R.Tensor((1, seq_len, 4096), dtype="float16") = lv622_1[0]
            lv1107 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight8, model_layers_28_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv626 = R.call_tir(cls.NT_matmul5, (rms_norm446, lv1107), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape880: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv626, R.shape([1, seq_len, 48, 128]))
            reshape881: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape880, R.shape([seq_len, 48, 128]))
            lv1108 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape881), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape882: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1108, R.shape([1, seq_len, 32, 128]))
            reshape883: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape882, R.shape([1, seq_len, 4096]))
            lv1109 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight8, model_layers_28_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv627 = R.call_tir(cls.NT_matmul6, (reshape883, lv1109), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv624_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv627, lv623_1, model_layers_28_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv625_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv624_1[1]
            rms_norm447: R.Tensor((1, seq_len, 4096), dtype="float16") = lv624_1[0]
            lv1110 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight8, model_layers_28_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv628 = R.call_tir(cls.NT_matmul7, (rms_norm447, lv1110), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split220: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv628, indices_or_sections=2, axis=-1)
            split_0220: R.Tensor((1, seq_len, 14336), dtype="float16") = split220[0]
            split_1220: R.Tensor((1, seq_len, 14336), dtype="float16") = split220[1]
            silu220: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0220)
            mul220: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu220, split_1220)
            lv1111 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight8, model_layers_28_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv629 = R.call_tir(cls.NT_matmul8, (mul220, lv1111), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv626_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv629, lv625_1, model_layers_29_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv627_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv626_1[1]
            rms_norm448: R.Tensor((1, seq_len, 4096), dtype="float16") = lv626_1[0]
            lv1112 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight8, model_layers_29_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv630 = R.call_tir(cls.NT_matmul5, (rms_norm448, lv1112), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape884: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv630, R.shape([1, seq_len, 48, 128]))
            reshape885: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape884, R.shape([seq_len, 48, 128]))
            lv1113 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape885), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape886: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1113, R.shape([1, seq_len, 32, 128]))
            reshape887: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape886, R.shape([1, seq_len, 4096]))
            lv1114 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight8, model_layers_29_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv631 = R.call_tir(cls.NT_matmul6, (reshape887, lv1114), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv628_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv631, lv627_1, model_layers_29_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv629_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv628_1[1]
            rms_norm449: R.Tensor((1, seq_len, 4096), dtype="float16") = lv628_1[0]
            lv1115 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight8, model_layers_29_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv632 = R.call_tir(cls.NT_matmul7, (rms_norm449, lv1115), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split221: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv632, indices_or_sections=2, axis=-1)
            split_0221: R.Tensor((1, seq_len, 14336), dtype="float16") = split221[0]
            split_1221: R.Tensor((1, seq_len, 14336), dtype="float16") = split221[1]
            silu221: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0221)
            mul221: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu221, split_1221)
            lv1116 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight8, model_layers_29_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv633 = R.call_tir(cls.NT_matmul8, (mul221, lv1116), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv630_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv633, lv629_1, model_layers_30_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv631_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv630_1[1]
            rms_norm450: R.Tensor((1, seq_len, 4096), dtype="float16") = lv630_1[0]
            lv1117 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight8, model_layers_30_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv634 = R.call_tir(cls.NT_matmul5, (rms_norm450, lv1117), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape888: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv634, R.shape([1, seq_len, 48, 128]))
            reshape889: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape888, R.shape([seq_len, 48, 128]))
            lv1118 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape889), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape890: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1118, R.shape([1, seq_len, 32, 128]))
            reshape891: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape890, R.shape([1, seq_len, 4096]))
            lv1119 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight8, model_layers_30_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv635 = R.call_tir(cls.NT_matmul6, (reshape891, lv1119), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv632_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv635, lv631_1, model_layers_30_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv633_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv632_1[1]
            rms_norm451: R.Tensor((1, seq_len, 4096), dtype="float16") = lv632_1[0]
            lv1120 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight8, model_layers_30_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv636 = R.call_tir(cls.NT_matmul7, (rms_norm451, lv1120), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split222: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv636, indices_or_sections=2, axis=-1)
            split_0222: R.Tensor((1, seq_len, 14336), dtype="float16") = split222[0]
            split_1222: R.Tensor((1, seq_len, 14336), dtype="float16") = split222[1]
            silu222: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0222)
            mul222: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu222, split_1222)
            lv1121 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight8, model_layers_30_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv637 = R.call_tir(cls.NT_matmul8, (mul222, lv1121), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv634_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv637, lv633_1, model_layers_31_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv635_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv634_1[1]
            rms_norm452: R.Tensor((1, seq_len, 4096), dtype="float16") = lv634_1[0]
            lv1122 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight8, model_layers_31_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv638 = R.call_tir(cls.NT_matmul5, (rms_norm452, lv1122), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape892: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv638, R.shape([1, seq_len, 48, 128]))
            reshape893: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape892, R.shape([seq_len, 48, 128]))
            lv1123 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape893), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape894: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1123, R.shape([1, seq_len, 32, 128]))
            reshape895: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape894, R.shape([1, seq_len, 4096]))
            lv1124 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight8, model_layers_31_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv639 = R.call_tir(cls.NT_matmul6, (reshape895, lv1124), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv636_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv639, lv635_1, model_layers_31_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv637_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv636_1[1]
            rms_norm453: R.Tensor((1, seq_len, 4096), dtype="float16") = lv636_1[0]
            lv1125 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight8, model_layers_31_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv640 = R.call_tir(cls.NT_matmul7, (rms_norm453, lv1125), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split223: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv640, indices_or_sections=2, axis=-1)
            split_0223: R.Tensor((1, seq_len, 14336), dtype="float16") = split223[0]
            split_1223: R.Tensor((1, seq_len, 14336), dtype="float16") = split223[1]
            silu223: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0223)
            mul223: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu223, split_1223)
            lv1126 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight8, model_layers_31_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv641 = R.call_tir(cls.NT_matmul8, (mul223, lv1126), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv638_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv641, lv637_1, model_norm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm454: R.Tensor((1, seq_len, 4096), dtype="float16") = lv638_1[0]
            lv1127 = R.call_tir(cls.dequantize, (lm_head_q_weight8, lm_head_q_scale8), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            lv642 = R.call_tir(cls.NT_matmul9, (rms_norm454, lv1127), out_sinfo=R.Tensor((1, seq_len, vocab_size), dtype="float16"))
            astype5: R.Tensor((1, seq_len, vocab_size), dtype="float32") = R.astype(lv642, dtype="float32")
            gv9: R.Tuple(R.Tensor((1, seq_len, vocab_size), dtype="float32"), R.Object) = astype5, paged_kv_cache
            R.output(gv9)
        return gv9

    @R.function
    def batch_verify_to_last_hidden_states(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size", "seq_len"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm585: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1448 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight11, model_layers_0_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv643 = R.call_tir(cls.NT_matmul5, (rms_norm585, lv1448), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1152: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv643, R.shape([1, seq_len, 48, 128]))
            reshape1153: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1152, R.shape([seq_len, 48, 128]))
            lv1449 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1153), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1154: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1449, R.shape([1, seq_len, 32, 128]))
            reshape1155: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1154, R.shape([1, seq_len, 4096]))
            lv1450 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight11, model_layers_0_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv644 = R.call_tir(cls.NT_matmul6, (reshape1155, lv1450), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv640 = R.call_tir(cls.fuse_add_norm_prefill, (lv644, input_embeds, model_layers_0_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv641: R.Tensor((1, seq_len, 4096), dtype="float16") = lv640[1]
            rms_norm586: R.Tensor((1, seq_len, 4096), dtype="float16") = lv640[0]
            lv1451 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight11, model_layers_0_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv645 = R.call_tir(cls.NT_matmul7, (rms_norm586, lv1451), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split288: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv645, indices_or_sections=2, axis=-1)
            split_0288: R.Tensor((1, seq_len, 14336), dtype="float16") = split288[0]
            split_1288: R.Tensor((1, seq_len, 14336), dtype="float16") = split288[1]
            silu288: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0288)
            mul288: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu288, split_1288)
            lv1452 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight11, model_layers_0_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv646 = R.call_tir(cls.NT_matmul8, (mul288, lv1452), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv642 = R.call_tir(cls.fuse_add_norm_prefill, (lv646, lv641, model_layers_1_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv643_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv642[1]
            rms_norm587: R.Tensor((1, seq_len, 4096), dtype="float16") = lv642[0]
            lv1453 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight11, model_layers_1_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv647 = R.call_tir(cls.NT_matmul5, (rms_norm587, lv1453), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1156: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv647, R.shape([1, seq_len, 48, 128]))
            reshape1157: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1156, R.shape([seq_len, 48, 128]))
            lv1454 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape1157), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1158: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1454, R.shape([1, seq_len, 32, 128]))
            reshape1159: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1158, R.shape([1, seq_len, 4096]))
            lv1455 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight11, model_layers_1_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv648 = R.call_tir(cls.NT_matmul6, (reshape1159, lv1455), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv644_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv648, lv643_1, model_layers_1_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv645_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv644_1[1]
            rms_norm588: R.Tensor((1, seq_len, 4096), dtype="float16") = lv644_1[0]
            lv1456 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight11, model_layers_1_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv649 = R.call_tir(cls.NT_matmul7, (rms_norm588, lv1456), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split289: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv649, indices_or_sections=2, axis=-1)
            split_0289: R.Tensor((1, seq_len, 14336), dtype="float16") = split289[0]
            split_1289: R.Tensor((1, seq_len, 14336), dtype="float16") = split289[1]
            silu289: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0289)
            mul289: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu289, split_1289)
            lv1457 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight11, model_layers_1_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv650 = R.call_tir(cls.NT_matmul8, (mul289, lv1457), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv646_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv650, lv645_1, model_layers_2_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv647_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv646_1[1]
            rms_norm589: R.Tensor((1, seq_len, 4096), dtype="float16") = lv646_1[0]
            lv1458 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight11, model_layers_2_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv651 = R.call_tir(cls.NT_matmul5, (rms_norm589, lv1458), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1160: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv651, R.shape([1, seq_len, 48, 128]))
            reshape1161: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1160, R.shape([seq_len, 48, 128]))
            lv1459 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape1161), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1162: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1459, R.shape([1, seq_len, 32, 128]))
            reshape1163: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1162, R.shape([1, seq_len, 4096]))
            lv1460 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight11, model_layers_2_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv652 = R.call_tir(cls.NT_matmul6, (reshape1163, lv1460), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv648_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv652, lv647_1, model_layers_2_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv649_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv648_1[1]
            rms_norm590: R.Tensor((1, seq_len, 4096), dtype="float16") = lv648_1[0]
            lv1461 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight11, model_layers_2_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv653 = R.call_tir(cls.NT_matmul7, (rms_norm590, lv1461), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split290: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv653, indices_or_sections=2, axis=-1)
            split_0290: R.Tensor((1, seq_len, 14336), dtype="float16") = split290[0]
            split_1290: R.Tensor((1, seq_len, 14336), dtype="float16") = split290[1]
            silu290: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0290)
            mul290: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu290, split_1290)
            lv1462 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight11, model_layers_2_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv654 = R.call_tir(cls.NT_matmul8, (mul290, lv1462), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv650_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv654, lv649_1, model_layers_3_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv651_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv650_1[1]
            rms_norm591: R.Tensor((1, seq_len, 4096), dtype="float16") = lv650_1[0]
            lv1463 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight11, model_layers_3_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv655 = R.call_tir(cls.NT_matmul5, (rms_norm591, lv1463), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1164: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv655, R.shape([1, seq_len, 48, 128]))
            reshape1165: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1164, R.shape([seq_len, 48, 128]))
            lv1464 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape1165), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1166: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1464, R.shape([1, seq_len, 32, 128]))
            reshape1167: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1166, R.shape([1, seq_len, 4096]))
            lv1465 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight11, model_layers_3_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv656 = R.call_tir(cls.NT_matmul6, (reshape1167, lv1465), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv652_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv656, lv651_1, model_layers_3_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv653_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv652_1[1]
            rms_norm592: R.Tensor((1, seq_len, 4096), dtype="float16") = lv652_1[0]
            lv1466 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight11, model_layers_3_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv657 = R.call_tir(cls.NT_matmul7, (rms_norm592, lv1466), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split291: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv657, indices_or_sections=2, axis=-1)
            split_0291: R.Tensor((1, seq_len, 14336), dtype="float16") = split291[0]
            split_1291: R.Tensor((1, seq_len, 14336), dtype="float16") = split291[1]
            silu291: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0291)
            mul291: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu291, split_1291)
            lv1467 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight11, model_layers_3_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv658 = R.call_tir(cls.NT_matmul8, (mul291, lv1467), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv654_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv658, lv653_1, model_layers_4_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv655_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv654_1[1]
            rms_norm593: R.Tensor((1, seq_len, 4096), dtype="float16") = lv654_1[0]
            lv1468 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight11, model_layers_4_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv659 = R.call_tir(cls.NT_matmul5, (rms_norm593, lv1468), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1168: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv659, R.shape([1, seq_len, 48, 128]))
            reshape1169: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1168, R.shape([seq_len, 48, 128]))
            lv1469 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape1169), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1170: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1469, R.shape([1, seq_len, 32, 128]))
            reshape1171: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1170, R.shape([1, seq_len, 4096]))
            lv1470 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight11, model_layers_4_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv660 = R.call_tir(cls.NT_matmul6, (reshape1171, lv1470), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv656_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv660, lv655_1, model_layers_4_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv657_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv656_1[1]
            rms_norm594: R.Tensor((1, seq_len, 4096), dtype="float16") = lv656_1[0]
            lv1471 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight11, model_layers_4_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv661 = R.call_tir(cls.NT_matmul7, (rms_norm594, lv1471), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split292: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv661, indices_or_sections=2, axis=-1)
            split_0292: R.Tensor((1, seq_len, 14336), dtype="float16") = split292[0]
            split_1292: R.Tensor((1, seq_len, 14336), dtype="float16") = split292[1]
            silu292: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0292)
            mul292: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu292, split_1292)
            lv1472 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight11, model_layers_4_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv662 = R.call_tir(cls.NT_matmul8, (mul292, lv1472), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv658_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv662, lv657_1, model_layers_5_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv659_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv658_1[1]
            rms_norm595: R.Tensor((1, seq_len, 4096), dtype="float16") = lv658_1[0]
            lv1473 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight11, model_layers_5_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv663 = R.call_tir(cls.NT_matmul5, (rms_norm595, lv1473), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1172: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv663, R.shape([1, seq_len, 48, 128]))
            reshape1173: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1172, R.shape([seq_len, 48, 128]))
            lv1474 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape1173), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1174: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1474, R.shape([1, seq_len, 32, 128]))
            reshape1175: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1174, R.shape([1, seq_len, 4096]))
            lv1475 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight11, model_layers_5_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv664 = R.call_tir(cls.NT_matmul6, (reshape1175, lv1475), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv660_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv664, lv659_1, model_layers_5_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv661_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv660_1[1]
            rms_norm596: R.Tensor((1, seq_len, 4096), dtype="float16") = lv660_1[0]
            lv1476 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight11, model_layers_5_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv665 = R.call_tir(cls.NT_matmul7, (rms_norm596, lv1476), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split293: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv665, indices_or_sections=2, axis=-1)
            split_0293: R.Tensor((1, seq_len, 14336), dtype="float16") = split293[0]
            split_1293: R.Tensor((1, seq_len, 14336), dtype="float16") = split293[1]
            silu293: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0293)
            mul293: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu293, split_1293)
            lv1477 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight11, model_layers_5_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv666 = R.call_tir(cls.NT_matmul8, (mul293, lv1477), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv662_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv666, lv661_1, model_layers_6_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv663_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv662_1[1]
            rms_norm597: R.Tensor((1, seq_len, 4096), dtype="float16") = lv662_1[0]
            lv1478 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight11, model_layers_6_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv667 = R.call_tir(cls.NT_matmul5, (rms_norm597, lv1478), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1176: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv667, R.shape([1, seq_len, 48, 128]))
            reshape1177: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1176, R.shape([seq_len, 48, 128]))
            lv1479 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape1177), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1178: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1479, R.shape([1, seq_len, 32, 128]))
            reshape1179: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1178, R.shape([1, seq_len, 4096]))
            lv1480 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight11, model_layers_6_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv668 = R.call_tir(cls.NT_matmul6, (reshape1179, lv1480), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv664_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv668, lv663_1, model_layers_6_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv665_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv664_1[1]
            rms_norm598: R.Tensor((1, seq_len, 4096), dtype="float16") = lv664_1[0]
            lv1481 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight11, model_layers_6_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv669 = R.call_tir(cls.NT_matmul7, (rms_norm598, lv1481), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split294: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv669, indices_or_sections=2, axis=-1)
            split_0294: R.Tensor((1, seq_len, 14336), dtype="float16") = split294[0]
            split_1294: R.Tensor((1, seq_len, 14336), dtype="float16") = split294[1]
            silu294: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0294)
            mul294: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu294, split_1294)
            lv1482 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight11, model_layers_6_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv670 = R.call_tir(cls.NT_matmul8, (mul294, lv1482), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv666_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv670, lv665_1, model_layers_7_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv667_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv666_1[1]
            rms_norm599: R.Tensor((1, seq_len, 4096), dtype="float16") = lv666_1[0]
            lv1483 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight11, model_layers_7_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv671 = R.call_tir(cls.NT_matmul5, (rms_norm599, lv1483), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1180: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv671, R.shape([1, seq_len, 48, 128]))
            reshape1181: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1180, R.shape([seq_len, 48, 128]))
            lv1484 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape1181), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1182: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1484, R.shape([1, seq_len, 32, 128]))
            reshape1183: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1182, R.shape([1, seq_len, 4096]))
            lv1485 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight11, model_layers_7_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv672 = R.call_tir(cls.NT_matmul6, (reshape1183, lv1485), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv668_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv672, lv667_1, model_layers_7_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv669_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv668_1[1]
            rms_norm600: R.Tensor((1, seq_len, 4096), dtype="float16") = lv668_1[0]
            lv1486 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight11, model_layers_7_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv673 = R.call_tir(cls.NT_matmul7, (rms_norm600, lv1486), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split295: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv673, indices_or_sections=2, axis=-1)
            split_0295: R.Tensor((1, seq_len, 14336), dtype="float16") = split295[0]
            split_1295: R.Tensor((1, seq_len, 14336), dtype="float16") = split295[1]
            silu295: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0295)
            mul295: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu295, split_1295)
            lv1487 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight11, model_layers_7_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv674 = R.call_tir(cls.NT_matmul8, (mul295, lv1487), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv670_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv674, lv669_1, model_layers_8_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv671_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv670_1[1]
            rms_norm601: R.Tensor((1, seq_len, 4096), dtype="float16") = lv670_1[0]
            lv1488 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight11, model_layers_8_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv675 = R.call_tir(cls.NT_matmul5, (rms_norm601, lv1488), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1184: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv675, R.shape([1, seq_len, 48, 128]))
            reshape1185: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1184, R.shape([seq_len, 48, 128]))
            lv1489 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape1185), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1186: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1489, R.shape([1, seq_len, 32, 128]))
            reshape1187: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1186, R.shape([1, seq_len, 4096]))
            lv1490 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight11, model_layers_8_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv676 = R.call_tir(cls.NT_matmul6, (reshape1187, lv1490), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv672_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv676, lv671_1, model_layers_8_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv673_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv672_1[1]
            rms_norm602: R.Tensor((1, seq_len, 4096), dtype="float16") = lv672_1[0]
            lv1491 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight11, model_layers_8_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv677 = R.call_tir(cls.NT_matmul7, (rms_norm602, lv1491), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split296: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv677, indices_or_sections=2, axis=-1)
            split_0296: R.Tensor((1, seq_len, 14336), dtype="float16") = split296[0]
            split_1296: R.Tensor((1, seq_len, 14336), dtype="float16") = split296[1]
            silu296: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0296)
            mul296: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu296, split_1296)
            lv1492 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight11, model_layers_8_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv678 = R.call_tir(cls.NT_matmul8, (mul296, lv1492), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv674_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv678, lv673_1, model_layers_9_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv675_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv674_1[1]
            rms_norm603: R.Tensor((1, seq_len, 4096), dtype="float16") = lv674_1[0]
            lv1493 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight11, model_layers_9_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv679 = R.call_tir(cls.NT_matmul5, (rms_norm603, lv1493), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1188: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv679, R.shape([1, seq_len, 48, 128]))
            reshape1189: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1188, R.shape([seq_len, 48, 128]))
            lv1494 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape1189), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1190: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1494, R.shape([1, seq_len, 32, 128]))
            reshape1191: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1190, R.shape([1, seq_len, 4096]))
            lv1495 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight11, model_layers_9_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv680 = R.call_tir(cls.NT_matmul6, (reshape1191, lv1495), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv676_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv680, lv675_1, model_layers_9_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv677_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv676_1[1]
            rms_norm604: R.Tensor((1, seq_len, 4096), dtype="float16") = lv676_1[0]
            lv1496 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight11, model_layers_9_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv681 = R.call_tir(cls.NT_matmul7, (rms_norm604, lv1496), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split297: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv681, indices_or_sections=2, axis=-1)
            split_0297: R.Tensor((1, seq_len, 14336), dtype="float16") = split297[0]
            split_1297: R.Tensor((1, seq_len, 14336), dtype="float16") = split297[1]
            silu297: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0297)
            mul297: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu297, split_1297)
            lv1497 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight11, model_layers_9_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv682 = R.call_tir(cls.NT_matmul8, (mul297, lv1497), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv678_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv682, lv677_1, model_layers_10_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv679_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv678_1[1]
            rms_norm605: R.Tensor((1, seq_len, 4096), dtype="float16") = lv678_1[0]
            lv1498 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight11, model_layers_10_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv683 = R.call_tir(cls.NT_matmul5, (rms_norm605, lv1498), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1192: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv683, R.shape([1, seq_len, 48, 128]))
            reshape1193: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1192, R.shape([seq_len, 48, 128]))
            lv1499 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape1193), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1194: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1499, R.shape([1, seq_len, 32, 128]))
            reshape1195: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1194, R.shape([1, seq_len, 4096]))
            lv1500 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight11, model_layers_10_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv684 = R.call_tir(cls.NT_matmul6, (reshape1195, lv1500), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv680_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv684, lv679_1, model_layers_10_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv681_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv680_1[1]
            rms_norm606: R.Tensor((1, seq_len, 4096), dtype="float16") = lv680_1[0]
            lv1501 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight11, model_layers_10_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv685 = R.call_tir(cls.NT_matmul7, (rms_norm606, lv1501), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split298: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv685, indices_or_sections=2, axis=-1)
            split_0298: R.Tensor((1, seq_len, 14336), dtype="float16") = split298[0]
            split_1298: R.Tensor((1, seq_len, 14336), dtype="float16") = split298[1]
            silu298: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0298)
            mul298: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu298, split_1298)
            lv1502 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight11, model_layers_10_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv686 = R.call_tir(cls.NT_matmul8, (mul298, lv1502), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv682_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv686, lv681_1, model_layers_11_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv683_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv682_1[1]
            rms_norm607: R.Tensor((1, seq_len, 4096), dtype="float16") = lv682_1[0]
            lv1503 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight11, model_layers_11_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv687 = R.call_tir(cls.NT_matmul5, (rms_norm607, lv1503), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1196: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv687, R.shape([1, seq_len, 48, 128]))
            reshape1197: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1196, R.shape([seq_len, 48, 128]))
            lv1504 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape1197), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1198: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1504, R.shape([1, seq_len, 32, 128]))
            reshape1199: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1198, R.shape([1, seq_len, 4096]))
            lv1505 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight11, model_layers_11_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv688 = R.call_tir(cls.NT_matmul6, (reshape1199, lv1505), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv684_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv688, lv683_1, model_layers_11_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv685_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv684_1[1]
            rms_norm608: R.Tensor((1, seq_len, 4096), dtype="float16") = lv684_1[0]
            lv1506 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight11, model_layers_11_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv689 = R.call_tir(cls.NT_matmul7, (rms_norm608, lv1506), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split299: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv689, indices_or_sections=2, axis=-1)
            split_0299: R.Tensor((1, seq_len, 14336), dtype="float16") = split299[0]
            split_1299: R.Tensor((1, seq_len, 14336), dtype="float16") = split299[1]
            silu299: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0299)
            mul299: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu299, split_1299)
            lv1507 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight11, model_layers_11_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv690 = R.call_tir(cls.NT_matmul8, (mul299, lv1507), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv686_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv690, lv685_1, model_layers_12_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv687_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv686_1[1]
            rms_norm609: R.Tensor((1, seq_len, 4096), dtype="float16") = lv686_1[0]
            lv1508 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight11, model_layers_12_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv691 = R.call_tir(cls.NT_matmul5, (rms_norm609, lv1508), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1200: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv691, R.shape([1, seq_len, 48, 128]))
            reshape1201: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1200, R.shape([seq_len, 48, 128]))
            lv1509 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape1201), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1202: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1509, R.shape([1, seq_len, 32, 128]))
            reshape1203: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1202, R.shape([1, seq_len, 4096]))
            lv1510 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight11, model_layers_12_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv692 = R.call_tir(cls.NT_matmul6, (reshape1203, lv1510), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv688_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv692, lv687_1, model_layers_12_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv689_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv688_1[1]
            rms_norm610: R.Tensor((1, seq_len, 4096), dtype="float16") = lv688_1[0]
            lv1511 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight11, model_layers_12_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv693 = R.call_tir(cls.NT_matmul7, (rms_norm610, lv1511), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split300: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv693, indices_or_sections=2, axis=-1)
            split_0300: R.Tensor((1, seq_len, 14336), dtype="float16") = split300[0]
            split_1300: R.Tensor((1, seq_len, 14336), dtype="float16") = split300[1]
            silu300: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0300)
            mul300: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu300, split_1300)
            lv1512 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight11, model_layers_12_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv694 = R.call_tir(cls.NT_matmul8, (mul300, lv1512), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv690_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv694, lv689_1, model_layers_13_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv691_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv690_1[1]
            rms_norm611: R.Tensor((1, seq_len, 4096), dtype="float16") = lv690_1[0]
            lv1513 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight11, model_layers_13_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv695 = R.call_tir(cls.NT_matmul5, (rms_norm611, lv1513), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1204: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv695, R.shape([1, seq_len, 48, 128]))
            reshape1205: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1204, R.shape([seq_len, 48, 128]))
            lv1514 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape1205), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1206: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1514, R.shape([1, seq_len, 32, 128]))
            reshape1207: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1206, R.shape([1, seq_len, 4096]))
            lv1515 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight11, model_layers_13_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv696 = R.call_tir(cls.NT_matmul6, (reshape1207, lv1515), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv692_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv696, lv691_1, model_layers_13_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv693_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv692_1[1]
            rms_norm612: R.Tensor((1, seq_len, 4096), dtype="float16") = lv692_1[0]
            lv1516 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight11, model_layers_13_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv697 = R.call_tir(cls.NT_matmul7, (rms_norm612, lv1516), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split301: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv697, indices_or_sections=2, axis=-1)
            split_0301: R.Tensor((1, seq_len, 14336), dtype="float16") = split301[0]
            split_1301: R.Tensor((1, seq_len, 14336), dtype="float16") = split301[1]
            silu301: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0301)
            mul301: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu301, split_1301)
            lv1517 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight11, model_layers_13_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv698 = R.call_tir(cls.NT_matmul8, (mul301, lv1517), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv694_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv698, lv693_1, model_layers_14_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv695_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv694_1[1]
            rms_norm613: R.Tensor((1, seq_len, 4096), dtype="float16") = lv694_1[0]
            lv1518 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight11, model_layers_14_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv699 = R.call_tir(cls.NT_matmul5, (rms_norm613, lv1518), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1208: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv699, R.shape([1, seq_len, 48, 128]))
            reshape1209: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1208, R.shape([seq_len, 48, 128]))
            lv1519 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape1209), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1210: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1519, R.shape([1, seq_len, 32, 128]))
            reshape1211: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1210, R.shape([1, seq_len, 4096]))
            lv1520 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight11, model_layers_14_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv700 = R.call_tir(cls.NT_matmul6, (reshape1211, lv1520), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv696_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv700, lv695_1, model_layers_14_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv697_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv696_1[1]
            rms_norm614: R.Tensor((1, seq_len, 4096), dtype="float16") = lv696_1[0]
            lv1521 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight11, model_layers_14_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv701 = R.call_tir(cls.NT_matmul7, (rms_norm614, lv1521), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split302: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv701, indices_or_sections=2, axis=-1)
            split_0302: R.Tensor((1, seq_len, 14336), dtype="float16") = split302[0]
            split_1302: R.Tensor((1, seq_len, 14336), dtype="float16") = split302[1]
            silu302: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0302)
            mul302: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu302, split_1302)
            lv1522 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight11, model_layers_14_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv702 = R.call_tir(cls.NT_matmul8, (mul302, lv1522), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv698_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv702, lv697_1, model_layers_15_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv699_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv698_1[1]
            rms_norm615: R.Tensor((1, seq_len, 4096), dtype="float16") = lv698_1[0]
            lv1523 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight11, model_layers_15_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv703 = R.call_tir(cls.NT_matmul5, (rms_norm615, lv1523), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1212: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv703, R.shape([1, seq_len, 48, 128]))
            reshape1213: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1212, R.shape([seq_len, 48, 128]))
            lv1524 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape1213), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1214: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1524, R.shape([1, seq_len, 32, 128]))
            reshape1215: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1214, R.shape([1, seq_len, 4096]))
            lv1525 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight11, model_layers_15_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv704 = R.call_tir(cls.NT_matmul6, (reshape1215, lv1525), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv700_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv704, lv699_1, model_layers_15_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv701_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv700_1[1]
            rms_norm616: R.Tensor((1, seq_len, 4096), dtype="float16") = lv700_1[0]
            lv1526 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight11, model_layers_15_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv705 = R.call_tir(cls.NT_matmul7, (rms_norm616, lv1526), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split303: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv705, indices_or_sections=2, axis=-1)
            split_0303: R.Tensor((1, seq_len, 14336), dtype="float16") = split303[0]
            split_1303: R.Tensor((1, seq_len, 14336), dtype="float16") = split303[1]
            silu303: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0303)
            mul303: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu303, split_1303)
            lv1527 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight11, model_layers_15_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv706 = R.call_tir(cls.NT_matmul8, (mul303, lv1527), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv702_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv706, lv701_1, model_layers_16_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv703_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv702_1[1]
            rms_norm617: R.Tensor((1, seq_len, 4096), dtype="float16") = lv702_1[0]
            lv1528 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight11, model_layers_16_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv707 = R.call_tir(cls.NT_matmul5, (rms_norm617, lv1528), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1216: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv707, R.shape([1, seq_len, 48, 128]))
            reshape1217: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1216, R.shape([seq_len, 48, 128]))
            lv1529 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape1217), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1218: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1529, R.shape([1, seq_len, 32, 128]))
            reshape1219: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1218, R.shape([1, seq_len, 4096]))
            lv1530 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight11, model_layers_16_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv708 = R.call_tir(cls.NT_matmul6, (reshape1219, lv1530), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv704_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv708, lv703_1, model_layers_16_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv705_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv704_1[1]
            rms_norm618: R.Tensor((1, seq_len, 4096), dtype="float16") = lv704_1[0]
            lv1531 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight11, model_layers_16_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv709 = R.call_tir(cls.NT_matmul7, (rms_norm618, lv1531), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split304: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv709, indices_or_sections=2, axis=-1)
            split_0304: R.Tensor((1, seq_len, 14336), dtype="float16") = split304[0]
            split_1304: R.Tensor((1, seq_len, 14336), dtype="float16") = split304[1]
            silu304: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0304)
            mul304: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu304, split_1304)
            lv1532 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight11, model_layers_16_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv710 = R.call_tir(cls.NT_matmul8, (mul304, lv1532), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv706_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv710, lv705_1, model_layers_17_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv707_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv706_1[1]
            rms_norm619: R.Tensor((1, seq_len, 4096), dtype="float16") = lv706_1[0]
            lv1533 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight11, model_layers_17_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv711 = R.call_tir(cls.NT_matmul5, (rms_norm619, lv1533), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1220: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv711, R.shape([1, seq_len, 48, 128]))
            reshape1221: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1220, R.shape([seq_len, 48, 128]))
            lv1534 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape1221), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1222: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1534, R.shape([1, seq_len, 32, 128]))
            reshape1223: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1222, R.shape([1, seq_len, 4096]))
            lv1535 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight11, model_layers_17_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv712 = R.call_tir(cls.NT_matmul6, (reshape1223, lv1535), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv708_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv712, lv707_1, model_layers_17_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv709_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv708_1[1]
            rms_norm620: R.Tensor((1, seq_len, 4096), dtype="float16") = lv708_1[0]
            lv1536 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight11, model_layers_17_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv713 = R.call_tir(cls.NT_matmul7, (rms_norm620, lv1536), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split305: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv713, indices_or_sections=2, axis=-1)
            split_0305: R.Tensor((1, seq_len, 14336), dtype="float16") = split305[0]
            split_1305: R.Tensor((1, seq_len, 14336), dtype="float16") = split305[1]
            silu305: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0305)
            mul305: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu305, split_1305)
            lv1537 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight11, model_layers_17_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv714 = R.call_tir(cls.NT_matmul8, (mul305, lv1537), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv710_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv714, lv709_1, model_layers_18_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv711_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv710_1[1]
            rms_norm621: R.Tensor((1, seq_len, 4096), dtype="float16") = lv710_1[0]
            lv1538 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight11, model_layers_18_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv715 = R.call_tir(cls.NT_matmul5, (rms_norm621, lv1538), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1224: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv715, R.shape([1, seq_len, 48, 128]))
            reshape1225: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1224, R.shape([seq_len, 48, 128]))
            lv1539 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape1225), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1226: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1539, R.shape([1, seq_len, 32, 128]))
            reshape1227: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1226, R.shape([1, seq_len, 4096]))
            lv1540 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight11, model_layers_18_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv716 = R.call_tir(cls.NT_matmul6, (reshape1227, lv1540), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv712_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv716, lv711_1, model_layers_18_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv713_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv712_1[1]
            rms_norm622: R.Tensor((1, seq_len, 4096), dtype="float16") = lv712_1[0]
            lv1541 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight11, model_layers_18_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv717 = R.call_tir(cls.NT_matmul7, (rms_norm622, lv1541), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split306: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv717, indices_or_sections=2, axis=-1)
            split_0306: R.Tensor((1, seq_len, 14336), dtype="float16") = split306[0]
            split_1306: R.Tensor((1, seq_len, 14336), dtype="float16") = split306[1]
            silu306: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0306)
            mul306: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu306, split_1306)
            lv1542 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight11, model_layers_18_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv718 = R.call_tir(cls.NT_matmul8, (mul306, lv1542), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv714_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv718, lv713_1, model_layers_19_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv715_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv714_1[1]
            rms_norm623: R.Tensor((1, seq_len, 4096), dtype="float16") = lv714_1[0]
            lv1543 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight11, model_layers_19_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv719 = R.call_tir(cls.NT_matmul5, (rms_norm623, lv1543), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1228: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv719, R.shape([1, seq_len, 48, 128]))
            reshape1229: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1228, R.shape([seq_len, 48, 128]))
            lv1544 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape1229), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1230: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1544, R.shape([1, seq_len, 32, 128]))
            reshape1231: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1230, R.shape([1, seq_len, 4096]))
            lv1545 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight11, model_layers_19_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv720 = R.call_tir(cls.NT_matmul6, (reshape1231, lv1545), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv716_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv720, lv715_1, model_layers_19_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv717_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv716_1[1]
            rms_norm624: R.Tensor((1, seq_len, 4096), dtype="float16") = lv716_1[0]
            lv1546 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight11, model_layers_19_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv721 = R.call_tir(cls.NT_matmul7, (rms_norm624, lv1546), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split307: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv721, indices_or_sections=2, axis=-1)
            split_0307: R.Tensor((1, seq_len, 14336), dtype="float16") = split307[0]
            split_1307: R.Tensor((1, seq_len, 14336), dtype="float16") = split307[1]
            silu307: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0307)
            mul307: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu307, split_1307)
            lv1547 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight11, model_layers_19_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv722 = R.call_tir(cls.NT_matmul8, (mul307, lv1547), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv718_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv722, lv717_1, model_layers_20_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv719_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv718_1[1]
            rms_norm625: R.Tensor((1, seq_len, 4096), dtype="float16") = lv718_1[0]
            lv1548 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight11, model_layers_20_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv723 = R.call_tir(cls.NT_matmul5, (rms_norm625, lv1548), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1232: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv723, R.shape([1, seq_len, 48, 128]))
            reshape1233: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1232, R.shape([seq_len, 48, 128]))
            lv1549 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape1233), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1234: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1549, R.shape([1, seq_len, 32, 128]))
            reshape1235: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1234, R.shape([1, seq_len, 4096]))
            lv1550 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight11, model_layers_20_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv724 = R.call_tir(cls.NT_matmul6, (reshape1235, lv1550), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv720_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv724, lv719_1, model_layers_20_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv721_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv720_1[1]
            rms_norm626: R.Tensor((1, seq_len, 4096), dtype="float16") = lv720_1[0]
            lv1551 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight11, model_layers_20_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv725 = R.call_tir(cls.NT_matmul7, (rms_norm626, lv1551), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split308: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv725, indices_or_sections=2, axis=-1)
            split_0308: R.Tensor((1, seq_len, 14336), dtype="float16") = split308[0]
            split_1308: R.Tensor((1, seq_len, 14336), dtype="float16") = split308[1]
            silu308: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0308)
            mul308: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu308, split_1308)
            lv1552 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight11, model_layers_20_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv726 = R.call_tir(cls.NT_matmul8, (mul308, lv1552), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv722_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv726, lv721_1, model_layers_21_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv723_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv722_1[1]
            rms_norm627: R.Tensor((1, seq_len, 4096), dtype="float16") = lv722_1[0]
            lv1553 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight11, model_layers_21_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv727 = R.call_tir(cls.NT_matmul5, (rms_norm627, lv1553), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1236: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv727, R.shape([1, seq_len, 48, 128]))
            reshape1237: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1236, R.shape([seq_len, 48, 128]))
            lv1554 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape1237), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1238: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1554, R.shape([1, seq_len, 32, 128]))
            reshape1239: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1238, R.shape([1, seq_len, 4096]))
            lv1555 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight11, model_layers_21_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv728 = R.call_tir(cls.NT_matmul6, (reshape1239, lv1555), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv724_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv728, lv723_1, model_layers_21_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv725_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv724_1[1]
            rms_norm628: R.Tensor((1, seq_len, 4096), dtype="float16") = lv724_1[0]
            lv1556 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight11, model_layers_21_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv729 = R.call_tir(cls.NT_matmul7, (rms_norm628, lv1556), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split309: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv729, indices_or_sections=2, axis=-1)
            split_0309: R.Tensor((1, seq_len, 14336), dtype="float16") = split309[0]
            split_1309: R.Tensor((1, seq_len, 14336), dtype="float16") = split309[1]
            silu309: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0309)
            mul309: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu309, split_1309)
            lv1557 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight11, model_layers_21_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv730 = R.call_tir(cls.NT_matmul8, (mul309, lv1557), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv726_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv730, lv725_1, model_layers_22_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv727_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv726_1[1]
            rms_norm629: R.Tensor((1, seq_len, 4096), dtype="float16") = lv726_1[0]
            lv1558 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight11, model_layers_22_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv731 = R.call_tir(cls.NT_matmul5, (rms_norm629, lv1558), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1240: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv731, R.shape([1, seq_len, 48, 128]))
            reshape1241: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1240, R.shape([seq_len, 48, 128]))
            lv1559 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape1241), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1242: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1559, R.shape([1, seq_len, 32, 128]))
            reshape1243: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1242, R.shape([1, seq_len, 4096]))
            lv1560 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight11, model_layers_22_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv732 = R.call_tir(cls.NT_matmul6, (reshape1243, lv1560), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv728_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv732, lv727_1, model_layers_22_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv729_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv728_1[1]
            rms_norm630: R.Tensor((1, seq_len, 4096), dtype="float16") = lv728_1[0]
            lv1561 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight11, model_layers_22_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv733 = R.call_tir(cls.NT_matmul7, (rms_norm630, lv1561), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split310: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv733, indices_or_sections=2, axis=-1)
            split_0310: R.Tensor((1, seq_len, 14336), dtype="float16") = split310[0]
            split_1310: R.Tensor((1, seq_len, 14336), dtype="float16") = split310[1]
            silu310: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0310)
            mul310: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu310, split_1310)
            lv1562 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight11, model_layers_22_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv734 = R.call_tir(cls.NT_matmul8, (mul310, lv1562), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv730_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv734, lv729_1, model_layers_23_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv731_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv730_1[1]
            rms_norm631: R.Tensor((1, seq_len, 4096), dtype="float16") = lv730_1[0]
            lv1563 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight11, model_layers_23_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv735 = R.call_tir(cls.NT_matmul5, (rms_norm631, lv1563), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1244: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv735, R.shape([1, seq_len, 48, 128]))
            reshape1245: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1244, R.shape([seq_len, 48, 128]))
            lv1564 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape1245), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1246: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1564, R.shape([1, seq_len, 32, 128]))
            reshape1247: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1246, R.shape([1, seq_len, 4096]))
            lv1565 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight11, model_layers_23_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv736 = R.call_tir(cls.NT_matmul6, (reshape1247, lv1565), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv732_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv736, lv731_1, model_layers_23_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv733_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv732_1[1]
            rms_norm632: R.Tensor((1, seq_len, 4096), dtype="float16") = lv732_1[0]
            lv1566 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight11, model_layers_23_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv737 = R.call_tir(cls.NT_matmul7, (rms_norm632, lv1566), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split311: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv737, indices_or_sections=2, axis=-1)
            split_0311: R.Tensor((1, seq_len, 14336), dtype="float16") = split311[0]
            split_1311: R.Tensor((1, seq_len, 14336), dtype="float16") = split311[1]
            silu311: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0311)
            mul311: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu311, split_1311)
            lv1567 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight11, model_layers_23_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv738 = R.call_tir(cls.NT_matmul8, (mul311, lv1567), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv734_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv738, lv733_1, model_layers_24_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv735_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv734_1[1]
            rms_norm633: R.Tensor((1, seq_len, 4096), dtype="float16") = lv734_1[0]
            lv1568 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight11, model_layers_24_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv739 = R.call_tir(cls.NT_matmul5, (rms_norm633, lv1568), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1248: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv739, R.shape([1, seq_len, 48, 128]))
            reshape1249: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1248, R.shape([seq_len, 48, 128]))
            lv1569 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape1249), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1250: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1569, R.shape([1, seq_len, 32, 128]))
            reshape1251: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1250, R.shape([1, seq_len, 4096]))
            lv1570 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight11, model_layers_24_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv740 = R.call_tir(cls.NT_matmul6, (reshape1251, lv1570), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv736_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv740, lv735_1, model_layers_24_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv737_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv736_1[1]
            rms_norm634: R.Tensor((1, seq_len, 4096), dtype="float16") = lv736_1[0]
            lv1571 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight11, model_layers_24_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv741 = R.call_tir(cls.NT_matmul7, (rms_norm634, lv1571), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split312: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv741, indices_or_sections=2, axis=-1)
            split_0312: R.Tensor((1, seq_len, 14336), dtype="float16") = split312[0]
            split_1312: R.Tensor((1, seq_len, 14336), dtype="float16") = split312[1]
            silu312: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0312)
            mul312: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu312, split_1312)
            lv1572 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight11, model_layers_24_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv742 = R.call_tir(cls.NT_matmul8, (mul312, lv1572), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv738_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv742, lv737_1, model_layers_25_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv739_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv738_1[1]
            rms_norm635: R.Tensor((1, seq_len, 4096), dtype="float16") = lv738_1[0]
            lv1573 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight11, model_layers_25_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv743 = R.call_tir(cls.NT_matmul5, (rms_norm635, lv1573), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1252: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv743, R.shape([1, seq_len, 48, 128]))
            reshape1253: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1252, R.shape([seq_len, 48, 128]))
            lv1574 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape1253), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1254: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1574, R.shape([1, seq_len, 32, 128]))
            reshape1255: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1254, R.shape([1, seq_len, 4096]))
            lv1575 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight11, model_layers_25_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv744 = R.call_tir(cls.NT_matmul6, (reshape1255, lv1575), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv740_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv744, lv739_1, model_layers_25_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv741_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv740_1[1]
            rms_norm636: R.Tensor((1, seq_len, 4096), dtype="float16") = lv740_1[0]
            lv1576 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight11, model_layers_25_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv745 = R.call_tir(cls.NT_matmul7, (rms_norm636, lv1576), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split313: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv745, indices_or_sections=2, axis=-1)
            split_0313: R.Tensor((1, seq_len, 14336), dtype="float16") = split313[0]
            split_1313: R.Tensor((1, seq_len, 14336), dtype="float16") = split313[1]
            silu313: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0313)
            mul313: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu313, split_1313)
            lv1577 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight11, model_layers_25_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv746 = R.call_tir(cls.NT_matmul8, (mul313, lv1577), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv742_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv746, lv741_1, model_layers_26_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv743_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv742_1[1]
            rms_norm637: R.Tensor((1, seq_len, 4096), dtype="float16") = lv742_1[0]
            lv1578 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight11, model_layers_26_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv747 = R.call_tir(cls.NT_matmul5, (rms_norm637, lv1578), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1256: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv747, R.shape([1, seq_len, 48, 128]))
            reshape1257: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1256, R.shape([seq_len, 48, 128]))
            lv1579 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1257), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1258: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1579, R.shape([1, seq_len, 32, 128]))
            reshape1259: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1258, R.shape([1, seq_len, 4096]))
            lv1580 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight11, model_layers_26_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv748 = R.call_tir(cls.NT_matmul6, (reshape1259, lv1580), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv744_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv748, lv743_1, model_layers_26_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv745_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv744_1[1]
            rms_norm638: R.Tensor((1, seq_len, 4096), dtype="float16") = lv744_1[0]
            lv1581 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight11, model_layers_26_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv749 = R.call_tir(cls.NT_matmul7, (rms_norm638, lv1581), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split314: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv749, indices_or_sections=2, axis=-1)
            split_0314: R.Tensor((1, seq_len, 14336), dtype="float16") = split314[0]
            split_1314: R.Tensor((1, seq_len, 14336), dtype="float16") = split314[1]
            silu314: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0314)
            mul314: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu314, split_1314)
            lv1582 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight11, model_layers_26_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv750 = R.call_tir(cls.NT_matmul8, (mul314, lv1582), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv746_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv750, lv745_1, model_layers_27_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv747_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv746_1[1]
            rms_norm639: R.Tensor((1, seq_len, 4096), dtype="float16") = lv746_1[0]
            lv1583 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight11, model_layers_27_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv751 = R.call_tir(cls.NT_matmul5, (rms_norm639, lv1583), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1260: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv751, R.shape([1, seq_len, 48, 128]))
            reshape1261: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1260, R.shape([seq_len, 48, 128]))
            lv1584 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1261), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1262: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1584, R.shape([1, seq_len, 32, 128]))
            reshape1263: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1262, R.shape([1, seq_len, 4096]))
            lv1585 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight11, model_layers_27_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv752 = R.call_tir(cls.NT_matmul6, (reshape1263, lv1585), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv748_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv752, lv747_1, model_layers_27_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv749_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv748_1[1]
            rms_norm640: R.Tensor((1, seq_len, 4096), dtype="float16") = lv748_1[0]
            lv1586 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight11, model_layers_27_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv753 = R.call_tir(cls.NT_matmul7, (rms_norm640, lv1586), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split315: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv753, indices_or_sections=2, axis=-1)
            split_0315: R.Tensor((1, seq_len, 14336), dtype="float16") = split315[0]
            split_1315: R.Tensor((1, seq_len, 14336), dtype="float16") = split315[1]
            silu315: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0315)
            mul315: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu315, split_1315)
            lv1587 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight11, model_layers_27_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv754 = R.call_tir(cls.NT_matmul8, (mul315, lv1587), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv750_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv754, lv749_1, model_layers_28_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv751_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv750_1[1]
            rms_norm641: R.Tensor((1, seq_len, 4096), dtype="float16") = lv750_1[0]
            lv1588 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight11, model_layers_28_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv755 = R.call_tir(cls.NT_matmul5, (rms_norm641, lv1588), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1264: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv755, R.shape([1, seq_len, 48, 128]))
            reshape1265: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1264, R.shape([seq_len, 48, 128]))
            lv1589 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1265), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1266: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1589, R.shape([1, seq_len, 32, 128]))
            reshape1267: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1266, R.shape([1, seq_len, 4096]))
            lv1590 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight11, model_layers_28_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv756 = R.call_tir(cls.NT_matmul6, (reshape1267, lv1590), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv752_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv756, lv751_1, model_layers_28_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv753_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv752_1[1]
            rms_norm642: R.Tensor((1, seq_len, 4096), dtype="float16") = lv752_1[0]
            lv1591 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight11, model_layers_28_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv757 = R.call_tir(cls.NT_matmul7, (rms_norm642, lv1591), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split316: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv757, indices_or_sections=2, axis=-1)
            split_0316: R.Tensor((1, seq_len, 14336), dtype="float16") = split316[0]
            split_1316: R.Tensor((1, seq_len, 14336), dtype="float16") = split316[1]
            silu316: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0316)
            mul316: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu316, split_1316)
            lv1592 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight11, model_layers_28_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv758 = R.call_tir(cls.NT_matmul8, (mul316, lv1592), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv754_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv758, lv753_1, model_layers_29_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv755_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv754_1[1]
            rms_norm643: R.Tensor((1, seq_len, 4096), dtype="float16") = lv754_1[0]
            lv1593 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight11, model_layers_29_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv759 = R.call_tir(cls.NT_matmul5, (rms_norm643, lv1593), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1268: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv759, R.shape([1, seq_len, 48, 128]))
            reshape1269: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1268, R.shape([seq_len, 48, 128]))
            lv1594 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1269), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1270: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1594, R.shape([1, seq_len, 32, 128]))
            reshape1271: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1270, R.shape([1, seq_len, 4096]))
            lv1595 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight11, model_layers_29_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv760 = R.call_tir(cls.NT_matmul6, (reshape1271, lv1595), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv756_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv760, lv755_1, model_layers_29_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv757_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv756_1[1]
            rms_norm644: R.Tensor((1, seq_len, 4096), dtype="float16") = lv756_1[0]
            lv1596 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight11, model_layers_29_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv761 = R.call_tir(cls.NT_matmul7, (rms_norm644, lv1596), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split317: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv761, indices_or_sections=2, axis=-1)
            split_0317: R.Tensor((1, seq_len, 14336), dtype="float16") = split317[0]
            split_1317: R.Tensor((1, seq_len, 14336), dtype="float16") = split317[1]
            silu317: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0317)
            mul317: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu317, split_1317)
            lv1597 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight11, model_layers_29_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv762 = R.call_tir(cls.NT_matmul8, (mul317, lv1597), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv758_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv762, lv757_1, model_layers_30_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv759_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv758_1[1]
            rms_norm645: R.Tensor((1, seq_len, 4096), dtype="float16") = lv758_1[0]
            lv1598 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight11, model_layers_30_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv763 = R.call_tir(cls.NT_matmul5, (rms_norm645, lv1598), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1272: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv763, R.shape([1, seq_len, 48, 128]))
            reshape1273: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1272, R.shape([seq_len, 48, 128]))
            lv1599 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1273), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1274: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1599, R.shape([1, seq_len, 32, 128]))
            reshape1275: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1274, R.shape([1, seq_len, 4096]))
            lv1600 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight11, model_layers_30_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv764 = R.call_tir(cls.NT_matmul6, (reshape1275, lv1600), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv760_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv764, lv759_1, model_layers_30_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv761_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv760_1[1]
            rms_norm646: R.Tensor((1, seq_len, 4096), dtype="float16") = lv760_1[0]
            lv1601 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight11, model_layers_30_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv765 = R.call_tir(cls.NT_matmul7, (rms_norm646, lv1601), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split318: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv765, indices_or_sections=2, axis=-1)
            split_0318: R.Tensor((1, seq_len, 14336), dtype="float16") = split318[0]
            split_1318: R.Tensor((1, seq_len, 14336), dtype="float16") = split318[1]
            silu318: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0318)
            mul318: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu318, split_1318)
            lv1602 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight11, model_layers_30_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv766 = R.call_tir(cls.NT_matmul8, (mul318, lv1602), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv762_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv766, lv761_1, model_layers_31_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv763_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv762_1[1]
            rms_norm647: R.Tensor((1, seq_len, 4096), dtype="float16") = lv762_1[0]
            lv1603 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight11, model_layers_31_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv767 = R.call_tir(cls.NT_matmul5, (rms_norm647, lv1603), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1276: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv767, R.shape([1, seq_len, 48, 128]))
            reshape1277: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1276, R.shape([seq_len, 48, 128]))
            lv1604 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1277), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1278: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1604, R.shape([1, seq_len, 32, 128]))
            reshape1279: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1278, R.shape([1, seq_len, 4096]))
            lv1605 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight11, model_layers_31_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv768 = R.call_tir(cls.NT_matmul6, (reshape1279, lv1605), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv764_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv768, lv763_1, model_layers_31_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv765_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv764_1[1]
            rms_norm648: R.Tensor((1, seq_len, 4096), dtype="float16") = lv764_1[0]
            lv1606 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight11, model_layers_31_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv769 = R.call_tir(cls.NT_matmul7, (rms_norm648, lv1606), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split319: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv769, indices_or_sections=2, axis=-1)
            split_0319: R.Tensor((1, seq_len, 14336), dtype="float16") = split319[0]
            split_1319: R.Tensor((1, seq_len, 14336), dtype="float16") = split319[1]
            silu319: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0319)
            mul319: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu319, split_1319)
            lv1607 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight11, model_layers_31_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv770 = R.call_tir(cls.NT_matmul8, (mul319, lv1607), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv766_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv770, lv765_1, model_norm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm649: R.Tensor((1, seq_len, 4096), dtype="float16") = lv766_1[0]
            gv12: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm649, paged_kv_cache
            R.output(gv12)
        return gv12

    @R.function
    def create_tir_paged_kv_cache(max_batch_size_: R.Shape(["max_batch_size"]), max_total_seq_len_: R.Shape(["max_total_seq_len"]), prefill_chunk_size_: R.Shape(["prefill_chunk_size"]), page_size_: R.Shape(["page_size"]), support_sliding_window_: R.Shape(["support_sliding_window"])) -> R.Object:
        max_batch_size = T.int64()
        max_total_seq_len = T.int64()
        prefill_chunk_size = T.int64()
        page_size = T.int64()
        support_sliding_window = T.int64()
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        gv: R.Tensor((), dtype="float16") = R.zeros(R.shape([]), dtype="float16")
        paged_kv_cache: R.Object = R.call_pure_packed("vm.builtin.paged_attention_kv_cache_create_reduced", R.shape([max_batch_size, max_total_seq_len, prefill_chunk_size, page_size, support_sliding_window]), R.shape([0, 32]), R.prim_value(32), R.prim_value(8), R.prim_value(128), R.prim_value(1), R.prim_value(1), R.prim_value(T.float32(500000.0)), gv, cls.tir_kv_cache_transpose_append, cls.batch_prefill_paged_kv, cls.batch_decode_paged_kv, cls.batch_prefill_paged_kv_sliding_window, cls.batch_decode_paged_kv_sliding_window, cls.batch_prefill_ragged_kv, cls.merge_state_inplace, cls.fused_rope, cls.copy_single_page, cls.tir_kv_cache_debug_get_kv, cls.compact_kv_copy, cls.batch_tree_attn, cls.tree_attn_paged_kv, R.prim_value(0), sinfo_args=(R.Object,))
        return paged_kv_cache

    @R.function
    def decode(input_embed: R.Tensor((1, 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight3: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale3: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm65: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv164 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight3, model_layers_0_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv771 = R.call_tir(cls.NT_matmul10, (rms_norm65, lv164), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape128: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv771, R.shape([1, 1, 48, 128]))
            reshape129: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape128, R.shape([1, 48, 128]))
            lv165 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape129), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape130: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv165, R.shape([1, 1, 32, 128]))
            reshape131: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape130, R.shape([1, 1, 4096]))
            lv166 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight3, model_layers_0_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv772 = R.call_tir(cls.NT_matmul11, (reshape131, lv166), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv768 = R.call_tir(cls.fuse_add_norm_prefill, (lv772, input_embed, model_layers_0_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv769: R.Tensor((1, 1, 4096), dtype="float16") = lv768[1]
            rms_norm66: R.Tensor((1, 1, 4096), dtype="float16") = lv768[0]
            lv167 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight3, model_layers_0_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv773 = R.call_tir(cls.NT_matmul12, (rms_norm66, lv167), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split32: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv773, indices_or_sections=2, axis=-1)
            split_032: R.Tensor((1, 1, 14336), dtype="float16") = split32[0]
            split_132: R.Tensor((1, 1, 14336), dtype="float16") = split32[1]
            silu32: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_032)
            mul32: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu32, split_132)
            lv168 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight3, model_layers_0_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv774 = R.call_tir(cls.NT_matmul13, (mul32, lv168), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv770 = R.call_tir(cls.fuse_add_norm_prefill, (lv774, lv769, model_layers_1_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv771_1: R.Tensor((1, 1, 4096), dtype="float16") = lv770[1]
            rms_norm67: R.Tensor((1, 1, 4096), dtype="float16") = lv770[0]
            lv169 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight3, model_layers_1_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv775 = R.call_tir(cls.NT_matmul10, (rms_norm67, lv169), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape132: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv775, R.shape([1, 1, 48, 128]))
            reshape133: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape132, R.shape([1, 48, 128]))
            lv170 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape133), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape134: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv170, R.shape([1, 1, 32, 128]))
            reshape135: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape134, R.shape([1, 1, 4096]))
            lv171 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight3, model_layers_1_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv776 = R.call_tir(cls.NT_matmul11, (reshape135, lv171), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv772_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv776, lv771_1, model_layers_1_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv773_1: R.Tensor((1, 1, 4096), dtype="float16") = lv772_1[1]
            rms_norm68: R.Tensor((1, 1, 4096), dtype="float16") = lv772_1[0]
            lv172 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight3, model_layers_1_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv777 = R.call_tir(cls.NT_matmul12, (rms_norm68, lv172), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split33: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv777, indices_or_sections=2, axis=-1)
            split_033: R.Tensor((1, 1, 14336), dtype="float16") = split33[0]
            split_133: R.Tensor((1, 1, 14336), dtype="float16") = split33[1]
            silu33: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_033)
            mul33: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu33, split_133)
            lv173 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight3, model_layers_1_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv778 = R.call_tir(cls.NT_matmul13, (mul33, lv173), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv774_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv778, lv773_1, model_layers_2_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv775_1: R.Tensor((1, 1, 4096), dtype="float16") = lv774_1[1]
            rms_norm69: R.Tensor((1, 1, 4096), dtype="float16") = lv774_1[0]
            lv174 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight3, model_layers_2_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv779 = R.call_tir(cls.NT_matmul10, (rms_norm69, lv174), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape136: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv779, R.shape([1, 1, 48, 128]))
            reshape137: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape136, R.shape([1, 48, 128]))
            lv175 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape137), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape138: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv175, R.shape([1, 1, 32, 128]))
            reshape139: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape138, R.shape([1, 1, 4096]))
            lv176 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight3, model_layers_2_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv780 = R.call_tir(cls.NT_matmul11, (reshape139, lv176), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv776_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv780, lv775_1, model_layers_2_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv777_1: R.Tensor((1, 1, 4096), dtype="float16") = lv776_1[1]
            rms_norm70: R.Tensor((1, 1, 4096), dtype="float16") = lv776_1[0]
            lv177 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight3, model_layers_2_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv781 = R.call_tir(cls.NT_matmul12, (rms_norm70, lv177), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split34: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv781, indices_or_sections=2, axis=-1)
            split_034: R.Tensor((1, 1, 14336), dtype="float16") = split34[0]
            split_134: R.Tensor((1, 1, 14336), dtype="float16") = split34[1]
            silu34: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_034)
            mul34: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu34, split_134)
            lv178 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight3, model_layers_2_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv782 = R.call_tir(cls.NT_matmul13, (mul34, lv178), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv778_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv782, lv777_1, model_layers_3_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv779_1: R.Tensor((1, 1, 4096), dtype="float16") = lv778_1[1]
            rms_norm71: R.Tensor((1, 1, 4096), dtype="float16") = lv778_1[0]
            lv179 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight3, model_layers_3_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv783 = R.call_tir(cls.NT_matmul10, (rms_norm71, lv179), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape140: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv783, R.shape([1, 1, 48, 128]))
            reshape141: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape140, R.shape([1, 48, 128]))
            lv180 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape141), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape142: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv180, R.shape([1, 1, 32, 128]))
            reshape143: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape142, R.shape([1, 1, 4096]))
            lv181 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight3, model_layers_3_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv784 = R.call_tir(cls.NT_matmul11, (reshape143, lv181), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv780_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv784, lv779_1, model_layers_3_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv781_1: R.Tensor((1, 1, 4096), dtype="float16") = lv780_1[1]
            rms_norm72: R.Tensor((1, 1, 4096), dtype="float16") = lv780_1[0]
            lv182 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight3, model_layers_3_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv785 = R.call_tir(cls.NT_matmul12, (rms_norm72, lv182), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split35: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv785, indices_or_sections=2, axis=-1)
            split_035: R.Tensor((1, 1, 14336), dtype="float16") = split35[0]
            split_135: R.Tensor((1, 1, 14336), dtype="float16") = split35[1]
            silu35: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_035)
            mul35: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu35, split_135)
            lv183 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight3, model_layers_3_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv786 = R.call_tir(cls.NT_matmul13, (mul35, lv183), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv782_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv786, lv781_1, model_layers_4_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv783_1: R.Tensor((1, 1, 4096), dtype="float16") = lv782_1[1]
            rms_norm73: R.Tensor((1, 1, 4096), dtype="float16") = lv782_1[0]
            lv184 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight3, model_layers_4_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv787 = R.call_tir(cls.NT_matmul10, (rms_norm73, lv184), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape144: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv787, R.shape([1, 1, 48, 128]))
            reshape145: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape144, R.shape([1, 48, 128]))
            lv185 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape145), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape146: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv185, R.shape([1, 1, 32, 128]))
            reshape147: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape146, R.shape([1, 1, 4096]))
            lv186 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight3, model_layers_4_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv788 = R.call_tir(cls.NT_matmul11, (reshape147, lv186), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv784_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv788, lv783_1, model_layers_4_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv785_1: R.Tensor((1, 1, 4096), dtype="float16") = lv784_1[1]
            rms_norm74: R.Tensor((1, 1, 4096), dtype="float16") = lv784_1[0]
            lv187 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight3, model_layers_4_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv789 = R.call_tir(cls.NT_matmul12, (rms_norm74, lv187), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split36: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv789, indices_or_sections=2, axis=-1)
            split_036: R.Tensor((1, 1, 14336), dtype="float16") = split36[0]
            split_136: R.Tensor((1, 1, 14336), dtype="float16") = split36[1]
            silu36: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_036)
            mul36: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu36, split_136)
            lv188 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight3, model_layers_4_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv790 = R.call_tir(cls.NT_matmul13, (mul36, lv188), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv786_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv790, lv785_1, model_layers_5_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv787_1: R.Tensor((1, 1, 4096), dtype="float16") = lv786_1[1]
            rms_norm75: R.Tensor((1, 1, 4096), dtype="float16") = lv786_1[0]
            lv189 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight3, model_layers_5_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv791 = R.call_tir(cls.NT_matmul10, (rms_norm75, lv189), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape148: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv791, R.shape([1, 1, 48, 128]))
            reshape149: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape148, R.shape([1, 48, 128]))
            lv190 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape149), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape150: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv190, R.shape([1, 1, 32, 128]))
            reshape151: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape150, R.shape([1, 1, 4096]))
            lv191 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight3, model_layers_5_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv792 = R.call_tir(cls.NT_matmul11, (reshape151, lv191), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv788_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv792, lv787_1, model_layers_5_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv789_1: R.Tensor((1, 1, 4096), dtype="float16") = lv788_1[1]
            rms_norm76: R.Tensor((1, 1, 4096), dtype="float16") = lv788_1[0]
            lv192 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight3, model_layers_5_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv793 = R.call_tir(cls.NT_matmul12, (rms_norm76, lv192), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split37: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv793, indices_or_sections=2, axis=-1)
            split_037: R.Tensor((1, 1, 14336), dtype="float16") = split37[0]
            split_137: R.Tensor((1, 1, 14336), dtype="float16") = split37[1]
            silu37: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_037)
            mul37: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu37, split_137)
            lv193 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight3, model_layers_5_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv794 = R.call_tir(cls.NT_matmul13, (mul37, lv193), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv790_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv794, lv789_1, model_layers_6_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv791_1: R.Tensor((1, 1, 4096), dtype="float16") = lv790_1[1]
            rms_norm77: R.Tensor((1, 1, 4096), dtype="float16") = lv790_1[0]
            lv194 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight3, model_layers_6_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv795 = R.call_tir(cls.NT_matmul10, (rms_norm77, lv194), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape152: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv795, R.shape([1, 1, 48, 128]))
            reshape153: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape152, R.shape([1, 48, 128]))
            lv195 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape153), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape154: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv195, R.shape([1, 1, 32, 128]))
            reshape155: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape154, R.shape([1, 1, 4096]))
            lv196 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight3, model_layers_6_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv796 = R.call_tir(cls.NT_matmul11, (reshape155, lv196), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv792_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv796, lv791_1, model_layers_6_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv793_1: R.Tensor((1, 1, 4096), dtype="float16") = lv792_1[1]
            rms_norm78: R.Tensor((1, 1, 4096), dtype="float16") = lv792_1[0]
            lv197 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight3, model_layers_6_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv797 = R.call_tir(cls.NT_matmul12, (rms_norm78, lv197), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split38: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv797, indices_or_sections=2, axis=-1)
            split_038: R.Tensor((1, 1, 14336), dtype="float16") = split38[0]
            split_138: R.Tensor((1, 1, 14336), dtype="float16") = split38[1]
            silu38: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_038)
            mul38: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu38, split_138)
            lv198 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight3, model_layers_6_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv798 = R.call_tir(cls.NT_matmul13, (mul38, lv198), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv794_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv798, lv793_1, model_layers_7_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv795_1: R.Tensor((1, 1, 4096), dtype="float16") = lv794_1[1]
            rms_norm79: R.Tensor((1, 1, 4096), dtype="float16") = lv794_1[0]
            lv199 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight3, model_layers_7_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv799 = R.call_tir(cls.NT_matmul10, (rms_norm79, lv199), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape156: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv799, R.shape([1, 1, 48, 128]))
            reshape157: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape156, R.shape([1, 48, 128]))
            lv200 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape157), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape158: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv200, R.shape([1, 1, 32, 128]))
            reshape159: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape158, R.shape([1, 1, 4096]))
            lv201 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight3, model_layers_7_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv800 = R.call_tir(cls.NT_matmul11, (reshape159, lv201), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv796_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv800, lv795_1, model_layers_7_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv797_1: R.Tensor((1, 1, 4096), dtype="float16") = lv796_1[1]
            rms_norm80: R.Tensor((1, 1, 4096), dtype="float16") = lv796_1[0]
            lv202 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight3, model_layers_7_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv801 = R.call_tir(cls.NT_matmul12, (rms_norm80, lv202), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split39: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv801, indices_or_sections=2, axis=-1)
            split_039: R.Tensor((1, 1, 14336), dtype="float16") = split39[0]
            split_139: R.Tensor((1, 1, 14336), dtype="float16") = split39[1]
            silu39: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_039)
            mul39: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu39, split_139)
            lv203 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight3, model_layers_7_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv802 = R.call_tir(cls.NT_matmul13, (mul39, lv203), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv798_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv802, lv797_1, model_layers_8_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv799_1: R.Tensor((1, 1, 4096), dtype="float16") = lv798_1[1]
            rms_norm81: R.Tensor((1, 1, 4096), dtype="float16") = lv798_1[0]
            lv204 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight3, model_layers_8_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv803 = R.call_tir(cls.NT_matmul10, (rms_norm81, lv204), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape160: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv803, R.shape([1, 1, 48, 128]))
            reshape161: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape160, R.shape([1, 48, 128]))
            lv205 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape161), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape162: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv205, R.shape([1, 1, 32, 128]))
            reshape163: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape162, R.shape([1, 1, 4096]))
            lv206 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight3, model_layers_8_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv804 = R.call_tir(cls.NT_matmul11, (reshape163, lv206), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv800_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv804, lv799_1, model_layers_8_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv801_1: R.Tensor((1, 1, 4096), dtype="float16") = lv800_1[1]
            rms_norm82: R.Tensor((1, 1, 4096), dtype="float16") = lv800_1[0]
            lv207 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight3, model_layers_8_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv805 = R.call_tir(cls.NT_matmul12, (rms_norm82, lv207), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split40: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv805, indices_or_sections=2, axis=-1)
            split_040: R.Tensor((1, 1, 14336), dtype="float16") = split40[0]
            split_140: R.Tensor((1, 1, 14336), dtype="float16") = split40[1]
            silu40: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_040)
            mul40: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu40, split_140)
            lv208 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight3, model_layers_8_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv806 = R.call_tir(cls.NT_matmul13, (mul40, lv208), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv802_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv806, lv801_1, model_layers_9_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv803_1: R.Tensor((1, 1, 4096), dtype="float16") = lv802_1[1]
            rms_norm83: R.Tensor((1, 1, 4096), dtype="float16") = lv802_1[0]
            lv209 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight3, model_layers_9_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv807 = R.call_tir(cls.NT_matmul10, (rms_norm83, lv209), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape164: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv807, R.shape([1, 1, 48, 128]))
            reshape165: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape164, R.shape([1, 48, 128]))
            lv210 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape165), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape166: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv210, R.shape([1, 1, 32, 128]))
            reshape167: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape166, R.shape([1, 1, 4096]))
            lv211 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight3, model_layers_9_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv808 = R.call_tir(cls.NT_matmul11, (reshape167, lv211), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv804_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv808, lv803_1, model_layers_9_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv805_1: R.Tensor((1, 1, 4096), dtype="float16") = lv804_1[1]
            rms_norm84: R.Tensor((1, 1, 4096), dtype="float16") = lv804_1[0]
            lv212 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight3, model_layers_9_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv809 = R.call_tir(cls.NT_matmul12, (rms_norm84, lv212), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split41: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv809, indices_or_sections=2, axis=-1)
            split_041: R.Tensor((1, 1, 14336), dtype="float16") = split41[0]
            split_141: R.Tensor((1, 1, 14336), dtype="float16") = split41[1]
            silu41: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_041)
            mul41: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu41, split_141)
            lv213 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight3, model_layers_9_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv810 = R.call_tir(cls.NT_matmul13, (mul41, lv213), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv806_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv810, lv805_1, model_layers_10_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv807_1: R.Tensor((1, 1, 4096), dtype="float16") = lv806_1[1]
            rms_norm85: R.Tensor((1, 1, 4096), dtype="float16") = lv806_1[0]
            lv214 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight3, model_layers_10_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv811 = R.call_tir(cls.NT_matmul10, (rms_norm85, lv214), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape168: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv811, R.shape([1, 1, 48, 128]))
            reshape169: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape168, R.shape([1, 48, 128]))
            lv215 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape169), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape170: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv215, R.shape([1, 1, 32, 128]))
            reshape171: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape170, R.shape([1, 1, 4096]))
            lv216 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight3, model_layers_10_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv812 = R.call_tir(cls.NT_matmul11, (reshape171, lv216), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv808_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv812, lv807_1, model_layers_10_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv809_1: R.Tensor((1, 1, 4096), dtype="float16") = lv808_1[1]
            rms_norm86: R.Tensor((1, 1, 4096), dtype="float16") = lv808_1[0]
            lv217 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight3, model_layers_10_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv813 = R.call_tir(cls.NT_matmul12, (rms_norm86, lv217), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split42: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv813, indices_or_sections=2, axis=-1)
            split_042: R.Tensor((1, 1, 14336), dtype="float16") = split42[0]
            split_142: R.Tensor((1, 1, 14336), dtype="float16") = split42[1]
            silu42: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_042)
            mul42: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu42, split_142)
            lv218 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight3, model_layers_10_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv814 = R.call_tir(cls.NT_matmul13, (mul42, lv218), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv810_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv814, lv809_1, model_layers_11_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv811_1: R.Tensor((1, 1, 4096), dtype="float16") = lv810_1[1]
            rms_norm87: R.Tensor((1, 1, 4096), dtype="float16") = lv810_1[0]
            lv219 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight3, model_layers_11_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv815 = R.call_tir(cls.NT_matmul10, (rms_norm87, lv219), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape172: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv815, R.shape([1, 1, 48, 128]))
            reshape173: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape172, R.shape([1, 48, 128]))
            lv220 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape173), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape174: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv220, R.shape([1, 1, 32, 128]))
            reshape175: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape174, R.shape([1, 1, 4096]))
            lv221 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight3, model_layers_11_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv816 = R.call_tir(cls.NT_matmul11, (reshape175, lv221), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv812_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv816, lv811_1, model_layers_11_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv813_1: R.Tensor((1, 1, 4096), dtype="float16") = lv812_1[1]
            rms_norm88: R.Tensor((1, 1, 4096), dtype="float16") = lv812_1[0]
            lv222 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight3, model_layers_11_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv817 = R.call_tir(cls.NT_matmul12, (rms_norm88, lv222), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split43: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv817, indices_or_sections=2, axis=-1)
            split_043: R.Tensor((1, 1, 14336), dtype="float16") = split43[0]
            split_143: R.Tensor((1, 1, 14336), dtype="float16") = split43[1]
            silu43: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_043)
            mul43: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu43, split_143)
            lv223 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight3, model_layers_11_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv818 = R.call_tir(cls.NT_matmul13, (mul43, lv223), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv814_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv818, lv813_1, model_layers_12_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv815_1: R.Tensor((1, 1, 4096), dtype="float16") = lv814_1[1]
            rms_norm89: R.Tensor((1, 1, 4096), dtype="float16") = lv814_1[0]
            lv224 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight3, model_layers_12_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv819 = R.call_tir(cls.NT_matmul10, (rms_norm89, lv224), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape176: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv819, R.shape([1, 1, 48, 128]))
            reshape177: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape176, R.shape([1, 48, 128]))
            lv225 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape177), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape178: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv225, R.shape([1, 1, 32, 128]))
            reshape179: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape178, R.shape([1, 1, 4096]))
            lv226 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight3, model_layers_12_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv820 = R.call_tir(cls.NT_matmul11, (reshape179, lv226), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv816_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv820, lv815_1, model_layers_12_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv817_1: R.Tensor((1, 1, 4096), dtype="float16") = lv816_1[1]
            rms_norm90: R.Tensor((1, 1, 4096), dtype="float16") = lv816_1[0]
            lv227 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight3, model_layers_12_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv821 = R.call_tir(cls.NT_matmul12, (rms_norm90, lv227), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split44: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv821, indices_or_sections=2, axis=-1)
            split_044: R.Tensor((1, 1, 14336), dtype="float16") = split44[0]
            split_144: R.Tensor((1, 1, 14336), dtype="float16") = split44[1]
            silu44: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_044)
            mul44: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu44, split_144)
            lv228 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight3, model_layers_12_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv822 = R.call_tir(cls.NT_matmul13, (mul44, lv228), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv818_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv822, lv817_1, model_layers_13_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv819_1: R.Tensor((1, 1, 4096), dtype="float16") = lv818_1[1]
            rms_norm91: R.Tensor((1, 1, 4096), dtype="float16") = lv818_1[0]
            lv229 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight3, model_layers_13_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv823 = R.call_tir(cls.NT_matmul10, (rms_norm91, lv229), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape180: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv823, R.shape([1, 1, 48, 128]))
            reshape181: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape180, R.shape([1, 48, 128]))
            lv230 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape181), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape182: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv230, R.shape([1, 1, 32, 128]))
            reshape183: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape182, R.shape([1, 1, 4096]))
            lv231 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight3, model_layers_13_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv824 = R.call_tir(cls.NT_matmul11, (reshape183, lv231), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv820_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv824, lv819_1, model_layers_13_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv821_1: R.Tensor((1, 1, 4096), dtype="float16") = lv820_1[1]
            rms_norm92: R.Tensor((1, 1, 4096), dtype="float16") = lv820_1[0]
            lv232 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight3, model_layers_13_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv825 = R.call_tir(cls.NT_matmul12, (rms_norm92, lv232), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split45: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv825, indices_or_sections=2, axis=-1)
            split_045: R.Tensor((1, 1, 14336), dtype="float16") = split45[0]
            split_145: R.Tensor((1, 1, 14336), dtype="float16") = split45[1]
            silu45: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_045)
            mul45: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu45, split_145)
            lv233 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight3, model_layers_13_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv826 = R.call_tir(cls.NT_matmul13, (mul45, lv233), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv822_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv826, lv821_1, model_layers_14_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv823_1: R.Tensor((1, 1, 4096), dtype="float16") = lv822_1[1]
            rms_norm93: R.Tensor((1, 1, 4096), dtype="float16") = lv822_1[0]
            lv234 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight3, model_layers_14_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv827 = R.call_tir(cls.NT_matmul10, (rms_norm93, lv234), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape184: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv827, R.shape([1, 1, 48, 128]))
            reshape185: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape184, R.shape([1, 48, 128]))
            lv235 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape185), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape186: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv235, R.shape([1, 1, 32, 128]))
            reshape187: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape186, R.shape([1, 1, 4096]))
            lv236 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight3, model_layers_14_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv828 = R.call_tir(cls.NT_matmul11, (reshape187, lv236), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv824_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv828, lv823_1, model_layers_14_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv825_1: R.Tensor((1, 1, 4096), dtype="float16") = lv824_1[1]
            rms_norm94: R.Tensor((1, 1, 4096), dtype="float16") = lv824_1[0]
            lv237 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight3, model_layers_14_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv829 = R.call_tir(cls.NT_matmul12, (rms_norm94, lv237), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split46: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv829, indices_or_sections=2, axis=-1)
            split_046: R.Tensor((1, 1, 14336), dtype="float16") = split46[0]
            split_146: R.Tensor((1, 1, 14336), dtype="float16") = split46[1]
            silu46: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_046)
            mul46: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu46, split_146)
            lv238 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight3, model_layers_14_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv830 = R.call_tir(cls.NT_matmul13, (mul46, lv238), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv826_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv830, lv825_1, model_layers_15_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv827_1: R.Tensor((1, 1, 4096), dtype="float16") = lv826_1[1]
            rms_norm95: R.Tensor((1, 1, 4096), dtype="float16") = lv826_1[0]
            lv239 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight3, model_layers_15_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv831 = R.call_tir(cls.NT_matmul10, (rms_norm95, lv239), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape188: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv831, R.shape([1, 1, 48, 128]))
            reshape189: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape188, R.shape([1, 48, 128]))
            lv240 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape189), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape190: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv240, R.shape([1, 1, 32, 128]))
            reshape191: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape190, R.shape([1, 1, 4096]))
            lv241 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight3, model_layers_15_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv832 = R.call_tir(cls.NT_matmul11, (reshape191, lv241), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv828_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv832, lv827_1, model_layers_15_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv829_1: R.Tensor((1, 1, 4096), dtype="float16") = lv828_1[1]
            rms_norm96: R.Tensor((1, 1, 4096), dtype="float16") = lv828_1[0]
            lv242 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight3, model_layers_15_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv833 = R.call_tir(cls.NT_matmul12, (rms_norm96, lv242), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split47: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv833, indices_or_sections=2, axis=-1)
            split_047: R.Tensor((1, 1, 14336), dtype="float16") = split47[0]
            split_147: R.Tensor((1, 1, 14336), dtype="float16") = split47[1]
            silu47: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_047)
            mul47: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu47, split_147)
            lv243 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight3, model_layers_15_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv834 = R.call_tir(cls.NT_matmul13, (mul47, lv243), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv830_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv834, lv829_1, model_layers_16_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv831_1: R.Tensor((1, 1, 4096), dtype="float16") = lv830_1[1]
            rms_norm97: R.Tensor((1, 1, 4096), dtype="float16") = lv830_1[0]
            lv244 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight3, model_layers_16_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv835 = R.call_tir(cls.NT_matmul10, (rms_norm97, lv244), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape192: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv835, R.shape([1, 1, 48, 128]))
            reshape193: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape192, R.shape([1, 48, 128]))
            lv245 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape193), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape194: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv245, R.shape([1, 1, 32, 128]))
            reshape195: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape194, R.shape([1, 1, 4096]))
            lv246 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight3, model_layers_16_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv836 = R.call_tir(cls.NT_matmul11, (reshape195, lv246), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv832_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv836, lv831_1, model_layers_16_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv833_1: R.Tensor((1, 1, 4096), dtype="float16") = lv832_1[1]
            rms_norm98: R.Tensor((1, 1, 4096), dtype="float16") = lv832_1[0]
            lv247 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight3, model_layers_16_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv837 = R.call_tir(cls.NT_matmul12, (rms_norm98, lv247), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split48: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv837, indices_or_sections=2, axis=-1)
            split_048: R.Tensor((1, 1, 14336), dtype="float16") = split48[0]
            split_148: R.Tensor((1, 1, 14336), dtype="float16") = split48[1]
            silu48: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_048)
            mul48: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu48, split_148)
            lv248 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight3, model_layers_16_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv838 = R.call_tir(cls.NT_matmul13, (mul48, lv248), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv834_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv838, lv833_1, model_layers_17_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv835_1: R.Tensor((1, 1, 4096), dtype="float16") = lv834_1[1]
            rms_norm99: R.Tensor((1, 1, 4096), dtype="float16") = lv834_1[0]
            lv249 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight3, model_layers_17_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv839 = R.call_tir(cls.NT_matmul10, (rms_norm99, lv249), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape196: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv839, R.shape([1, 1, 48, 128]))
            reshape197: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape196, R.shape([1, 48, 128]))
            lv250 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape197), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape198: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv250, R.shape([1, 1, 32, 128]))
            reshape199: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape198, R.shape([1, 1, 4096]))
            lv251 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight3, model_layers_17_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv840 = R.call_tir(cls.NT_matmul11, (reshape199, lv251), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv836_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv840, lv835_1, model_layers_17_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv837_1: R.Tensor((1, 1, 4096), dtype="float16") = lv836_1[1]
            rms_norm100: R.Tensor((1, 1, 4096), dtype="float16") = lv836_1[0]
            lv252 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight3, model_layers_17_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv841 = R.call_tir(cls.NT_matmul12, (rms_norm100, lv252), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split49: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv841, indices_or_sections=2, axis=-1)
            split_049: R.Tensor((1, 1, 14336), dtype="float16") = split49[0]
            split_149: R.Tensor((1, 1, 14336), dtype="float16") = split49[1]
            silu49: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_049)
            mul49: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu49, split_149)
            lv253 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight3, model_layers_17_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv842 = R.call_tir(cls.NT_matmul13, (mul49, lv253), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv838_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv842, lv837_1, model_layers_18_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv839_1: R.Tensor((1, 1, 4096), dtype="float16") = lv838_1[1]
            rms_norm101: R.Tensor((1, 1, 4096), dtype="float16") = lv838_1[0]
            lv254 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight3, model_layers_18_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv843 = R.call_tir(cls.NT_matmul10, (rms_norm101, lv254), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape200: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv843, R.shape([1, 1, 48, 128]))
            reshape201: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape200, R.shape([1, 48, 128]))
            lv255 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape201), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape202: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv255, R.shape([1, 1, 32, 128]))
            reshape203: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape202, R.shape([1, 1, 4096]))
            lv256 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight3, model_layers_18_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv844 = R.call_tir(cls.NT_matmul11, (reshape203, lv256), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv840_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv844, lv839_1, model_layers_18_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv841_1: R.Tensor((1, 1, 4096), dtype="float16") = lv840_1[1]
            rms_norm102: R.Tensor((1, 1, 4096), dtype="float16") = lv840_1[0]
            lv257 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight3, model_layers_18_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv845 = R.call_tir(cls.NT_matmul12, (rms_norm102, lv257), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split50: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv845, indices_or_sections=2, axis=-1)
            split_050: R.Tensor((1, 1, 14336), dtype="float16") = split50[0]
            split_150: R.Tensor((1, 1, 14336), dtype="float16") = split50[1]
            silu50: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_050)
            mul50: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu50, split_150)
            lv258 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight3, model_layers_18_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv846 = R.call_tir(cls.NT_matmul13, (mul50, lv258), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv842_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv846, lv841_1, model_layers_19_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv843_1: R.Tensor((1, 1, 4096), dtype="float16") = lv842_1[1]
            rms_norm103: R.Tensor((1, 1, 4096), dtype="float16") = lv842_1[0]
            lv259 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight3, model_layers_19_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv847 = R.call_tir(cls.NT_matmul10, (rms_norm103, lv259), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape204: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv847, R.shape([1, 1, 48, 128]))
            reshape205: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape204, R.shape([1, 48, 128]))
            lv260 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape205), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape206: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv260, R.shape([1, 1, 32, 128]))
            reshape207: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape206, R.shape([1, 1, 4096]))
            lv261 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight3, model_layers_19_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv848 = R.call_tir(cls.NT_matmul11, (reshape207, lv261), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv844_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv848, lv843_1, model_layers_19_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv845_1: R.Tensor((1, 1, 4096), dtype="float16") = lv844_1[1]
            rms_norm104: R.Tensor((1, 1, 4096), dtype="float16") = lv844_1[0]
            lv262 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight3, model_layers_19_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv849 = R.call_tir(cls.NT_matmul12, (rms_norm104, lv262), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split51: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv849, indices_or_sections=2, axis=-1)
            split_051: R.Tensor((1, 1, 14336), dtype="float16") = split51[0]
            split_151: R.Tensor((1, 1, 14336), dtype="float16") = split51[1]
            silu51: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_051)
            mul51: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu51, split_151)
            lv263 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight3, model_layers_19_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv850 = R.call_tir(cls.NT_matmul13, (mul51, lv263), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv846_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv850, lv845_1, model_layers_20_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv847_1: R.Tensor((1, 1, 4096), dtype="float16") = lv846_1[1]
            rms_norm105: R.Tensor((1, 1, 4096), dtype="float16") = lv846_1[0]
            lv264 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight3, model_layers_20_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv851 = R.call_tir(cls.NT_matmul10, (rms_norm105, lv264), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape208: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv851, R.shape([1, 1, 48, 128]))
            reshape209: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape208, R.shape([1, 48, 128]))
            lv265 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape209), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape210: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv265, R.shape([1, 1, 32, 128]))
            reshape211: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape210, R.shape([1, 1, 4096]))
            lv266 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight3, model_layers_20_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv852 = R.call_tir(cls.NT_matmul11, (reshape211, lv266), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv848_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv852, lv847_1, model_layers_20_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv849_1: R.Tensor((1, 1, 4096), dtype="float16") = lv848_1[1]
            rms_norm106: R.Tensor((1, 1, 4096), dtype="float16") = lv848_1[0]
            lv267 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight3, model_layers_20_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv853 = R.call_tir(cls.NT_matmul12, (rms_norm106, lv267), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split52: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv853, indices_or_sections=2, axis=-1)
            split_052: R.Tensor((1, 1, 14336), dtype="float16") = split52[0]
            split_152: R.Tensor((1, 1, 14336), dtype="float16") = split52[1]
            silu52: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_052)
            mul52: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu52, split_152)
            lv268 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight3, model_layers_20_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv854 = R.call_tir(cls.NT_matmul13, (mul52, lv268), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv850_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv854, lv849_1, model_layers_21_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv851_1: R.Tensor((1, 1, 4096), dtype="float16") = lv850_1[1]
            rms_norm107: R.Tensor((1, 1, 4096), dtype="float16") = lv850_1[0]
            lv269 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight3, model_layers_21_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv855 = R.call_tir(cls.NT_matmul10, (rms_norm107, lv269), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape212: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv855, R.shape([1, 1, 48, 128]))
            reshape213: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape212, R.shape([1, 48, 128]))
            lv270 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape213), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape214: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv270, R.shape([1, 1, 32, 128]))
            reshape215: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape214, R.shape([1, 1, 4096]))
            lv271 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight3, model_layers_21_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv856 = R.call_tir(cls.NT_matmul11, (reshape215, lv271), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv852_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv856, lv851_1, model_layers_21_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv853_1: R.Tensor((1, 1, 4096), dtype="float16") = lv852_1[1]
            rms_norm108: R.Tensor((1, 1, 4096), dtype="float16") = lv852_1[0]
            lv272 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight3, model_layers_21_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv857 = R.call_tir(cls.NT_matmul12, (rms_norm108, lv272), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split53: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv857, indices_or_sections=2, axis=-1)
            split_053: R.Tensor((1, 1, 14336), dtype="float16") = split53[0]
            split_153: R.Tensor((1, 1, 14336), dtype="float16") = split53[1]
            silu53: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_053)
            mul53: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu53, split_153)
            lv273 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight3, model_layers_21_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv858 = R.call_tir(cls.NT_matmul13, (mul53, lv273), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv854_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv858, lv853_1, model_layers_22_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv855_1: R.Tensor((1, 1, 4096), dtype="float16") = lv854_1[1]
            rms_norm109: R.Tensor((1, 1, 4096), dtype="float16") = lv854_1[0]
            lv274 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight3, model_layers_22_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv859 = R.call_tir(cls.NT_matmul10, (rms_norm109, lv274), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape216: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv859, R.shape([1, 1, 48, 128]))
            reshape217: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape216, R.shape([1, 48, 128]))
            lv275 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape217), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape218: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv275, R.shape([1, 1, 32, 128]))
            reshape219: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape218, R.shape([1, 1, 4096]))
            lv276 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight3, model_layers_22_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv860 = R.call_tir(cls.NT_matmul11, (reshape219, lv276), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv856_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv860, lv855_1, model_layers_22_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv857_1: R.Tensor((1, 1, 4096), dtype="float16") = lv856_1[1]
            rms_norm110: R.Tensor((1, 1, 4096), dtype="float16") = lv856_1[0]
            lv277 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight3, model_layers_22_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv861 = R.call_tir(cls.NT_matmul12, (rms_norm110, lv277), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split54: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv861, indices_or_sections=2, axis=-1)
            split_054: R.Tensor((1, 1, 14336), dtype="float16") = split54[0]
            split_154: R.Tensor((1, 1, 14336), dtype="float16") = split54[1]
            silu54: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_054)
            mul54: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu54, split_154)
            lv278 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight3, model_layers_22_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv862 = R.call_tir(cls.NT_matmul13, (mul54, lv278), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv858_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv862, lv857_1, model_layers_23_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv859_1: R.Tensor((1, 1, 4096), dtype="float16") = lv858_1[1]
            rms_norm111: R.Tensor((1, 1, 4096), dtype="float16") = lv858_1[0]
            lv279 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight3, model_layers_23_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv863 = R.call_tir(cls.NT_matmul10, (rms_norm111, lv279), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape220: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv863, R.shape([1, 1, 48, 128]))
            reshape221: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape220, R.shape([1, 48, 128]))
            lv280 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape221), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape222: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv280, R.shape([1, 1, 32, 128]))
            reshape223: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape222, R.shape([1, 1, 4096]))
            lv281 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight3, model_layers_23_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv864 = R.call_tir(cls.NT_matmul11, (reshape223, lv281), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv860_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv864, lv859_1, model_layers_23_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv861_1: R.Tensor((1, 1, 4096), dtype="float16") = lv860_1[1]
            rms_norm112: R.Tensor((1, 1, 4096), dtype="float16") = lv860_1[0]
            lv282 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight3, model_layers_23_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv865 = R.call_tir(cls.NT_matmul12, (rms_norm112, lv282), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split55: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv865, indices_or_sections=2, axis=-1)
            split_055: R.Tensor((1, 1, 14336), dtype="float16") = split55[0]
            split_155: R.Tensor((1, 1, 14336), dtype="float16") = split55[1]
            silu55: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_055)
            mul55: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu55, split_155)
            lv283 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight3, model_layers_23_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv866 = R.call_tir(cls.NT_matmul13, (mul55, lv283), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv862_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv866, lv861_1, model_layers_24_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv863_1: R.Tensor((1, 1, 4096), dtype="float16") = lv862_1[1]
            rms_norm113: R.Tensor((1, 1, 4096), dtype="float16") = lv862_1[0]
            lv284 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight3, model_layers_24_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv867 = R.call_tir(cls.NT_matmul10, (rms_norm113, lv284), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape224: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv867, R.shape([1, 1, 48, 128]))
            reshape225: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape224, R.shape([1, 48, 128]))
            lv285 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape225), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape226: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv285, R.shape([1, 1, 32, 128]))
            reshape227: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape226, R.shape([1, 1, 4096]))
            lv286 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight3, model_layers_24_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv868 = R.call_tir(cls.NT_matmul11, (reshape227, lv286), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv864_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv868, lv863_1, model_layers_24_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv865_1: R.Tensor((1, 1, 4096), dtype="float16") = lv864_1[1]
            rms_norm114: R.Tensor((1, 1, 4096), dtype="float16") = lv864_1[0]
            lv287 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight3, model_layers_24_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv869 = R.call_tir(cls.NT_matmul12, (rms_norm114, lv287), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split56: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv869, indices_or_sections=2, axis=-1)
            split_056: R.Tensor((1, 1, 14336), dtype="float16") = split56[0]
            split_156: R.Tensor((1, 1, 14336), dtype="float16") = split56[1]
            silu56: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_056)
            mul56: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu56, split_156)
            lv288 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight3, model_layers_24_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv870 = R.call_tir(cls.NT_matmul13, (mul56, lv288), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv866_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv870, lv865_1, model_layers_25_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv867_1: R.Tensor((1, 1, 4096), dtype="float16") = lv866_1[1]
            rms_norm115: R.Tensor((1, 1, 4096), dtype="float16") = lv866_1[0]
            lv289 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight3, model_layers_25_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv871 = R.call_tir(cls.NT_matmul10, (rms_norm115, lv289), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape228: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv871, R.shape([1, 1, 48, 128]))
            reshape229: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape228, R.shape([1, 48, 128]))
            lv290 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape229), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape230: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv290, R.shape([1, 1, 32, 128]))
            reshape231: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape230, R.shape([1, 1, 4096]))
            lv291 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight3, model_layers_25_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv872 = R.call_tir(cls.NT_matmul11, (reshape231, lv291), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv868_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv872, lv867_1, model_layers_25_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv869_1: R.Tensor((1, 1, 4096), dtype="float16") = lv868_1[1]
            rms_norm116: R.Tensor((1, 1, 4096), dtype="float16") = lv868_1[0]
            lv292 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight3, model_layers_25_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv873 = R.call_tir(cls.NT_matmul12, (rms_norm116, lv292), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split57: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv873, indices_or_sections=2, axis=-1)
            split_057: R.Tensor((1, 1, 14336), dtype="float16") = split57[0]
            split_157: R.Tensor((1, 1, 14336), dtype="float16") = split57[1]
            silu57: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_057)
            mul57: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu57, split_157)
            lv293 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight3, model_layers_25_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv874 = R.call_tir(cls.NT_matmul13, (mul57, lv293), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv870_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv874, lv869_1, model_layers_26_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv871_1: R.Tensor((1, 1, 4096), dtype="float16") = lv870_1[1]
            rms_norm117: R.Tensor((1, 1, 4096), dtype="float16") = lv870_1[0]
            lv294 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight3, model_layers_26_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv875 = R.call_tir(cls.NT_matmul10, (rms_norm117, lv294), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape232: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv875, R.shape([1, 1, 48, 128]))
            reshape233: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape232, R.shape([1, 48, 128]))
            lv295 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape233), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape234: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv295, R.shape([1, 1, 32, 128]))
            reshape235: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape234, R.shape([1, 1, 4096]))
            lv296 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight3, model_layers_26_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv876 = R.call_tir(cls.NT_matmul11, (reshape235, lv296), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv872_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv876, lv871_1, model_layers_26_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv873_1: R.Tensor((1, 1, 4096), dtype="float16") = lv872_1[1]
            rms_norm118: R.Tensor((1, 1, 4096), dtype="float16") = lv872_1[0]
            lv297 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight3, model_layers_26_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv877 = R.call_tir(cls.NT_matmul12, (rms_norm118, lv297), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split58: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv877, indices_or_sections=2, axis=-1)
            split_058: R.Tensor((1, 1, 14336), dtype="float16") = split58[0]
            split_158: R.Tensor((1, 1, 14336), dtype="float16") = split58[1]
            silu58: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_058)
            mul58: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu58, split_158)
            lv298 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight3, model_layers_26_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv878 = R.call_tir(cls.NT_matmul13, (mul58, lv298), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv874_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv878, lv873_1, model_layers_27_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv875_1: R.Tensor((1, 1, 4096), dtype="float16") = lv874_1[1]
            rms_norm119: R.Tensor((1, 1, 4096), dtype="float16") = lv874_1[0]
            lv299 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight3, model_layers_27_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv879 = R.call_tir(cls.NT_matmul10, (rms_norm119, lv299), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape236: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv879, R.shape([1, 1, 48, 128]))
            reshape237: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape236, R.shape([1, 48, 128]))
            lv300 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape237), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape238: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv300, R.shape([1, 1, 32, 128]))
            reshape239: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape238, R.shape([1, 1, 4096]))
            lv301 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight3, model_layers_27_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv880 = R.call_tir(cls.NT_matmul11, (reshape239, lv301), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv876_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv880, lv875_1, model_layers_27_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv877_1: R.Tensor((1, 1, 4096), dtype="float16") = lv876_1[1]
            rms_norm120: R.Tensor((1, 1, 4096), dtype="float16") = lv876_1[0]
            lv302 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight3, model_layers_27_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv881 = R.call_tir(cls.NT_matmul12, (rms_norm120, lv302), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split59: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv881, indices_or_sections=2, axis=-1)
            split_059: R.Tensor((1, 1, 14336), dtype="float16") = split59[0]
            split_159: R.Tensor((1, 1, 14336), dtype="float16") = split59[1]
            silu59: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_059)
            mul59: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu59, split_159)
            lv303 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight3, model_layers_27_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv882 = R.call_tir(cls.NT_matmul13, (mul59, lv303), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv878_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv882, lv877_1, model_layers_28_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv879_1: R.Tensor((1, 1, 4096), dtype="float16") = lv878_1[1]
            rms_norm121: R.Tensor((1, 1, 4096), dtype="float16") = lv878_1[0]
            lv304 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight3, model_layers_28_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv883 = R.call_tir(cls.NT_matmul10, (rms_norm121, lv304), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape240: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv883, R.shape([1, 1, 48, 128]))
            reshape241: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape240, R.shape([1, 48, 128]))
            lv305 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape241), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape242: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv305, R.shape([1, 1, 32, 128]))
            reshape243: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape242, R.shape([1, 1, 4096]))
            lv306 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight3, model_layers_28_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv884 = R.call_tir(cls.NT_matmul11, (reshape243, lv306), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv880_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv884, lv879_1, model_layers_28_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv881_1: R.Tensor((1, 1, 4096), dtype="float16") = lv880_1[1]
            rms_norm122: R.Tensor((1, 1, 4096), dtype="float16") = lv880_1[0]
            lv307 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight3, model_layers_28_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv885 = R.call_tir(cls.NT_matmul12, (rms_norm122, lv307), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split60: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv885, indices_or_sections=2, axis=-1)
            split_060: R.Tensor((1, 1, 14336), dtype="float16") = split60[0]
            split_160: R.Tensor((1, 1, 14336), dtype="float16") = split60[1]
            silu60: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_060)
            mul60: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu60, split_160)
            lv308 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight3, model_layers_28_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv886 = R.call_tir(cls.NT_matmul13, (mul60, lv308), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv882_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv886, lv881_1, model_layers_29_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv883_1: R.Tensor((1, 1, 4096), dtype="float16") = lv882_1[1]
            rms_norm123: R.Tensor((1, 1, 4096), dtype="float16") = lv882_1[0]
            lv309 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight3, model_layers_29_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv887 = R.call_tir(cls.NT_matmul10, (rms_norm123, lv309), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape244: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv887, R.shape([1, 1, 48, 128]))
            reshape245: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape244, R.shape([1, 48, 128]))
            lv310 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape245), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape246: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv310, R.shape([1, 1, 32, 128]))
            reshape247: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape246, R.shape([1, 1, 4096]))
            lv311 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight3, model_layers_29_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv888 = R.call_tir(cls.NT_matmul11, (reshape247, lv311), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv884_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv888, lv883_1, model_layers_29_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv885_1: R.Tensor((1, 1, 4096), dtype="float16") = lv884_1[1]
            rms_norm124: R.Tensor((1, 1, 4096), dtype="float16") = lv884_1[0]
            lv312 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight3, model_layers_29_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv889 = R.call_tir(cls.NT_matmul12, (rms_norm124, lv312), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split61: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv889, indices_or_sections=2, axis=-1)
            split_061: R.Tensor((1, 1, 14336), dtype="float16") = split61[0]
            split_161: R.Tensor((1, 1, 14336), dtype="float16") = split61[1]
            silu61: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_061)
            mul61: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu61, split_161)
            lv313 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight3, model_layers_29_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv890 = R.call_tir(cls.NT_matmul13, (mul61, lv313), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv886_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv890, lv885_1, model_layers_30_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv887_1: R.Tensor((1, 1, 4096), dtype="float16") = lv886_1[1]
            rms_norm125: R.Tensor((1, 1, 4096), dtype="float16") = lv886_1[0]
            lv314 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight3, model_layers_30_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv891 = R.call_tir(cls.NT_matmul10, (rms_norm125, lv314), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape248: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv891, R.shape([1, 1, 48, 128]))
            reshape249: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape248, R.shape([1, 48, 128]))
            lv315 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape249), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape250: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv315, R.shape([1, 1, 32, 128]))
            reshape251: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape250, R.shape([1, 1, 4096]))
            lv316 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight3, model_layers_30_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv892 = R.call_tir(cls.NT_matmul11, (reshape251, lv316), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv888_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv892, lv887_1, model_layers_30_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv889_1: R.Tensor((1, 1, 4096), dtype="float16") = lv888_1[1]
            rms_norm126: R.Tensor((1, 1, 4096), dtype="float16") = lv888_1[0]
            lv317 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight3, model_layers_30_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv893 = R.call_tir(cls.NT_matmul12, (rms_norm126, lv317), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split62: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv893, indices_or_sections=2, axis=-1)
            split_062: R.Tensor((1, 1, 14336), dtype="float16") = split62[0]
            split_162: R.Tensor((1, 1, 14336), dtype="float16") = split62[1]
            silu62: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_062)
            mul62: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu62, split_162)
            lv318 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight3, model_layers_30_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv894 = R.call_tir(cls.NT_matmul13, (mul62, lv318), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv890_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv894, lv889_1, model_layers_31_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv891_1: R.Tensor((1, 1, 4096), dtype="float16") = lv890_1[1]
            rms_norm127: R.Tensor((1, 1, 4096), dtype="float16") = lv890_1[0]
            lv319 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight3, model_layers_31_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv895 = R.call_tir(cls.NT_matmul10, (rms_norm127, lv319), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape252: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv895, R.shape([1, 1, 48, 128]))
            reshape253: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape252, R.shape([1, 48, 128]))
            lv320 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape253), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape254: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv320, R.shape([1, 1, 32, 128]))
            reshape255: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape254, R.shape([1, 1, 4096]))
            lv321 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight3, model_layers_31_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv896 = R.call_tir(cls.NT_matmul11, (reshape255, lv321), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv892_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv896, lv891_1, model_layers_31_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv893_1: R.Tensor((1, 1, 4096), dtype="float16") = lv892_1[1]
            rms_norm128: R.Tensor((1, 1, 4096), dtype="float16") = lv892_1[0]
            lv322 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight3, model_layers_31_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv897 = R.call_tir(cls.NT_matmul12, (rms_norm128, lv322), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split63: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv897, indices_or_sections=2, axis=-1)
            split_063: R.Tensor((1, 1, 14336), dtype="float16") = split63[0]
            split_163: R.Tensor((1, 1, 14336), dtype="float16") = split63[1]
            silu63: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_063)
            mul63: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu63, split_163)
            lv323 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight3, model_layers_31_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv898 = R.call_tir(cls.NT_matmul13, (mul63, lv323), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv894_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv898, lv893_1, model_norm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            rms_norm129: R.Tensor((1, 1, 4096), dtype="float16") = lv894_1[0]
            lv324 = R.call_tir(cls.dequantize, (lm_head_q_weight3, lm_head_q_scale3), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            lv899 = R.call_tir(cls.NT_matmul14, (rms_norm129, lv324), out_sinfo=R.Tensor((1, 1, vocab_size), dtype="float16"))
            astype2: R.Tensor((1, 1, vocab_size), dtype="float32") = R.astype(lv899, dtype="float32")
            gv4: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = astype2, paged_kv_cache
            R.output(gv4)
        return gv4

    @R.function
    def decode_to_last_hidden_states(input_embed: R.Tensor((1, 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, 4096), dtype="float16"), R.Object):
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm195: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv485 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight5, model_layers_0_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv900 = R.call_tir(cls.NT_matmul10, (rms_norm195, lv485), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape384: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv900, R.shape([1, 1, 48, 128]))
            reshape385: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape384, R.shape([1, 48, 128]))
            lv486 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape385), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape386: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv486, R.shape([1, 1, 32, 128]))
            reshape387: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape386, R.shape([1, 1, 4096]))
            lv487 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight5, model_layers_0_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv901 = R.call_tir(cls.NT_matmul11, (reshape387, lv487), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv896 = R.call_tir(cls.fuse_add_norm_prefill, (lv901, input_embed, model_layers_0_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv897: R.Tensor((1, 1, 4096), dtype="float16") = lv896[1]
            rms_norm196: R.Tensor((1, 1, 4096), dtype="float16") = lv896[0]
            lv488 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight5, model_layers_0_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv902 = R.call_tir(cls.NT_matmul12, (rms_norm196, lv488), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split96: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv902, indices_or_sections=2, axis=-1)
            split_096: R.Tensor((1, 1, 14336), dtype="float16") = split96[0]
            split_196: R.Tensor((1, 1, 14336), dtype="float16") = split96[1]
            silu96: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_096)
            mul96: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu96, split_196)
            lv489 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight5, model_layers_0_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv903 = R.call_tir(cls.NT_matmul13, (mul96, lv489), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv898 = R.call_tir(cls.fuse_add_norm_prefill, (lv903, lv897, model_layers_1_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv899: R.Tensor((1, 1, 4096), dtype="float16") = lv898[1]
            rms_norm197: R.Tensor((1, 1, 4096), dtype="float16") = lv898[0]
            lv490 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight5, model_layers_1_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv904 = R.call_tir(cls.NT_matmul10, (rms_norm197, lv490), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape388: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv904, R.shape([1, 1, 48, 128]))
            reshape389: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape388, R.shape([1, 48, 128]))
            lv491 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape389), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape390: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv491, R.shape([1, 1, 32, 128]))
            reshape391: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape390, R.shape([1, 1, 4096]))
            lv492 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight5, model_layers_1_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv905 = R.call_tir(cls.NT_matmul11, (reshape391, lv492), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv900_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv905, lv899, model_layers_1_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv901_1: R.Tensor((1, 1, 4096), dtype="float16") = lv900_1[1]
            rms_norm198: R.Tensor((1, 1, 4096), dtype="float16") = lv900_1[0]
            lv493 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight5, model_layers_1_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv906 = R.call_tir(cls.NT_matmul12, (rms_norm198, lv493), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split97: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv906, indices_or_sections=2, axis=-1)
            split_097: R.Tensor((1, 1, 14336), dtype="float16") = split97[0]
            split_197: R.Tensor((1, 1, 14336), dtype="float16") = split97[1]
            silu97: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_097)
            mul97: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu97, split_197)
            lv494 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight5, model_layers_1_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv907 = R.call_tir(cls.NT_matmul13, (mul97, lv494), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv902_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv907, lv901_1, model_layers_2_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv903_1: R.Tensor((1, 1, 4096), dtype="float16") = lv902_1[1]
            rms_norm199: R.Tensor((1, 1, 4096), dtype="float16") = lv902_1[0]
            lv495 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight5, model_layers_2_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv908 = R.call_tir(cls.NT_matmul10, (rms_norm199, lv495), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape392: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv908, R.shape([1, 1, 48, 128]))
            reshape393: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape392, R.shape([1, 48, 128]))
            lv496 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape393), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape394: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv496, R.shape([1, 1, 32, 128]))
            reshape395: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape394, R.shape([1, 1, 4096]))
            lv497 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight5, model_layers_2_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv909 = R.call_tir(cls.NT_matmul11, (reshape395, lv497), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv904_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv909, lv903_1, model_layers_2_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv905_1: R.Tensor((1, 1, 4096), dtype="float16") = lv904_1[1]
            rms_norm200: R.Tensor((1, 1, 4096), dtype="float16") = lv904_1[0]
            lv498 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight5, model_layers_2_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv910 = R.call_tir(cls.NT_matmul12, (rms_norm200, lv498), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split98: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv910, indices_or_sections=2, axis=-1)
            split_098: R.Tensor((1, 1, 14336), dtype="float16") = split98[0]
            split_198: R.Tensor((1, 1, 14336), dtype="float16") = split98[1]
            silu98: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_098)
            mul98: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu98, split_198)
            lv499 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight5, model_layers_2_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv911 = R.call_tir(cls.NT_matmul13, (mul98, lv499), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv906_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv911, lv905_1, model_layers_3_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv907_1: R.Tensor((1, 1, 4096), dtype="float16") = lv906_1[1]
            rms_norm201: R.Tensor((1, 1, 4096), dtype="float16") = lv906_1[0]
            lv500 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight5, model_layers_3_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv912 = R.call_tir(cls.NT_matmul10, (rms_norm201, lv500), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape396: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv912, R.shape([1, 1, 48, 128]))
            reshape397: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape396, R.shape([1, 48, 128]))
            lv501 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape397), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape398: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv501, R.shape([1, 1, 32, 128]))
            reshape399: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape398, R.shape([1, 1, 4096]))
            lv502 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight5, model_layers_3_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv913 = R.call_tir(cls.NT_matmul11, (reshape399, lv502), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv908_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv913, lv907_1, model_layers_3_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv909_1: R.Tensor((1, 1, 4096), dtype="float16") = lv908_1[1]
            rms_norm202: R.Tensor((1, 1, 4096), dtype="float16") = lv908_1[0]
            lv503 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight5, model_layers_3_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv914 = R.call_tir(cls.NT_matmul12, (rms_norm202, lv503), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split99: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv914, indices_or_sections=2, axis=-1)
            split_099: R.Tensor((1, 1, 14336), dtype="float16") = split99[0]
            split_199: R.Tensor((1, 1, 14336), dtype="float16") = split99[1]
            silu99: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_099)
            mul99: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu99, split_199)
            lv504 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight5, model_layers_3_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv915 = R.call_tir(cls.NT_matmul13, (mul99, lv504), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv910_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv915, lv909_1, model_layers_4_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv911_1: R.Tensor((1, 1, 4096), dtype="float16") = lv910_1[1]
            rms_norm203: R.Tensor((1, 1, 4096), dtype="float16") = lv910_1[0]
            lv505 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight5, model_layers_4_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv916 = R.call_tir(cls.NT_matmul10, (rms_norm203, lv505), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape400: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv916, R.shape([1, 1, 48, 128]))
            reshape401: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape400, R.shape([1, 48, 128]))
            lv506 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape401), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape402: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv506, R.shape([1, 1, 32, 128]))
            reshape403: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape402, R.shape([1, 1, 4096]))
            lv507 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight5, model_layers_4_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv917 = R.call_tir(cls.NT_matmul11, (reshape403, lv507), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv912_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv917, lv911_1, model_layers_4_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv913_1: R.Tensor((1, 1, 4096), dtype="float16") = lv912_1[1]
            rms_norm204: R.Tensor((1, 1, 4096), dtype="float16") = lv912_1[0]
            lv508 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight5, model_layers_4_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv918 = R.call_tir(cls.NT_matmul12, (rms_norm204, lv508), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split100: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv918, indices_or_sections=2, axis=-1)
            split_0100: R.Tensor((1, 1, 14336), dtype="float16") = split100[0]
            split_1100: R.Tensor((1, 1, 14336), dtype="float16") = split100[1]
            silu100: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0100)
            mul100: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu100, split_1100)
            lv509 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight5, model_layers_4_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv919 = R.call_tir(cls.NT_matmul13, (mul100, lv509), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv914_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv919, lv913_1, model_layers_5_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv915_1: R.Tensor((1, 1, 4096), dtype="float16") = lv914_1[1]
            rms_norm205: R.Tensor((1, 1, 4096), dtype="float16") = lv914_1[0]
            lv510 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight5, model_layers_5_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv920 = R.call_tir(cls.NT_matmul10, (rms_norm205, lv510), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape404: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv920, R.shape([1, 1, 48, 128]))
            reshape405: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape404, R.shape([1, 48, 128]))
            lv511 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape405), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape406: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv511, R.shape([1, 1, 32, 128]))
            reshape407: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape406, R.shape([1, 1, 4096]))
            lv512 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight5, model_layers_5_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv921 = R.call_tir(cls.NT_matmul11, (reshape407, lv512), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv916_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv921, lv915_1, model_layers_5_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv917_1: R.Tensor((1, 1, 4096), dtype="float16") = lv916_1[1]
            rms_norm206: R.Tensor((1, 1, 4096), dtype="float16") = lv916_1[0]
            lv513 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight5, model_layers_5_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv922 = R.call_tir(cls.NT_matmul12, (rms_norm206, lv513), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split101: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv922, indices_or_sections=2, axis=-1)
            split_0101: R.Tensor((1, 1, 14336), dtype="float16") = split101[0]
            split_1101: R.Tensor((1, 1, 14336), dtype="float16") = split101[1]
            silu101: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0101)
            mul101: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu101, split_1101)
            lv514 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight5, model_layers_5_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv923 = R.call_tir(cls.NT_matmul13, (mul101, lv514), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv918_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv923, lv917_1, model_layers_6_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv919_1: R.Tensor((1, 1, 4096), dtype="float16") = lv918_1[1]
            rms_norm207: R.Tensor((1, 1, 4096), dtype="float16") = lv918_1[0]
            lv515 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight5, model_layers_6_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv924 = R.call_tir(cls.NT_matmul10, (rms_norm207, lv515), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape408: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv924, R.shape([1, 1, 48, 128]))
            reshape409: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape408, R.shape([1, 48, 128]))
            lv516 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape409), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape410: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv516, R.shape([1, 1, 32, 128]))
            reshape411: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape410, R.shape([1, 1, 4096]))
            lv517 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight5, model_layers_6_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv925 = R.call_tir(cls.NT_matmul11, (reshape411, lv517), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv920_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv925, lv919_1, model_layers_6_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv921_1: R.Tensor((1, 1, 4096), dtype="float16") = lv920_1[1]
            rms_norm208: R.Tensor((1, 1, 4096), dtype="float16") = lv920_1[0]
            lv518 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight5, model_layers_6_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv926 = R.call_tir(cls.NT_matmul12, (rms_norm208, lv518), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split102: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv926, indices_or_sections=2, axis=-1)
            split_0102: R.Tensor((1, 1, 14336), dtype="float16") = split102[0]
            split_1102: R.Tensor((1, 1, 14336), dtype="float16") = split102[1]
            silu102: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0102)
            mul102: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu102, split_1102)
            lv519 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight5, model_layers_6_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv927 = R.call_tir(cls.NT_matmul13, (mul102, lv519), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv922_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv927, lv921_1, model_layers_7_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv923_1: R.Tensor((1, 1, 4096), dtype="float16") = lv922_1[1]
            rms_norm209: R.Tensor((1, 1, 4096), dtype="float16") = lv922_1[0]
            lv520 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight5, model_layers_7_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv928 = R.call_tir(cls.NT_matmul10, (rms_norm209, lv520), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape412: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv928, R.shape([1, 1, 48, 128]))
            reshape413: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape412, R.shape([1, 48, 128]))
            lv521 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape413), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape414: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv521, R.shape([1, 1, 32, 128]))
            reshape415: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape414, R.shape([1, 1, 4096]))
            lv522 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight5, model_layers_7_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv929 = R.call_tir(cls.NT_matmul11, (reshape415, lv522), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv924_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv929, lv923_1, model_layers_7_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv925_1: R.Tensor((1, 1, 4096), dtype="float16") = lv924_1[1]
            rms_norm210: R.Tensor((1, 1, 4096), dtype="float16") = lv924_1[0]
            lv523 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight5, model_layers_7_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv930 = R.call_tir(cls.NT_matmul12, (rms_norm210, lv523), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split103: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv930, indices_or_sections=2, axis=-1)
            split_0103: R.Tensor((1, 1, 14336), dtype="float16") = split103[0]
            split_1103: R.Tensor((1, 1, 14336), dtype="float16") = split103[1]
            silu103: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0103)
            mul103: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu103, split_1103)
            lv524 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight5, model_layers_7_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv931 = R.call_tir(cls.NT_matmul13, (mul103, lv524), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv926_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv931, lv925_1, model_layers_8_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv927_1: R.Tensor((1, 1, 4096), dtype="float16") = lv926_1[1]
            rms_norm211: R.Tensor((1, 1, 4096), dtype="float16") = lv926_1[0]
            lv525 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight5, model_layers_8_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv932 = R.call_tir(cls.NT_matmul10, (rms_norm211, lv525), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape416: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv932, R.shape([1, 1, 48, 128]))
            reshape417: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape416, R.shape([1, 48, 128]))
            lv526 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape417), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape418: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv526, R.shape([1, 1, 32, 128]))
            reshape419: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape418, R.shape([1, 1, 4096]))
            lv527 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight5, model_layers_8_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv933 = R.call_tir(cls.NT_matmul11, (reshape419, lv527), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv928_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv933, lv927_1, model_layers_8_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv929_1: R.Tensor((1, 1, 4096), dtype="float16") = lv928_1[1]
            rms_norm212: R.Tensor((1, 1, 4096), dtype="float16") = lv928_1[0]
            lv528 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight5, model_layers_8_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv934 = R.call_tir(cls.NT_matmul12, (rms_norm212, lv528), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split104: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv934, indices_or_sections=2, axis=-1)
            split_0104: R.Tensor((1, 1, 14336), dtype="float16") = split104[0]
            split_1104: R.Tensor((1, 1, 14336), dtype="float16") = split104[1]
            silu104: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0104)
            mul104: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu104, split_1104)
            lv529 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight5, model_layers_8_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv935 = R.call_tir(cls.NT_matmul13, (mul104, lv529), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv930_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv935, lv929_1, model_layers_9_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv931_1: R.Tensor((1, 1, 4096), dtype="float16") = lv930_1[1]
            rms_norm213: R.Tensor((1, 1, 4096), dtype="float16") = lv930_1[0]
            lv530 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight5, model_layers_9_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv936 = R.call_tir(cls.NT_matmul10, (rms_norm213, lv530), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape420: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv936, R.shape([1, 1, 48, 128]))
            reshape421: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape420, R.shape([1, 48, 128]))
            lv531 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape421), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape422: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv531, R.shape([1, 1, 32, 128]))
            reshape423: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape422, R.shape([1, 1, 4096]))
            lv532 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight5, model_layers_9_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv937 = R.call_tir(cls.NT_matmul11, (reshape423, lv532), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv932_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv937, lv931_1, model_layers_9_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv933_1: R.Tensor((1, 1, 4096), dtype="float16") = lv932_1[1]
            rms_norm214: R.Tensor((1, 1, 4096), dtype="float16") = lv932_1[0]
            lv533 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight5, model_layers_9_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv938 = R.call_tir(cls.NT_matmul12, (rms_norm214, lv533), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split105: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv938, indices_or_sections=2, axis=-1)
            split_0105: R.Tensor((1, 1, 14336), dtype="float16") = split105[0]
            split_1105: R.Tensor((1, 1, 14336), dtype="float16") = split105[1]
            silu105: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0105)
            mul105: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu105, split_1105)
            lv534 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight5, model_layers_9_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv939 = R.call_tir(cls.NT_matmul13, (mul105, lv534), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv934_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv939, lv933_1, model_layers_10_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv935_1: R.Tensor((1, 1, 4096), dtype="float16") = lv934_1[1]
            rms_norm215: R.Tensor((1, 1, 4096), dtype="float16") = lv934_1[0]
            lv535 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight5, model_layers_10_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv940 = R.call_tir(cls.NT_matmul10, (rms_norm215, lv535), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape424: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv940, R.shape([1, 1, 48, 128]))
            reshape425: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape424, R.shape([1, 48, 128]))
            lv536 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape425), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape426: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv536, R.shape([1, 1, 32, 128]))
            reshape427: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape426, R.shape([1, 1, 4096]))
            lv537 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight5, model_layers_10_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv941 = R.call_tir(cls.NT_matmul11, (reshape427, lv537), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv936_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv941, lv935_1, model_layers_10_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv937_1: R.Tensor((1, 1, 4096), dtype="float16") = lv936_1[1]
            rms_norm216: R.Tensor((1, 1, 4096), dtype="float16") = lv936_1[0]
            lv538 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight5, model_layers_10_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv942 = R.call_tir(cls.NT_matmul12, (rms_norm216, lv538), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split106: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv942, indices_or_sections=2, axis=-1)
            split_0106: R.Tensor((1, 1, 14336), dtype="float16") = split106[0]
            split_1106: R.Tensor((1, 1, 14336), dtype="float16") = split106[1]
            silu106: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0106)
            mul106: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu106, split_1106)
            lv539 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight5, model_layers_10_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv943 = R.call_tir(cls.NT_matmul13, (mul106, lv539), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv938_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv943, lv937_1, model_layers_11_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv939_1: R.Tensor((1, 1, 4096), dtype="float16") = lv938_1[1]
            rms_norm217: R.Tensor((1, 1, 4096), dtype="float16") = lv938_1[0]
            lv540 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight5, model_layers_11_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv944 = R.call_tir(cls.NT_matmul10, (rms_norm217, lv540), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape428: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv944, R.shape([1, 1, 48, 128]))
            reshape429: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape428, R.shape([1, 48, 128]))
            lv541 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape429), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape430: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv541, R.shape([1, 1, 32, 128]))
            reshape431: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape430, R.shape([1, 1, 4096]))
            lv542 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight5, model_layers_11_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv945 = R.call_tir(cls.NT_matmul11, (reshape431, lv542), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv940_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv945, lv939_1, model_layers_11_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv941_1: R.Tensor((1, 1, 4096), dtype="float16") = lv940_1[1]
            rms_norm218: R.Tensor((1, 1, 4096), dtype="float16") = lv940_1[0]
            lv543 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight5, model_layers_11_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv946 = R.call_tir(cls.NT_matmul12, (rms_norm218, lv543), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split107: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv946, indices_or_sections=2, axis=-1)
            split_0107: R.Tensor((1, 1, 14336), dtype="float16") = split107[0]
            split_1107: R.Tensor((1, 1, 14336), dtype="float16") = split107[1]
            silu107: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0107)
            mul107: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu107, split_1107)
            lv544 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight5, model_layers_11_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv947 = R.call_tir(cls.NT_matmul13, (mul107, lv544), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv942_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv947, lv941_1, model_layers_12_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv943_1: R.Tensor((1, 1, 4096), dtype="float16") = lv942_1[1]
            rms_norm219: R.Tensor((1, 1, 4096), dtype="float16") = lv942_1[0]
            lv545 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight5, model_layers_12_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv948 = R.call_tir(cls.NT_matmul10, (rms_norm219, lv545), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape432: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv948, R.shape([1, 1, 48, 128]))
            reshape433: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape432, R.shape([1, 48, 128]))
            lv546 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape433), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape434: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv546, R.shape([1, 1, 32, 128]))
            reshape435: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape434, R.shape([1, 1, 4096]))
            lv547 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight5, model_layers_12_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv949 = R.call_tir(cls.NT_matmul11, (reshape435, lv547), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv944_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv949, lv943_1, model_layers_12_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv945_1: R.Tensor((1, 1, 4096), dtype="float16") = lv944_1[1]
            rms_norm220: R.Tensor((1, 1, 4096), dtype="float16") = lv944_1[0]
            lv548 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight5, model_layers_12_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv950 = R.call_tir(cls.NT_matmul12, (rms_norm220, lv548), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split108: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv950, indices_or_sections=2, axis=-1)
            split_0108: R.Tensor((1, 1, 14336), dtype="float16") = split108[0]
            split_1108: R.Tensor((1, 1, 14336), dtype="float16") = split108[1]
            silu108: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0108)
            mul108: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu108, split_1108)
            lv549 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight5, model_layers_12_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv951 = R.call_tir(cls.NT_matmul13, (mul108, lv549), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv946_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv951, lv945_1, model_layers_13_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv947_1: R.Tensor((1, 1, 4096), dtype="float16") = lv946_1[1]
            rms_norm221: R.Tensor((1, 1, 4096), dtype="float16") = lv946_1[0]
            lv550 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight5, model_layers_13_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv952 = R.call_tir(cls.NT_matmul10, (rms_norm221, lv550), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape436: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv952, R.shape([1, 1, 48, 128]))
            reshape437: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape436, R.shape([1, 48, 128]))
            lv551 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape437), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape438: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv551, R.shape([1, 1, 32, 128]))
            reshape439: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape438, R.shape([1, 1, 4096]))
            lv552 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight5, model_layers_13_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv953 = R.call_tir(cls.NT_matmul11, (reshape439, lv552), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv948_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv953, lv947_1, model_layers_13_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv949_1: R.Tensor((1, 1, 4096), dtype="float16") = lv948_1[1]
            rms_norm222: R.Tensor((1, 1, 4096), dtype="float16") = lv948_1[0]
            lv553 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight5, model_layers_13_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv954 = R.call_tir(cls.NT_matmul12, (rms_norm222, lv553), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split109: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv954, indices_or_sections=2, axis=-1)
            split_0109: R.Tensor((1, 1, 14336), dtype="float16") = split109[0]
            split_1109: R.Tensor((1, 1, 14336), dtype="float16") = split109[1]
            silu109: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0109)
            mul109: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu109, split_1109)
            lv554 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight5, model_layers_13_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv955 = R.call_tir(cls.NT_matmul13, (mul109, lv554), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv950_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv955, lv949_1, model_layers_14_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv951_1: R.Tensor((1, 1, 4096), dtype="float16") = lv950_1[1]
            rms_norm223: R.Tensor((1, 1, 4096), dtype="float16") = lv950_1[0]
            lv555 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight5, model_layers_14_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv956 = R.call_tir(cls.NT_matmul10, (rms_norm223, lv555), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape440: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv956, R.shape([1, 1, 48, 128]))
            reshape441: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape440, R.shape([1, 48, 128]))
            lv556 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape441), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape442: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv556, R.shape([1, 1, 32, 128]))
            reshape443: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape442, R.shape([1, 1, 4096]))
            lv557 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight5, model_layers_14_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv957 = R.call_tir(cls.NT_matmul11, (reshape443, lv557), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv952_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv957, lv951_1, model_layers_14_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv953_1: R.Tensor((1, 1, 4096), dtype="float16") = lv952_1[1]
            rms_norm224: R.Tensor((1, 1, 4096), dtype="float16") = lv952_1[0]
            lv558 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight5, model_layers_14_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv958 = R.call_tir(cls.NT_matmul12, (rms_norm224, lv558), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split110: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv958, indices_or_sections=2, axis=-1)
            split_0110: R.Tensor((1, 1, 14336), dtype="float16") = split110[0]
            split_1110: R.Tensor((1, 1, 14336), dtype="float16") = split110[1]
            silu110: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0110)
            mul110: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu110, split_1110)
            lv559 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight5, model_layers_14_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv959 = R.call_tir(cls.NT_matmul13, (mul110, lv559), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv954_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv959, lv953_1, model_layers_15_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv955_1: R.Tensor((1, 1, 4096), dtype="float16") = lv954_1[1]
            rms_norm225: R.Tensor((1, 1, 4096), dtype="float16") = lv954_1[0]
            lv560 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight5, model_layers_15_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv960 = R.call_tir(cls.NT_matmul10, (rms_norm225, lv560), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape444: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv960, R.shape([1, 1, 48, 128]))
            reshape445: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape444, R.shape([1, 48, 128]))
            lv561 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape445), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape446: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv561, R.shape([1, 1, 32, 128]))
            reshape447: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape446, R.shape([1, 1, 4096]))
            lv562 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight5, model_layers_15_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv961 = R.call_tir(cls.NT_matmul11, (reshape447, lv562), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv956_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv961, lv955_1, model_layers_15_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv957_1: R.Tensor((1, 1, 4096), dtype="float16") = lv956_1[1]
            rms_norm226: R.Tensor((1, 1, 4096), dtype="float16") = lv956_1[0]
            lv563 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight5, model_layers_15_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv962 = R.call_tir(cls.NT_matmul12, (rms_norm226, lv563), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split111: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv962, indices_or_sections=2, axis=-1)
            split_0111: R.Tensor((1, 1, 14336), dtype="float16") = split111[0]
            split_1111: R.Tensor((1, 1, 14336), dtype="float16") = split111[1]
            silu111: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0111)
            mul111: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu111, split_1111)
            lv564 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight5, model_layers_15_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv963 = R.call_tir(cls.NT_matmul13, (mul111, lv564), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv958_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv963, lv957_1, model_layers_16_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv959_1: R.Tensor((1, 1, 4096), dtype="float16") = lv958_1[1]
            rms_norm227: R.Tensor((1, 1, 4096), dtype="float16") = lv958_1[0]
            lv565 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight5, model_layers_16_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv964 = R.call_tir(cls.NT_matmul10, (rms_norm227, lv565), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape448: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv964, R.shape([1, 1, 48, 128]))
            reshape449: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape448, R.shape([1, 48, 128]))
            lv566 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape449), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape450: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv566, R.shape([1, 1, 32, 128]))
            reshape451: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape450, R.shape([1, 1, 4096]))
            lv567 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight5, model_layers_16_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv965 = R.call_tir(cls.NT_matmul11, (reshape451, lv567), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv960_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv965, lv959_1, model_layers_16_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv961_1: R.Tensor((1, 1, 4096), dtype="float16") = lv960_1[1]
            rms_norm228: R.Tensor((1, 1, 4096), dtype="float16") = lv960_1[0]
            lv568 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight5, model_layers_16_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv966 = R.call_tir(cls.NT_matmul12, (rms_norm228, lv568), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split112: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv966, indices_or_sections=2, axis=-1)
            split_0112: R.Tensor((1, 1, 14336), dtype="float16") = split112[0]
            split_1112: R.Tensor((1, 1, 14336), dtype="float16") = split112[1]
            silu112: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0112)
            mul112: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu112, split_1112)
            lv569 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight5, model_layers_16_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv967 = R.call_tir(cls.NT_matmul13, (mul112, lv569), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv962_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv967, lv961_1, model_layers_17_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv963_1: R.Tensor((1, 1, 4096), dtype="float16") = lv962_1[1]
            rms_norm229: R.Tensor((1, 1, 4096), dtype="float16") = lv962_1[0]
            lv570 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight5, model_layers_17_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv968 = R.call_tir(cls.NT_matmul10, (rms_norm229, lv570), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape452: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv968, R.shape([1, 1, 48, 128]))
            reshape453: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape452, R.shape([1, 48, 128]))
            lv571 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape453), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape454: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv571, R.shape([1, 1, 32, 128]))
            reshape455: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape454, R.shape([1, 1, 4096]))
            lv572 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight5, model_layers_17_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv969 = R.call_tir(cls.NT_matmul11, (reshape455, lv572), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv964_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv969, lv963_1, model_layers_17_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv965_1: R.Tensor((1, 1, 4096), dtype="float16") = lv964_1[1]
            rms_norm230: R.Tensor((1, 1, 4096), dtype="float16") = lv964_1[0]
            lv573 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight5, model_layers_17_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv970 = R.call_tir(cls.NT_matmul12, (rms_norm230, lv573), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split113: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv970, indices_or_sections=2, axis=-1)
            split_0113: R.Tensor((1, 1, 14336), dtype="float16") = split113[0]
            split_1113: R.Tensor((1, 1, 14336), dtype="float16") = split113[1]
            silu113: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0113)
            mul113: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu113, split_1113)
            lv574 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight5, model_layers_17_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv971 = R.call_tir(cls.NT_matmul13, (mul113, lv574), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv966_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv971, lv965_1, model_layers_18_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv967_1: R.Tensor((1, 1, 4096), dtype="float16") = lv966_1[1]
            rms_norm231: R.Tensor((1, 1, 4096), dtype="float16") = lv966_1[0]
            lv575 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight5, model_layers_18_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv972 = R.call_tir(cls.NT_matmul10, (rms_norm231, lv575), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape456: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv972, R.shape([1, 1, 48, 128]))
            reshape457: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape456, R.shape([1, 48, 128]))
            lv576 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape457), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape458: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv576, R.shape([1, 1, 32, 128]))
            reshape459: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape458, R.shape([1, 1, 4096]))
            lv577 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight5, model_layers_18_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv973 = R.call_tir(cls.NT_matmul11, (reshape459, lv577), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv968_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv973, lv967_1, model_layers_18_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv969_1: R.Tensor((1, 1, 4096), dtype="float16") = lv968_1[1]
            rms_norm232: R.Tensor((1, 1, 4096), dtype="float16") = lv968_1[0]
            lv578 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight5, model_layers_18_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv974 = R.call_tir(cls.NT_matmul12, (rms_norm232, lv578), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split114: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv974, indices_or_sections=2, axis=-1)
            split_0114: R.Tensor((1, 1, 14336), dtype="float16") = split114[0]
            split_1114: R.Tensor((1, 1, 14336), dtype="float16") = split114[1]
            silu114: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0114)
            mul114: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu114, split_1114)
            lv579 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight5, model_layers_18_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv975 = R.call_tir(cls.NT_matmul13, (mul114, lv579), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv970_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv975, lv969_1, model_layers_19_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv971_1: R.Tensor((1, 1, 4096), dtype="float16") = lv970_1[1]
            rms_norm233: R.Tensor((1, 1, 4096), dtype="float16") = lv970_1[0]
            lv580 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight5, model_layers_19_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv976 = R.call_tir(cls.NT_matmul10, (rms_norm233, lv580), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape460: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv976, R.shape([1, 1, 48, 128]))
            reshape461: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape460, R.shape([1, 48, 128]))
            lv581 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape461), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape462: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv581, R.shape([1, 1, 32, 128]))
            reshape463: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape462, R.shape([1, 1, 4096]))
            lv582 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight5, model_layers_19_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv977 = R.call_tir(cls.NT_matmul11, (reshape463, lv582), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv972_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv977, lv971_1, model_layers_19_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv973_1: R.Tensor((1, 1, 4096), dtype="float16") = lv972_1[1]
            rms_norm234: R.Tensor((1, 1, 4096), dtype="float16") = lv972_1[0]
            lv583 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight5, model_layers_19_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv978 = R.call_tir(cls.NT_matmul12, (rms_norm234, lv583), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split115: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv978, indices_or_sections=2, axis=-1)
            split_0115: R.Tensor((1, 1, 14336), dtype="float16") = split115[0]
            split_1115: R.Tensor((1, 1, 14336), dtype="float16") = split115[1]
            silu115: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0115)
            mul115: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu115, split_1115)
            lv584 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight5, model_layers_19_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv979 = R.call_tir(cls.NT_matmul13, (mul115, lv584), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv974_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv979, lv973_1, model_layers_20_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv975_1: R.Tensor((1, 1, 4096), dtype="float16") = lv974_1[1]
            rms_norm235: R.Tensor((1, 1, 4096), dtype="float16") = lv974_1[0]
            lv585 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight5, model_layers_20_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv980 = R.call_tir(cls.NT_matmul10, (rms_norm235, lv585), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape464: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv980, R.shape([1, 1, 48, 128]))
            reshape465: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape464, R.shape([1, 48, 128]))
            lv586 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape465), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape466: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv586, R.shape([1, 1, 32, 128]))
            reshape467: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape466, R.shape([1, 1, 4096]))
            lv587 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight5, model_layers_20_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv981 = R.call_tir(cls.NT_matmul11, (reshape467, lv587), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv976_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv981, lv975_1, model_layers_20_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv977_1: R.Tensor((1, 1, 4096), dtype="float16") = lv976_1[1]
            rms_norm236: R.Tensor((1, 1, 4096), dtype="float16") = lv976_1[0]
            lv588 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight5, model_layers_20_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv982 = R.call_tir(cls.NT_matmul12, (rms_norm236, lv588), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split116: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv982, indices_or_sections=2, axis=-1)
            split_0116: R.Tensor((1, 1, 14336), dtype="float16") = split116[0]
            split_1116: R.Tensor((1, 1, 14336), dtype="float16") = split116[1]
            silu116: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0116)
            mul116: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu116, split_1116)
            lv589 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight5, model_layers_20_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv983 = R.call_tir(cls.NT_matmul13, (mul116, lv589), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv978_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv983, lv977_1, model_layers_21_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv979_1: R.Tensor((1, 1, 4096), dtype="float16") = lv978_1[1]
            rms_norm237: R.Tensor((1, 1, 4096), dtype="float16") = lv978_1[0]
            lv590 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight5, model_layers_21_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv984 = R.call_tir(cls.NT_matmul10, (rms_norm237, lv590), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape468: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv984, R.shape([1, 1, 48, 128]))
            reshape469: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape468, R.shape([1, 48, 128]))
            lv591 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape469), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape470: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv591, R.shape([1, 1, 32, 128]))
            reshape471: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape470, R.shape([1, 1, 4096]))
            lv592 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight5, model_layers_21_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv985 = R.call_tir(cls.NT_matmul11, (reshape471, lv592), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv980_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv985, lv979_1, model_layers_21_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv981_1: R.Tensor((1, 1, 4096), dtype="float16") = lv980_1[1]
            rms_norm238: R.Tensor((1, 1, 4096), dtype="float16") = lv980_1[0]
            lv593 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight5, model_layers_21_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv986 = R.call_tir(cls.NT_matmul12, (rms_norm238, lv593), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split117: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv986, indices_or_sections=2, axis=-1)
            split_0117: R.Tensor((1, 1, 14336), dtype="float16") = split117[0]
            split_1117: R.Tensor((1, 1, 14336), dtype="float16") = split117[1]
            silu117: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0117)
            mul117: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu117, split_1117)
            lv594 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight5, model_layers_21_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv987 = R.call_tir(cls.NT_matmul13, (mul117, lv594), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv982_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv987, lv981_1, model_layers_22_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv983_1: R.Tensor((1, 1, 4096), dtype="float16") = lv982_1[1]
            rms_norm239: R.Tensor((1, 1, 4096), dtype="float16") = lv982_1[0]
            lv595 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight5, model_layers_22_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv988 = R.call_tir(cls.NT_matmul10, (rms_norm239, lv595), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape472: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv988, R.shape([1, 1, 48, 128]))
            reshape473: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape472, R.shape([1, 48, 128]))
            lv596 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape473), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape474: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv596, R.shape([1, 1, 32, 128]))
            reshape475: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape474, R.shape([1, 1, 4096]))
            lv597 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight5, model_layers_22_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv989 = R.call_tir(cls.NT_matmul11, (reshape475, lv597), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv984_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv989, lv983_1, model_layers_22_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv985_1: R.Tensor((1, 1, 4096), dtype="float16") = lv984_1[1]
            rms_norm240: R.Tensor((1, 1, 4096), dtype="float16") = lv984_1[0]
            lv598 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight5, model_layers_22_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv990 = R.call_tir(cls.NT_matmul12, (rms_norm240, lv598), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split118: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv990, indices_or_sections=2, axis=-1)
            split_0118: R.Tensor((1, 1, 14336), dtype="float16") = split118[0]
            split_1118: R.Tensor((1, 1, 14336), dtype="float16") = split118[1]
            silu118: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0118)
            mul118: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu118, split_1118)
            lv599 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight5, model_layers_22_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv991 = R.call_tir(cls.NT_matmul13, (mul118, lv599), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv986_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv991, lv985_1, model_layers_23_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv987_1: R.Tensor((1, 1, 4096), dtype="float16") = lv986_1[1]
            rms_norm241: R.Tensor((1, 1, 4096), dtype="float16") = lv986_1[0]
            lv600 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight5, model_layers_23_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv992 = R.call_tir(cls.NT_matmul10, (rms_norm241, lv600), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape476: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv992, R.shape([1, 1, 48, 128]))
            reshape477: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape476, R.shape([1, 48, 128]))
            lv601 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape477), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape478: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv601, R.shape([1, 1, 32, 128]))
            reshape479: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape478, R.shape([1, 1, 4096]))
            lv602 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight5, model_layers_23_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv993 = R.call_tir(cls.NT_matmul11, (reshape479, lv602), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv988_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv993, lv987_1, model_layers_23_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv989_1: R.Tensor((1, 1, 4096), dtype="float16") = lv988_1[1]
            rms_norm242: R.Tensor((1, 1, 4096), dtype="float16") = lv988_1[0]
            lv603 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight5, model_layers_23_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv994 = R.call_tir(cls.NT_matmul12, (rms_norm242, lv603), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split119: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv994, indices_or_sections=2, axis=-1)
            split_0119: R.Tensor((1, 1, 14336), dtype="float16") = split119[0]
            split_1119: R.Tensor((1, 1, 14336), dtype="float16") = split119[1]
            silu119: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0119)
            mul119: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu119, split_1119)
            lv604 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight5, model_layers_23_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv995 = R.call_tir(cls.NT_matmul13, (mul119, lv604), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv990_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv995, lv989_1, model_layers_24_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv991_1: R.Tensor((1, 1, 4096), dtype="float16") = lv990_1[1]
            rms_norm243: R.Tensor((1, 1, 4096), dtype="float16") = lv990_1[0]
            lv605 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight5, model_layers_24_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv996 = R.call_tir(cls.NT_matmul10, (rms_norm243, lv605), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape480: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv996, R.shape([1, 1, 48, 128]))
            reshape481: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape480, R.shape([1, 48, 128]))
            lv606 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape481), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape482: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv606, R.shape([1, 1, 32, 128]))
            reshape483: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape482, R.shape([1, 1, 4096]))
            lv607 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight5, model_layers_24_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv997 = R.call_tir(cls.NT_matmul11, (reshape483, lv607), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv992_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv997, lv991_1, model_layers_24_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv993_1: R.Tensor((1, 1, 4096), dtype="float16") = lv992_1[1]
            rms_norm244: R.Tensor((1, 1, 4096), dtype="float16") = lv992_1[0]
            lv608 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight5, model_layers_24_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv998 = R.call_tir(cls.NT_matmul12, (rms_norm244, lv608), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split120: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv998, indices_or_sections=2, axis=-1)
            split_0120: R.Tensor((1, 1, 14336), dtype="float16") = split120[0]
            split_1120: R.Tensor((1, 1, 14336), dtype="float16") = split120[1]
            silu120: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0120)
            mul120: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu120, split_1120)
            lv609 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight5, model_layers_24_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv999 = R.call_tir(cls.NT_matmul13, (mul120, lv609), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv994_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv999, lv993_1, model_layers_25_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv995_1: R.Tensor((1, 1, 4096), dtype="float16") = lv994_1[1]
            rms_norm245: R.Tensor((1, 1, 4096), dtype="float16") = lv994_1[0]
            lv610 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight5, model_layers_25_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1000 = R.call_tir(cls.NT_matmul10, (rms_norm245, lv610), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape484: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv1000, R.shape([1, 1, 48, 128]))
            reshape485: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape484, R.shape([1, 48, 128]))
            lv611 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape485), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape486: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv611, R.shape([1, 1, 32, 128]))
            reshape487: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape486, R.shape([1, 1, 4096]))
            lv612 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight5, model_layers_25_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1001 = R.call_tir(cls.NT_matmul11, (reshape487, lv612), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv996_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1001, lv995_1, model_layers_25_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv997_1: R.Tensor((1, 1, 4096), dtype="float16") = lv996_1[1]
            rms_norm246: R.Tensor((1, 1, 4096), dtype="float16") = lv996_1[0]
            lv613 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight5, model_layers_25_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1002 = R.call_tir(cls.NT_matmul12, (rms_norm246, lv613), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split121: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv1002, indices_or_sections=2, axis=-1)
            split_0121: R.Tensor((1, 1, 14336), dtype="float16") = split121[0]
            split_1121: R.Tensor((1, 1, 14336), dtype="float16") = split121[1]
            silu121: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0121)
            mul121: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu121, split_1121)
            lv614 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight5, model_layers_25_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1003 = R.call_tir(cls.NT_matmul13, (mul121, lv614), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv998_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1003, lv997_1, model_layers_26_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv999_1: R.Tensor((1, 1, 4096), dtype="float16") = lv998_1[1]
            rms_norm247: R.Tensor((1, 1, 4096), dtype="float16") = lv998_1[0]
            lv615 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight5, model_layers_26_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1004 = R.call_tir(cls.NT_matmul10, (rms_norm247, lv615), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape488: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv1004, R.shape([1, 1, 48, 128]))
            reshape489: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape488, R.shape([1, 48, 128]))
            lv616 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape489), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape490: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv616, R.shape([1, 1, 32, 128]))
            reshape491: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape490, R.shape([1, 1, 4096]))
            lv617 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight5, model_layers_26_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1005 = R.call_tir(cls.NT_matmul11, (reshape491, lv617), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1000_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1005, lv999_1, model_layers_26_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1001_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1000_1[1]
            rms_norm248: R.Tensor((1, 1, 4096), dtype="float16") = lv1000_1[0]
            lv618 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight5, model_layers_26_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1006 = R.call_tir(cls.NT_matmul12, (rms_norm248, lv618), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split122: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv1006, indices_or_sections=2, axis=-1)
            split_0122: R.Tensor((1, 1, 14336), dtype="float16") = split122[0]
            split_1122: R.Tensor((1, 1, 14336), dtype="float16") = split122[1]
            silu122: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0122)
            mul122: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu122, split_1122)
            lv619 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight5, model_layers_26_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1007 = R.call_tir(cls.NT_matmul13, (mul122, lv619), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1002_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1007, lv1001_1, model_layers_27_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1003_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1002_1[1]
            rms_norm249: R.Tensor((1, 1, 4096), dtype="float16") = lv1002_1[0]
            lv620 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight5, model_layers_27_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1008 = R.call_tir(cls.NT_matmul10, (rms_norm249, lv620), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape492: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv1008, R.shape([1, 1, 48, 128]))
            reshape493: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape492, R.shape([1, 48, 128]))
            lv621 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape493), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape494: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv621, R.shape([1, 1, 32, 128]))
            reshape495: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape494, R.shape([1, 1, 4096]))
            lv622 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight5, model_layers_27_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1009 = R.call_tir(cls.NT_matmul11, (reshape495, lv622), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1004_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1009, lv1003_1, model_layers_27_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1005_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1004_1[1]
            rms_norm250: R.Tensor((1, 1, 4096), dtype="float16") = lv1004_1[0]
            lv623 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight5, model_layers_27_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1010 = R.call_tir(cls.NT_matmul12, (rms_norm250, lv623), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split123: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv1010, indices_or_sections=2, axis=-1)
            split_0123: R.Tensor((1, 1, 14336), dtype="float16") = split123[0]
            split_1123: R.Tensor((1, 1, 14336), dtype="float16") = split123[1]
            silu123: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0123)
            mul123: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu123, split_1123)
            lv624 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight5, model_layers_27_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1011 = R.call_tir(cls.NT_matmul13, (mul123, lv624), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1006_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1011, lv1005_1, model_layers_28_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1007_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1006_1[1]
            rms_norm251: R.Tensor((1, 1, 4096), dtype="float16") = lv1006_1[0]
            lv625 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight5, model_layers_28_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1012 = R.call_tir(cls.NT_matmul10, (rms_norm251, lv625), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape496: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv1012, R.shape([1, 1, 48, 128]))
            reshape497: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape496, R.shape([1, 48, 128]))
            lv626 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape497), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape498: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv626, R.shape([1, 1, 32, 128]))
            reshape499: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape498, R.shape([1, 1, 4096]))
            lv627 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight5, model_layers_28_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1013 = R.call_tir(cls.NT_matmul11, (reshape499, lv627), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1008_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1013, lv1007_1, model_layers_28_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1009_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1008_1[1]
            rms_norm252: R.Tensor((1, 1, 4096), dtype="float16") = lv1008_1[0]
            lv628 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight5, model_layers_28_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1014 = R.call_tir(cls.NT_matmul12, (rms_norm252, lv628), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split124: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv1014, indices_or_sections=2, axis=-1)
            split_0124: R.Tensor((1, 1, 14336), dtype="float16") = split124[0]
            split_1124: R.Tensor((1, 1, 14336), dtype="float16") = split124[1]
            silu124: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0124)
            mul124: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu124, split_1124)
            lv629 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight5, model_layers_28_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1015 = R.call_tir(cls.NT_matmul13, (mul124, lv629), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1010_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1015, lv1009_1, model_layers_29_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1011_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1010_1[1]
            rms_norm253: R.Tensor((1, 1, 4096), dtype="float16") = lv1010_1[0]
            lv630 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight5, model_layers_29_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1016 = R.call_tir(cls.NT_matmul10, (rms_norm253, lv630), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape500: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv1016, R.shape([1, 1, 48, 128]))
            reshape501: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape500, R.shape([1, 48, 128]))
            lv631 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape501), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape502: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv631, R.shape([1, 1, 32, 128]))
            reshape503: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape502, R.shape([1, 1, 4096]))
            lv632 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight5, model_layers_29_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1017 = R.call_tir(cls.NT_matmul11, (reshape503, lv632), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1012_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1017, lv1011_1, model_layers_29_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1013_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1012_1[1]
            rms_norm254: R.Tensor((1, 1, 4096), dtype="float16") = lv1012_1[0]
            lv633 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight5, model_layers_29_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1018 = R.call_tir(cls.NT_matmul12, (rms_norm254, lv633), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split125: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv1018, indices_or_sections=2, axis=-1)
            split_0125: R.Tensor((1, 1, 14336), dtype="float16") = split125[0]
            split_1125: R.Tensor((1, 1, 14336), dtype="float16") = split125[1]
            silu125: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0125)
            mul125: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu125, split_1125)
            lv634 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight5, model_layers_29_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1019 = R.call_tir(cls.NT_matmul13, (mul125, lv634), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1014_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1019, lv1013_1, model_layers_30_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1015_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1014_1[1]
            rms_norm255: R.Tensor((1, 1, 4096), dtype="float16") = lv1014_1[0]
            lv635 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight5, model_layers_30_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1020 = R.call_tir(cls.NT_matmul10, (rms_norm255, lv635), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape504: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv1020, R.shape([1, 1, 48, 128]))
            reshape505: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape504, R.shape([1, 48, 128]))
            lv636 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape505), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape506: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv636, R.shape([1, 1, 32, 128]))
            reshape507: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape506, R.shape([1, 1, 4096]))
            lv637 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight5, model_layers_30_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1021 = R.call_tir(cls.NT_matmul11, (reshape507, lv637), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1016_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1021, lv1015_1, model_layers_30_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1017_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1016_1[1]
            rms_norm256: R.Tensor((1, 1, 4096), dtype="float16") = lv1016_1[0]
            lv638 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight5, model_layers_30_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1022 = R.call_tir(cls.NT_matmul12, (rms_norm256, lv638), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split126: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv1022, indices_or_sections=2, axis=-1)
            split_0126: R.Tensor((1, 1, 14336), dtype="float16") = split126[0]
            split_1126: R.Tensor((1, 1, 14336), dtype="float16") = split126[1]
            silu126: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0126)
            mul126: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu126, split_1126)
            lv639 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight5, model_layers_30_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1023 = R.call_tir(cls.NT_matmul13, (mul126, lv639), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1018_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1023, lv1017_1, model_layers_31_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1019_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1018_1[1]
            rms_norm257: R.Tensor((1, 1, 4096), dtype="float16") = lv1018_1[0]
            lv640 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight5, model_layers_31_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1024 = R.call_tir(cls.NT_matmul10, (rms_norm257, lv640), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            reshape508: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(lv1024, R.shape([1, 1, 48, 128]))
            reshape509: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape508, R.shape([1, 48, 128]))
            lv641 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape509), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape510: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv641, R.shape([1, 1, 32, 128]))
            reshape511: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape510, R.shape([1, 1, 4096]))
            lv642 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight5, model_layers_31_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1025 = R.call_tir(cls.NT_matmul11, (reshape511, lv642), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1020_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1025, lv1019_1, model_layers_31_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1021_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1020_1[1]
            rms_norm258: R.Tensor((1, 1, 4096), dtype="float16") = lv1020_1[0]
            lv643 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight5, model_layers_31_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1026 = R.call_tir(cls.NT_matmul12, (rms_norm258, lv643), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            split127: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(lv1026, indices_or_sections=2, axis=-1)
            split_0127: R.Tensor((1, 1, 14336), dtype="float16") = split127[0]
            split_1127: R.Tensor((1, 1, 14336), dtype="float16") = split127[1]
            silu127: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0127)
            mul127: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu127, split_1127)
            lv644 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight5, model_layers_31_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1027 = R.call_tir(cls.NT_matmul13, (mul127, lv644), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1022_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1027, lv1021_1, model_norm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            rms_norm259: R.Tensor((1, 1, 4096), dtype="float16") = lv1022_1[0]
            gv6: R.Tuple(R.Tensor((1, 1, 4096), dtype="float16"), R.Object) = rms_norm259, paged_kv_cache
            R.output(gv6)
        return gv6

    @R.function
    def embed(input_ids: R.Tensor(("seq_len",), dtype="int32"), packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tensor(("seq_len", 4096), dtype="float16"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            lv = R.call_tir(cls.dequantize, (model_embed_tokens_q_weight, model_embed_tokens_q_scale), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            take: R.Tensor((seq_len, 4096), dtype="float16") = R.take(lv, input_ids, axis=0)
            gv: R.Tensor((seq_len, 4096), dtype="float16") = take
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul(lv806: R.Tensor((6144, 4096), dtype="float16"), rms_norm325: R.Tensor(("batch_size", 1, 4096), dtype="float16")) -> R.Tensor(("batch_size", 1, 6144), dtype="float16"):
        batch_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims644: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv806, axes=None)
            gv: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm325, permute_dims644, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul1(lv808: R.Tensor((4096, 4096), dtype="float16"), reshape643: R.Tensor(("batch_size", 1, 4096), dtype="float16")) -> R.Tensor(("batch_size", 1, 4096), dtype="float16"):
        batch_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims645: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv808, axes=None)
            gv: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape643, permute_dims645, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul10(lv164: R.Tensor((6144, 4096), dtype="float16"), rms_norm65: R.Tensor((1, 1, 4096), dtype="float16")) -> R.Tensor((1, 1, 6144), dtype="float16"):
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims130: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv164, axes=None)
            gv: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm65, permute_dims130, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul11(lv166: R.Tensor((4096, 4096), dtype="float16"), reshape131: R.Tensor((1, 1, 4096), dtype="float16")) -> R.Tensor((1, 1, 4096), dtype="float16"):
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims131: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv166, axes=None)
            gv: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape131, permute_dims131, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul12(lv167: R.Tensor((28672, 4096), dtype="float16"), rms_norm66: R.Tensor((1, 1, 4096), dtype="float16")) -> R.Tensor((1, 1, 28672), dtype="float16"):
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims132: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv167, axes=None)
            gv: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm66, permute_dims132, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul13(lv168: R.Tensor((4096, 14336), dtype="float16"), mul32: R.Tensor((1, 1, 14336), dtype="float16")) -> R.Tensor((1, 1, 4096), dtype="float16"):
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims133: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv168, axes=None)
            gv: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul32, permute_dims133, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul14(lv324: R.Tensor(("vocab_size", 4096), dtype="float16"), rms_norm129: R.Tensor((1, 1, 4096), dtype="float16")) -> R.Tensor((1, 1, "vocab_size"), dtype="float16"):
        vocab_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims258: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv324, axes=None)
            gv: R.Tensor((1, 1, vocab_size), dtype="float16") = R.matmul(rms_norm129, permute_dims258, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul15(lv1: R.Tensor(("vocab_size", 4096), dtype="float16"), hidden_states: R.Tensor(("seq_len", 4096), dtype="float16")) -> R.Tensor(("seq_len", "vocab_size"), dtype="float16"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv1, axes=None)
            gv: R.Tensor((seq_len, vocab_size), dtype="float16") = R.matmul(hidden_states, permute_dims, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul2(lv809: R.Tensor((28672, 4096), dtype="float16"), rms_norm326: R.Tensor(("batch_size", 1, 4096), dtype="float16")) -> R.Tensor(("batch_size", 1, 28672), dtype="float16"):
        batch_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims646: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv809, axes=None)
            gv: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm326, permute_dims646, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul3(lv810: R.Tensor((4096, 14336), dtype="float16"), mul160: R.Tensor(("batch_size", 1, 14336), dtype="float16")) -> R.Tensor(("batch_size", 1, 4096), dtype="float16"):
        batch_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims647: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv810, axes=None)
            gv: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul160, permute_dims647, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul4(lv966: R.Tensor(("vocab_size", 4096), dtype="float16"), rms_norm389: R.Tensor(("batch_size", 1, 4096), dtype="float16")) -> R.Tensor(("batch_size", 1, "vocab_size"), dtype="float16"):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims772: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv966, axes=None)
            gv: R.Tensor((batch_size, 1, vocab_size), dtype="float16") = R.matmul(rms_norm389, permute_dims772, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul5(lv645: R.Tensor((6144, 4096), dtype="float16"), rms_norm260: R.Tensor((1, "seq_len", 4096), dtype="float16")) -> R.Tensor((1, "seq_len", 6144), dtype="float16"):
        seq_len = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims515: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv645, axes=None)
            gv: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm260, permute_dims515, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul6(lv647: R.Tensor((4096, 4096), dtype="float16"), reshape515: R.Tensor((1, "seq_len", 4096), dtype="float16")) -> R.Tensor((1, "seq_len", 4096), dtype="float16"):
        seq_len = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims516: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv647, axes=None)
            gv: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape515, permute_dims516, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul7(lv648: R.Tensor((28672, 4096), dtype="float16"), rms_norm261: R.Tensor((1, "seq_len", 4096), dtype="float16")) -> R.Tensor((1, "seq_len", 28672), dtype="float16"):
        seq_len = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims517: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv648, axes=None)
            gv: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm261, permute_dims517, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul8(lv649: R.Tensor((4096, 14336), dtype="float16"), mul128: R.Tensor((1, "seq_len", 14336), dtype="float16")) -> R.Tensor((1, "seq_len", 4096), dtype="float16"):
        seq_len = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims518: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv649, axes=None)
            gv: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul128, permute_dims518, out_dtype="void")
            R.output(gv)
        return gv

    @R.function(private=True)
    def fused_relax_permute_dims_relax_matmul9(lv805: R.Tensor(("vocab_size", 4096), dtype="float16"), take2: R.Tensor((1, "batch_size", 4096), dtype="float16")) -> R.Tensor((1, "batch_size", "vocab_size"), dtype="float16"):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"Composite": "transpose_matmul_fuse", "Primitive": 1})
        with R.dataflow():
            permute_dims643: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv805, axes=None)
            gv: R.Tensor((1, batch_size, vocab_size), dtype="float16") = R.matmul(take2, permute_dims643, out_dtype="void")
            R.output(gv)
        return gv

    @R.function
    def gather_hidden_states(src: R.Tensor(("m", "n"), dtype="float16"), indices: R.Tensor(("batch_size",), dtype="int32"), dst: R.Tensor(("batch_size", "n"), dtype="float16")) -> R.Tensor(("batch_size", "n"), dtype="float16"):
        batch_size = T.int64(is_size_var=True)
        n = T.int64(is_size_var=True)
        m = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        cls = Module
        with R.dataflow():
            gv: R.Tensor((batch_size, n), dtype="float16") = R.call_tir_inplace(cls._gather_hidden_states, (src, indices, dst), out_sinfo=R.Tensor((batch_size, n), dtype="float16"), inplace_indices=[2])
            R.output(gv)
        return gv

    @R.function
    def get_logits(hidden_states: R.Tensor(("seq_len", 4096), dtype="float16"), packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tensor(("seq_len", "vocab_size"), dtype="float32"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            lm_head_q_weight1: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale1: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            lv1 = R.call_tir(cls.dequantize, (lm_head_q_weight1, lm_head_q_scale1), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            lv1028 = R.call_tir(cls.NT_matmul15, (hidden_states, lv1), out_sinfo=R.Tensor((seq_len, vocab_size), dtype="float16"))
            astype: R.Tensor((seq_len, vocab_size), dtype="float32") = R.astype(lv1028, dtype="float32")
            gv1: R.Tensor((seq_len, vocab_size), dtype="float32") = astype
            R.output(gv1)
        return gv1

    @R.function
    def multinomial_from_uniform(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        with R.dataflow():
            probs_1: R.Tensor((batch_size, vocab_size), dtype="float32") = probs
            uniform_samples_1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices_1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            nn_multinomial_from_uniform: R.Tensor((num_samples, 1), dtype="int32") = R.multinomial_from_uniform(probs_1, uniform_samples_1, sample_indices_1, dtype="int32")
            lv: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", nn_multinomial_from_uniform, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            gv: R.Tensor((num_samples,), dtype="int32") = lv
            R.output(gv)
        return gv

    @R.function
    def prefill(input_embed: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight2: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale2: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv2 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight2, model_layers_0_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1029 = R.call_tir(cls.NT_matmul5, (rms_norm, lv2), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1029, R.shape([1, seq_len, 48, 128]))
            reshape1: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape, R.shape([seq_len, 48, 128]))
            lv3 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape2: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv3, R.shape([1, seq_len, 32, 128]))
            reshape3: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape2, R.shape([1, seq_len, 4096]))
            lv4 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight2, model_layers_0_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1030 = R.call_tir(cls.NT_matmul6, (reshape3, lv4), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1024 = R.call_tir(cls.fuse_add_norm_prefill, (lv1030, input_embed, model_layers_0_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1025: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1024[1]
            rms_norm1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1024[0]
            lv5 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight2, model_layers_0_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1031 = R.call_tir(cls.NT_matmul7, (rms_norm1, lv5), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1031, indices_or_sections=2, axis=-1)
            split_0: R.Tensor((1, seq_len, 14336), dtype="float16") = split[0]
            split_1: R.Tensor((1, seq_len, 14336), dtype="float16") = split[1]
            silu: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0)
            mul: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu, split_1)
            lv6 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight2, model_layers_0_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1032 = R.call_tir(cls.NT_matmul8, (mul, lv6), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1026 = R.call_tir(cls.fuse_add_norm_prefill, (lv1032, lv1025, model_layers_1_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1027: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1026[1]
            rms_norm2: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1026[0]
            lv7 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight2, model_layers_1_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1033 = R.call_tir(cls.NT_matmul5, (rms_norm2, lv7), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape4: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1033, R.shape([1, seq_len, 48, 128]))
            reshape5: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape4, R.shape([seq_len, 48, 128]))
            lv8 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape5), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape6: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv8, R.shape([1, seq_len, 32, 128]))
            reshape7: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape6, R.shape([1, seq_len, 4096]))
            lv9 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight2, model_layers_1_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1034 = R.call_tir(cls.NT_matmul6, (reshape7, lv9), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1028 = R.call_tir(cls.fuse_add_norm_prefill, (lv1034, lv1027, model_layers_1_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1029_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1028[1]
            rms_norm3: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1028[0]
            lv10 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight2, model_layers_1_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1035 = R.call_tir(cls.NT_matmul7, (rms_norm3, lv10), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split1: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1035, indices_or_sections=2, axis=-1)
            split_01: R.Tensor((1, seq_len, 14336), dtype="float16") = split1[0]
            split_11: R.Tensor((1, seq_len, 14336), dtype="float16") = split1[1]
            silu1: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_01)
            mul1: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu1, split_11)
            lv11 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight2, model_layers_1_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1036 = R.call_tir(cls.NT_matmul8, (mul1, lv11), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1030_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1036, lv1029_1, model_layers_2_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1031_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1030_1[1]
            rms_norm4: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1030_1[0]
            lv12 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight2, model_layers_2_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1037 = R.call_tir(cls.NT_matmul5, (rms_norm4, lv12), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape8: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1037, R.shape([1, seq_len, 48, 128]))
            reshape9: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape8, R.shape([seq_len, 48, 128]))
            lv13 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape9), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape10: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv13, R.shape([1, seq_len, 32, 128]))
            reshape11: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape10, R.shape([1, seq_len, 4096]))
            lv14 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight2, model_layers_2_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1038 = R.call_tir(cls.NT_matmul6, (reshape11, lv14), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1032_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1038, lv1031_1, model_layers_2_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1033_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1032_1[1]
            rms_norm5: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1032_1[0]
            lv15 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight2, model_layers_2_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1039 = R.call_tir(cls.NT_matmul7, (rms_norm5, lv15), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split2: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1039, indices_or_sections=2, axis=-1)
            split_02: R.Tensor((1, seq_len, 14336), dtype="float16") = split2[0]
            split_12: R.Tensor((1, seq_len, 14336), dtype="float16") = split2[1]
            silu2: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_02)
            mul2: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu2, split_12)
            lv16 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight2, model_layers_2_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1040 = R.call_tir(cls.NT_matmul8, (mul2, lv16), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1034_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1040, lv1033_1, model_layers_3_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1035_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1034_1[1]
            rms_norm6: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1034_1[0]
            lv17 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight2, model_layers_3_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1041 = R.call_tir(cls.NT_matmul5, (rms_norm6, lv17), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape12: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1041, R.shape([1, seq_len, 48, 128]))
            reshape13: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape12, R.shape([seq_len, 48, 128]))
            lv18 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape13), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape14: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv18, R.shape([1, seq_len, 32, 128]))
            reshape15: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape14, R.shape([1, seq_len, 4096]))
            lv19 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight2, model_layers_3_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1042 = R.call_tir(cls.NT_matmul6, (reshape15, lv19), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1036_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1042, lv1035_1, model_layers_3_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1037_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1036_1[1]
            rms_norm7: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1036_1[0]
            lv20 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight2, model_layers_3_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1043 = R.call_tir(cls.NT_matmul7, (rms_norm7, lv20), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split3: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1043, indices_or_sections=2, axis=-1)
            split_03: R.Tensor((1, seq_len, 14336), dtype="float16") = split3[0]
            split_13: R.Tensor((1, seq_len, 14336), dtype="float16") = split3[1]
            silu3: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_03)
            mul3: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu3, split_13)
            lv21 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight2, model_layers_3_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1044 = R.call_tir(cls.NT_matmul8, (mul3, lv21), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1038_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1044, lv1037_1, model_layers_4_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1039_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1038_1[1]
            rms_norm8: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1038_1[0]
            lv22 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight2, model_layers_4_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1045 = R.call_tir(cls.NT_matmul5, (rms_norm8, lv22), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape16: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1045, R.shape([1, seq_len, 48, 128]))
            reshape17: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape16, R.shape([seq_len, 48, 128]))
            lv23 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape17), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape18: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv23, R.shape([1, seq_len, 32, 128]))
            reshape19: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape18, R.shape([1, seq_len, 4096]))
            lv24 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight2, model_layers_4_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1046 = R.call_tir(cls.NT_matmul6, (reshape19, lv24), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1040_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1046, lv1039_1, model_layers_4_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1041_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1040_1[1]
            rms_norm9: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1040_1[0]
            lv25 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight2, model_layers_4_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1047 = R.call_tir(cls.NT_matmul7, (rms_norm9, lv25), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split4: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1047, indices_or_sections=2, axis=-1)
            split_04: R.Tensor((1, seq_len, 14336), dtype="float16") = split4[0]
            split_14: R.Tensor((1, seq_len, 14336), dtype="float16") = split4[1]
            silu4: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_04)
            mul4: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu4, split_14)
            lv26 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight2, model_layers_4_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1048 = R.call_tir(cls.NT_matmul8, (mul4, lv26), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1042_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1048, lv1041_1, model_layers_5_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1043_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1042_1[1]
            rms_norm10: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1042_1[0]
            lv27 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight2, model_layers_5_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1049 = R.call_tir(cls.NT_matmul5, (rms_norm10, lv27), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape20: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1049, R.shape([1, seq_len, 48, 128]))
            reshape21: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape20, R.shape([seq_len, 48, 128]))
            lv28 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape21), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape22: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv28, R.shape([1, seq_len, 32, 128]))
            reshape23: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape22, R.shape([1, seq_len, 4096]))
            lv29 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight2, model_layers_5_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1050 = R.call_tir(cls.NT_matmul6, (reshape23, lv29), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1044_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1050, lv1043_1, model_layers_5_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1045_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1044_1[1]
            rms_norm11: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1044_1[0]
            lv30 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight2, model_layers_5_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1051 = R.call_tir(cls.NT_matmul7, (rms_norm11, lv30), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split5: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1051, indices_or_sections=2, axis=-1)
            split_05: R.Tensor((1, seq_len, 14336), dtype="float16") = split5[0]
            split_15: R.Tensor((1, seq_len, 14336), dtype="float16") = split5[1]
            silu5: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_05)
            mul5: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu5, split_15)
            lv31 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight2, model_layers_5_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1052 = R.call_tir(cls.NT_matmul8, (mul5, lv31), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1046_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1052, lv1045_1, model_layers_6_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1047_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1046_1[1]
            rms_norm12: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1046_1[0]
            lv32 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight2, model_layers_6_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1053 = R.call_tir(cls.NT_matmul5, (rms_norm12, lv32), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape24: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1053, R.shape([1, seq_len, 48, 128]))
            reshape25: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape24, R.shape([seq_len, 48, 128]))
            lv33 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape25), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape26: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv33, R.shape([1, seq_len, 32, 128]))
            reshape27: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape26, R.shape([1, seq_len, 4096]))
            lv34 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight2, model_layers_6_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1054 = R.call_tir(cls.NT_matmul6, (reshape27, lv34), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1048_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1054, lv1047_1, model_layers_6_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1049_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1048_1[1]
            rms_norm13: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1048_1[0]
            lv35 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight2, model_layers_6_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1055 = R.call_tir(cls.NT_matmul7, (rms_norm13, lv35), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split6: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1055, indices_or_sections=2, axis=-1)
            split_06: R.Tensor((1, seq_len, 14336), dtype="float16") = split6[0]
            split_16: R.Tensor((1, seq_len, 14336), dtype="float16") = split6[1]
            silu6: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_06)
            mul6: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu6, split_16)
            lv36 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight2, model_layers_6_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1056 = R.call_tir(cls.NT_matmul8, (mul6, lv36), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1050_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1056, lv1049_1, model_layers_7_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1051_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1050_1[1]
            rms_norm14: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1050_1[0]
            lv37 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight2, model_layers_7_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1057 = R.call_tir(cls.NT_matmul5, (rms_norm14, lv37), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape28: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1057, R.shape([1, seq_len, 48, 128]))
            reshape29: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape28, R.shape([seq_len, 48, 128]))
            lv38 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape29), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape30: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv38, R.shape([1, seq_len, 32, 128]))
            reshape31: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape30, R.shape([1, seq_len, 4096]))
            lv39 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight2, model_layers_7_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1058 = R.call_tir(cls.NT_matmul6, (reshape31, lv39), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1052_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1058, lv1051_1, model_layers_7_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1053_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1052_1[1]
            rms_norm15: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1052_1[0]
            lv40 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight2, model_layers_7_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1059 = R.call_tir(cls.NT_matmul7, (rms_norm15, lv40), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split7: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1059, indices_or_sections=2, axis=-1)
            split_07: R.Tensor((1, seq_len, 14336), dtype="float16") = split7[0]
            split_17: R.Tensor((1, seq_len, 14336), dtype="float16") = split7[1]
            silu7: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_07)
            mul7: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu7, split_17)
            lv41 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight2, model_layers_7_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1060 = R.call_tir(cls.NT_matmul8, (mul7, lv41), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1054_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1060, lv1053_1, model_layers_8_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1055_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1054_1[1]
            rms_norm16: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1054_1[0]
            lv42 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight2, model_layers_8_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1061 = R.call_tir(cls.NT_matmul5, (rms_norm16, lv42), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape32: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1061, R.shape([1, seq_len, 48, 128]))
            reshape33: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape32, R.shape([seq_len, 48, 128]))
            lv43 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape33), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape34: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv43, R.shape([1, seq_len, 32, 128]))
            reshape35: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape34, R.shape([1, seq_len, 4096]))
            lv44 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight2, model_layers_8_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1062 = R.call_tir(cls.NT_matmul6, (reshape35, lv44), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1056_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1062, lv1055_1, model_layers_8_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1057_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1056_1[1]
            rms_norm17: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1056_1[0]
            lv45 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight2, model_layers_8_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1063 = R.call_tir(cls.NT_matmul7, (rms_norm17, lv45), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split8: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1063, indices_or_sections=2, axis=-1)
            split_08: R.Tensor((1, seq_len, 14336), dtype="float16") = split8[0]
            split_18: R.Tensor((1, seq_len, 14336), dtype="float16") = split8[1]
            silu8: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_08)
            mul8: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu8, split_18)
            lv46 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight2, model_layers_8_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1064 = R.call_tir(cls.NT_matmul8, (mul8, lv46), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1058_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1064, lv1057_1, model_layers_9_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1059_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1058_1[1]
            rms_norm18: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1058_1[0]
            lv47 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight2, model_layers_9_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1065 = R.call_tir(cls.NT_matmul5, (rms_norm18, lv47), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape36: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1065, R.shape([1, seq_len, 48, 128]))
            reshape37: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape36, R.shape([seq_len, 48, 128]))
            lv48 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape37), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape38: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv48, R.shape([1, seq_len, 32, 128]))
            reshape39: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape38, R.shape([1, seq_len, 4096]))
            lv49 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight2, model_layers_9_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1066 = R.call_tir(cls.NT_matmul6, (reshape39, lv49), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1060_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1066, lv1059_1, model_layers_9_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1061_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1060_1[1]
            rms_norm19: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1060_1[0]
            lv50 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight2, model_layers_9_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1067 = R.call_tir(cls.NT_matmul7, (rms_norm19, lv50), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split9: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1067, indices_or_sections=2, axis=-1)
            split_09: R.Tensor((1, seq_len, 14336), dtype="float16") = split9[0]
            split_19: R.Tensor((1, seq_len, 14336), dtype="float16") = split9[1]
            silu9: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_09)
            mul9: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu9, split_19)
            lv51 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight2, model_layers_9_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1068 = R.call_tir(cls.NT_matmul8, (mul9, lv51), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1062_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1068, lv1061_1, model_layers_10_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1063_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1062_1[1]
            rms_norm20: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1062_1[0]
            lv52 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight2, model_layers_10_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1069 = R.call_tir(cls.NT_matmul5, (rms_norm20, lv52), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape40: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1069, R.shape([1, seq_len, 48, 128]))
            reshape41: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape40, R.shape([seq_len, 48, 128]))
            lv53 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape41), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape42: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv53, R.shape([1, seq_len, 32, 128]))
            reshape43: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape42, R.shape([1, seq_len, 4096]))
            lv54 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight2, model_layers_10_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1070 = R.call_tir(cls.NT_matmul6, (reshape43, lv54), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1064_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1070, lv1063_1, model_layers_10_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1065_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1064_1[1]
            rms_norm21: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1064_1[0]
            lv55 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight2, model_layers_10_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1071 = R.call_tir(cls.NT_matmul7, (rms_norm21, lv55), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split10: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1071, indices_or_sections=2, axis=-1)
            split_010: R.Tensor((1, seq_len, 14336), dtype="float16") = split10[0]
            split_110: R.Tensor((1, seq_len, 14336), dtype="float16") = split10[1]
            silu10: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_010)
            mul10: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu10, split_110)
            lv56 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight2, model_layers_10_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1072 = R.call_tir(cls.NT_matmul8, (mul10, lv56), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1066_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1072, lv1065_1, model_layers_11_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1067_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1066_1[1]
            rms_norm22: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1066_1[0]
            lv57 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight2, model_layers_11_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1073 = R.call_tir(cls.NT_matmul5, (rms_norm22, lv57), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape44: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1073, R.shape([1, seq_len, 48, 128]))
            reshape45: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape44, R.shape([seq_len, 48, 128]))
            lv58 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape45), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape46: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv58, R.shape([1, seq_len, 32, 128]))
            reshape47: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape46, R.shape([1, seq_len, 4096]))
            lv59 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight2, model_layers_11_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1074 = R.call_tir(cls.NT_matmul6, (reshape47, lv59), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1068_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1074, lv1067_1, model_layers_11_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1069_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1068_1[1]
            rms_norm23: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1068_1[0]
            lv60 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight2, model_layers_11_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1075 = R.call_tir(cls.NT_matmul7, (rms_norm23, lv60), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split11: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1075, indices_or_sections=2, axis=-1)
            split_011: R.Tensor((1, seq_len, 14336), dtype="float16") = split11[0]
            split_111: R.Tensor((1, seq_len, 14336), dtype="float16") = split11[1]
            silu11: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_011)
            mul11: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu11, split_111)
            lv61 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight2, model_layers_11_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1076 = R.call_tir(cls.NT_matmul8, (mul11, lv61), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1070_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1076, lv1069_1, model_layers_12_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1071_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1070_1[1]
            rms_norm24: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1070_1[0]
            lv62 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight2, model_layers_12_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1077 = R.call_tir(cls.NT_matmul5, (rms_norm24, lv62), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape48: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1077, R.shape([1, seq_len, 48, 128]))
            reshape49: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape48, R.shape([seq_len, 48, 128]))
            lv63 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape49), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape50: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv63, R.shape([1, seq_len, 32, 128]))
            reshape51: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape50, R.shape([1, seq_len, 4096]))
            lv64 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight2, model_layers_12_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1078 = R.call_tir(cls.NT_matmul6, (reshape51, lv64), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1072_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1078, lv1071_1, model_layers_12_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1073_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1072_1[1]
            rms_norm25: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1072_1[0]
            lv65 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight2, model_layers_12_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1079 = R.call_tir(cls.NT_matmul7, (rms_norm25, lv65), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split12: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1079, indices_or_sections=2, axis=-1)
            split_012: R.Tensor((1, seq_len, 14336), dtype="float16") = split12[0]
            split_112: R.Tensor((1, seq_len, 14336), dtype="float16") = split12[1]
            silu12: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_012)
            mul12: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu12, split_112)
            lv66 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight2, model_layers_12_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1080 = R.call_tir(cls.NT_matmul8, (mul12, lv66), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1074_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1080, lv1073_1, model_layers_13_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1075_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1074_1[1]
            rms_norm26: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1074_1[0]
            lv67 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight2, model_layers_13_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1081 = R.call_tir(cls.NT_matmul5, (rms_norm26, lv67), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape52: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1081, R.shape([1, seq_len, 48, 128]))
            reshape53: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape52, R.shape([seq_len, 48, 128]))
            lv68 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape53), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape54: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv68, R.shape([1, seq_len, 32, 128]))
            reshape55: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape54, R.shape([1, seq_len, 4096]))
            lv69 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight2, model_layers_13_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1082 = R.call_tir(cls.NT_matmul6, (reshape55, lv69), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1076_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1082, lv1075_1, model_layers_13_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1077_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1076_1[1]
            rms_norm27: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1076_1[0]
            lv70 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight2, model_layers_13_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1083 = R.call_tir(cls.NT_matmul7, (rms_norm27, lv70), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split13: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1083, indices_or_sections=2, axis=-1)
            split_013: R.Tensor((1, seq_len, 14336), dtype="float16") = split13[0]
            split_113: R.Tensor((1, seq_len, 14336), dtype="float16") = split13[1]
            silu13: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_013)
            mul13: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu13, split_113)
            lv71 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight2, model_layers_13_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1084 = R.call_tir(cls.NT_matmul8, (mul13, lv71), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1078_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1084, lv1077_1, model_layers_14_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1079_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1078_1[1]
            rms_norm28: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1078_1[0]
            lv72 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight2, model_layers_14_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1085 = R.call_tir(cls.NT_matmul5, (rms_norm28, lv72), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape56: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1085, R.shape([1, seq_len, 48, 128]))
            reshape57: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape56, R.shape([seq_len, 48, 128]))
            lv73 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape57), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape58: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv73, R.shape([1, seq_len, 32, 128]))
            reshape59: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape58, R.shape([1, seq_len, 4096]))
            lv74 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight2, model_layers_14_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1086 = R.call_tir(cls.NT_matmul6, (reshape59, lv74), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1080_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1086, lv1079_1, model_layers_14_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1081_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1080_1[1]
            rms_norm29: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1080_1[0]
            lv75 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight2, model_layers_14_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1087 = R.call_tir(cls.NT_matmul7, (rms_norm29, lv75), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split14: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1087, indices_or_sections=2, axis=-1)
            split_014: R.Tensor((1, seq_len, 14336), dtype="float16") = split14[0]
            split_114: R.Tensor((1, seq_len, 14336), dtype="float16") = split14[1]
            silu14: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_014)
            mul14: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu14, split_114)
            lv76 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight2, model_layers_14_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1088 = R.call_tir(cls.NT_matmul8, (mul14, lv76), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1082_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1088, lv1081_1, model_layers_15_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1083_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1082_1[1]
            rms_norm30: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1082_1[0]
            lv77 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight2, model_layers_15_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1089 = R.call_tir(cls.NT_matmul5, (rms_norm30, lv77), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape60: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1089, R.shape([1, seq_len, 48, 128]))
            reshape61: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape60, R.shape([seq_len, 48, 128]))
            lv78 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape61), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape62: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv78, R.shape([1, seq_len, 32, 128]))
            reshape63: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape62, R.shape([1, seq_len, 4096]))
            lv79 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight2, model_layers_15_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1090 = R.call_tir(cls.NT_matmul6, (reshape63, lv79), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1084_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1090, lv1083_1, model_layers_15_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1085_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1084_1[1]
            rms_norm31: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1084_1[0]
            lv80 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight2, model_layers_15_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1091 = R.call_tir(cls.NT_matmul7, (rms_norm31, lv80), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split15: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1091, indices_or_sections=2, axis=-1)
            split_015: R.Tensor((1, seq_len, 14336), dtype="float16") = split15[0]
            split_115: R.Tensor((1, seq_len, 14336), dtype="float16") = split15[1]
            silu15: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_015)
            mul15: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu15, split_115)
            lv81 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight2, model_layers_15_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1092 = R.call_tir(cls.NT_matmul8, (mul15, lv81), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1086_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1092, lv1085_1, model_layers_16_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1087_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1086_1[1]
            rms_norm32: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1086_1[0]
            lv82 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight2, model_layers_16_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1093 = R.call_tir(cls.NT_matmul5, (rms_norm32, lv82), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape64: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1093, R.shape([1, seq_len, 48, 128]))
            reshape65: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape64, R.shape([seq_len, 48, 128]))
            lv83 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape65), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape66: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv83, R.shape([1, seq_len, 32, 128]))
            reshape67: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape66, R.shape([1, seq_len, 4096]))
            lv84 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight2, model_layers_16_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1094 = R.call_tir(cls.NT_matmul6, (reshape67, lv84), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1088_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1094, lv1087_1, model_layers_16_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1089_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1088_1[1]
            rms_norm33: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1088_1[0]
            lv85 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight2, model_layers_16_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1095 = R.call_tir(cls.NT_matmul7, (rms_norm33, lv85), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split16: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1095, indices_or_sections=2, axis=-1)
            split_016: R.Tensor((1, seq_len, 14336), dtype="float16") = split16[0]
            split_116: R.Tensor((1, seq_len, 14336), dtype="float16") = split16[1]
            silu16: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_016)
            mul16: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu16, split_116)
            lv86 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight2, model_layers_16_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1096 = R.call_tir(cls.NT_matmul8, (mul16, lv86), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1090_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1096, lv1089_1, model_layers_17_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1091_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1090_1[1]
            rms_norm34: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1090_1[0]
            lv87 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight2, model_layers_17_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1097 = R.call_tir(cls.NT_matmul5, (rms_norm34, lv87), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape68: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1097, R.shape([1, seq_len, 48, 128]))
            reshape69: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape68, R.shape([seq_len, 48, 128]))
            lv88 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape69), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape70: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv88, R.shape([1, seq_len, 32, 128]))
            reshape71: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape70, R.shape([1, seq_len, 4096]))
            lv89 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight2, model_layers_17_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1098 = R.call_tir(cls.NT_matmul6, (reshape71, lv89), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1092_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1098, lv1091_1, model_layers_17_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1093_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1092_1[1]
            rms_norm35: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1092_1[0]
            lv90 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight2, model_layers_17_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1099 = R.call_tir(cls.NT_matmul7, (rms_norm35, lv90), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split17: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1099, indices_or_sections=2, axis=-1)
            split_017: R.Tensor((1, seq_len, 14336), dtype="float16") = split17[0]
            split_117: R.Tensor((1, seq_len, 14336), dtype="float16") = split17[1]
            silu17: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_017)
            mul17: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu17, split_117)
            lv91 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight2, model_layers_17_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1100 = R.call_tir(cls.NT_matmul8, (mul17, lv91), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1094_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1100, lv1093_1, model_layers_18_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1095_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1094_1[1]
            rms_norm36: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1094_1[0]
            lv92 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight2, model_layers_18_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1101 = R.call_tir(cls.NT_matmul5, (rms_norm36, lv92), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape72: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1101, R.shape([1, seq_len, 48, 128]))
            reshape73: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape72, R.shape([seq_len, 48, 128]))
            lv93 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape73), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape74: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv93, R.shape([1, seq_len, 32, 128]))
            reshape75: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape74, R.shape([1, seq_len, 4096]))
            lv94 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight2, model_layers_18_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1102 = R.call_tir(cls.NT_matmul6, (reshape75, lv94), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1096_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1102, lv1095_1, model_layers_18_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1097_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1096_1[1]
            rms_norm37: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1096_1[0]
            lv95 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight2, model_layers_18_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1103 = R.call_tir(cls.NT_matmul7, (rms_norm37, lv95), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split18: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1103, indices_or_sections=2, axis=-1)
            split_018: R.Tensor((1, seq_len, 14336), dtype="float16") = split18[0]
            split_118: R.Tensor((1, seq_len, 14336), dtype="float16") = split18[1]
            silu18: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_018)
            mul18: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu18, split_118)
            lv96 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight2, model_layers_18_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1104 = R.call_tir(cls.NT_matmul8, (mul18, lv96), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1098_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1104, lv1097_1, model_layers_19_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1099_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1098_1[1]
            rms_norm38: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1098_1[0]
            lv97 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight2, model_layers_19_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1105 = R.call_tir(cls.NT_matmul5, (rms_norm38, lv97), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape76: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1105, R.shape([1, seq_len, 48, 128]))
            reshape77: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape76, R.shape([seq_len, 48, 128]))
            lv98 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape77), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape78: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv98, R.shape([1, seq_len, 32, 128]))
            reshape79: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape78, R.shape([1, seq_len, 4096]))
            lv99 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight2, model_layers_19_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1106 = R.call_tir(cls.NT_matmul6, (reshape79, lv99), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1100_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1106, lv1099_1, model_layers_19_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1101_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1100_1[1]
            rms_norm39: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1100_1[0]
            lv100 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight2, model_layers_19_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1107 = R.call_tir(cls.NT_matmul7, (rms_norm39, lv100), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split19: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1107, indices_or_sections=2, axis=-1)
            split_019: R.Tensor((1, seq_len, 14336), dtype="float16") = split19[0]
            split_119: R.Tensor((1, seq_len, 14336), dtype="float16") = split19[1]
            silu19: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_019)
            mul19: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu19, split_119)
            lv101 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight2, model_layers_19_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1108 = R.call_tir(cls.NT_matmul8, (mul19, lv101), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1102_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1108, lv1101_1, model_layers_20_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1103_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1102_1[1]
            rms_norm40: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1102_1[0]
            lv102 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight2, model_layers_20_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1109 = R.call_tir(cls.NT_matmul5, (rms_norm40, lv102), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape80: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1109, R.shape([1, seq_len, 48, 128]))
            reshape81: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape80, R.shape([seq_len, 48, 128]))
            lv103 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape81), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape82: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv103, R.shape([1, seq_len, 32, 128]))
            reshape83: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape82, R.shape([1, seq_len, 4096]))
            lv104 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight2, model_layers_20_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1110 = R.call_tir(cls.NT_matmul6, (reshape83, lv104), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1104_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1110, lv1103_1, model_layers_20_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1105_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1104_1[1]
            rms_norm41: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1104_1[0]
            lv105 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight2, model_layers_20_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1111 = R.call_tir(cls.NT_matmul7, (rms_norm41, lv105), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split20: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1111, indices_or_sections=2, axis=-1)
            split_020: R.Tensor((1, seq_len, 14336), dtype="float16") = split20[0]
            split_120: R.Tensor((1, seq_len, 14336), dtype="float16") = split20[1]
            silu20: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_020)
            mul20: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu20, split_120)
            lv106 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight2, model_layers_20_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1112 = R.call_tir(cls.NT_matmul8, (mul20, lv106), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1106_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1112, lv1105_1, model_layers_21_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1107_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1106_1[1]
            rms_norm42: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1106_1[0]
            lv107 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight2, model_layers_21_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1113 = R.call_tir(cls.NT_matmul5, (rms_norm42, lv107), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape84: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1113, R.shape([1, seq_len, 48, 128]))
            reshape85: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape84, R.shape([seq_len, 48, 128]))
            lv108 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape85), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape86: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv108, R.shape([1, seq_len, 32, 128]))
            reshape87: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape86, R.shape([1, seq_len, 4096]))
            lv109 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight2, model_layers_21_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1114 = R.call_tir(cls.NT_matmul6, (reshape87, lv109), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1108_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1114, lv1107_1, model_layers_21_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1109_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1108_1[1]
            rms_norm43: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1108_1[0]
            lv110 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight2, model_layers_21_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1115 = R.call_tir(cls.NT_matmul7, (rms_norm43, lv110), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split21: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1115, indices_or_sections=2, axis=-1)
            split_021: R.Tensor((1, seq_len, 14336), dtype="float16") = split21[0]
            split_121: R.Tensor((1, seq_len, 14336), dtype="float16") = split21[1]
            silu21: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_021)
            mul21: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu21, split_121)
            lv111 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight2, model_layers_21_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1116 = R.call_tir(cls.NT_matmul8, (mul21, lv111), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1110_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1116, lv1109_1, model_layers_22_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1111_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1110_1[1]
            rms_norm44: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1110_1[0]
            lv112 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight2, model_layers_22_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1117 = R.call_tir(cls.NT_matmul5, (rms_norm44, lv112), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape88: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1117, R.shape([1, seq_len, 48, 128]))
            reshape89: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape88, R.shape([seq_len, 48, 128]))
            lv113 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape89), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape90: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv113, R.shape([1, seq_len, 32, 128]))
            reshape91: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape90, R.shape([1, seq_len, 4096]))
            lv114 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight2, model_layers_22_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1118 = R.call_tir(cls.NT_matmul6, (reshape91, lv114), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1112_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1118, lv1111_1, model_layers_22_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1113_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1112_1[1]
            rms_norm45: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1112_1[0]
            lv115 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight2, model_layers_22_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1119 = R.call_tir(cls.NT_matmul7, (rms_norm45, lv115), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split22: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1119, indices_or_sections=2, axis=-1)
            split_022: R.Tensor((1, seq_len, 14336), dtype="float16") = split22[0]
            split_122: R.Tensor((1, seq_len, 14336), dtype="float16") = split22[1]
            silu22: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_022)
            mul22: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu22, split_122)
            lv116 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight2, model_layers_22_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1120 = R.call_tir(cls.NT_matmul8, (mul22, lv116), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1114_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1120, lv1113_1, model_layers_23_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1115_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1114_1[1]
            rms_norm46: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1114_1[0]
            lv117 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight2, model_layers_23_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1121 = R.call_tir(cls.NT_matmul5, (rms_norm46, lv117), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape92: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1121, R.shape([1, seq_len, 48, 128]))
            reshape93: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape92, R.shape([seq_len, 48, 128]))
            lv118 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape93), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape94: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv118, R.shape([1, seq_len, 32, 128]))
            reshape95: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape94, R.shape([1, seq_len, 4096]))
            lv119 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight2, model_layers_23_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1122 = R.call_tir(cls.NT_matmul6, (reshape95, lv119), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1116_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1122, lv1115_1, model_layers_23_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1117_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1116_1[1]
            rms_norm47: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1116_1[0]
            lv120 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight2, model_layers_23_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1123 = R.call_tir(cls.NT_matmul7, (rms_norm47, lv120), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split23: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1123, indices_or_sections=2, axis=-1)
            split_023: R.Tensor((1, seq_len, 14336), dtype="float16") = split23[0]
            split_123: R.Tensor((1, seq_len, 14336), dtype="float16") = split23[1]
            silu23: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_023)
            mul23: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu23, split_123)
            lv121 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight2, model_layers_23_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1124 = R.call_tir(cls.NT_matmul8, (mul23, lv121), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1118_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1124, lv1117_1, model_layers_24_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1119_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1118_1[1]
            rms_norm48: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1118_1[0]
            lv122 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight2, model_layers_24_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1125 = R.call_tir(cls.NT_matmul5, (rms_norm48, lv122), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape96: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1125, R.shape([1, seq_len, 48, 128]))
            reshape97: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape96, R.shape([seq_len, 48, 128]))
            lv123 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape97), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape98: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv123, R.shape([1, seq_len, 32, 128]))
            reshape99: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape98, R.shape([1, seq_len, 4096]))
            lv124 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight2, model_layers_24_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1126 = R.call_tir(cls.NT_matmul6, (reshape99, lv124), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1120_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1126, lv1119_1, model_layers_24_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1121_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1120_1[1]
            rms_norm49: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1120_1[0]
            lv125 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight2, model_layers_24_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1127 = R.call_tir(cls.NT_matmul7, (rms_norm49, lv125), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split24: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1127, indices_or_sections=2, axis=-1)
            split_024: R.Tensor((1, seq_len, 14336), dtype="float16") = split24[0]
            split_124: R.Tensor((1, seq_len, 14336), dtype="float16") = split24[1]
            silu24: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_024)
            mul24: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu24, split_124)
            lv126 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight2, model_layers_24_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1128 = R.call_tir(cls.NT_matmul8, (mul24, lv126), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1122_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1128, lv1121_1, model_layers_25_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1123_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1122_1[1]
            rms_norm50: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1122_1[0]
            lv127 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight2, model_layers_25_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1129 = R.call_tir(cls.NT_matmul5, (rms_norm50, lv127), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape100: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1129, R.shape([1, seq_len, 48, 128]))
            reshape101: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape100, R.shape([seq_len, 48, 128]))
            lv128 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape101), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape102: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv128, R.shape([1, seq_len, 32, 128]))
            reshape103: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape102, R.shape([1, seq_len, 4096]))
            lv129 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight2, model_layers_25_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1130 = R.call_tir(cls.NT_matmul6, (reshape103, lv129), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1124_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1130, lv1123_1, model_layers_25_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1125_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1124_1[1]
            rms_norm51: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1124_1[0]
            lv130 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight2, model_layers_25_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1131 = R.call_tir(cls.NT_matmul7, (rms_norm51, lv130), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split25: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1131, indices_or_sections=2, axis=-1)
            split_025: R.Tensor((1, seq_len, 14336), dtype="float16") = split25[0]
            split_125: R.Tensor((1, seq_len, 14336), dtype="float16") = split25[1]
            silu25: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_025)
            mul25: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu25, split_125)
            lv131 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight2, model_layers_25_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1132 = R.call_tir(cls.NT_matmul8, (mul25, lv131), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1126_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1132, lv1125_1, model_layers_26_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1127_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1126_1[1]
            rms_norm52: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1126_1[0]
            lv132 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight2, model_layers_26_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1133 = R.call_tir(cls.NT_matmul5, (rms_norm52, lv132), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape104: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1133, R.shape([1, seq_len, 48, 128]))
            reshape105: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape104, R.shape([seq_len, 48, 128]))
            lv133 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape105), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape106: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv133, R.shape([1, seq_len, 32, 128]))
            reshape107: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape106, R.shape([1, seq_len, 4096]))
            lv134 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight2, model_layers_26_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1134 = R.call_tir(cls.NT_matmul6, (reshape107, lv134), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1128_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1134, lv1127_1, model_layers_26_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1129_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1128_1[1]
            rms_norm53: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1128_1[0]
            lv135 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight2, model_layers_26_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1135 = R.call_tir(cls.NT_matmul7, (rms_norm53, lv135), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split26: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1135, indices_or_sections=2, axis=-1)
            split_026: R.Tensor((1, seq_len, 14336), dtype="float16") = split26[0]
            split_126: R.Tensor((1, seq_len, 14336), dtype="float16") = split26[1]
            silu26: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_026)
            mul26: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu26, split_126)
            lv136 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight2, model_layers_26_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1136 = R.call_tir(cls.NT_matmul8, (mul26, lv136), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1130_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1136, lv1129_1, model_layers_27_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1131_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1130_1[1]
            rms_norm54: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1130_1[0]
            lv137 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight2, model_layers_27_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1137 = R.call_tir(cls.NT_matmul5, (rms_norm54, lv137), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape108: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1137, R.shape([1, seq_len, 48, 128]))
            reshape109: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape108, R.shape([seq_len, 48, 128]))
            lv138 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape109), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape110: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv138, R.shape([1, seq_len, 32, 128]))
            reshape111: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape110, R.shape([1, seq_len, 4096]))
            lv139 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight2, model_layers_27_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1138 = R.call_tir(cls.NT_matmul6, (reshape111, lv139), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1132_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1138, lv1131_1, model_layers_27_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1133_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1132_1[1]
            rms_norm55: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1132_1[0]
            lv140 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight2, model_layers_27_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1139 = R.call_tir(cls.NT_matmul7, (rms_norm55, lv140), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split27: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1139, indices_or_sections=2, axis=-1)
            split_027: R.Tensor((1, seq_len, 14336), dtype="float16") = split27[0]
            split_127: R.Tensor((1, seq_len, 14336), dtype="float16") = split27[1]
            silu27: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_027)
            mul27: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu27, split_127)
            lv141 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight2, model_layers_27_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1140 = R.call_tir(cls.NT_matmul8, (mul27, lv141), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1134_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1140, lv1133_1, model_layers_28_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1135_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1134_1[1]
            rms_norm56: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1134_1[0]
            lv142 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight2, model_layers_28_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1141 = R.call_tir(cls.NT_matmul5, (rms_norm56, lv142), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape112: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1141, R.shape([1, seq_len, 48, 128]))
            reshape113: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape112, R.shape([seq_len, 48, 128]))
            lv143 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape113), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape114: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv143, R.shape([1, seq_len, 32, 128]))
            reshape115: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape114, R.shape([1, seq_len, 4096]))
            lv144 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight2, model_layers_28_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1142 = R.call_tir(cls.NT_matmul6, (reshape115, lv144), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1136_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1142, lv1135_1, model_layers_28_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1137_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1136_1[1]
            rms_norm57: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1136_1[0]
            lv145 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight2, model_layers_28_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1143 = R.call_tir(cls.NT_matmul7, (rms_norm57, lv145), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split28: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1143, indices_or_sections=2, axis=-1)
            split_028: R.Tensor((1, seq_len, 14336), dtype="float16") = split28[0]
            split_128: R.Tensor((1, seq_len, 14336), dtype="float16") = split28[1]
            silu28: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_028)
            mul28: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu28, split_128)
            lv146 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight2, model_layers_28_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1144 = R.call_tir(cls.NT_matmul8, (mul28, lv146), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1138_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1144, lv1137_1, model_layers_29_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1139_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1138_1[1]
            rms_norm58: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1138_1[0]
            lv147 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight2, model_layers_29_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1145 = R.call_tir(cls.NT_matmul5, (rms_norm58, lv147), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape116: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1145, R.shape([1, seq_len, 48, 128]))
            reshape117: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape116, R.shape([seq_len, 48, 128]))
            lv148 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape117), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape118: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv148, R.shape([1, seq_len, 32, 128]))
            reshape119: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape118, R.shape([1, seq_len, 4096]))
            lv149 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight2, model_layers_29_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1146 = R.call_tir(cls.NT_matmul6, (reshape119, lv149), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1140_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1146, lv1139_1, model_layers_29_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1141_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1140_1[1]
            rms_norm59: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1140_1[0]
            lv150 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight2, model_layers_29_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1147 = R.call_tir(cls.NT_matmul7, (rms_norm59, lv150), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split29: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1147, indices_or_sections=2, axis=-1)
            split_029: R.Tensor((1, seq_len, 14336), dtype="float16") = split29[0]
            split_129: R.Tensor((1, seq_len, 14336), dtype="float16") = split29[1]
            silu29: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_029)
            mul29: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu29, split_129)
            lv151 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight2, model_layers_29_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1148 = R.call_tir(cls.NT_matmul8, (mul29, lv151), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1142_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1148, lv1141_1, model_layers_30_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1143_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1142_1[1]
            rms_norm60: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1142_1[0]
            lv152 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight2, model_layers_30_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1149 = R.call_tir(cls.NT_matmul5, (rms_norm60, lv152), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape120: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1149, R.shape([1, seq_len, 48, 128]))
            reshape121: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape120, R.shape([seq_len, 48, 128]))
            lv153 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape121), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape122: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv153, R.shape([1, seq_len, 32, 128]))
            reshape123: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape122, R.shape([1, seq_len, 4096]))
            lv154 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight2, model_layers_30_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1150 = R.call_tir(cls.NT_matmul6, (reshape123, lv154), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1144_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1150, lv1143_1, model_layers_30_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1145_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1144_1[1]
            rms_norm61: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1144_1[0]
            lv155 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight2, model_layers_30_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1151 = R.call_tir(cls.NT_matmul7, (rms_norm61, lv155), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split30: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1151, indices_or_sections=2, axis=-1)
            split_030: R.Tensor((1, seq_len, 14336), dtype="float16") = split30[0]
            split_130: R.Tensor((1, seq_len, 14336), dtype="float16") = split30[1]
            silu30: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_030)
            mul30: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu30, split_130)
            lv156 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight2, model_layers_30_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1152 = R.call_tir(cls.NT_matmul8, (mul30, lv156), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1146_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1152, lv1145_1, model_layers_31_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1147_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1146_1[1]
            rms_norm62: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1146_1[0]
            lv157 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight2, model_layers_31_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1153 = R.call_tir(cls.NT_matmul5, (rms_norm62, lv157), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape124: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1153, R.shape([1, seq_len, 48, 128]))
            reshape125: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape124, R.shape([seq_len, 48, 128]))
            lv158 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape125), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape126: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv158, R.shape([1, seq_len, 32, 128]))
            reshape127: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape126, R.shape([1, seq_len, 4096]))
            lv159 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight2, model_layers_31_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1154 = R.call_tir(cls.NT_matmul6, (reshape127, lv159), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1148_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1154, lv1147_1, model_layers_31_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1149_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1148_1[1]
            rms_norm63: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1148_1[0]
            lv160 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight2, model_layers_31_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1155 = R.call_tir(cls.NT_matmul7, (rms_norm63, lv160), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split31: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1155, indices_or_sections=2, axis=-1)
            split_031: R.Tensor((1, seq_len, 14336), dtype="float16") = split31[0]
            split_131: R.Tensor((1, seq_len, 14336), dtype="float16") = split31[1]
            silu31: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_031)
            mul31: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu31, split_131)
            lv161 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight2, model_layers_31_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1156 = R.call_tir(cls.NT_matmul8, (mul31, lv161), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1150_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1156, lv1149_1, model_norm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm64: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1150_1[0]
            lv162 = R.call_tir(cls.index, (rms_norm64,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv163 = R.call_tir(cls.dequantize, (lm_head_q_weight2, lm_head_q_scale2), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            lv1157 = R.call_tir(cls.NT_matmul14, (lv162, lv163), out_sinfo=R.Tensor((1, 1, vocab_size), dtype="float16"))
            astype1: R.Tensor((1, 1, vocab_size), dtype="float32") = R.astype(lv1157, dtype="float32")
            gv3: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = astype1, paged_kv_cache
            R.output(gv3)
        return gv3

    @R.function
    def prefill_to_last_hidden_states(input_embed: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm130: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv325 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight4, model_layers_0_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1158 = R.call_tir(cls.NT_matmul5, (rms_norm130, lv325), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape256: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1158, R.shape([1, seq_len, 48, 128]))
            reshape257: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape256, R.shape([seq_len, 48, 128]))
            lv326 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape257), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape258: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv326, R.shape([1, seq_len, 32, 128]))
            reshape259: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape258, R.shape([1, seq_len, 4096]))
            lv327 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight4, model_layers_0_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1159 = R.call_tir(cls.NT_matmul6, (reshape259, lv327), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1152 = R.call_tir(cls.fuse_add_norm_prefill, (lv1159, input_embed, model_layers_0_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1153: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1152[1]
            rms_norm131: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1152[0]
            lv328 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight4, model_layers_0_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1160 = R.call_tir(cls.NT_matmul7, (rms_norm131, lv328), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split64: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1160, indices_or_sections=2, axis=-1)
            split_064: R.Tensor((1, seq_len, 14336), dtype="float16") = split64[0]
            split_164: R.Tensor((1, seq_len, 14336), dtype="float16") = split64[1]
            silu64: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_064)
            mul64: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu64, split_164)
            lv329 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight4, model_layers_0_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1161 = R.call_tir(cls.NT_matmul8, (mul64, lv329), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1154 = R.call_tir(cls.fuse_add_norm_prefill, (lv1161, lv1153, model_layers_1_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1155: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1154[1]
            rms_norm132: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1154[0]
            lv330 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight4, model_layers_1_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1162 = R.call_tir(cls.NT_matmul5, (rms_norm132, lv330), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape260: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1162, R.shape([1, seq_len, 48, 128]))
            reshape261: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape260, R.shape([seq_len, 48, 128]))
            lv331 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape261), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape262: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv331, R.shape([1, seq_len, 32, 128]))
            reshape263: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape262, R.shape([1, seq_len, 4096]))
            lv332 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight4, model_layers_1_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1163 = R.call_tir(cls.NT_matmul6, (reshape263, lv332), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1156 = R.call_tir(cls.fuse_add_norm_prefill, (lv1163, lv1155, model_layers_1_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1157: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1156[1]
            rms_norm133: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1156[0]
            lv333 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight4, model_layers_1_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1164 = R.call_tir(cls.NT_matmul7, (rms_norm133, lv333), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split65: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1164, indices_or_sections=2, axis=-1)
            split_065: R.Tensor((1, seq_len, 14336), dtype="float16") = split65[0]
            split_165: R.Tensor((1, seq_len, 14336), dtype="float16") = split65[1]
            silu65: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_065)
            mul65: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu65, split_165)
            lv334 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight4, model_layers_1_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1165 = R.call_tir(cls.NT_matmul8, (mul65, lv334), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1158_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1165, lv1157, model_layers_2_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1159_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1158_1[1]
            rms_norm134: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1158_1[0]
            lv335 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight4, model_layers_2_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1166 = R.call_tir(cls.NT_matmul5, (rms_norm134, lv335), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape264: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1166, R.shape([1, seq_len, 48, 128]))
            reshape265: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape264, R.shape([seq_len, 48, 128]))
            lv336 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape265), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape266: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv336, R.shape([1, seq_len, 32, 128]))
            reshape267: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape266, R.shape([1, seq_len, 4096]))
            lv337 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight4, model_layers_2_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1167 = R.call_tir(cls.NT_matmul6, (reshape267, lv337), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1160_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1167, lv1159_1, model_layers_2_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1161_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1160_1[1]
            rms_norm135: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1160_1[0]
            lv338 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight4, model_layers_2_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1168 = R.call_tir(cls.NT_matmul7, (rms_norm135, lv338), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split66: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1168, indices_or_sections=2, axis=-1)
            split_066: R.Tensor((1, seq_len, 14336), dtype="float16") = split66[0]
            split_166: R.Tensor((1, seq_len, 14336), dtype="float16") = split66[1]
            silu66: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_066)
            mul66: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu66, split_166)
            lv339 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight4, model_layers_2_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1169 = R.call_tir(cls.NT_matmul8, (mul66, lv339), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1162_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1169, lv1161_1, model_layers_3_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1163_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1162_1[1]
            rms_norm136: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1162_1[0]
            lv340 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight4, model_layers_3_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1170 = R.call_tir(cls.NT_matmul5, (rms_norm136, lv340), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape268: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1170, R.shape([1, seq_len, 48, 128]))
            reshape269: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape268, R.shape([seq_len, 48, 128]))
            lv341 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape269), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape270: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv341, R.shape([1, seq_len, 32, 128]))
            reshape271: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape270, R.shape([1, seq_len, 4096]))
            lv342 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight4, model_layers_3_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1171 = R.call_tir(cls.NT_matmul6, (reshape271, lv342), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1164_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1171, lv1163_1, model_layers_3_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1165_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1164_1[1]
            rms_norm137: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1164_1[0]
            lv343 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight4, model_layers_3_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1172 = R.call_tir(cls.NT_matmul7, (rms_norm137, lv343), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split67: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1172, indices_or_sections=2, axis=-1)
            split_067: R.Tensor((1, seq_len, 14336), dtype="float16") = split67[0]
            split_167: R.Tensor((1, seq_len, 14336), dtype="float16") = split67[1]
            silu67: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_067)
            mul67: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu67, split_167)
            lv344 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight4, model_layers_3_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1173 = R.call_tir(cls.NT_matmul8, (mul67, lv344), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1166_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1173, lv1165_1, model_layers_4_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1167_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1166_1[1]
            rms_norm138: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1166_1[0]
            lv345 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight4, model_layers_4_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1174 = R.call_tir(cls.NT_matmul5, (rms_norm138, lv345), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape272: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1174, R.shape([1, seq_len, 48, 128]))
            reshape273: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape272, R.shape([seq_len, 48, 128]))
            lv346 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape273), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape274: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv346, R.shape([1, seq_len, 32, 128]))
            reshape275: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape274, R.shape([1, seq_len, 4096]))
            lv347 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight4, model_layers_4_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1175 = R.call_tir(cls.NT_matmul6, (reshape275, lv347), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1168_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1175, lv1167_1, model_layers_4_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1169_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1168_1[1]
            rms_norm139: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1168_1[0]
            lv348 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight4, model_layers_4_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1176 = R.call_tir(cls.NT_matmul7, (rms_norm139, lv348), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split68: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1176, indices_or_sections=2, axis=-1)
            split_068: R.Tensor((1, seq_len, 14336), dtype="float16") = split68[0]
            split_168: R.Tensor((1, seq_len, 14336), dtype="float16") = split68[1]
            silu68: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_068)
            mul68: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu68, split_168)
            lv349 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight4, model_layers_4_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1177 = R.call_tir(cls.NT_matmul8, (mul68, lv349), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1170_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1177, lv1169_1, model_layers_5_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1171_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1170_1[1]
            rms_norm140: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1170_1[0]
            lv350 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight4, model_layers_5_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1178 = R.call_tir(cls.NT_matmul5, (rms_norm140, lv350), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape276: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1178, R.shape([1, seq_len, 48, 128]))
            reshape277: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape276, R.shape([seq_len, 48, 128]))
            lv351 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape277), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape278: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv351, R.shape([1, seq_len, 32, 128]))
            reshape279: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape278, R.shape([1, seq_len, 4096]))
            lv352 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight4, model_layers_5_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1179 = R.call_tir(cls.NT_matmul6, (reshape279, lv352), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1172_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1179, lv1171_1, model_layers_5_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1173_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1172_1[1]
            rms_norm141: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1172_1[0]
            lv353 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight4, model_layers_5_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1180 = R.call_tir(cls.NT_matmul7, (rms_norm141, lv353), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split69: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1180, indices_or_sections=2, axis=-1)
            split_069: R.Tensor((1, seq_len, 14336), dtype="float16") = split69[0]
            split_169: R.Tensor((1, seq_len, 14336), dtype="float16") = split69[1]
            silu69: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_069)
            mul69: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu69, split_169)
            lv354 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight4, model_layers_5_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1181 = R.call_tir(cls.NT_matmul8, (mul69, lv354), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1174_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1181, lv1173_1, model_layers_6_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1175_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1174_1[1]
            rms_norm142: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1174_1[0]
            lv355 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight4, model_layers_6_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1182 = R.call_tir(cls.NT_matmul5, (rms_norm142, lv355), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape280: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1182, R.shape([1, seq_len, 48, 128]))
            reshape281: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape280, R.shape([seq_len, 48, 128]))
            lv356 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape281), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape282: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv356, R.shape([1, seq_len, 32, 128]))
            reshape283: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape282, R.shape([1, seq_len, 4096]))
            lv357 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight4, model_layers_6_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1183 = R.call_tir(cls.NT_matmul6, (reshape283, lv357), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1176_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1183, lv1175_1, model_layers_6_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1177_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1176_1[1]
            rms_norm143: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1176_1[0]
            lv358 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight4, model_layers_6_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1184 = R.call_tir(cls.NT_matmul7, (rms_norm143, lv358), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split70: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1184, indices_or_sections=2, axis=-1)
            split_070: R.Tensor((1, seq_len, 14336), dtype="float16") = split70[0]
            split_170: R.Tensor((1, seq_len, 14336), dtype="float16") = split70[1]
            silu70: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_070)
            mul70: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu70, split_170)
            lv359 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight4, model_layers_6_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1185 = R.call_tir(cls.NT_matmul8, (mul70, lv359), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1178_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1185, lv1177_1, model_layers_7_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1179_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1178_1[1]
            rms_norm144: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1178_1[0]
            lv360 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight4, model_layers_7_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1186 = R.call_tir(cls.NT_matmul5, (rms_norm144, lv360), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape284: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1186, R.shape([1, seq_len, 48, 128]))
            reshape285: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape284, R.shape([seq_len, 48, 128]))
            lv361 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape285), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape286: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv361, R.shape([1, seq_len, 32, 128]))
            reshape287: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape286, R.shape([1, seq_len, 4096]))
            lv362 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight4, model_layers_7_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1187 = R.call_tir(cls.NT_matmul6, (reshape287, lv362), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1180_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1187, lv1179_1, model_layers_7_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1181_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1180_1[1]
            rms_norm145: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1180_1[0]
            lv363 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight4, model_layers_7_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1188 = R.call_tir(cls.NT_matmul7, (rms_norm145, lv363), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split71: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1188, indices_or_sections=2, axis=-1)
            split_071: R.Tensor((1, seq_len, 14336), dtype="float16") = split71[0]
            split_171: R.Tensor((1, seq_len, 14336), dtype="float16") = split71[1]
            silu71: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_071)
            mul71: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu71, split_171)
            lv364 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight4, model_layers_7_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1189 = R.call_tir(cls.NT_matmul8, (mul71, lv364), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1182_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1189, lv1181_1, model_layers_8_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1183_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1182_1[1]
            rms_norm146: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1182_1[0]
            lv365 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight4, model_layers_8_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1190 = R.call_tir(cls.NT_matmul5, (rms_norm146, lv365), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape288: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1190, R.shape([1, seq_len, 48, 128]))
            reshape289: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape288, R.shape([seq_len, 48, 128]))
            lv366 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape289), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape290: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv366, R.shape([1, seq_len, 32, 128]))
            reshape291: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape290, R.shape([1, seq_len, 4096]))
            lv367 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight4, model_layers_8_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1191 = R.call_tir(cls.NT_matmul6, (reshape291, lv367), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1184_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1191, lv1183_1, model_layers_8_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1185_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1184_1[1]
            rms_norm147: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1184_1[0]
            lv368 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight4, model_layers_8_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1192 = R.call_tir(cls.NT_matmul7, (rms_norm147, lv368), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split72: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1192, indices_or_sections=2, axis=-1)
            split_072: R.Tensor((1, seq_len, 14336), dtype="float16") = split72[0]
            split_172: R.Tensor((1, seq_len, 14336), dtype="float16") = split72[1]
            silu72: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_072)
            mul72: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu72, split_172)
            lv369 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight4, model_layers_8_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1193 = R.call_tir(cls.NT_matmul8, (mul72, lv369), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1186_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1193, lv1185_1, model_layers_9_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1187_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1186_1[1]
            rms_norm148: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1186_1[0]
            lv370 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight4, model_layers_9_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1194 = R.call_tir(cls.NT_matmul5, (rms_norm148, lv370), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape292: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1194, R.shape([1, seq_len, 48, 128]))
            reshape293: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape292, R.shape([seq_len, 48, 128]))
            lv371 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape293), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape294: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv371, R.shape([1, seq_len, 32, 128]))
            reshape295: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape294, R.shape([1, seq_len, 4096]))
            lv372 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight4, model_layers_9_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1195 = R.call_tir(cls.NT_matmul6, (reshape295, lv372), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1188_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1195, lv1187_1, model_layers_9_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1189_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1188_1[1]
            rms_norm149: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1188_1[0]
            lv373 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight4, model_layers_9_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1196 = R.call_tir(cls.NT_matmul7, (rms_norm149, lv373), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split73: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1196, indices_or_sections=2, axis=-1)
            split_073: R.Tensor((1, seq_len, 14336), dtype="float16") = split73[0]
            split_173: R.Tensor((1, seq_len, 14336), dtype="float16") = split73[1]
            silu73: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_073)
            mul73: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu73, split_173)
            lv374 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight4, model_layers_9_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1197 = R.call_tir(cls.NT_matmul8, (mul73, lv374), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1190_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1197, lv1189_1, model_layers_10_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1191_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1190_1[1]
            rms_norm150: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1190_1[0]
            lv375 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight4, model_layers_10_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1198 = R.call_tir(cls.NT_matmul5, (rms_norm150, lv375), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape296: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1198, R.shape([1, seq_len, 48, 128]))
            reshape297: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape296, R.shape([seq_len, 48, 128]))
            lv376 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape297), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape298: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv376, R.shape([1, seq_len, 32, 128]))
            reshape299: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape298, R.shape([1, seq_len, 4096]))
            lv377 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight4, model_layers_10_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1199 = R.call_tir(cls.NT_matmul6, (reshape299, lv377), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1192_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1199, lv1191_1, model_layers_10_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1193_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1192_1[1]
            rms_norm151: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1192_1[0]
            lv378 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight4, model_layers_10_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1200 = R.call_tir(cls.NT_matmul7, (rms_norm151, lv378), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split74: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1200, indices_or_sections=2, axis=-1)
            split_074: R.Tensor((1, seq_len, 14336), dtype="float16") = split74[0]
            split_174: R.Tensor((1, seq_len, 14336), dtype="float16") = split74[1]
            silu74: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_074)
            mul74: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu74, split_174)
            lv379 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight4, model_layers_10_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1201 = R.call_tir(cls.NT_matmul8, (mul74, lv379), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1194_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1201, lv1193_1, model_layers_11_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1195_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1194_1[1]
            rms_norm152: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1194_1[0]
            lv380 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight4, model_layers_11_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1202 = R.call_tir(cls.NT_matmul5, (rms_norm152, lv380), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape300: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1202, R.shape([1, seq_len, 48, 128]))
            reshape301: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape300, R.shape([seq_len, 48, 128]))
            lv381 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape301), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape302: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv381, R.shape([1, seq_len, 32, 128]))
            reshape303: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape302, R.shape([1, seq_len, 4096]))
            lv382 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight4, model_layers_11_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1203 = R.call_tir(cls.NT_matmul6, (reshape303, lv382), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1196_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1203, lv1195_1, model_layers_11_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1197_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1196_1[1]
            rms_norm153: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1196_1[0]
            lv383 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight4, model_layers_11_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1204 = R.call_tir(cls.NT_matmul7, (rms_norm153, lv383), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split75: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1204, indices_or_sections=2, axis=-1)
            split_075: R.Tensor((1, seq_len, 14336), dtype="float16") = split75[0]
            split_175: R.Tensor((1, seq_len, 14336), dtype="float16") = split75[1]
            silu75: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_075)
            mul75: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu75, split_175)
            lv384 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight4, model_layers_11_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1205 = R.call_tir(cls.NT_matmul8, (mul75, lv384), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1198_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1205, lv1197_1, model_layers_12_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1199_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1198_1[1]
            rms_norm154: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1198_1[0]
            lv385 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight4, model_layers_12_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1206 = R.call_tir(cls.NT_matmul5, (rms_norm154, lv385), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape304: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1206, R.shape([1, seq_len, 48, 128]))
            reshape305: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape304, R.shape([seq_len, 48, 128]))
            lv386 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape305), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape306: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv386, R.shape([1, seq_len, 32, 128]))
            reshape307: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape306, R.shape([1, seq_len, 4096]))
            lv387 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight4, model_layers_12_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1207 = R.call_tir(cls.NT_matmul6, (reshape307, lv387), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1200_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1207, lv1199_1, model_layers_12_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1201_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1200_1[1]
            rms_norm155: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1200_1[0]
            lv388 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight4, model_layers_12_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1208 = R.call_tir(cls.NT_matmul7, (rms_norm155, lv388), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split76: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1208, indices_or_sections=2, axis=-1)
            split_076: R.Tensor((1, seq_len, 14336), dtype="float16") = split76[0]
            split_176: R.Tensor((1, seq_len, 14336), dtype="float16") = split76[1]
            silu76: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_076)
            mul76: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu76, split_176)
            lv389 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight4, model_layers_12_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1209 = R.call_tir(cls.NT_matmul8, (mul76, lv389), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1202_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1209, lv1201_1, model_layers_13_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1203_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1202_1[1]
            rms_norm156: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1202_1[0]
            lv390 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight4, model_layers_13_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1210 = R.call_tir(cls.NT_matmul5, (rms_norm156, lv390), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape308: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1210, R.shape([1, seq_len, 48, 128]))
            reshape309: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape308, R.shape([seq_len, 48, 128]))
            lv391 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape309), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape310: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv391, R.shape([1, seq_len, 32, 128]))
            reshape311: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape310, R.shape([1, seq_len, 4096]))
            lv392 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight4, model_layers_13_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1211 = R.call_tir(cls.NT_matmul6, (reshape311, lv392), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1204_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1211, lv1203_1, model_layers_13_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1205_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1204_1[1]
            rms_norm157: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1204_1[0]
            lv393 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight4, model_layers_13_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1212 = R.call_tir(cls.NT_matmul7, (rms_norm157, lv393), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split77: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1212, indices_or_sections=2, axis=-1)
            split_077: R.Tensor((1, seq_len, 14336), dtype="float16") = split77[0]
            split_177: R.Tensor((1, seq_len, 14336), dtype="float16") = split77[1]
            silu77: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_077)
            mul77: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu77, split_177)
            lv394 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight4, model_layers_13_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1213 = R.call_tir(cls.NT_matmul8, (mul77, lv394), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1206_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1213, lv1205_1, model_layers_14_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1207_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1206_1[1]
            rms_norm158: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1206_1[0]
            lv395 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight4, model_layers_14_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1214 = R.call_tir(cls.NT_matmul5, (rms_norm158, lv395), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape312: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1214, R.shape([1, seq_len, 48, 128]))
            reshape313: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape312, R.shape([seq_len, 48, 128]))
            lv396 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape313), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape314: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv396, R.shape([1, seq_len, 32, 128]))
            reshape315: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape314, R.shape([1, seq_len, 4096]))
            lv397 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight4, model_layers_14_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1215 = R.call_tir(cls.NT_matmul6, (reshape315, lv397), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1208_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1215, lv1207_1, model_layers_14_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1209_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1208_1[1]
            rms_norm159: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1208_1[0]
            lv398 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight4, model_layers_14_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1216 = R.call_tir(cls.NT_matmul7, (rms_norm159, lv398), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split78: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1216, indices_or_sections=2, axis=-1)
            split_078: R.Tensor((1, seq_len, 14336), dtype="float16") = split78[0]
            split_178: R.Tensor((1, seq_len, 14336), dtype="float16") = split78[1]
            silu78: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_078)
            mul78: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu78, split_178)
            lv399 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight4, model_layers_14_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1217 = R.call_tir(cls.NT_matmul8, (mul78, lv399), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1210_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1217, lv1209_1, model_layers_15_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1211_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1210_1[1]
            rms_norm160: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1210_1[0]
            lv400 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight4, model_layers_15_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1218 = R.call_tir(cls.NT_matmul5, (rms_norm160, lv400), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape316: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1218, R.shape([1, seq_len, 48, 128]))
            reshape317: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape316, R.shape([seq_len, 48, 128]))
            lv401 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape317), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape318: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv401, R.shape([1, seq_len, 32, 128]))
            reshape319: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape318, R.shape([1, seq_len, 4096]))
            lv402 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight4, model_layers_15_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1219 = R.call_tir(cls.NT_matmul6, (reshape319, lv402), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1212_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1219, lv1211_1, model_layers_15_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1213_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1212_1[1]
            rms_norm161: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1212_1[0]
            lv403 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight4, model_layers_15_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1220 = R.call_tir(cls.NT_matmul7, (rms_norm161, lv403), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split79: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1220, indices_or_sections=2, axis=-1)
            split_079: R.Tensor((1, seq_len, 14336), dtype="float16") = split79[0]
            split_179: R.Tensor((1, seq_len, 14336), dtype="float16") = split79[1]
            silu79: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_079)
            mul79: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu79, split_179)
            lv404 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight4, model_layers_15_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1221 = R.call_tir(cls.NT_matmul8, (mul79, lv404), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1214_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1221, lv1213_1, model_layers_16_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1215_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1214_1[1]
            rms_norm162: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1214_1[0]
            lv405 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight4, model_layers_16_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1222 = R.call_tir(cls.NT_matmul5, (rms_norm162, lv405), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape320: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1222, R.shape([1, seq_len, 48, 128]))
            reshape321: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape320, R.shape([seq_len, 48, 128]))
            lv406 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape321), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape322: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv406, R.shape([1, seq_len, 32, 128]))
            reshape323: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape322, R.shape([1, seq_len, 4096]))
            lv407 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight4, model_layers_16_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1223 = R.call_tir(cls.NT_matmul6, (reshape323, lv407), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1216_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1223, lv1215_1, model_layers_16_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1217_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1216_1[1]
            rms_norm163: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1216_1[0]
            lv408 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight4, model_layers_16_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1224 = R.call_tir(cls.NT_matmul7, (rms_norm163, lv408), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split80: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1224, indices_or_sections=2, axis=-1)
            split_080: R.Tensor((1, seq_len, 14336), dtype="float16") = split80[0]
            split_180: R.Tensor((1, seq_len, 14336), dtype="float16") = split80[1]
            silu80: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_080)
            mul80: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu80, split_180)
            lv409 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight4, model_layers_16_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1225 = R.call_tir(cls.NT_matmul8, (mul80, lv409), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1218_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1225, lv1217_1, model_layers_17_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1219_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1218_1[1]
            rms_norm164: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1218_1[0]
            lv410 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight4, model_layers_17_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1226 = R.call_tir(cls.NT_matmul5, (rms_norm164, lv410), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape324: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1226, R.shape([1, seq_len, 48, 128]))
            reshape325: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape324, R.shape([seq_len, 48, 128]))
            lv411 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape325), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape326: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv411, R.shape([1, seq_len, 32, 128]))
            reshape327: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape326, R.shape([1, seq_len, 4096]))
            lv412 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight4, model_layers_17_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1227 = R.call_tir(cls.NT_matmul6, (reshape327, lv412), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1220_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1227, lv1219_1, model_layers_17_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1221_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1220_1[1]
            rms_norm165: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1220_1[0]
            lv413 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight4, model_layers_17_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1228 = R.call_tir(cls.NT_matmul7, (rms_norm165, lv413), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split81: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1228, indices_or_sections=2, axis=-1)
            split_081: R.Tensor((1, seq_len, 14336), dtype="float16") = split81[0]
            split_181: R.Tensor((1, seq_len, 14336), dtype="float16") = split81[1]
            silu81: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_081)
            mul81: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu81, split_181)
            lv414 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight4, model_layers_17_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1229 = R.call_tir(cls.NT_matmul8, (mul81, lv414), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1222_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1229, lv1221_1, model_layers_18_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1223_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1222_1[1]
            rms_norm166: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1222_1[0]
            lv415 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight4, model_layers_18_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1230 = R.call_tir(cls.NT_matmul5, (rms_norm166, lv415), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape328: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1230, R.shape([1, seq_len, 48, 128]))
            reshape329: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape328, R.shape([seq_len, 48, 128]))
            lv416 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape329), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape330: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv416, R.shape([1, seq_len, 32, 128]))
            reshape331: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape330, R.shape([1, seq_len, 4096]))
            lv417 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight4, model_layers_18_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1231 = R.call_tir(cls.NT_matmul6, (reshape331, lv417), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1224_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1231, lv1223_1, model_layers_18_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1225_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1224_1[1]
            rms_norm167: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1224_1[0]
            lv418 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight4, model_layers_18_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1232 = R.call_tir(cls.NT_matmul7, (rms_norm167, lv418), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split82: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1232, indices_or_sections=2, axis=-1)
            split_082: R.Tensor((1, seq_len, 14336), dtype="float16") = split82[0]
            split_182: R.Tensor((1, seq_len, 14336), dtype="float16") = split82[1]
            silu82: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_082)
            mul82: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu82, split_182)
            lv419 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight4, model_layers_18_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1233 = R.call_tir(cls.NT_matmul8, (mul82, lv419), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1226_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1233, lv1225_1, model_layers_19_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1227_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1226_1[1]
            rms_norm168: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1226_1[0]
            lv420 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight4, model_layers_19_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1234 = R.call_tir(cls.NT_matmul5, (rms_norm168, lv420), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape332: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1234, R.shape([1, seq_len, 48, 128]))
            reshape333: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape332, R.shape([seq_len, 48, 128]))
            lv421 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape333), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape334: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv421, R.shape([1, seq_len, 32, 128]))
            reshape335: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape334, R.shape([1, seq_len, 4096]))
            lv422 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight4, model_layers_19_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1235 = R.call_tir(cls.NT_matmul6, (reshape335, lv422), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1228_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1235, lv1227_1, model_layers_19_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1229_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1228_1[1]
            rms_norm169: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1228_1[0]
            lv423 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight4, model_layers_19_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1236 = R.call_tir(cls.NT_matmul7, (rms_norm169, lv423), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split83: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1236, indices_or_sections=2, axis=-1)
            split_083: R.Tensor((1, seq_len, 14336), dtype="float16") = split83[0]
            split_183: R.Tensor((1, seq_len, 14336), dtype="float16") = split83[1]
            silu83: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_083)
            mul83: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu83, split_183)
            lv424 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight4, model_layers_19_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1237 = R.call_tir(cls.NT_matmul8, (mul83, lv424), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1230_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1237, lv1229_1, model_layers_20_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1231_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1230_1[1]
            rms_norm170: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1230_1[0]
            lv425 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight4, model_layers_20_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1238 = R.call_tir(cls.NT_matmul5, (rms_norm170, lv425), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape336: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1238, R.shape([1, seq_len, 48, 128]))
            reshape337: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape336, R.shape([seq_len, 48, 128]))
            lv426 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape337), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape338: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv426, R.shape([1, seq_len, 32, 128]))
            reshape339: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape338, R.shape([1, seq_len, 4096]))
            lv427 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight4, model_layers_20_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1239 = R.call_tir(cls.NT_matmul6, (reshape339, lv427), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1232_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1239, lv1231_1, model_layers_20_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1233_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1232_1[1]
            rms_norm171: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1232_1[0]
            lv428 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight4, model_layers_20_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1240 = R.call_tir(cls.NT_matmul7, (rms_norm171, lv428), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split84: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1240, indices_or_sections=2, axis=-1)
            split_084: R.Tensor((1, seq_len, 14336), dtype="float16") = split84[0]
            split_184: R.Tensor((1, seq_len, 14336), dtype="float16") = split84[1]
            silu84: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_084)
            mul84: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu84, split_184)
            lv429 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight4, model_layers_20_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1241 = R.call_tir(cls.NT_matmul8, (mul84, lv429), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1234_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1241, lv1233_1, model_layers_21_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1235_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1234_1[1]
            rms_norm172: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1234_1[0]
            lv430 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight4, model_layers_21_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1242 = R.call_tir(cls.NT_matmul5, (rms_norm172, lv430), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape340: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1242, R.shape([1, seq_len, 48, 128]))
            reshape341: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape340, R.shape([seq_len, 48, 128]))
            lv431 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape341), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape342: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv431, R.shape([1, seq_len, 32, 128]))
            reshape343: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape342, R.shape([1, seq_len, 4096]))
            lv432 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight4, model_layers_21_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1243 = R.call_tir(cls.NT_matmul6, (reshape343, lv432), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1236_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1243, lv1235_1, model_layers_21_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1237_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1236_1[1]
            rms_norm173: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1236_1[0]
            lv433 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight4, model_layers_21_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1244 = R.call_tir(cls.NT_matmul7, (rms_norm173, lv433), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split85: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1244, indices_or_sections=2, axis=-1)
            split_085: R.Tensor((1, seq_len, 14336), dtype="float16") = split85[0]
            split_185: R.Tensor((1, seq_len, 14336), dtype="float16") = split85[1]
            silu85: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_085)
            mul85: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu85, split_185)
            lv434 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight4, model_layers_21_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1245 = R.call_tir(cls.NT_matmul8, (mul85, lv434), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1238_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1245, lv1237_1, model_layers_22_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1239_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1238_1[1]
            rms_norm174: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1238_1[0]
            lv435 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight4, model_layers_22_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1246 = R.call_tir(cls.NT_matmul5, (rms_norm174, lv435), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape344: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1246, R.shape([1, seq_len, 48, 128]))
            reshape345: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape344, R.shape([seq_len, 48, 128]))
            lv436 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape345), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape346: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv436, R.shape([1, seq_len, 32, 128]))
            reshape347: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape346, R.shape([1, seq_len, 4096]))
            lv437 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight4, model_layers_22_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1247 = R.call_tir(cls.NT_matmul6, (reshape347, lv437), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1240_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1247, lv1239_1, model_layers_22_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1241_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1240_1[1]
            rms_norm175: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1240_1[0]
            lv438 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight4, model_layers_22_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1248 = R.call_tir(cls.NT_matmul7, (rms_norm175, lv438), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split86: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1248, indices_or_sections=2, axis=-1)
            split_086: R.Tensor((1, seq_len, 14336), dtype="float16") = split86[0]
            split_186: R.Tensor((1, seq_len, 14336), dtype="float16") = split86[1]
            silu86: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_086)
            mul86: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu86, split_186)
            lv439 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight4, model_layers_22_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1249 = R.call_tir(cls.NT_matmul8, (mul86, lv439), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1242_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1249, lv1241_1, model_layers_23_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1243_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1242_1[1]
            rms_norm176: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1242_1[0]
            lv440 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight4, model_layers_23_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1250 = R.call_tir(cls.NT_matmul5, (rms_norm176, lv440), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape348: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1250, R.shape([1, seq_len, 48, 128]))
            reshape349: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape348, R.shape([seq_len, 48, 128]))
            lv441 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape349), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape350: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv441, R.shape([1, seq_len, 32, 128]))
            reshape351: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape350, R.shape([1, seq_len, 4096]))
            lv442 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight4, model_layers_23_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1251 = R.call_tir(cls.NT_matmul6, (reshape351, lv442), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1244_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1251, lv1243_1, model_layers_23_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1245_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1244_1[1]
            rms_norm177: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1244_1[0]
            lv443 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight4, model_layers_23_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1252 = R.call_tir(cls.NT_matmul7, (rms_norm177, lv443), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split87: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1252, indices_or_sections=2, axis=-1)
            split_087: R.Tensor((1, seq_len, 14336), dtype="float16") = split87[0]
            split_187: R.Tensor((1, seq_len, 14336), dtype="float16") = split87[1]
            silu87: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_087)
            mul87: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu87, split_187)
            lv444 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight4, model_layers_23_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1253 = R.call_tir(cls.NT_matmul8, (mul87, lv444), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1246_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1253, lv1245_1, model_layers_24_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1247_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1246_1[1]
            rms_norm178: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1246_1[0]
            lv445 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight4, model_layers_24_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1254 = R.call_tir(cls.NT_matmul5, (rms_norm178, lv445), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape352: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1254, R.shape([1, seq_len, 48, 128]))
            reshape353: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape352, R.shape([seq_len, 48, 128]))
            lv446 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape353), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape354: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv446, R.shape([1, seq_len, 32, 128]))
            reshape355: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape354, R.shape([1, seq_len, 4096]))
            lv447 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight4, model_layers_24_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1255 = R.call_tir(cls.NT_matmul6, (reshape355, lv447), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1248_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1255, lv1247_1, model_layers_24_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1249_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1248_1[1]
            rms_norm179: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1248_1[0]
            lv448 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight4, model_layers_24_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1256 = R.call_tir(cls.NT_matmul7, (rms_norm179, lv448), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split88: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1256, indices_or_sections=2, axis=-1)
            split_088: R.Tensor((1, seq_len, 14336), dtype="float16") = split88[0]
            split_188: R.Tensor((1, seq_len, 14336), dtype="float16") = split88[1]
            silu88: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_088)
            mul88: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu88, split_188)
            lv449 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight4, model_layers_24_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1257 = R.call_tir(cls.NT_matmul8, (mul88, lv449), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1250_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1257, lv1249_1, model_layers_25_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1251_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1250_1[1]
            rms_norm180: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1250_1[0]
            lv450 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight4, model_layers_25_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1258 = R.call_tir(cls.NT_matmul5, (rms_norm180, lv450), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape356: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1258, R.shape([1, seq_len, 48, 128]))
            reshape357: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape356, R.shape([seq_len, 48, 128]))
            lv451 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape357), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape358: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv451, R.shape([1, seq_len, 32, 128]))
            reshape359: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape358, R.shape([1, seq_len, 4096]))
            lv452 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight4, model_layers_25_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1259 = R.call_tir(cls.NT_matmul6, (reshape359, lv452), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1252_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1259, lv1251_1, model_layers_25_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1253_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1252_1[1]
            rms_norm181: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1252_1[0]
            lv453 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight4, model_layers_25_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1260 = R.call_tir(cls.NT_matmul7, (rms_norm181, lv453), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split89: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1260, indices_or_sections=2, axis=-1)
            split_089: R.Tensor((1, seq_len, 14336), dtype="float16") = split89[0]
            split_189: R.Tensor((1, seq_len, 14336), dtype="float16") = split89[1]
            silu89: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_089)
            mul89: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu89, split_189)
            lv454 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight4, model_layers_25_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1261 = R.call_tir(cls.NT_matmul8, (mul89, lv454), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1254_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1261, lv1253_1, model_layers_26_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1255_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1254_1[1]
            rms_norm182: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1254_1[0]
            lv455 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight4, model_layers_26_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1262 = R.call_tir(cls.NT_matmul5, (rms_norm182, lv455), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape360: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1262, R.shape([1, seq_len, 48, 128]))
            reshape361: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape360, R.shape([seq_len, 48, 128]))
            lv456 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape361), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape362: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv456, R.shape([1, seq_len, 32, 128]))
            reshape363: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape362, R.shape([1, seq_len, 4096]))
            lv457 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight4, model_layers_26_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1263 = R.call_tir(cls.NT_matmul6, (reshape363, lv457), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1256_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1263, lv1255_1, model_layers_26_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1257_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1256_1[1]
            rms_norm183: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1256_1[0]
            lv458 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight4, model_layers_26_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1264 = R.call_tir(cls.NT_matmul7, (rms_norm183, lv458), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split90: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1264, indices_or_sections=2, axis=-1)
            split_090: R.Tensor((1, seq_len, 14336), dtype="float16") = split90[0]
            split_190: R.Tensor((1, seq_len, 14336), dtype="float16") = split90[1]
            silu90: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_090)
            mul90: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu90, split_190)
            lv459 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight4, model_layers_26_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1265 = R.call_tir(cls.NT_matmul8, (mul90, lv459), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1258_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1265, lv1257_1, model_layers_27_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1259_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1258_1[1]
            rms_norm184: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1258_1[0]
            lv460 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight4, model_layers_27_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1266 = R.call_tir(cls.NT_matmul5, (rms_norm184, lv460), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape364: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1266, R.shape([1, seq_len, 48, 128]))
            reshape365: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape364, R.shape([seq_len, 48, 128]))
            lv461 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape365), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape366: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv461, R.shape([1, seq_len, 32, 128]))
            reshape367: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape366, R.shape([1, seq_len, 4096]))
            lv462 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight4, model_layers_27_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1267 = R.call_tir(cls.NT_matmul6, (reshape367, lv462), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1260_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1267, lv1259_1, model_layers_27_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1261_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1260_1[1]
            rms_norm185: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1260_1[0]
            lv463 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight4, model_layers_27_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1268 = R.call_tir(cls.NT_matmul7, (rms_norm185, lv463), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split91: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1268, indices_or_sections=2, axis=-1)
            split_091: R.Tensor((1, seq_len, 14336), dtype="float16") = split91[0]
            split_191: R.Tensor((1, seq_len, 14336), dtype="float16") = split91[1]
            silu91: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_091)
            mul91: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu91, split_191)
            lv464 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight4, model_layers_27_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1269 = R.call_tir(cls.NT_matmul8, (mul91, lv464), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1262_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1269, lv1261_1, model_layers_28_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1263_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1262_1[1]
            rms_norm186: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1262_1[0]
            lv465 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight4, model_layers_28_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1270 = R.call_tir(cls.NT_matmul5, (rms_norm186, lv465), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape368: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1270, R.shape([1, seq_len, 48, 128]))
            reshape369: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape368, R.shape([seq_len, 48, 128]))
            lv466 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape369), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape370: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv466, R.shape([1, seq_len, 32, 128]))
            reshape371: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape370, R.shape([1, seq_len, 4096]))
            lv467 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight4, model_layers_28_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1271 = R.call_tir(cls.NT_matmul6, (reshape371, lv467), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1264_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1271, lv1263_1, model_layers_28_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1265_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1264_1[1]
            rms_norm187: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1264_1[0]
            lv468 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight4, model_layers_28_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1272 = R.call_tir(cls.NT_matmul7, (rms_norm187, lv468), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split92: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1272, indices_or_sections=2, axis=-1)
            split_092: R.Tensor((1, seq_len, 14336), dtype="float16") = split92[0]
            split_192: R.Tensor((1, seq_len, 14336), dtype="float16") = split92[1]
            silu92: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_092)
            mul92: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu92, split_192)
            lv469 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight4, model_layers_28_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1273 = R.call_tir(cls.NT_matmul8, (mul92, lv469), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1266_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1273, lv1265_1, model_layers_29_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1267_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1266_1[1]
            rms_norm188: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1266_1[0]
            lv470 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight4, model_layers_29_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1274 = R.call_tir(cls.NT_matmul5, (rms_norm188, lv470), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape372: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1274, R.shape([1, seq_len, 48, 128]))
            reshape373: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape372, R.shape([seq_len, 48, 128]))
            lv471 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape373), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape374: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv471, R.shape([1, seq_len, 32, 128]))
            reshape375: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape374, R.shape([1, seq_len, 4096]))
            lv472 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight4, model_layers_29_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1275 = R.call_tir(cls.NT_matmul6, (reshape375, lv472), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1268_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1275, lv1267_1, model_layers_29_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1269_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1268_1[1]
            rms_norm189: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1268_1[0]
            lv473 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight4, model_layers_29_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1276 = R.call_tir(cls.NT_matmul7, (rms_norm189, lv473), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split93: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1276, indices_or_sections=2, axis=-1)
            split_093: R.Tensor((1, seq_len, 14336), dtype="float16") = split93[0]
            split_193: R.Tensor((1, seq_len, 14336), dtype="float16") = split93[1]
            silu93: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_093)
            mul93: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu93, split_193)
            lv474 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight4, model_layers_29_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1277 = R.call_tir(cls.NT_matmul8, (mul93, lv474), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1270_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1277, lv1269_1, model_layers_30_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1271_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1270_1[1]
            rms_norm190: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1270_1[0]
            lv475 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight4, model_layers_30_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1278 = R.call_tir(cls.NT_matmul5, (rms_norm190, lv475), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape376: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1278, R.shape([1, seq_len, 48, 128]))
            reshape377: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape376, R.shape([seq_len, 48, 128]))
            lv476 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape377), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape378: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv476, R.shape([1, seq_len, 32, 128]))
            reshape379: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape378, R.shape([1, seq_len, 4096]))
            lv477 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight4, model_layers_30_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1279 = R.call_tir(cls.NT_matmul6, (reshape379, lv477), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1272_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1279, lv1271_1, model_layers_30_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1273_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1272_1[1]
            rms_norm191: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1272_1[0]
            lv478 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight4, model_layers_30_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1280 = R.call_tir(cls.NT_matmul7, (rms_norm191, lv478), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split94: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1280, indices_or_sections=2, axis=-1)
            split_094: R.Tensor((1, seq_len, 14336), dtype="float16") = split94[0]
            split_194: R.Tensor((1, seq_len, 14336), dtype="float16") = split94[1]
            silu94: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_094)
            mul94: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu94, split_194)
            lv479 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight4, model_layers_30_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1281 = R.call_tir(cls.NT_matmul8, (mul94, lv479), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1274_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1281, lv1273_1, model_layers_31_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1275_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1274_1[1]
            rms_norm192: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1274_1[0]
            lv480 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight4, model_layers_31_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            lv1282 = R.call_tir(cls.NT_matmul5, (rms_norm192, lv480), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape380: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(lv1282, R.shape([1, seq_len, 48, 128]))
            reshape381: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape380, R.shape([seq_len, 48, 128]))
            lv481 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape381), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape382: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv481, R.shape([1, seq_len, 32, 128]))
            reshape383: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape382, R.shape([1, seq_len, 4096]))
            lv482 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight4, model_layers_31_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            lv1283 = R.call_tir(cls.NT_matmul6, (reshape383, lv482), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1276_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1283, lv1275_1, model_layers_31_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1277_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1276_1[1]
            rms_norm193: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1276_1[0]
            lv483 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight4, model_layers_31_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            lv1284 = R.call_tir(cls.NT_matmul7, (rms_norm193, lv483), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            split95: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(lv1284, indices_or_sections=2, axis=-1)
            split_095: R.Tensor((1, seq_len, 14336), dtype="float16") = split95[0]
            split_195: R.Tensor((1, seq_len, 14336), dtype="float16") = split95[1]
            silu95: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_095)
            mul95: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu95, split_195)
            lv484 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight4, model_layers_31_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            lv1285 = R.call_tir(cls.NT_matmul8, (mul95, lv484), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1278_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1285, lv1277_1, model_norm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm194: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1278_1[0]
            gv5: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm194, paged_kv_cache
            R.output(gv5)
        return gv5

    @R.function
    def renormalize_by_top_p(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), top_p: R.Tensor(("batch_size",), dtype="float32"), init_pivots: R.Tensor(("batch_size", 3), dtype="float32")) -> R.Tensor(("batch_size", "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv6 = R.call_tir(cls.top_p_pivot_cutoff, (probs, top_p, init_pivots), out_sinfo=[R.Tensor((batch_size,), dtype="float32"), R.Tensor((batch_size,), dtype="float32")])
            lv7: R.Tensor((batch_size,), dtype="float32") = lv6[0]
            lv8: R.Tensor((batch_size,), dtype="float32") = lv6[1]
            gv5 = R.call_tir(cls.top_p_renorm_after_cutoff, (probs, lv7, lv8), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            R.output(gv5)
        return gv5

    @R.function
    def sample_with_top_p(sorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), top_p: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            sorted_probs_1: R.Tensor((batch_size, vocab_size), dtype="float32") = sorted_probs
            sorted_indices_1: R.Tensor((batch_size, vocab_size), dtype="int32") = sorted_indices
            uniform_samples1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            sample_indices2: R.Tensor((batch_size, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", top_p, R.shape([batch_size, 1]), sinfo_args=(R.Tensor((batch_size, 1), dtype="float32"),))
            lv3 = R.call_tir(cls.full, R.tuple(), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"), tir_vars=R.shape([vocab_size]))
            cumsum: R.Tensor((batch_size, vocab_size), dtype="float32") = R.cumsum(sorted_probs_1, axis=1, dtype="void", exclusive=False)
            lv4 = R.call_tir(cls.get_renorm_prob, (cumsum, sample_indices2, lv3), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv5 = R.call_tir(cls.get_index_from_sorted, (cumsum, sorted_indices_1, lv4, uniform_samples1, sample_indices1), out_sinfo=R.Tensor((num_samples, 1), dtype="int32"))
            gv2: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", lv5, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            R.output(gv2)
        return gv2

    @R.function
    def sampler_take_probs(unsorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), sampling_result: R.Tensor(("num_samples",), dtype="int32"), lobprob_offsets: R.Tensor(("num_positions",), dtype="int32")) -> R.Tuple(R.Tensor(("num_samples",), dtype="float32"), R.Tensor(("num_positions",), dtype="float32"), R.Tensor(("num_positions",), dtype="int32")):
        num_samples = T.int64(is_size_var=True)
        num_positions = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv3 = R.call_tir(cls.sampler_take_probs_tir, (unsorted_probs, sorted_indices, sample_indices, sampling_result, lobprob_offsets), out_sinfo=[R.Tensor((num_samples,), dtype="float32"), R.Tensor((num_positions,), dtype="float32"), R.Tensor((num_positions,), dtype="int32")])
            R.output(gv3)
        return gv3

    @R.function
    def sampler_verify_draft_tokens(draft_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), draft_tokens: R.Tensor(("num_nodes",), dtype="int32"), model_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), token_tree_first_child: R.Tensor(("num_nodes",), dtype="int32"), token_tree_next_sibling: R.Tensor(("num_nodes",), dtype="int32"), uniform_samples: R.Tensor(("num_nodes",), dtype="float32"), token_tree_parent_ptr: R.Tensor(("nbatch",), dtype="int32")) -> R.Tuple(R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), R.Tensor(("nbatch",), dtype="int32")):
        num_nodes = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        nbatch = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv4: R.Tuple(R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")) = R.call_tir_inplace(cls.batch_verify_on_gpu_single_kernel, (draft_probs, draft_tokens, model_probs, token_tree_first_child, token_tree_next_sibling, uniform_samples, token_tree_parent_ptr), out_sinfo=[R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")], inplace_indices=[2, 6])
            R.output(gv4)
        return gv4

    @R.function
    def scatter_hidden_states(src: R.Tensor(("batch_size", "n"), dtype="float16"), indices: R.Tensor(("batch_size",), dtype="int32"), dst: R.Tensor(("m", "n"), dtype="float16")) -> R.Tensor(("m", "n"), dtype="float16"):
        m = T.int64(is_size_var=True)
        n = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        cls = Module
        with R.dataflow():
            gv1: R.Tensor((m, n), dtype="float16") = R.call_tir_inplace(cls._scatter_hidden_states, (src, indices, dst), out_sinfo=R.Tensor((m, n), dtype="float16"), inplace_indices=[2])
            R.output(gv1)
        return gv1

    @R.function
    def softmax_with_temperature(logits: R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), temperature: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            lv: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", logits, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv1 = R.call_tir(cls.chunk_lse, (lv, temperature), out_sinfo=[R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32"), R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32")])
            lv2: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[0]
            lv3: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[1]
            lv4 = R.call_tir(cls.softmax_with_chunked_sum, (lv, temperature, lv2, lv3), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv: R.Tensor((batch_size, 1, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", lv4, R.shape([batch_size, 1, vocab_size]), sinfo_args=(R.Tensor((batch_size, 1, vocab_size), dtype="float32"),))
            R.output(gv)
        return gv