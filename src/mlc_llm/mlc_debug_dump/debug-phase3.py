# from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func
    def _gather_hidden_states(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        m, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (m, n), "float16")
        batch_size = T.int32(is_size_var=True)
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        dst = T.match_buffer(var_dst, (batch_size, n), "float16")
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("gather_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[indices[vb], vj], indices[vb])
                T.writes(dst[vb, vj])
                dst[vb, vj] = src[indices[vb], vj]

    @T.prim_func
    def _scatter_hidden_states(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (batch_size, n), "float16")
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        m = T.int32(is_size_var=True)
        dst = T.match_buffer(var_dst, (m, n), "float16")
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("scatter_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[vb, vj], indices[vb])
                T.writes(dst[indices[vb], vj])
                dst[indices[vb], vj] = src[vb, vj]

    @T.prim_func
    def apply_bitmask_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_bitmask: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        bitmask = T.match_buffer(var_bitmask, (batch_size, (vocab_size + 31) // 32), "int32")
        # with T.block("root"):
        for fused_s_v_0 in T.thread_binding((num_seq * vocab_size + 1023) // 1024, thread="blockIdx.x"):
            for fused_s_v_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vs = T.axis.spatial(num_seq, (fused_s_v_0 * 1024 + fused_s_v_1) // vocab_size)
                    vv = T.axis.spatial(vocab_size, (fused_s_v_0 * 1024 + fused_s_v_1) % vocab_size)
                    T.where(fused_s_v_0 * 1024 + fused_s_v_1 < num_seq * vocab_size)
                    T.reads(bitmask[seq_ids[vs], vv // 32], seq_ids[vs], logits[seq_ids[vs], vv])
                    T.writes(logits[seq_ids[vs], vv])
                    logits[seq_ids[vs], vv] = T.if_then_else(T.bitwise_and(T.shift_right(bitmask[seq_ids[vs], vv // 32], vv % 32), 1) == 1, logits[seq_ids[vs], vv], T.float32(-340282346638528859811704183484516925440.0))

    @T.prim_func
    def apply_logit_bias_inplace(var_logits: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_logit_bias: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        logit_bias = T.match_buffer(var_logit_bias, (num_token,))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[pos2seq_id[vp], token_ids[vp]], pos2seq_id[vp], token_ids[vp], logit_bias[vp])
                    T.writes(logits[pos2seq_id[vp], token_ids[vp]])
                    logits[pos2seq_id[vp], token_ids[vp]] = logits[pos2seq_id[vp], token_ids[vp]] + logit_bias[vp]

    @T.prim_func
    def apply_penalty_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_token_cnt: T.handle, var_penalties: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        token_cnt = T.match_buffer(var_token_cnt, (num_token,), "int32")
        penalties = T.match_buffer(var_penalties, (num_seq, 3))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]], seq_ids[pos2seq_id[vp]], pos2seq_id[vp], token_ids[vp], penalties[pos2seq_id[vp], 0:3], token_cnt[vp])
                    T.writes(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] - (penalties[pos2seq_id[vp], 0] + T.Cast("float32", token_cnt[vp]) * penalties[pos2seq_id[vp], 1])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = T.if_then_else(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] < T.float32(0.0), logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] * penalties[pos2seq_id[vp], 2], logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] / penalties[pos2seq_id[vp], 2])

    @T.prim_func(private=True)
    def argsort(var_probs: T.handle, var_argsort_gpu_v1: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(), T.int64()
        probs = T.match_buffer(var_probs, (batch_size, vocab_size), offset_factor=1)
        out_buf = T.match_buffer(var_argsort_gpu_v1, (batch_size, vocab_size), "int32", align=8)
        # with T.block("root"):
        value_buf = T.alloc_buffer((batch_size, vocab_size), align=8)
        value_swap_buf = T.alloc_buffer((batch_size, vocab_size), align=8)
        out_swap_buf = T.alloc_buffer((batch_size, vocab_size), "int32", align=8)
        with T.block("argsort_gpu"):
            T.reads()
            T.writes()
            if vocab_size > T.int64(0):
                with T.launch_thread("threadIdx.x", T.int64(256)) as threadIdx_x:
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (vocab_size + T.int64(255)) // T.int64(256)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size))
                    if blockIdx_x * T.int64(256) + threadIdx_x < vocab_size:
                        value_buf[(blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x) + blockIdx_y // batch_size) // vocab_size, (blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x) + blockIdx_y // batch_size) % vocab_size] = probs[(blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x) + blockIdx_y // batch_size) // vocab_size, (blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x) + blockIdx_y // batch_size) % vocab_size]
                        out_buf[(blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x) + blockIdx_y // batch_size) // vocab_size, (blockIdx_y % batch_size * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x) + blockIdx_y // batch_size) % vocab_size] = T.Cast("int32", blockIdx_x * T.int64(256) + threadIdx_x)
                with T.attr(0, "hand_threaded", 0):
                    threadIdx_x = T.launch_thread("threadIdx.x", T.int64(64))
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (vocab_size + T.int64(127)) // T.int64(128)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size))
                    temp_keys_swap = T.allocate([T.int64(128)], "float32", "shared")
                    temp_values_swap = T.allocate([T.int64(128)], "int32", "shared")
                    temp_keys = T.allocate([T.int64(1)], "float32", "local")
                    temp_values = T.allocate([T.int64(1)], "int32", "local")
                    temp_cond1 = T.allocate([T.int64(1)], "float32", "local")
                    temp_cond2 = T.allocate([T.int64(1)], "float32", "local")
                    temp_keys_swap_1 = T.Buffer((128,), data=temp_keys_swap, scope="shared")
                    temp_values_swap_1 = T.Buffer((128,), "int32", data=temp_values_swap, scope="shared")
                    for i in range(T.int64(2)):
                        if T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128) < vocab_size:
                            temp_keys_swap_1[T.int64(2) * threadIdx_x + i] = value_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) % vocab_size]
                            temp_values_swap_1[T.int64(2) * threadIdx_x + i] = out_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + i + blockIdx_x * T.int64(128))) % vocab_size]
                    T.tvm_storage_sync("shared")
                    for j in range(T.min(T.int64(128), vocab_size - blockIdx_x * T.int64(128))):
                        if T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) < T.min(T.int64(128), vocab_size - blockIdx_x * T.int64(128)) - T.int64(1):
                            temp_cond1_1 = T.Buffer((1,), data=temp_cond1, scope="local")
                            temp_cond1_1[T.int64(0)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)]
                            temp_cond2_1 = T.Buffer((1,), data=temp_cond2, scope="local")
                            temp_cond2_1[T.int64(0)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)]
                            if temp_cond1_1[T.int64(0)] < temp_cond2_1[T.int64(0)]:
                                temp_keys_1 = T.Buffer((1,), data=temp_keys, scope="local")
                                temp_keys_1[T.int64(0)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)]
                                temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)] = temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)]
                                temp_keys_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)] = temp_keys_1[T.int64(0)]
                                temp_values_1 = T.Buffer((1,), "int32", data=temp_values, scope="local")
                                temp_values_1[T.int64(0)] = temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)]
                                temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2)] = temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)]
                                temp_values_swap_1[T.int64(2) * threadIdx_x + (T.int64(2) * threadIdx_x + j) % T.int64(2) + T.int64(1)] = temp_values_1[T.int64(0)]
                        T.tvm_storage_sync("shared")
                    for k in range(T.int64(2)):
                        if T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128) < vocab_size:
                            value_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_keys_swap_1[T.int64(2) * threadIdx_x + k]
                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_keys_swap_1[T.int64(2) * threadIdx_x + k]
                            out_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_values_swap_1[T.int64(2) * threadIdx_x + k]
                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) // vocab_size, (blockIdx_y % batch_size * vocab_size + blockIdx_y // batch_size + (T.int64(2) * threadIdx_x + k + blockIdx_x * T.int64(128))) % vocab_size] = temp_values_swap_1[T.int64(2) * threadIdx_x + k]
                for i_0 in range(T.if_then_else(T.bitwise_and(vocab_size, vocab_size - T.int64(1)) == T.int64(0), T.int64(64) - T.Cast("int64", T.clz(vocab_size) - 64 + 64) - T.int64(1), T.int64(64) - T.Cast("int64", T.clz(vocab_size) - 64 + 64)) - (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))):
                    threadIdx_x = T.launch_thread("threadIdx.x", T.int64(256))
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(1023)) // T.int64(1024)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size * ((vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) - T.int64(1))) // T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))))))
                    if T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) < vocab_size:
                        if (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(1023)) // T.int64(1024) == T.int64(1):
                            if i_0 % T.int64(2) == T.int64(0):
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)), T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                i = T.allocate([T.int64(1)], "int64", "local")
                                j = T.allocate([T.int64(1)], "int64", "local")
                                i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]
                                j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - last_1[T.int64(0)]
                                for i_1_1 in range(T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)) - threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)), (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256))):
                                    if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)):
                                        if value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                    else:
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size)):
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_1_1)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                            else:
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)), T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                i = T.allocate([T.int64(1)], "int64", "local")
                                j = T.allocate([T.int64(1)], "int64", "local")
                                i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]
                                j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) - last_1[T.int64(0)]
                                for i_2 in range(T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)) - threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)), (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256))):
                                    if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)):
                                        if value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                    else:
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size)):
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                            i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                        else:
                                            value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + threadIdx_x * ((T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) + T.int64(255)) // T.int64(256)) + i_2)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                            j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                        else:
                            if i_0 % T.int64(2) == T.int64(0):
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), blockIdx_x * T.int64(1024) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(blockIdx_x * T.int64(1024), T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                if i_0 % T.int64(2) == T.int64(0):
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_3 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)):
                                            if value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)):
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_3)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                else:
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_4 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)):
                                            if value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)):
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_4)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                            else:
                                first = T.allocate([T.int64(1)], "int64", "local")
                                mid = T.allocate([T.int64(1)], "int64", "local")
                                last = T.allocate([T.int64(1)], "int64", "local")
                                first_1 = T.Buffer((1,), "int64", data=first, scope="local")
                                first_1[T.int64(0)] = T.max(T.int64(0), blockIdx_x * T.int64(1024) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size)))
                                last_1 = T.Buffer((1,), "int64", data=last, scope="local")
                                last_1[T.int64(0)] = T.min(blockIdx_x * T.int64(1024), T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size))
                                while first_1[T.int64(0)] < last_1[T.int64(0)]:
                                    if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - T.int64(1) - T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)))) % vocab_size]:
                                        first_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1)) + T.int64(1)
                                    else:
                                        last_1[T.int64(0)] = T.shift_right(first_1[T.int64(0)] + last_1[T.int64(0)], T.int64(1))
                                if i_0 % T.int64(2) == T.int64(0):
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_5 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)):
                                            if value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)):
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = value_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_5)) % vocab_size] = out_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                else:
                                    first_2 = T.allocate([T.int64(1)], "int64", "local")
                                    mid_1 = T.allocate([T.int64(1)], "int64", "local")
                                    last_2 = T.allocate([T.int64(1)], "int64", "local")
                                    first_3 = T.Buffer((1,), "int64", data=first_2, scope="local")
                                    first_3[T.int64(0)] = T.max(T.int64(0), threadIdx_x * T.int64(4) - T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)))
                                    last_3 = T.Buffer((1,), "int64", data=last_2, scope="local")
                                    last_3[T.int64(0)] = T.min(threadIdx_x * T.int64(4), T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)))
                                    while first_3[T.int64(0)] < last_3[T.int64(0)]:
                                        if value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - T.int64(1) - T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)))) % vocab_size]:
                                            first_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1)) + T.int64(1)
                                        else:
                                            last_3[T.int64(0)] = T.shift_right(first_3[T.int64(0)] + last_3[T.int64(0)], T.int64(1))
                                    i = T.allocate([T.int64(1)], "int64", "local")
                                    j = T.allocate([T.int64(1)], "int64", "local")
                                    i_1 = T.Buffer((1,), "int64", data=i, scope="local")
                                    i_1[T.int64(0)] = T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + first_3[T.int64(0)]
                                    j_1 = T.Buffer((1,), "int64", data=j, scope="local")
                                    j_1[T.int64(0)] = T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + threadIdx_x * T.int64(4) - last_3[T.int64(0)]
                                    for i_6 in range(T.min(T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)) - threadIdx_x * T.int64(4), T.int64(4))):
                                        if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)) and j_1[T.int64(0)] < T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))), vocab_size) - (T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) + blockIdx_x * T.int64(1024) - last_1[T.int64(0)]), T.int64(1024)):
                                            if value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size] <= value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                                        else:
                                            if i_1[T.int64(0)] < T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)] + T.min(T.min(T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) // T.int64(2), vocab_size) - (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + first_1[T.int64(0)]), T.int64(1024)):
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + i_1[T.int64(0)]) % vocab_size]
                                                i_1[T.int64(0)] = i_1[T.int64(0)] + T.int64(1)
                                            else:
                                                value_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = value_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                out_buf[(blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) // vocab_size, (blockIdx_y % batch_size * vocab_size + (T.shift_left(T.int64(2), i_0 + (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) * (blockIdx_y // batch_size) + blockIdx_x * T.int64(1024) + threadIdx_x * T.int64(4) + i_6)) % vocab_size] = out_swap_buf[(blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) // vocab_size, (blockIdx_y % batch_size * vocab_size + j_1[T.int64(0)]) % vocab_size]
                                                j_1[T.int64(0)] = j_1[T.int64(0)] + T.int64(1)
                if T.if_then_else(T.bitwise_and(vocab_size, vocab_size - T.int64(1)) == T.int64(0), T.int64(64) - T.Cast("int64", T.clz(vocab_size) - 64 + 64) - T.int64(1), T.int64(64) - T.Cast("int64", T.clz(vocab_size) - 64 + 64)) > T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1) and (T.if_then_else(T.bitwise_and(vocab_size, vocab_size - T.int64(1)) == T.int64(0), T.int64(64) - T.Cast("int64", T.clz(vocab_size) - 64 + 64) - T.int64(1), T.int64(64) - T.Cast("int64", T.clz(vocab_size) - 64 + 64)) - (T.int64(32) - T.Cast("int64", T.clz(T.int64(128)) - 64 + 32) - T.int64(1))) % T.int64(2) == T.int64(1):
                    threadIdx_x = T.launch_thread("threadIdx.x", T.int64(256))
                    blockIdx_x = T.launch_thread("blockIdx.x", T.max(T.int64(1), (vocab_size + T.int64(255)) // T.int64(256)))
                    blockIdx_y = T.launch_thread("blockIdx.y", T.max(T.int64(1), batch_size))
                    if blockIdx_x * T.int64(256) + threadIdx_x < vocab_size:
                        value_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) % vocab_size] = value_swap_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) % vocab_size]
                        out_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) % vocab_size] = out_swap_buf[(blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) // vocab_size, (blockIdx_y * vocab_size + (blockIdx_x * T.int64(256) + threadIdx_x)) % vocab_size]

    @T.prim_func
    def batch_decode_paged_kv(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 128), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 8, 16, 128), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (B,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 128), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.12751743082459868)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(8, thread="blockIdx.y"):
                for ty in T.thread_binding(4, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        for tz in T.thread_binding(4, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[bx], q_rope_position[bx], Q[bx, fused_by_bz // 8 * 4 + fused_by_bz % 8 * 4 + ty, tx * 4 - 64:tx * 4 - 64 + 132])
                                T.writes(output[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((4, 4, 128), scope="shared")
                                md_allreduce = T.alloc_buffer((4, 4, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((4,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 8
                                bz: T.int32 = fused_by_bz // 8
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec + 64] * T.float16(-1.0), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 15) // 16):
                                    tile_start_s: T.int32 = tz * 4 + ty
                                    tile_start_g: T.int32 = (iterator * 4 + tz) * 4 + ty
                                    for j in range(1):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = row_g
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, pages[page_no, 0, by, page_offset, tx * 4 + vec + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 4 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 4 + tz) * 4 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(4):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 4 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(4):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by * 4 + bz * 4 + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by * 4 + bz * 4 + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_decode_paged_kv_sliding_window(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 128), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 8, 16, 128), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, B), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 128), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.12751743082459868)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(8, thread="blockIdx.y"):
                for ty in T.thread_binding(4, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        for tz in T.thread_binding(4, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[0:3, bx], q_rope_position[bx], Q[bx, fused_by_bz // 8 * 4 + fused_by_bz % 8 * 4 + ty, tx * 4 - 64:tx * 4 - 64 + 132])
                                T.writes(output[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((4, 4, 128), scope="shared")
                                md_allreduce = T.alloc_buffer((4, 4, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((4,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 8
                                bz: T.int32 = fused_by_bz // 8
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, batch_idx] - length_info[1, batch_idx] + length_info[2, batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec + 64] * T.float16(-1.0), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 15) // 16):
                                    tile_start_s: T.int32 = tz * 4 + ty
                                    tile_start_g: T.int32 = (iterator * 4 + tz) * 4 + ty
                                    for j in range(1):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = T.if_then_else(row_g < length_info[2, batch_idx], row_g, row_g - length_info[2, batch_idx] + length_info[1, batch_idx])
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, pages[page_no, 0, by, page_offset, tx * 4 + vec + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 4 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 4 + tz) * 4 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(4):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 4 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(4):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by * 4 + bz * 4 + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by * 4 + bz * 4 + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_prefill_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, pages[page_no, 0, by, page_offset, j + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_paged_kv_sliding_window(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, batch_size), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, b_idx] - length_info[1, b_idx] + length_info[2, b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, pages[page_no, 0, by, page_offset, j + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_ragged_kv(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_k_rope_pos_offset: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (kv_len, 8, 128), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[L_kv_base + cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, k[L_kv_base + cur_L, by, j + 64] * T.float16(-1.0), k[L_kv_base + cur_L, by, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), k[L_kv_base + cur_L, by, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                V_smem[i, j] = v[L_kv_base + cur_L, by, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_tree_attn(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_mn_indptr: T.handle, var_mask: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, batch_size: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 128), "float16")
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (kv_len, 8, 128), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        mn_indptr = T.match_buffer(var_mn_indptr, (batch_size + 1,), "int32", offset_factor=1)
        tree_size = T.int32(is_size_var=True)
        mask = T.match_buffer(var_mask, (tree_size, 2), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("KV_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_base + L_kv_start + i
                                                            if L_kv_start + i < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, k[cur_L, by, j + 64] * T.float16(-1.0), k[cur_L, by, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), k[cur_L, by, j])
                                                                V_smem[i, j] = v[cur_L, by, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func(private=True)
    def batch_verify_on_gpu_single_kernel(var_draft_probs: T.handle, var_draft_tokens: T.handle, var_model_probs: T.handle, var_token_tree_first_child: T.handle, var_token_tree_next_sibling: T.handle, var_uniform_samples: T.handle, var_token_tree_parent_ptr: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        num_nodes, vocab_size = T.int32(is_size_var=True), T.int64(is_size_var=True)
        draft_probs = T.match_buffer(var_draft_probs, (num_nodes, vocab_size))
        draft_tokens = T.match_buffer(var_draft_tokens, (num_nodes,), "int32")
        model_probs = T.match_buffer(var_model_probs, (num_nodes, vocab_size))
        token_tree_first_child = T.match_buffer(var_token_tree_first_child, (num_nodes,), "int32")
        token_tree_next_sibling = T.match_buffer(var_token_tree_next_sibling, (num_nodes,), "int32")
        uniform_samples = T.match_buffer(var_uniform_samples, (num_nodes,))
        nbatch = T.int32(is_size_var=True)
        token_tree_parent_ptr = T.match_buffer(var_token_tree_parent_ptr, (nbatch,), "int32")
        # with T.block("root"):
        child_ptr = T.alloc_buffer((1,), "int32", scope="local")
        parent_ptr = T.alloc_buffer((1,), "int32", scope="local")
        child_token = T.alloc_buffer((1,), "int32", scope="local")
        done = T.alloc_buffer((1,), "bool", scope="local")
        psum = T.alloc_buffer((1,), scope="local")
        t0 = T.alloc_buffer((1,), scope="local")
        model_prob_local = T.alloc_buffer((1,), scope="local")
        draft_prob_local = T.alloc_buffer((1,), scope="local")
        p_child = T.alloc_buffer((1,), scope="local")
        q_child = T.alloc_buffer((1,), scope="local")
        uniform_sample = T.alloc_buffer((1,), scope="local")
        pred_shared = T.alloc_buffer((1,), "bool", scope="shared")
        pred_local = T.alloc_buffer((1,), "bool", scope="local")
        for _bx in T.thread_binding(nbatch, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(token_tree_parent_ptr[b], token_tree_first_child[T.min(parent_ptr[0], child_ptr[0]):T.min(parent_ptr[0], child_ptr[0]) + (T.max(parent_ptr[0], child_ptr[0]) + 1 - T.min(parent_ptr[0], child_ptr[0]))], parent_ptr[0], done[0], child_ptr[0], draft_tokens[child_ptr[0]], model_probs[parent_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], child_token[0], draft_probs[child_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], uniform_samples[child_ptr[0]], p_child[0], uniform_sample[0], q_child[0], pred_shared[0], pred_local[0], model_prob_local[0], draft_prob_local[0], psum[0], t0[0], token_tree_next_sibling[child_ptr[0]])
                    T.writes(parent_ptr[0], child_ptr[0], done[0], child_token[0], p_child[0], q_child[0], uniform_sample[0], pred_shared[0], pred_local[0], psum[0], model_prob_local[0], draft_prob_local[0], t0[0], model_probs[parent_ptr[0], T.Cast("int64", tx):T.Cast("int64", tx) + ((vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) - T.int64(1023))], token_tree_parent_ptr[b])
                    parent_ptr[0] = token_tree_parent_ptr[b]
                    child_ptr[0] = token_tree_first_child[parent_ptr[0]]
                    done[0] = T.bool(False)
                    while not done[0]:
                        T.tvm_storage_sync("shared")
                        if child_ptr[0] == -1:
                            done[0] = T.bool(True)
                            T.tvm_storage_sync("shared")
                        else:
                            if tx == 0:
                                child_token[0] = draft_tokens[child_ptr[0]]
                                p_child[0] = model_probs[parent_ptr[0], child_token[0]]
                                q_child[0] = draft_probs[child_ptr[0], child_token[0]]
                                uniform_sample[0] = uniform_samples[child_ptr[0]]
                                pred_shared[0] = p_child[0] >= uniform_sample[0] * q_child[0]
                            T.tvm_storage_sync("shared")
                            pred_local[0] = pred_shared[0]
                            if pred_local[0]:
                                parent_ptr[0] = child_ptr[0]
                                child_ptr[0] = token_tree_first_child[child_ptr[0]]
                            else:
                                psum[0] = T.float32(0.0)
                                for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                    if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                        model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                        psum[0] = psum[0] + model_prob_local[0]
                                with T.block("block_cross_thread"):
                                    T.reads(psum[0])
                                    T.writes(t0[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), psum[0], T.bool(True), t0[0], tx)
                                if t0[0] < T.float32(9.9999999999999995e-08):
                                    parent_ptr[0] = child_ptr[0]
                                    child_ptr[0] = token_tree_first_child[child_ptr[0]]
                                else:
                                    for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                        if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                            model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                            model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)] = model_prob_local[0] / t0[0]
                                    child_ptr[0] = token_tree_next_sibling[child_ptr[0]]
                    if tx == 0:
                        token_tree_parent_ptr[b] = parent_ptr[0]

    @T.prim_func
    def chunk_lse(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        # with T.block("root"):
        A_pad = T.alloc_buffer((batch_size, num_chunks, T.int64(4096)))
        temp_max = T.alloc_buffer((batch_size, num_chunks))
        temp_sum = T.alloc_buffer((batch_size, num_chunks))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("pad"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2])
                T.writes(A_pad[v0, v1, v2])
                A_pad[v0, v1, v2] = T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), A[v0, v1 * T.int64(4096) + v2] / temperature[v0], A[v0, v1 * T.int64(4096) + v2]), T.float32(-340282346638528859811704183484516925440.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("max"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(A_pad[v0, v1, v2])
                T.writes(temp_max[v0, v1])
                with T.init():
                    temp_max[v0, v1] = T.float32(-340282346638528859811704183484516925440.0)
                temp_max[v0, v1] = T.max(temp_max[v0, v1], A_pad[v0, v1, v2])
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("sum_exp"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(temperature[v0], A_pad[v0, v1, v2], temp_max[v0, v1])
                T.writes(temp_sum[v0, v1])
                with T.init():
                    temp_sum[v0, v1] = T.float32(0.0)
                temp_sum[v0, v1] = temp_sum[v0, v1] + T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A_pad[v0, v1, v2] - temp_max[v0, v1]), T.Cast("float32", A_pad[v0, v1, v2] == temp_max[v0, v1])), T.float32(0.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(1)):
            with T.block("log"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], temp_sum[v0, v1], temp_max[v0, v1])
                T.writes(chunked_sum[v0, v1], chunked_max[v0, v1])
                chunked_sum[v0, v1] = T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.log(temp_sum[v0, v1]), temp_sum[v0, v1])
                chunked_max[v0, v1] = temp_max[v0, v1]

    @T.prim_func
    def compact_kv_copy(var_pages: T.handle, var_copy_length_indptr: T.handle, var_copy_src_dst_pos: T.handle, batch_size: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        num_pages = T.int32()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, 16, 128), "float16")
        copy_length_indptr = T.match_buffer(var_copy_length_indptr, (batch_size + 1,), "int32", offset_factor=1)
        total_copy_length = T.int32()
        copy_src_dst_pos = T.match_buffer(var_copy_src_dst_pos, (2, total_copy_length), "int32", offset_factor=1)
        with T.block("root"):
            T.reads()
            T.writes()
            for bhd_o in T.thread_binding(batch_size, thread="blockIdx.x"):
                for bhd_i in T.thread_binding(1024, thread="threadIdx.x"):
                    b: T.int32 = (bhd_o * 1024 + bhd_i) // 1024
                    h: T.int32 = (bhd_o * 1024 + bhd_i) // 128 % 8
                    d: T.int32 = (bhd_o * 1024 + bhd_i) % 128
                    if bhd_o * 1024 + bhd_i < batch_size * 8 * 128:
                        for i in range(copy_length_indptr[b + 1] - copy_length_indptr[b]):
                            src_pos: T.int32 = copy_src_dst_pos[0, copy_length_indptr[b] + i]
                            dst_pos: T.int32 = copy_src_dst_pos[1, copy_length_indptr[b] + i]
                            pages[dst_pos // 16, 0, h, dst_pos % 16, d] = pages[src_pos // 16, 0, h, src_pos % 16, d]
                            pages[dst_pos // 16, 1, h, dst_pos % 16, d] = pages[src_pos // 16, 1, h, src_pos % 16, d]

    @T.prim_func
    def copy_single_page(var_pages: T.handle, src_page_id: T.int64, tgt_page_id: T.int64, copy_length: T.int64):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        num_pages, page_size = T.int32(), T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, page_size, 128), "float16")
        # with T.block("root"):
        for b in T.thread_binding(copy_length, thread="blockIdx.x"):
            for t in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("copy"):
                    vh = T.axis.spatial(8, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) // (copy_length * T.int64(128))))
                    vp = T.axis.spatial(copy_length, (b * T.int64(1024) + T.Cast("int64", t)) % (copy_length * T.int64(128)) // T.int64(128))
                    vd = T.axis.spatial(128, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) % T.int64(128)))
                    T.where(b * T.int64(1024) + T.Cast("int64", t) < copy_length * T.int64(8) * T.int64(128))
                    T.reads(pages[src_page_id, 0:2, vh, vp, vd])
                    T.writes(pages[tgt_page_id, 0:2, vh, vp, vd])
                    pages[tgt_page_id, 0, vh, vp, vd] = pages[src_page_id, 0, vh, vp, vd]
                    pages[tgt_page_id, 1, vh, vp, vd] = pages[src_page_id, 1, vh, vp, vd]

    @T.prim_func
    def full(var_result: T.handle, value: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch_size = T.int32(is_size_var=True)
        result = T.match_buffer(var_result, (batch_size, 1), "int32")
        # with T.block("root"):
        for i in range(batch_size):
            with T.block("block"):
                vi = T.axis.spatial(batch_size, i)
                T.reads()
                T.writes(result[vi, 0])
                result[vi, 0] = value

    @T.prim_func(private=True)
    def fuse_add_norm_decode(pA: T.handle, pB: T.handle, C: T.Buffer((4096,), "float16"), pO: T.handle, pAdd: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int32()
        A = T.match_buffer(pA, (batch_size, 1, 4096), "float16")
        B = T.match_buffer(pB, (batch_size, 1, 4096), "float16")
        O = T.match_buffer(pO, (batch_size, 1, 4096), "float16")
        add = T.match_buffer(pAdd, (batch_size, 1, 4096), "float16")
        # with T.block("root"):
        add_local = T.alloc_buffer((4,), "float16", scope="local")
        sum_shared = T.alloc_buffer((batch_size, 1), scope="shared")
        sum_local = T.alloc_buffer((1024, batch_size, 1), scope="local")
        for v_bx in T.thread_binding(batch_size, thread="blockIdx.x"):
            for v_tx in T.thread_binding(1024, thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                for i in range(4):
                    with T.block("T_add"):
                        bx = T.axis.spatial(batch_size, v_bx)
                        h = T.axis.spatial(4096, i * 1024 + v_tx)
                        T.reads(A[bx, 0, h], B[bx, 0, h])
                        T.writes(add_local[h // 1024])
                        add_local[h // 1024] = A[bx, 0, h] + B[bx, 0, h]
                    with T.block("T_write_back"):
                        bx = T.axis.spatial(batch_size, v_bx)
                        v_ax1 = T.axis.spatial(1, 0)
                        h = T.axis.spatial(4096, i * 1024 + v_tx)
                        T.reads(add_local[h // 1024])
                        T.writes(add[bx, v_ax1, h])
                        add[bx, v_ax1, h] = add_local[h // 1024]
                with T.block("T_multiply_red_rf_init"):
                    tx, bx = T.axis.remap("SS", [v_tx, v_bx])
                    T.reads()
                    T.writes(sum_local[tx, bx, 0])
                    sum_local[tx, bx, 0] = T.float32(0.0)
                for v_i, _j in T.grid(4, 1):
                    with T.block("T_multiply_red_rf_update"):
                        tx, bx, i = T.axis.remap("SSR", [v_tx, v_bx, v_i])
                        T.reads(sum_local[tx, bx, 0], add_local[i])
                        T.writes(sum_local[tx, bx, 0])
                        sum_local[tx, bx, 0] = sum_local[tx, bx, 0] + T.Cast("float32", add_local[i]) * T.Cast("float32", add_local[i])
            for _j in range(1):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_multiply_red"):
                        tx, bx = T.axis.remap("RS", [v_tx_2, v_bx])
                        T.reads(sum_local[tx, bx, 0])
                        T.writes(sum_shared[bx, 0])
                        with T.init():
                            sum_shared[bx, 0] = T.float32(0.0)
                        sum_shared[bx, 0] = sum_shared[bx, 0] + sum_local[tx, bx, 0]
            for i in range(4):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_cast_2"):
                        bx = T.axis.spatial(batch_size, v_bx)
                        h = T.axis.spatial(4096, i * 1024 + v_tx_2)
                        T.reads(sum_shared[bx, 0], add_local[h // 1024], C[h])
                        T.writes(O[bx, 0, h])
                        O[bx, 0, h] = T.Cast("float16", T.rsqrt(sum_shared[bx, 0] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05)) * T.Cast("float32", add_local[h // 1024]) * T.Cast("float32", C[h]))

    @T.prim_func(private=True)
    def fuse_add_norm_prefill(pA: T.handle, pB: T.handle, C: T.Buffer((4096,), "float16"), pO: T.handle, pAdd: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        seq_len = T.int32()
        A = T.match_buffer(pA, (1, seq_len, 4096), "float16")
        B = T.match_buffer(pB, (1, seq_len, 4096), "float16")
        O = T.match_buffer(pO, (1, seq_len, 4096), "float16")
        add = T.match_buffer(pAdd, (1, seq_len, 4096), "float16")
        # with T.block("root"):
        add_local = T.alloc_buffer((4,), "float16", scope="local")
        sum_shared = T.alloc_buffer((1, seq_len), scope="shared")
        sum_local = T.alloc_buffer((1024, 1, seq_len), scope="local")
        for v_bx in T.thread_binding(seq_len, thread="blockIdx.x"):
            for v_tx in T.thread_binding(1024, thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                for v_i in range(4):
                    with T.block("T_add"):
                        bx = T.axis.spatial(seq_len, v_bx)
                        h = T.axis.spatial(4096, v_i * 1024 + v_tx)
                        T.reads(A[0, bx, h], B[0, bx, h])
                        T.writes(add_local[h // 1024])
                        add_local[h // 1024] = A[0, bx, h] + B[0, bx, h]
                    with T.block("T_write_back"):
                        bx = T.axis.spatial(seq_len, v_bx)
                        h = T.axis.spatial(4096, v_i * 1024 + v_tx)
                        T.reads(add_local[h // 1024])
                        T.writes(add[0, bx, h])
                        add[0, bx, h] = add_local[h // 1024]
                with T.block("T_multiply_red_rf_init"):
                    tx, bx = T.axis.remap("SS", [v_tx, v_bx])
                    T.reads()
                    T.writes(sum_local[tx, 0, bx])
                    sum_local[tx, 0, bx] = T.float32(0.0)
                for v_i, _j in T.grid(4, 1):
                    with T.block("T_multiply_red_rf_update"):
                        tx, bx, i = T.axis.remap("SSR", [v_tx, v_bx, v_i])
                        T.reads(sum_local[tx, 0, bx], add_local[i])
                        T.writes(sum_local[tx, 0, bx])
                        sum_local[tx, 0, bx] = sum_local[tx, 0, bx] + T.Cast("float32", add_local[i]) * T.Cast("float32", add_local[i])
            for _j in range(1):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_multiply_red"):
                        tx, bx = T.axis.remap("RS", [v_tx_2, v_bx])
                        T.reads(sum_local[tx, 0, bx])
                        T.writes(sum_shared[0, bx])
                        with T.init():
                            sum_shared[0, bx] = T.float32(0.0)
                        sum_shared[0, bx] = sum_shared[0, bx] + sum_local[tx, 0, bx]
            for v_i in range(4):
                for v_tx_2 in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("T_cast_2"):
                        bx = T.axis.spatial(seq_len, v_bx)
                        v1 = T.axis.spatial(4096, v_i * 1024 + v_tx_2)
                        T.reads(sum_shared[0, bx], add_local[v1 // 1024], C[v1])
                        T.writes(O[0, bx, v1])
                        O[0, bx, v1] = T.Cast("float16", T.rsqrt(sum_shared[0, bx] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05)) * T.Cast("float32", add_local[v1 // 1024]) * T.Cast("float32", C[v1]))

    @T.prim_func(private=True)
    def fused_dequantize1_NT_matmul(model_layers_0_self_attn_qkv_proj_q_weight7: T.Buffer((T.int64(6144), T.int64(412)), "uint32"), model_layers_0_self_attn_qkv_proj_q_scale7: T.Buffer((T.int64(6144), T.int64(103)), "float16"), p_rms_norm325: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        rms_norm325 = T.match_buffer(p_rms_norm325, (batch_size, T.int64(1), T.int64(4096)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(6144)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_qkv_proj_q_weight7[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_qkv_proj_q_weight7[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_qkv_proj_q_scale7[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_qkv_proj_q_scale7[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(6144), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm325[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm325[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize1_NT_matmul10(model_layers_0_self_attn_qkv_proj_q_weight3: T.Buffer((T.int64(6144), T.int64(412)), "uint32"), model_layers_0_self_attn_qkv_proj_q_scale3: T.Buffer((T.int64(6144), T.int64(103)), "float16"), rms_norm65: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), NT_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(6144)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_qkv_proj_q_weight3[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_qkv_proj_q_weight3[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_qkv_proj_q_scale3[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_qkv_proj_q_scale3[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(6144), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm65[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm65[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize1_NT_matmul5(model_layers_0_self_attn_qkv_proj_q_weight6: T.Buffer((T.int64(6144), T.int64(412)), "uint32"), model_layers_0_self_attn_qkv_proj_q_scale6: T.Buffer((T.int64(6144), T.int64(103)), "float16"), p_rms_norm260: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm260 = T.match_buffer(p_rms_norm260, (T.int64(1), seq_len, T.int64(4096)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(6144)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_qkv_proj_q_weight6[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_qkv_proj_q_weight6[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_qkv_proj_q_scale6[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_qkv_proj_q_scale6[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(6144), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm260[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm260[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize2_NT_matmul1(model_layers_0_self_attn_o_proj_q_weight7: T.Buffer((T.int64(4096), T.int64(412)), "uint32"), model_layers_0_self_attn_o_proj_q_scale7: T.Buffer((T.int64(4096), T.int64(103)), "float16"), p_reshape643: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape643 = T.match_buffer(p_reshape643, (batch_size, T.int64(1), T.int64(4096)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(4096)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_o_proj_q_weight7[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_o_proj_q_weight7[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_o_proj_q_scale7[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_o_proj_q_scale7[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(reshape643[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + reshape643[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize2_NT_matmul11(model_layers_0_self_attn_o_proj_q_weight3: T.Buffer((T.int64(4096), T.int64(412)), "uint32"), model_layers_0_self_attn_o_proj_q_scale3: T.Buffer((T.int64(4096), T.int64(103)), "float16"), lv196: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), NT_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_o_proj_q_weight3[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_o_proj_q_weight3[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_o_proj_q_scale3[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_o_proj_q_scale3[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv196[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + lv196[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize2_NT_matmul6(model_layers_0_self_attn_o_proj_q_weight6: T.Buffer((T.int64(4096), T.int64(412)), "uint32"), model_layers_0_self_attn_o_proj_q_scale6: T.Buffer((T.int64(4096), T.int64(103)), "float16"), p_reshape515: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        reshape515 = T.match_buffer(p_reshape515, (T.int64(1), seq_len, T.int64(4096)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_o_proj_q_weight6[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_o_proj_q_weight6[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_o_proj_q_scale6[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_o_proj_q_scale6[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(4096), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(reshape515[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + reshape515[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize3_NT_matmul12(model_layers_0_mlp_gate_up_proj_q_weight3: T.Buffer((T.int64(28672), T.int64(412)), "uint32"), model_layers_0_mlp_gate_up_proj_q_scale3: T.Buffer((T.int64(28672), T.int64(103)), "float16"), rms_norm66: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), NT_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(28672)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_gate_up_proj_q_weight3[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_gate_up_proj_q_weight3[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_gate_up_proj_q_scale3[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_gate_up_proj_q_scale3[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(28672), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm66[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm66[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize3_NT_matmul2(model_layers_0_mlp_gate_up_proj_q_weight7: T.Buffer((T.int64(28672), T.int64(412)), "uint32"), model_layers_0_mlp_gate_up_proj_q_scale7: T.Buffer((T.int64(28672), T.int64(103)), "float16"), p_rms_norm326: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        rms_norm326 = T.match_buffer(p_rms_norm326, (batch_size, T.int64(1), T.int64(4096)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(28672)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_gate_up_proj_q_weight7[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_gate_up_proj_q_weight7[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_gate_up_proj_q_scale7[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_gate_up_proj_q_scale7[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(28672), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm326[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm326[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize3_NT_matmul7(model_layers_0_mlp_gate_up_proj_q_weight6: T.Buffer((T.int64(28672), T.int64(412)), "uint32"), model_layers_0_mlp_gate_up_proj_q_scale6: T.Buffer((T.int64(28672), T.int64(103)), "float16"), p_rms_norm261: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm261 = T.match_buffer(p_rms_norm261, (T.int64(1), seq_len, T.int64(4096)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(28672)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_gate_up_proj_q_weight6[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_gate_up_proj_q_weight6[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_gate_up_proj_q_scale6[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_gate_up_proj_q_scale6[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(28672), T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm261[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm261[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize4_NT_matmul13(model_layers_0_mlp_down_proj_q_weight3: T.Buffer((T.int64(4096), T.int64(1436)), "uint32"), model_layers_0_mlp_down_proj_q_scale3: T.Buffer((T.int64(4096), T.int64(359)), "float16"), lv197: T.Buffer((T.int64(1), T.int64(1), T.int64(14336)), "float16"), NT_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_down_proj_q_weight3[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_down_proj_q_weight3[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_down_proj_q_scale3[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_down_proj_q_scale3[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(4096), T.int64(14336)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv197[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + lv197[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize4_NT_matmul3(model_layers_0_mlp_down_proj_q_weight7: T.Buffer((T.int64(4096), T.int64(1436)), "uint32"), model_layers_0_mlp_down_proj_q_scale7: T.Buffer((T.int64(4096), T.int64(359)), "float16"), p_lv: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv = T.match_buffer(p_lv, (batch_size, T.int64(1), T.int64(14336)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(4096)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_down_proj_q_weight7[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_down_proj_q_weight7[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_down_proj_q_scale7[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_down_proj_q_scale7[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), T.int64(4096), T.int64(14336)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + lv[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize4_NT_matmul8(model_layers_0_mlp_down_proj_q_weight6: T.Buffer((T.int64(4096), T.int64(1436)), "uint32"), model_layers_0_mlp_down_proj_q_scale6: T.Buffer((T.int64(4096), T.int64(359)), "float16"), p_lv65: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        lv65 = T.match_buffer(p_lv65, (T.int64(1), seq_len, T.int64(14336)), "float16")
        NT_matmul_intermediate = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        dequantize_intermediate = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_down_proj_q_weight6[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_down_proj_q_weight6[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_down_proj_q_scale6[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_down_proj_q_scale6[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), seq_len, T.int64(4096), T.int64(14336)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(lv65[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + lv65[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]

    @T.prim_func(private=True)
    def fused_dequantize_fused_NT_matmul14_cast2(p_lm_head_q_weight3: T.handle, p_lm_head_q_scale3: T.handle, rms_norm129: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        lm_head_q_weight3 = T.match_buffer(p_lm_head_q_weight3, (vocab_size, T.int64(412)), "uint32")
        lm_head_q_scale3 = T.match_buffer(p_lm_head_q_scale3, (vocab_size, T.int64(103)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), T.int64(1), vocab_size))
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), vocab_size), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(lm_head_q_weight3[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(lm_head_q_weight3[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], lm_head_q_scale3[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * lm_head_q_scale3[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm129[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm129[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), vocab_size):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float32", NT_matmul_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize_fused_NT_matmul15_cast3(p_lm_head_q_weight1: T.handle, p_lm_head_q_scale1: T.handle, p_hidden_states: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        lm_head_q_weight1 = T.match_buffer(p_lm_head_q_weight1, (vocab_size, T.int64(412)), "uint32")
        lm_head_q_scale1 = T.match_buffer(p_lm_head_q_scale1, (vocab_size, T.int64(103)), "float16")
        seq_len = T.int64()
        hidden_states = T.match_buffer(p_hidden_states, (seq_len, T.int64(4096)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (seq_len, vocab_size))
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((seq_len, vocab_size), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(lm_head_q_weight1[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(lm_head_q_weight1[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], lm_head_q_scale1[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * lm_head_q_scale1[v_i0, v_i1 // T.int64(40)]
        for i0, i1, k in T.grid(seq_len, vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_k = T.axis.remap("SSR", [i0, i1, k])
                T.reads(hidden_states[v_i0, v_k], dequantize_intermediate[v_i1, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1] = NT_matmul_intermediate[v_i0, v_i1] + hidden_states[v_i0, v_k] * dequantize_intermediate[v_i1, v_k]
        for i0, i1 in T.grid(seq_len, vocab_size):
            with T.block("compute_1"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(NT_matmul_intermediate[v_i0, v_i1])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1])
                compute_intermediate_intermediate[v_i0, v_i1] = T.Cast("float32", NT_matmul_intermediate[v_i0, v_i1])

    @T.prim_func(private=True)
    def fused_dequantize_fused_NT_matmul4_cast(p_lm_head_q_weight7: T.handle, p_lm_head_q_scale7: T.handle, p_rms_norm389: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        lm_head_q_weight7 = T.match_buffer(p_lm_head_q_weight7, (vocab_size, T.int64(412)), "uint32")
        lm_head_q_scale7 = T.match_buffer(p_lm_head_q_scale7, (vocab_size, T.int64(103)), "float16")
        batch_size = T.int64()
        rms_norm389 = T.match_buffer(p_rms_norm389, (batch_size, T.int64(1), T.int64(4096)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), vocab_size))
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((batch_size, T.int64(1), vocab_size), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(lm_head_q_weight7[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(lm_head_q_weight7[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], lm_head_q_scale7[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * lm_head_q_scale7[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(batch_size, T.int64(1), vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(rms_norm389[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + rms_norm389[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for i0, i1, i2 in T.grid(batch_size, T.int64(1), vocab_size):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float32", NT_matmul_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize_fused_NT_matmul9_cast1(p_lm_head_q_weight6: T.handle, p_lm_head_q_scale6: T.handle, p_take2: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        lm_head_q_weight6 = T.match_buffer(p_lm_head_q_weight6, (vocab_size, T.int64(412)), "uint32")
        lm_head_q_scale6 = T.match_buffer(p_lm_head_q_scale6, (vocab_size, T.int64(103)), "float16")
        batch_size = T.int64()
        take2 = T.match_buffer(p_take2, (T.int64(1), batch_size, T.int64(4096)), "float16")
        compute_intermediate_intermediate = T.match_buffer(p_output0, (T.int64(1), batch_size, vocab_size))
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        dequantize_intermediate = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        NT_matmul_intermediate = T.alloc_buffer((T.int64(1), batch_size, vocab_size), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(lm_head_q_weight6[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(lm_head_q_weight6[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], lm_head_q_scale6[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize_intermediate[v_i0, v_i1])
                dequantize_intermediate[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * lm_head_q_scale6[v_i0, v_i1 // T.int64(40)]
        for i0, i1, i2, k in T.grid(T.int64(1), batch_size, vocab_size, T.int64(4096)):
            with T.block("NT_matmul"):
                v_i0, v_i1, v_i2, v_k = T.axis.remap("SSSR", [i0, i1, i2, k])
                T.reads(take2[v_i0, v_i1, v_k], dequantize_intermediate[v_i2, v_k])
                T.writes(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                with T.init():
                    NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0.0)
                NT_matmul_intermediate[v_i0, v_i1, v_i2] = NT_matmul_intermediate[v_i0, v_i1, v_i2] + take2[v_i0, v_i1, v_k] * dequantize_intermediate[v_i2, v_k]
        for i0, i1, i2 in T.grid(T.int64(1), batch_size, vocab_size):
            with T.block("compute_1"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(NT_matmul_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute_intermediate_intermediate[v_i0, v_i1, v_i2])
                compute_intermediate_intermediate[v_i0, v_i1, v_i2] = T.Cast("float32", NT_matmul_intermediate[v_i0, v_i1, v_i2])

    @T.prim_func(private=True)
    def fused_dequantize_take1(p_model_embed_tokens_q_weight: T.handle, p_model_embed_tokens_q_scale: T.handle, p_input_ids: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        vocab_size = T.int32()
        model_embed_tokens_q_weight = T.match_buffer(p_model_embed_tokens_q_weight, (vocab_size, 412), "uint32")
        model_embed_tokens_q_scale = T.match_buffer(p_model_embed_tokens_q_scale, (vocab_size, 103), "float16")
        seq_len = T.int32()
        input_ids = T.match_buffer(p_input_ids, (seq_len,), "int32")
        T_take_intermediate = T.match_buffer(p_output0, (seq_len, 4096), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, 4096), "float16")
        for i0, i1 in T.grid(vocab_size, 4096):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_embed_tokens_q_weight[v_i0, v_i1 // 10])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_embed_tokens_q_weight[v_i0, v_i1 // 10], T.Cast("uint32", v_i1 % 10 * 3)), T.uint32(7)))
        for ax0, ax1 in T.grid(seq_len, 4096):
            with T.block("T_take"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(input_ids[v_ax0], compute[input_ids[v_ax0], v_ax1], model_embed_tokens_q_scale[input_ids[v_ax0], v_ax1 // 40])
                T.writes(T_take_intermediate[v_ax0, v_ax1])
                T_take_intermediate[v_ax0, v_ax1] = (compute[input_ids[v_ax0], v_ax1] - T.float16(3.0)) * model_embed_tokens_q_scale[input_ids[v_ax0], v_ax1 // 40]

    @T.prim_func(private=True)
    def fused_reshape10_reshape11(lv165: T.Buffer((T.int64(1), T.int64(32), T.int64(128)), "float16"), T_reshape_intermediate_1: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(32), T.int64(128)), "float16")
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(32), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv165[T.int64(0), (v_ax3 // T.int64(128) + v_ax2) % T.int64(32), v_ax3 % T.int64(128)])
                T.writes(T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv165[T.int64(0), (v_ax3 // T.int64(128) + v_ax2) % T.int64(32), v_ax3 % T.int64(128)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_reshape_intermediate[T.int64(0), T.int64(0), v_ax2 % T.int64(4096) // T.int64(128), v_ax2 % T.int64(128)])
                T.writes(T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = T_reshape_intermediate[T.int64(0), T.int64(0), v_ax2 % T.int64(4096) // T.int64(128), v_ax2 % T.int64(128)]

    @T.prim_func(private=True)
    def fused_reshape8_reshape9(lv771: T.Buffer((T.int64(1), T.int64(1), T.int64(6144)), "float16"), T_reshape_intermediate_1: T.Buffer((T.int64(1), T.int64(48), T.int64(128)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_reshape_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(48), T.int64(128)), "float16")
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(48), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv771[T.int64(0), T.int64(0), (v_ax2 * T.int64(128) + v_ax3) % T.int64(6144)])
                T.writes(T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = lv771[T.int64(0), T.int64(0), (v_ax2 * T.int64(128) + v_ax3) % T.int64(6144)]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(48), T.int64(128)):
            with T.block("T_reshape_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_reshape_intermediate[T.int64(0), T.int64(0), (v_ax2 // T.int64(128) + v_ax1) % T.int64(48), v_ax2 % T.int64(128)])
                T.writes(T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_reshape_intermediate_1[v_ax0, v_ax1, v_ax2] = T_reshape_intermediate[T.int64(0), T.int64(0), (v_ax2 // T.int64(128) + v_ax1) % T.int64(48), v_ax2 % T.int64(128)]

    @T.prim_func
    def fused_rope(var_qkv: T.handle, var_position_map: T.handle, var_q: T.handle, var_k: T.handle, var_v: T.handle, apply_rope: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        seq_len = T.int32()
        qkv = T.match_buffer(var_qkv, (seq_len, 48, 128), "float16")
        position_map = T.match_buffer(var_position_map, (seq_len,), "int32", offset_factor=1)
        q = T.match_buffer(var_q, (seq_len, 32, 128), "float16")
        k = T.match_buffer(var_k, (seq_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (seq_len, 8, 128), "float16")
        # with T.block("root"):
        for iters_0, iters_1, iters_2 in T.grid(seq_len, 48, 128):
            with T.block("llama_fused_rope"):
                s, h, d = T.axis.remap("SSS", [iters_0, iters_1, iters_2])
                T.reads(position_map[s], qkv[s, h, d - 64:d - 64 + 129])
                T.writes(q[s, h, d], k[s, h - 32, d], v[s, h - 40, d])
                if h < 32:
                    freq = T.float32()
                    q[s, h, d] = T.if_then_else(apply_rope > 0 and d < 128, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 64, qkv[s, h, d + 64] * T.float16(-1.0), qkv[s, h, d - 64]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(500000.0), T.Cast("float32", d * 2 % 128) / T.float32(128.0))}), qkv[s, h, d])
                else:
                    if h < 40:
                        freq = T.float32()
                        k[s, h - 32, d] = T.if_then_else(apply_rope > 0 and d < 128, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 64, qkv[s, h, d + 64] * T.float16(-1.0), qkv[s, h, d - 64]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(500000.0), T.Cast("float32", d * 2 % 128) / T.float32(128.0))}), qkv[s, h, d])
                    else:
                        v[s, h - 40, d] = qkv[s, h, d]

    @T.prim_func(private=True)
    def fused_split1_silu1_multiply1(p_lv259: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        lv259 = T.match_buffer(p_lv259, (T.int64(1), seq_len, T.int64(28672)), "float16")
        T_multiply_intermediate_1 = T.match_buffer(p_output0, (T.int64(1), seq_len, T.int64(14336)), "float16")
        # with T.block("root"):
        T_split_sections_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(14336)), "float16")
        T_split_sections_intermediate_1 = T.alloc_buffer((T.int64(1), seq_len, T.int64(14336)), "float16")
        compute = T.alloc_buffer((T.int64(1), seq_len, T.int64(14336)), "float16")
        T_multiply_intermediate = T.alloc_buffer((T.int64(1), seq_len, T.int64(14336)), "float16")
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(14336)):
            with T.block("T_split_sections"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv259[v_ax0, v_ax1, v_ax2])
                T.writes(T_split_sections_intermediate[v_ax0, v_ax1, v_ax2])
                T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] = lv259[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(14336)):
            with T.block("T_split_sections_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv259[v_ax0, v_ax1, v_ax2 + T.int64(14336)])
                T.writes(T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] = lv259[v_ax0, v_ax1, v_ax2 + T.int64(14336)]
        for i0, i1, i2 in T.grid(T.int64(1), seq_len, T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_split_sections_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.sigmoid(T_split_sections_intermediate[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(14336)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_split_sections_intermediate[v_ax0, v_ax1, v_ax2], compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] * compute[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(14336)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_intermediate[v_ax0, v_ax1, v_ax2], T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2] = T_multiply_intermediate[v_ax0, v_ax1, v_ax2] * T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_split2_silu2_multiply2(lv773: T.Buffer((T.int64(1), T.int64(1), T.int64(28672)), "float16"), T_multiply_intermediate_1: T.Buffer((T.int64(1), T.int64(1), T.int64(14336)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_split_sections_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(14336)), "float16")
        T_split_sections_intermediate_1 = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(14336)), "float16")
        compute = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(14336)), "float16")
        T_multiply_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(14336)), "float16")
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(14336)):
            with T.block("T_split_sections"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv773[v_ax0, v_ax1, v_ax2])
                T.writes(T_split_sections_intermediate[v_ax0, v_ax1, v_ax2])
                T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] = lv773[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(14336)):
            with T.block("T_split_sections_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv773[v_ax0, v_ax1, v_ax2 + T.int64(14336)])
                T.writes(T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] = lv773[v_ax0, v_ax1, v_ax2 + T.int64(14336)]
        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_split_sections_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.sigmoid(T_split_sections_intermediate[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(14336)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_split_sections_intermediate[v_ax0, v_ax1, v_ax2], compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] * compute[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(14336)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_intermediate[v_ax0, v_ax1, v_ax2], T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2] = T_multiply_intermediate[v_ax0, v_ax1, v_ax2] * T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func(private=True)
    def fused_split_silu_multiply(p_lv2: T.handle, p_output0: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv2 = T.match_buffer(p_lv2, (batch_size, T.int64(1), T.int64(28672)), "float16")
        T_multiply_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(14336)), "float16")
        # with T.block("root"):
        T_split_sections_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(14336)), "float16")
        T_split_sections_intermediate_1 = T.alloc_buffer((batch_size, T.int64(1), T.int64(14336)), "float16")
        compute = T.alloc_buffer((batch_size, T.int64(1), T.int64(14336)), "float16")
        T_multiply_intermediate = T.alloc_buffer((batch_size, T.int64(1), T.int64(14336)), "float16")
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(14336)):
            with T.block("T_split_sections"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv2[v_ax0, v_ax1, v_ax2])
                T.writes(T_split_sections_intermediate[v_ax0, v_ax1, v_ax2])
                T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] = lv2[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(14336)):
            with T.block("T_split_sections_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(lv2[v_ax0, v_ax1, v_ax2 + T.int64(14336)])
                T.writes(T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2] = lv2[v_ax0, v_ax1, v_ax2 + T.int64(14336)]
        for i0, i1, i2 in T.grid(batch_size, T.int64(1), T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1, v_i2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_split_sections_intermediate[v_i0, v_i1, v_i2])
                T.writes(compute[v_i0, v_i1, v_i2])
                compute[v_i0, v_i1, v_i2] = T.sigmoid(T_split_sections_intermediate[v_i0, v_i1, v_i2])
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(14336)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_split_sections_intermediate[v_ax0, v_ax1, v_ax2], compute[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = T_split_sections_intermediate[v_ax0, v_ax1, v_ax2] * compute[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(14336)):
            with T.block("T_multiply_1"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_multiply_intermediate[v_ax0, v_ax1, v_ax2], T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2])
                T_multiply_intermediate_1[v_ax0, v_ax1, v_ax2] = T_multiply_intermediate[v_ax0, v_ax1, v_ax2] * T_split_sections_intermediate_1[v_ax0, v_ax1, v_ax2]

    @T.prim_func
    def gather_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        m, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (m, n))
        batch_size = T.int32(is_size_var=True)
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        dst = T.match_buffer(var_dst, (batch_size, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("gather_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[indices[vb], vj], indices[vb])
                T.writes(dst[vb, vj])
                dst[vb, vj] = src[indices[vb], vj]

    @T.prim_func(private=True)
    def get_index_from_sorted(A: T.handle, B: T.handle, C: T.handle, D: T.handle, E: T.handle, F: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        indices = T.match_buffer(B, (batch, vocab_size), "int32")
        renorm_prob = T.match_buffer(C, (batch, 1))
        out_batch = T.int64(is_size_var=True)
        usample = T.match_buffer(D, (out_batch, 1))
        sample_indices = T.match_buffer(E, (out_batch, 1), "int32")
        output_index = T.match_buffer(F, (out_batch, 1), "int32")
        # with T.block("root"):
        for ax0, ax1 in T.grid(out_batch, vocab_size):
            with T.block("T_get_index_from_sorted"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(usample[v_ax0, T.int64(0)], cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1):v_ax1 - T.int64(1) + T.int64(2)], sample_indices[v_ax0, T.int64(0)], renorm_prob[sample_indices[v_ax0, T.int64(0)], 0], indices[sample_indices[v_ax0, T.int64(0)], T.min(T.int64(0), v_ax1):T.min(T.int64(0), v_ax1) + (T.max(T.int64(0), v_ax1) + T.int64(1) - T.min(T.int64(0), v_ax1))])
                T.writes(output_index[v_ax0, 0])
                if usample[v_ax0, T.int64(0)] < cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0] or v_ax1 + T.int64(1) == vocab_size:
                    if v_ax1 == T.int64(0):
                        output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], 0]
                    else:
                        if usample[v_ax0, T.int64(0)] >= cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1)] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0]:
                            output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], v_ax1]

    @T.prim_func(private=True)
    def get_renorm_prob(A: T.handle, B: T.handle, C: T.handle, D: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        top_p = T.match_buffer(B, (batch, 1))
        top_k = T.match_buffer(C, (batch, 1), "int32")
        renorm_prob = T.match_buffer(D, (batch, 1))
        # with T.block("root"):
        for ax0, ax1 in T.grid(batch, vocab_size):
            with T.block("T_get_renorm_prob"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(cumsum_sorted[v_ax0, T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)):T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + (T.max(T.max(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + T.int64(1) - T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)))], top_p[v_ax0, 0], top_k[v_ax0, 0])
                T.writes(renorm_prob[v_ax0, 0])
                if not (cumsum_sorted[v_ax0, 0] < top_p[v_ax0, 0] and top_k[v_ax0, 0] > 1):
                    renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, 0]
                else:
                    if cumsum_sorted[v_ax0, v_ax1] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0]):
                        if v_ax1 + T.int64(1) == vocab_size:
                            renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1]
                        else:
                            if not (cumsum_sorted[v_ax0, v_ax1 + T.int64(1)] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0])):
                                renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1 + T.int64(1)]

    @T.prim_func(private=True)
    def gpu_2d_continuous_cumsum(var_a: T.handle, var_out: T.handle):
        T.func_attr({"tir.is_scheduled": 1})
        m, n = T.int64(), T.int64()
        A = T.match_buffer(var_a, (m, n))
        Out = T.match_buffer(var_out, (m, n))
        # with T.block("root"):
        Tmp = T.alloc_buffer((m, n))
        ceil_log2: T.int64 = T.Cast("int64", T.ceil(T.log2(T.Cast("float32", n))))
        total_rounds: T.int64 = ceil_log2 // T.int64(9)
        for by in T.thread_binding(m, thread="blockIdx.y"):
            for bx in T.thread_binding((n + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                with T.block(""):
                    T.reads(A[by, bx * T.int64(512):bx * T.int64(512) + T.int64(512)])
                    T.writes(Out[by, bx * T.int64(512):bx * T.int64(512) + T.int64(512)], Tmp[by, bx])
                    local_buf = T.alloc_buffer((4,), scope="local")
                    shared_buf = T.alloc_buffer((T.int64(512),), scope="shared")
                    for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                        for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            tx_idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + tx * T.int64(4)
                            for i in T.vectorized(T.int64(4)):
                                local_buf[i] = T.if_then_else(tx_idx + i < n, T.Cast("float32", A[by, tx_idx + i]), T.Cast("float32", 0))
                            for i in T.unroll(T.int64(1), T.int64(4)):
                                local_buf[i] = local_buf[i] + local_buf[i - T.int64(1)]
                            for i in T.vectorized(T.int64(4)):
                                shared_buf[ty * T.int64(128) + tx * T.int64(4) + i] = local_buf[i]
                            for i in T.unroll(T.int64(5)):
                                for j in T.vectorized(T.int64(4)):
                                    idx: T.int64 = ty * T.int64(128) + tx * T.int64(4)
                                    if tx >= T.shift_left(T.int64(1), i):
                                        shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[idx - T.shift_left(T.int64(1), i) * T.int64(4) + T.int64(4) - T.int64(1)]
                            for i in T.unroll(T.int64(1), T.int64(4)):
                                for j in T.vectorized(T.int64(4)):
                                    if ty == T.int64(0):
                                        idx: T.int64 = i * T.int64(128) + tx * T.int64(4)
                                        shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[i * T.int64(128) - T.int64(1)]
                            for i in T.vectorized(T.int64(4)):
                                idx: T.int64 = ty * T.int64(128) + tx * T.int64(4) + i
                                if bx * T.int64(512) + idx < n:
                                    Out[by, bx * T.int64(512) + idx] = shared_buf[idx]
                            if tx == T.int64(0) and ty == T.int64(0):
                                for i in T.vectorized(T.int64(4)):
                                    Tmp[by, bx] = shared_buf[T.int64(511)]
        for i in range(total_rounds):
            cur_len: T.int64 = (n + T.shift_left(T.int64(1), T.int64(9) * (i + T.int64(1))) - T.int64(1)) // T.shift_left(T.int64(1), T.int64(9) * (i + T.int64(1)))
            for by in T.thread_binding(m, thread="blockIdx.y"):
                for bx in T.thread_binding((cur_len + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                    with T.block(""):
                        T.reads(Tmp[by, bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)):bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)) + T.int64(512)])
                        T.writes(Tmp[by, T.min(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx):T.min(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx) + (T.max(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)) + T.int64(511), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx) + T.int64(1) - T.min(bx * T.int64(512) + i * ((n + T.int64(511)) // T.int64(512)), (i + T.int64(1)) * ((n + T.int64(511)) // T.int64(512)) + bx))])
                        local_buf = T.alloc_buffer((4,), scope="local")
                        shared_buf = T.alloc_buffer((T.int64(512),), scope="shared")
                        for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                            for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                tx_idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + tx * T.int64(4)
                                for i_1 in T.vectorized(T.int64(4)):
                                    local_buf[i_1] = T.if_then_else(tx_idx + i_1 < cur_len, T.Cast("float32", Tmp[by, i * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + tx_idx + i_1]), T.Cast("float32", 0))
                                for i_1 in T.unroll(T.int64(1), T.int64(4)):
                                    local_buf[i_1] = local_buf[i_1] + local_buf[i_1 - T.int64(1)]
                                for i_1 in T.vectorized(T.int64(4)):
                                    shared_buf[ty * T.int64(128) + tx * T.int64(4) + i_1] = local_buf[i_1]
                                for i_1 in T.unroll(T.int64(5)):
                                    for j in T.vectorized(T.int64(4)):
                                        idx: T.int64 = ty * T.int64(128) + tx * T.int64(4)
                                        if tx >= T.shift_left(T.int64(1), i_1):
                                            shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[idx - T.shift_left(T.int64(1), i_1) * T.int64(4) + T.int64(4) - T.int64(1)]
                                for i_1 in T.unroll(T.int64(1), T.int64(4)):
                                    for j in T.vectorized(T.int64(4)):
                                        if ty == T.int64(0):
                                            idx: T.int64 = i_1 * T.int64(128) + tx * T.int64(4)
                                            shared_buf[idx + j] = shared_buf[idx + j] + shared_buf[i_1 * T.int64(128) - T.int64(1)]
                                for i_1 in T.vectorized(T.int64(4)):
                                    idx: T.int64 = ty * T.int64(128) + tx * T.int64(4) + i_1
                                    if bx * T.int64(512) + idx < cur_len:
                                        Tmp[by, i * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + bx * T.int64(512) + idx] = shared_buf[idx]
                                if tx == T.int64(0) and ty == T.int64(0):
                                    for i_1 in T.vectorized(T.int64(4)):
                                        Tmp[by, (i + T.int64(1)) * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + bx] = shared_buf[T.int64(511)]
        for i in range(total_rounds - T.int64(1)):
            real_idx: T.int64 = total_rounds - T.int64(1) - i - T.int64(1)
            cur_len: T.int64 = (n + T.shift_left(T.int64(1), T.int64(9) * (real_idx + T.int64(1))) - T.int64(1)) // T.shift_left(T.int64(1), T.int64(9) * (real_idx + T.int64(1)))
            for by in T.thread_binding(m, thread="blockIdx.y"):
                for bx in T.thread_binding((cur_len + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                    for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                        for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                            for i_1 in range(T.int64(4)):
                                idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + i_1 * T.int64(32) + tx
                                if idx < cur_len:
                                    Tmp[by, real_idx * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + idx] = Tmp[by, real_idx * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + idx] + T.if_then_else(bx > T.int64(0), Tmp[by, (real_idx + T.int64(1)) * ((n + T.int64(512) - T.int64(1)) // T.int64(512)) + bx - T.int64(1)], T.float32(0.0))
        for by in T.thread_binding(m, thread="blockIdx.y"):
            for bx in T.thread_binding((n + T.int64(511)) // T.int64(512), thread="blockIdx.x"):
                for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                    for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        for i in range(T.int64(4)):
                            idx: T.int64 = bx * T.int64(512) + ty * T.int64(128) + i * T.int64(32) + tx
                            if idx < n:
                                Out[by, idx] = Out[by, idx] + T.if_then_else(bx > T.int64(0), Tmp[by, bx - T.int64(1)], T.float32(0.0))

    @T.prim_func(private=True)
    def index(var_rms_norm64: T.handle, index: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm64 = T.match_buffer(var_rms_norm64, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        for i, _, k in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("index"):
                v_i, v__, v_k = T.axis.remap("SSS", [i, _, k])
                T.reads(rms_norm64[v_i, seq_len - T.int64(1), v_k])
                T.writes(index[v_i, v__, v_k])
                index[v_i, v__, v_k] = rms_norm64[v_i, seq_len - T.int64(1), v_k]

    @T.prim_func
    def merge_state_inplace(v: T.handle, s: T.handle, v_other: T.handle, s_other: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        N, H, D = T.int32(is_size_var=True), T.int32(is_size_var=True), T.int32(is_size_var=True)
        V = T.match_buffer(v, (N, H, D), "float16")
        S = T.match_buffer(s, (N, H))
        V_other = T.match_buffer(v_other, (N, H, D), "float16")
        S_other = T.match_buffer(s_other, (N, H))
        # with T.block("root"):
        for bx in T.thread_binding(N, thread="blockIdx.x"):
            for by in T.thread_binding(1, thread="blockIdx.y"):
                for ty in T.thread_binding(32, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("merge"):
                            T.reads(S[bx, ty + by * 32], S_other[bx, ty + by * 32], V[bx, ty + by * 32, tx * 4:tx * 4 + 4], V_other[bx, ty + by * 32, tx * 4:tx * 4 + 4])
                            T.writes(V[bx, ty + by * 32, tx * 4:tx * 4 + 4], S[bx, ty + by * 32])
                            s_val = T.alloc_buffer((1,), scope="local")
                            s_other_val = T.alloc_buffer((1,), scope="local")
                            s_max = T.alloc_buffer((1,), scope="local")
                            scale = T.alloc_buffer((1,), scope="local")
                            other_scale = T.alloc_buffer((1,), scope="local")
                            v_vec = T.alloc_buffer((4,), "float16", scope="local")
                            v_other_vec = T.alloc_buffer((4,), "float16", scope="local")
                            s_val[0] = S[bx, ty + by * 32]
                            s_other_val[0] = S_other[bx, ty + by * 32]
                            s_max[0] = T.max(s_val[0], s_other_val[0])
                            s_val[0] = T.exp2(s_val[0] - s_max[0])
                            s_other_val[0] = T.exp2(s_other_val[0] - s_max[0])
                            scale[0] = s_val[0] / (s_val[0] + s_other_val[0])
                            other_scale[0] = s_other_val[0] / (s_val[0] + s_other_val[0])
                            for vec in T.vectorized(4):
                                v_vec[vec] = V[bx, ty + by * 32, tx * 4 + vec]
                            for vec in T.vectorized(4):
                                v_other_vec[vec] = V_other[bx, ty + by * 32, tx * 4 + vec]
                            for vec in range(4):
                                v_vec[vec] = T.Cast("float16", T.Cast("float32", v_vec[vec]) * scale[0] + T.Cast("float32", v_other_vec[vec]) * other_scale[0])
                            for vec in T.vectorized(4):
                                V[bx, ty + by * 32, tx * 4 + vec] = v_vec[vec]
                            S[bx, ty + by * 32] = T.log2(s_val[0] + s_other_val[0]) + s_max[0]

    @T.prim_func
    def parallel_sampling_from_prob(var_prob: T.handle, var_uniform_samples: T.handle, var_row_indices: T.handle, var_sampled_token_ids: T.handle):
        T.func_attr({"tir.is_scheduled": 1})
        n, vocab_size = T.int64(), T.int64()
        prob = T.match_buffer(var_prob, (n, vocab_size))
        batch_size = T.int64()
        uniform_samples = T.match_buffer(var_uniform_samples, (batch_size, 1))
        row_indices = T.match_buffer(var_row_indices, (batch_size, 1), "int32")
        token_ids = T.match_buffer(var_sampled_token_ids, (batch_size, 1), "int32")
        # with T.block("root"):
        aggregate = T.alloc_buffer((), scope="local")
        sample_id_local = T.alloc_buffer((), "int32", scope="local")
        step_iter = T.alloc_buffer((), "int32", scope="local")
        for bx in T.thread_binding(batch_size, thread="blockIdx.x"):
            row_idx: T.int32 = row_indices[bx, 0]
            for ty in T.thread_binding(T.int64(4), thread="threadIdx.y"):
                for tx in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    u: T.float32 = uniform_samples[bx, 0]
                    aggregate[()] = T.Cast("float32", 0)
                    step_iter[()] = 0
                    while T.tvm_thread_invariant((step_iter[()] == 0 or aggregate[()] < u - T.float32(9.9999999999999995e-07)) and T.Cast("int64", step_iter[()]) < (vocab_size + T.int64(512) - T.int64(1)) // T.int64(512)):
                        with T.block(""):
                            T.reads(step_iter[()], prob[row_idx, T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4):T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4) + T.int64(4)], aggregate[()])
                            T.writes(sample_id_local[()], aggregate[()])
                            prob_gt_threshold = T.alloc_buffer((T.int64(4),), scope="local")
                            cumsum = T.alloc_buffer((T.int64(512),), scope="shared")
                            greater_than_u = T.alloc_buffer((T.int64(4),), "bool", scope="local")
                            mask = T.alloc_buffer((T.int64(4),), "bool", scope="local")
                            valid = T.alloc_buffer((T.int64(4),), "bool", scope="local")
                            indices = T.alloc_buffer((T.int64(4),), "int32", scope="local")
                            step_aggregate = T.alloc_buffer((), scope="local")
                            for v in T.unroll(T.int64(4)):
                                idx: T.int64 = T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4) + v
                                prob_local: T.float32 = T.if_then_else(idx < vocab_size, prob[row_idx, idx], T.Cast("float32", 0))
                                prob_gt_threshold[v] = T.if_then_else(prob_local > T.float32(0.0), prob_local, T.Cast("float32", 0))
                                valid[v] = prob_local > T.float32(0.0) and idx < vocab_size
                            with T.block(""):
                                T.reads(prob_gt_threshold[T.int64(0):T.int64(4)])
                                T.writes(step_aggregate[()])
                                local_sum = T.alloc_buffer((), scope="local")
                                shared_buf = T.alloc_buffer((T.int64(128),), scope="shared")
                                idx: T.int64 = ty * T.int64(32) + tx
                                local_sum[()] = T.Cast("float32", 0)
                                for i in T.unroll(T.int64(4)):
                                    local_sum[()] = local_sum[()] + prob_gt_threshold[i]
                                shared_buf[idx] = local_sum[()]
                                for i in T.unroll(T.int64(7)):
                                    if idx % T.shift_left(T.int64(1), i + T.int64(1)) == T.int64(0):
                                        shared_buf[idx] = shared_buf[idx] + shared_buf[idx + T.shift_left(T.int64(1), i)]
                                step_aggregate[()] = shared_buf[0]
                            if T.tvm_thread_invariant(aggregate[()] + step_aggregate[()] >= u - T.float32(9.9999999999999995e-07)):
                                for i in T.unroll(T.int64(1), T.int64(4)):
                                    prob_gt_threshold[i] = prob_gt_threshold[i] + prob_gt_threshold[i - T.int64(1)]
                                for i in T.vectorized(T.int64(4)):
                                    cumsum[ty * T.int64(128) + tx * T.int64(4) + i] = prob_gt_threshold[i]
                                for i in T.unroll(T.int64(5)):
                                    for j in T.vectorized(T.int64(4)):
                                        idx: T.int64 = ty * T.int64(128) + tx * T.int64(4)
                                        if tx >= T.shift_left(T.int64(1), i):
                                            cumsum[idx + j] = cumsum[idx + j] + cumsum[idx - T.shift_left(T.int64(1), i) * T.int64(4) + T.int64(4) - T.int64(1)]
                                for i in T.unroll(T.int64(1), T.int64(4)):
                                    for j in T.vectorized(T.int64(4)):
                                        if ty == T.int64(0):
                                            idx: T.int64 = i * T.int64(128) + tx * T.int64(4)
                                            cumsum[idx + j] = cumsum[idx + j] + cumsum[i * T.int64(128) - T.int64(1)]
                                for v in T.unroll(T.int64(4)):
                                    greater_than_u[v] = cumsum[ty * T.int64(128) + tx * T.int64(4) + v] + aggregate[()] >= u - T.float32(9.9999999999999995e-07)
                                with T.block(""):
                                    T.reads(greater_than_u[T.int64(0):T.int64(4)])
                                    T.writes(mask[T.int64(0):T.int64(4)])
                                    shared_buf = T.alloc_buffer((T.int64(128),), "bool", scope="shared")
                                    tx_idx: T.int64 = ty * T.int64(32) + tx
                                    shared_buf[tx_idx] = greater_than_u[T.int64(3)]
                                    mask[0] = T.if_then_else(tx_idx != T.int64(0), T.Cast("int8", greater_than_u[0]) != T.Cast("int8", shared_buf[tx_idx - T.int64(1)]), greater_than_u[0])
                                    for i in T.unroll(T.int64(1), T.int64(4)):
                                        mask[i] = T.Cast("int8", greater_than_u[i]) != T.Cast("int8", greater_than_u[i - T.int64(1)])
                                for v in T.unroll(T.int64(4)):
                                    mask[v] = mask[v] and valid[v]
                                    indices[v] = T.Cast("int32", T.Cast("int64", step_iter[()]) * T.int64(512) + ty * T.int64(128) + tx * T.int64(4) + v)
                                with T.block(""):
                                    T.reads(mask[T.int64(0):T.int64(4)], indices[T.int64(0):T.int64(4)])
                                    T.writes(sample_id_local[()])
                                    local_sum = T.alloc_buffer((), "int32", scope="local")
                                    shared_buf = T.alloc_buffer((T.int64(128),), "int32", scope="shared")
                                    idx: T.int64 = ty * T.int64(32) + tx
                                    local_sum[()] = T.Cast("int32", vocab_size - T.int64(1))
                                    for i in T.unroll(T.int64(4)):
                                        if mask[i]:
                                            local_sum[()] = T.min(local_sum[()], indices[i])
                                    shared_buf[idx] = local_sum[()]
                                    for i in T.unroll(T.int64(7)):
                                        if idx % T.shift_left(T.int64(1), i + T.int64(1)) == T.int64(0):
                                            shared_buf[idx] = T.min(shared_buf[idx], shared_buf[idx + T.shift_left(T.int64(1), i)])
                                    sample_id_local[()] = shared_buf[0]
                            aggregate[()] = aggregate[()] + step_aggregate[()]
                        step_iter[()] = step_iter[()] + 1
                    if tx == T.int64(0) and ty == T.int64(0):
                        token_ids[bx, 0] = sample_id_local[()]

    @T.prim_func(private=True)
    def reshape(var_lv: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv = T.match_buffer(var_lv, (batch_size, T.int64(1), T.int64(6144)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1), T.int64(48), T.int64(128)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(batch_size, T.int64(1), T.int64(48), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv[((v_ax2 * T.int64(128) + v_ax3) // T.int64(6144) + v_ax0 + v_ax1) % batch_size, T.int64(0), (v_ax2 * T.int64(128) + v_ax3) % T.int64(6144)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv[((v_ax2 * T.int64(128) + v_ax3) // T.int64(6144) + v_ax0 + v_ax1) % batch_size, T.int64(0), (v_ax2 * T.int64(128) + v_ax3) % T.int64(6144)]

    @T.prim_func(private=True)
    def reshape1(var_reshape640: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape640 = T.match_buffer(var_reshape640, (batch_size, T.int64(1), T.int64(48), T.int64(128)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(48), T.int64(128)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(48), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape640[((v_ax2 // T.int64(128) + v_ax1) // T.int64(48) + v_ax0) % batch_size, T.int64(0), (v_ax2 // T.int64(128) + v_ax1) % T.int64(48), v_ax2 % T.int64(128)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape640[((v_ax2 // T.int64(128) + v_ax1) // T.int64(48) + v_ax0) % batch_size, T.int64(0), (v_ax2 // T.int64(128) + v_ax1) % T.int64(48), v_ax2 % T.int64(128)]

    @T.prim_func(private=True)
    def reshape2(var_lv807: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv807 = T.match_buffer(var_lv807, (batch_size, T.int64(32), T.int64(128)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1), T.int64(32), T.int64(128)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(batch_size, T.int64(1), T.int64(32), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv807[((v_ax3 // T.int64(128) + v_ax2) // T.int64(32) + v_ax0 + v_ax1) % batch_size, (v_ax3 // T.int64(128) + v_ax2) % T.int64(32), v_ax3 % T.int64(128)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv807[((v_ax3 // T.int64(128) + v_ax2) // T.int64(32) + v_ax0 + v_ax1) % batch_size, (v_ax3 // T.int64(128) + v_ax2) % T.int64(32), v_ax3 % T.int64(128)]

    @T.prim_func(private=True)
    def reshape3(var_reshape642: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape642 = T.match_buffer(var_reshape642, (batch_size, T.int64(1), T.int64(32), T.int64(128)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1), T.int64(4096)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(4096)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape642[(v_ax2 // T.int64(4096) + v_ax0 + v_ax1) % batch_size, T.int64(0), v_ax2 % T.int64(4096) // T.int64(128), v_ax2 % T.int64(128)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape642[(v_ax2 // T.int64(4096) + v_ax0 + v_ax1) % batch_size, T.int64(0), v_ax2 % T.int64(4096) // T.int64(128), v_ax2 % T.int64(128)]

    @T.prim_func(private=True)
    def reshape4(var_lv257: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        lv257 = T.match_buffer(var_lv257, (T.int64(1), seq_len, T.int64(6144)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), seq_len, T.int64(48), T.int64(128)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), seq_len, T.int64(48), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv257[T.int64(0), ((v_ax2 * T.int64(128) + v_ax3) // T.int64(6144) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax2 * T.int64(128) + v_ax3) % T.int64(6144)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv257[T.int64(0), ((v_ax2 * T.int64(128) + v_ax3) // T.int64(6144) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax2 * T.int64(128) + v_ax3) % T.int64(6144)]

    @T.prim_func(private=True)
    def reshape5(var_reshape512: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        reshape512 = T.match_buffer(var_reshape512, (T.int64(1), seq_len, T.int64(48), T.int64(128)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (seq_len, T.int64(48), T.int64(128)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(seq_len, T.int64(48), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape512[T.int64(0), ((v_ax2 // T.int64(128) + v_ax1) // T.int64(48) + v_ax0) % seq_len, (v_ax2 // T.int64(128) + v_ax1) % T.int64(48), v_ax2 % T.int64(128)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape512[T.int64(0), ((v_ax2 // T.int64(128) + v_ax1) // T.int64(48) + v_ax0) % seq_len, (v_ax2 // T.int64(128) + v_ax1) % T.int64(48), v_ax2 % T.int64(128)]

    @T.prim_func(private=True)
    def reshape6(var_lv646: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        lv646 = T.match_buffer(var_lv646, (seq_len, T.int64(32), T.int64(128)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), seq_len, T.int64(32), T.int64(128)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), seq_len, T.int64(32), T.int64(128)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(lv646[((v_ax3 // T.int64(128) + v_ax2) // T.int64(32) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax3 // T.int64(128) + v_ax2) % T.int64(32), v_ax3 % T.int64(128)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2, v_ax3])
                T_reshape[v_ax0, v_ax1, v_ax2, v_ax3] = lv646[((v_ax3 // T.int64(128) + v_ax2) // T.int64(32) + v_ax0 * seq_len + v_ax1) % seq_len, (v_ax3 // T.int64(128) + v_ax2) % T.int64(32), v_ax3 % T.int64(128)]

    @T.prim_func(private=True)
    def reshape7(var_reshape514: T.handle, var_T_reshape: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        reshape514 = T.match_buffer(var_reshape514, (T.int64(1), seq_len, T.int64(32), T.int64(128)), "float16")
        T_reshape = T.match_buffer(var_T_reshape, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(4096)):
            with T.block("T_reshape"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(reshape514[T.int64(0), (v_ax2 // T.int64(4096) + v_ax0 * seq_len + v_ax1) % seq_len, v_ax2 % T.int64(4096) // T.int64(128), v_ax2 % T.int64(128)])
                T.writes(T_reshape[v_ax0, v_ax1, v_ax2])
                T_reshape[v_ax0, v_ax1, v_ax2] = reshape514[T.int64(0), (v_ax2 // T.int64(4096) + v_ax0 * seq_len + v_ax1) % seq_len, v_ax2 % T.int64(4096) // T.int64(128), v_ax2 % T.int64(128)]

    @T.prim_func(private=True)
    def rms_norm(var_input_embeds: T.handle, model_layers_0_input_layernorm_weight7: T.Buffer((T.int64(4096),), "float16"), var_T_cast: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        batch_size = T.int64()
        input_embeds = T.match_buffer(var_input_embeds, (batch_size, T.int64(1), T.int64(4096)), "float16")
        T_cast = T.match_buffer(var_T_cast, (batch_size, T.int64(1), T.int64(4096)), "float16")
        # with T.block("root"):
        T_cast_1 = T.alloc_buffer((batch_size, T.int64(1), T.int64(4096)))
        T_multiply = T.alloc_buffer((batch_size, T.int64(1), T.int64(4096)))
        T_multiply_red = T.alloc_buffer((batch_size, T.int64(1)))
        rsqrt = T.alloc_buffer((batch_size, T.int64(1)))
        T_cast_2 = T.alloc_buffer((T.int64(4096),))
        T_rms_norm = T.alloc_buffer((batch_size, T.int64(1), T.int64(4096)))
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(4096)):
            with T.block("T_cast"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input_embeds[v_ax0, v_ax1, v_ax2])
                T.writes(T_cast_1[v_ax0, v_ax1, v_ax2])
                T_cast_1[v_ax0, v_ax1, v_ax2] = T.Cast("float32", input_embeds[v_ax0, v_ax1, v_ax2])
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(4096)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_cast_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = T_cast_1[v_ax0, v_ax1, v_ax2] * T_cast_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, k2 in T.grid(batch_size, T.int64(1), T.int64(4096)):
            with T.block("T_multiply_red"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(T_multiply[v_ax0, v_ax1, v_k2])
                T.writes(T_multiply_red[v_ax0, v_ax1])
                with T.init():
                    T_multiply_red[v_ax0, v_ax1] = T.float32(0.0)
                T_multiply_red[v_ax0, v_ax1] = T_multiply_red[v_ax0, v_ax1] + T_multiply[v_ax0, v_ax1, v_k2]
        for ax0, ax1 in T.grid(batch_size, T.int64(1)):
            with T.block("rsqrt"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_multiply_red[v_ax0, v_ax1])
                T.writes(rsqrt[v_ax0, v_ax1])
                rsqrt[v_ax0, v_ax1] = T.rsqrt(T_multiply_red[v_ax0, v_ax1] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05))
        for ax0 in range(T.int64(4096)):
            with T.block("T_cast_1"):
                v_ax0 = T.axis.spatial(T.int64(4096), ax0)
                T.reads(model_layers_0_input_layernorm_weight7[v_ax0])
                T.writes(T_cast_2[v_ax0])
                T_cast_2[v_ax0] = T.Cast("float32", model_layers_0_input_layernorm_weight7[v_ax0])
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(4096)):
            with T.block("T_rms_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(rsqrt[v_ax0, v_ax1], T_cast_1[v_ax0, v_ax1, v_ax2], T_cast_2[v_ax2])
                T.writes(T_rms_norm[v_ax0, v_ax1, v_ax2])
                T_rms_norm[v_ax0, v_ax1, v_ax2] = rsqrt[v_ax0, v_ax1] * T_cast_1[v_ax0, v_ax1, v_ax2] * T_cast_2[v_ax2]
        for ax0, ax1, ax2 in T.grid(batch_size, T.int64(1), T.int64(4096)):
            with T.block("T_cast_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_rms_norm[v_ax0, v_ax1, v_ax2])
                T.writes(T_cast[v_ax0, v_ax1, v_ax2])
                T_cast[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T_rms_norm[v_ax0, v_ax1, v_ax2])

    @T.prim_func(private=True)
    def rms_norm1(var_input_embeds: T.handle, model_layers_0_input_layernorm_weight6: T.Buffer((T.int64(4096),), "float16"), var_T_cast: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        input_embeds = T.match_buffer(var_input_embeds, (T.int64(1), seq_len, T.int64(4096)), "float16")
        T_cast = T.match_buffer(var_T_cast, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        T_cast_1 = T.alloc_buffer((T.int64(1), seq_len, T.int64(4096)))
        T_multiply = T.alloc_buffer((T.int64(1), seq_len, T.int64(4096)))
        T_multiply_red = T.alloc_buffer((T.int64(1), seq_len))
        rsqrt = T.alloc_buffer((T.int64(1), seq_len))
        T_cast_2 = T.alloc_buffer((T.int64(4096),))
        T_rms_norm = T.alloc_buffer((T.int64(1), seq_len, T.int64(4096)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(4096)):
            with T.block("T_cast"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input_embeds[v_ax0, v_ax1, v_ax2])
                T.writes(T_cast_1[v_ax0, v_ax1, v_ax2])
                T_cast_1[v_ax0, v_ax1, v_ax2] = T.Cast("float32", input_embeds[v_ax0, v_ax1, v_ax2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(4096)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_cast_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = T_cast_1[v_ax0, v_ax1, v_ax2] * T_cast_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, k2 in T.grid(T.int64(1), seq_len, T.int64(4096)):
            with T.block("T_multiply_red"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(T_multiply[v_ax0, v_ax1, v_k2])
                T.writes(T_multiply_red[v_ax0, v_ax1])
                with T.init():
                    T_multiply_red[v_ax0, v_ax1] = T.float32(0.0)
                T_multiply_red[v_ax0, v_ax1] = T_multiply_red[v_ax0, v_ax1] + T_multiply[v_ax0, v_ax1, v_k2]
        for ax0, ax1 in T.grid(T.int64(1), seq_len):
            with T.block("rsqrt"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_multiply_red[v_ax0, v_ax1])
                T.writes(rsqrt[v_ax0, v_ax1])
                rsqrt[v_ax0, v_ax1] = T.rsqrt(T_multiply_red[v_ax0, v_ax1] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05))
        for ax0 in range(T.int64(4096)):
            with T.block("T_cast_1"):
                v_ax0 = T.axis.spatial(T.int64(4096), ax0)
                T.reads(model_layers_0_input_layernorm_weight6[v_ax0])
                T.writes(T_cast_2[v_ax0])
                T_cast_2[v_ax0] = T.Cast("float32", model_layers_0_input_layernorm_weight6[v_ax0])
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(4096)):
            with T.block("T_rms_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(rsqrt[v_ax0, v_ax1], T_cast_1[v_ax0, v_ax1, v_ax2], T_cast_2[v_ax2])
                T.writes(T_rms_norm[v_ax0, v_ax1, v_ax2])
                T_rms_norm[v_ax0, v_ax1, v_ax2] = rsqrt[v_ax0, v_ax1] * T_cast_1[v_ax0, v_ax1, v_ax2] * T_cast_2[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), seq_len, T.int64(4096)):
            with T.block("T_cast_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_rms_norm[v_ax0, v_ax1, v_ax2])
                T.writes(T_cast[v_ax0, v_ax1, v_ax2])
                T_cast[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T_rms_norm[v_ax0, v_ax1, v_ax2])

    @T.prim_func(private=True)
    def rms_norm2(input_embed: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16"), model_layers_0_input_layernorm_weight3: T.Buffer((T.int64(4096),), "float16"), T_cast: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"tir.noalias": T.bool(True)})
        # with T.block("root"):
        T_cast_1 = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)))
        T_multiply_red = T.alloc_buffer((T.int64(1), T.int64(1)))
        rsqrt = T.alloc_buffer((T.int64(1), T.int64(1)))
        T_cast_2 = T.alloc_buffer((T.int64(4096),))
        T_rms_norm = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(4096)))
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("T_cast"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(input_embed[v_ax0, v_ax1, v_ax2])
                T.writes(T_cast_1[v_ax0, v_ax1, v_ax2])
                T_cast_1[v_ax0, v_ax1, v_ax2] = T.Cast("float32", input_embed[v_ax0, v_ax1, v_ax2])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_cast_1[v_ax0, v_ax1, v_ax2])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2])
                T_multiply[v_ax0, v_ax1, v_ax2] = T_cast_1[v_ax0, v_ax1, v_ax2] * T_cast_1[v_ax0, v_ax1, v_ax2]
        for ax0, ax1, k2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("T_multiply_red"):
                v_ax0, v_ax1, v_k2 = T.axis.remap("SSR", [ax0, ax1, k2])
                T.reads(T_multiply[v_ax0, v_ax1, v_k2])
                T.writes(T_multiply_red[v_ax0, v_ax1])
                with T.init():
                    T_multiply_red[v_ax0, v_ax1] = T.float32(0.0)
                T_multiply_red[v_ax0, v_ax1] = T_multiply_red[v_ax0, v_ax1] + T_multiply[v_ax0, v_ax1, v_k2]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
            with T.block("rsqrt"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(T_multiply_red[v_ax0, v_ax1])
                T.writes(rsqrt[v_ax0, v_ax1])
                rsqrt[v_ax0, v_ax1] = T.rsqrt(T_multiply_red[v_ax0, v_ax1] * T.float32(0.000244140625) + T.float32(1.0000000000000001e-05))
        for ax0 in range(T.int64(4096)):
            with T.block("T_cast_1"):
                v_ax0 = T.axis.spatial(T.int64(4096), ax0)
                T.reads(model_layers_0_input_layernorm_weight3[v_ax0])
                T.writes(T_cast_2[v_ax0])
                T_cast_2[v_ax0] = T.Cast("float32", model_layers_0_input_layernorm_weight3[v_ax0])
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("T_rms_norm"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(rsqrt[v_ax0, v_ax1], T_cast_1[v_ax0, v_ax1, v_ax2], T_cast_2[v_ax2])
                T.writes(T_rms_norm[v_ax0, v_ax1, v_ax2])
                T_rms_norm[v_ax0, v_ax1, v_ax2] = rsqrt[v_ax0, v_ax1] * T_cast_1[v_ax0, v_ax1, v_ax2] * T_cast_2[v_ax2]
        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("T_cast_2"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(T_rms_norm[v_ax0, v_ax1, v_ax2])
                T.writes(T_cast[v_ax0, v_ax1, v_ax2])
                T_cast[v_ax0, v_ax1, v_ax2] = T.Cast("float16", T_rms_norm[v_ax0, v_ax1, v_ax2])

    @T.prim_func
    def sampler_take_probs_tir(var_unsorted_probs: T.handle, var_sorted_indices: T.handle, var_sample_indices: T.handle, var_sampling_results: T.handle, var_top_prob_offsets: T.handle, var_sampled_values: T.handle, var_top_prob_probs: T.handle, var_top_prob_indices: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        unsorted_probs = T.match_buffer(var_unsorted_probs, (batch_size, vocab_size))
        sorted_indices = T.match_buffer(var_sorted_indices, (batch_size, vocab_size), "int32")
        num_samples = T.int32(is_size_var=True)
        sample_indices = T.match_buffer(var_sample_indices, (num_samples,), "int32")
        sampling_results = T.match_buffer(var_sampling_results, (num_samples,), "int32")
        num_positions = T.int32(is_size_var=True)
        top_prob_offsets = T.match_buffer(var_top_prob_offsets, (num_positions,), "int32")
        sampled_values = T.match_buffer(var_sampled_values, (num_samples,))
        top_prob_probs = T.match_buffer(var_top_prob_probs, (num_positions,))
        top_prob_indices = T.match_buffer(var_top_prob_indices, (num_positions,), "int32")
        # with T.block("root"):
        for i in range(num_positions + num_samples):
            with T.block("block"):
                vi = T.axis.spatial(num_positions + num_samples, i)
                T.reads(top_prob_offsets[vi], sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], unsorted_probs[T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]):T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + (T.max(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + 1 - T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions])), T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]):T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + (T.max(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + 1 - T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]))], sample_indices[vi - num_positions], sampling_results[vi - num_positions])
                T.writes(top_prob_indices[vi], top_prob_probs[vi], sampled_values[vi - num_positions])
                if vi < num_positions:
                    row: T.int32 = top_prob_offsets[vi] // vocab_size
                    col: T.int32 = top_prob_offsets[vi] % vocab_size
                    top_prob_indices[vi] = sorted_indices[row, col]
                    top_prob_probs[vi] = unsorted_probs[row, sorted_indices[row, col]]
                else:
                    vj: T.int32 = vi - num_positions
                    sampled_values[vj] = unsorted_probs[sample_indices[vj], sampling_results[vj]]

    @T.prim_func
    def scatter_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (batch_size, n))
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        m = T.int32(is_size_var=True)
        dst = T.match_buffer(var_dst, (m, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("scatter_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[vb, vj], indices[vb])
                T.writes(dst[indices[vb], vj])
                dst[indices[vb], vj] = src[vb, vj]

    @T.prim_func
    def softmax_with_chunked_sum(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle, var_softmax: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        softmax = T.match_buffer(var_softmax, (batch_size, vocab_size))
        # with T.block("root"):
        temp_max_shared = T.alloc_buffer((batch_size,), scope="shared")
        temp_sum_shared = T.alloc_buffer((batch_size,), scope="shared")
        for l0_l1_fused in T.thread_binding(batch_size * num_chunks, thread="blockIdx.x"):
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("max"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(chunked_max[v0, v1])
                        T.writes(temp_max_shared[v0])
                        with T.init():
                            temp_max_shared[v0] = T.float32(-340282346638528859811704183484516925440.0)
                        temp_max_shared[v0] = T.max(temp_max_shared[v0], chunked_max[v0, v1])
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("sum_exp"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(temperature[v0], chunked_sum[v0, v1], chunked_max[v0, v1], temp_max_shared[v0])
                        T.writes(temp_sum_shared[v0])
                        with T.init():
                            temp_sum_shared[v0] = T.float32(0.0)
                        temp_sum_shared[v0] = temp_sum_shared[v0] + T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(chunked_sum[v0, v1] + chunked_max[v0, v1] - temp_max_shared[v0]), T.Cast("float32", chunked_max[v0, v1] == temp_max_shared[v0]) * chunked_sum[v0, v1])
            for l2_0 in T.serial(T.int64(4), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                for l2_1 in T.thread_binding(T.int64(32), thread="threadIdx.y"):
                    for l2_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        with T.block("log_pad"):
                            v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                            v1 = T.axis.spatial(num_chunks, l0_l1_fused % num_chunks)
                            v2 = T.axis.spatial(T.int64(4096), l2_0 * T.int64(1024) + l2_1 * T.int64(32) + l2_2)
                            T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2], temp_sum_shared[v0], temp_max_shared[v0])
                            T.writes(softmax[v0, v1 * T.int64(4096) + v2])
                            if v1 * T.int64(4096) + v2 < vocab_size:
                                softmax[v0, v1 * T.int64(4096) + v2] = T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A[v0, v1 * T.int64(4096) + v2] / temperature[v0] - (T.log(temp_sum_shared[v0]) + temp_max_shared[v0])), T.Cast("float32", A[v0, v1 * T.int64(4096) + v2] == temp_max_shared[v0]) / temp_sum_shared[v0])

    @T.prim_func(private=True)
    def take(var_rms_norm324: T.handle, var_logit_positions: T.handle, var_T_take: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm324 = T.match_buffer(var_rms_norm324, (T.int64(1), seq_len, T.int64(4096)), "float16")
        batch_size = T.int64()
        logit_positions = T.match_buffer(var_logit_positions, (batch_size,), "int32")
        T_take = T.match_buffer(var_T_take, (T.int64(1), batch_size, T.int64(4096)), "float16")
        # with T.block("root"):
        for ax0, ax1, ax2 in T.grid(T.int64(1), batch_size, T.int64(4096)):
            with T.block("T_take"):
                v_ax0, v_ax1, v_ax2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                T.reads(rms_norm324[v_ax0, logit_positions[v_ax1], v_ax2], logit_positions[v_ax1])
                T.writes(T_take[v_ax0, v_ax1, v_ax2])
                T_take[v_ax0, v_ax1, v_ax2] = rms_norm324[v_ax0, logit_positions[v_ax1], v_ax2]

    @T.prim_func(private=True)
    def take1(var_hidden_states: T.handle, var_logit_positions: T.handle, var_T_take: T.handle):
        T.func_attr({"tir.noalias": T.bool(True)})
        seq_len = T.int64()
        hidden_states = T.match_buffer(var_hidden_states, (seq_len, T.int64(4096)), "float16")
        batch_size = T.int64()
        logit_positions = T.match_buffer(var_logit_positions, (batch_size,), "int32")
        T_take = T.match_buffer(var_T_take, (batch_size, T.int64(4096)), "float16")
        # with T.block("root"):
        for ax0, ax1 in T.grid(batch_size, T.int64(4096)):
            with T.block("T_take"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(hidden_states[logit_positions[v_ax0], v_ax1], logit_positions[v_ax0])
                T.writes(T_take[v_ax0, v_ax1])
                T_take[v_ax0, v_ax1] = hidden_states[logit_positions[v_ax0], v_ax1]

    @T.prim_func(private=True)
    def take_sorted_probs(var_probs: T.handle, var_lv1: T.handle, var_take_sorted_probs: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(), T.int64()
        probs = T.match_buffer(var_probs, (batch_size, vocab_size))
        lv1 = T.match_buffer(var_lv1, (batch_size, vocab_size), "int32")
        batch_size_1, vocab_size_1 = T.int64(is_size_var=True), T.int64(is_size_var=True)
        take_sorted_probs = T.match_buffer(var_take_sorted_probs, (batch_size_1, vocab_size_1))
        # with T.block("root"):
        for i, j in T.grid(batch_size_1, vocab_size_1):
            with T.block("take_sorted_probs"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads(probs[v_i, lv1[v_i, v_j]], lv1[v_i, v_j])
                T.writes(take_sorted_probs[v_i, v_j])
                take_sorted_probs[v_i, v_j] = probs[v_i, lv1[v_i, v_j]]

    @T.prim_func
    def tir_kv_cache_debug_get_kv(var_pages: T.handle, var_position_map: T.handle, var_k_data: T.handle, var_v_data: T.handle, layer_id: T.int64):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        num_pages, page_size = T.int64(), T.int64(is_size_var=True)
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, page_size, 128), "float16")
        seqlen = T.int64(is_size_var=True)
        position_map = T.match_buffer(var_position_map, (seqlen,), "int32", offset_factor=1)
        k_data = T.match_buffer(var_k_data, (32, seqlen, 8, 128), "float16")
        v_data = T.match_buffer(var_v_data, (32, seqlen, 8, 128), "float16")
        # with T.block("root"):
        for p, h, d in T.grid(seqlen, 8, 128):
            with T.block("copy0"):
                vp, vh, vd = T.axis.remap("SSS", [p, h, d])
                T.reads(position_map[vp], pages[T.Cast("int64", position_map[vp]) // page_size, 0:2, vh, T.Cast("int64", position_map[vp]) % page_size, vd])
                T.writes(k_data[layer_id, vp, vh, vd], v_data[layer_id, vp, vh, vd])
                position: T.int32 = position_map[vp]
                k_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 0, vh, T.Cast("int64", position) % page_size, vd]
                v_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 1, vh, T.Cast("int64", position) % page_size, vd]

    @T.prim_func
    def tir_kv_cache_transpose_append(var_pages: T.handle, var_k_data: T.handle, var_v_data: T.handle, var_position_map: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        num_pages = T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, 16, 128), "float16")
        ntoken = T.int64(is_size_var=True)
        k_data = T.match_buffer(var_k_data, (ntoken, 8, 128), "float16")
        v_data = T.match_buffer(var_v_data, (ntoken, 8, 128), "float16")
        position_map = T.match_buffer(var_position_map, (ntoken,), "int32", offset_factor=1)
        # with T.block("root"):
        for global_pos, h, f in T.grid(ntoken, 8, 128):
            if position_map[global_pos] != -1:
                with T.block("k_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], k_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 0, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 0, vh, position % 16, vf] = k_data[vgpos, vh, vf]
                with T.block("v_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], v_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 1, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 1, vh, position % 16, vf] = v_data[vgpos, vh, vf]

    @T.prim_func(private=True)
    def top_p_pivot_cutoff(var_prob: T.handle, var_top_p_arr: T.handle, var_init_pivots: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        top_p_arr = T.match_buffer(var_top_p_arr, (B,))
        init_pivots = T.match_buffer(var_init_pivots, (B, 3))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        # with T.block("root"):
        pivot = T.alloc_buffer((3,), scope="local")
        top_p = T.alloc_buffer((1,), scope="local")
        L = T.alloc_buffer((1,), scope="shared")
        R_1 = T.alloc_buffer((1,), scope="shared")
        L_local = T.alloc_buffer((1,), scope="local")
        R_local = T.alloc_buffer((1,), scope="local")
        q = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((3,), scope="local")
        lmin_broadcast = T.alloc_buffer((1,), scope="shared")
        lmin_broadcast_local = T.alloc_buffer((1,), scope="local")
        lmin = T.alloc_buffer((3,), scope="local")
        cmin = T.alloc_buffer((3,), "int32", scope="local")
        total_sum = T.alloc_buffer((1,), scope="local")
        it = T.alloc_buffer((1,), "int32", scope="local")
        es_local = T.alloc_buffer((1,), "bool", scope="local")
        es = T.alloc_buffer((1,), "bool", scope="shared")
        find_pivot_local = T.alloc_buffer((1,), "bool", scope="local")
        find_pivot = T.alloc_buffer((1,), "bool", scope="shared")
        total_sum_reduce = T.alloc_buffer((1,), scope="local")
        lsum_reduce = T.alloc_buffer((1,), scope="local")
        lmin_reduce = T.alloc_buffer((1,), scope="local")
        cmin_reduce = T.alloc_buffer((1,), "int32", scope="local")
        for _bx in T.thread_binding(B, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(top_p_arr[b], top_p[0], L[0], R_1[0], init_pivots[b, 0:3], L_local[0], R_local[0], find_pivot_local[0], it[0], es_local[0], prob[b, it[0] * 1024 + tx], total_sum[0], q[0], pivot[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lsum[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], cmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], total_sum_reduce[0], es[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], lsum_reduce[0], cmin_reduce[0], find_pivot[0])
                    T.writes(top_p[0], L[0], R_1[0], find_pivot[0], L_local[0], R_local[0], pivot[0:3], find_pivot_local[0], final_lsum[b], final_pivot[b], lsum[0:3], lmin[0:3], cmin[0:3], total_sum[0], it[0], es_local[0], q[0], total_sum_reduce[0], es[0], lsum_reduce[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], cmin_reduce[0])
                    top_p[0] = top_p_arr[b]
                    if tx == 0:
                        L[0] = T.float32(1.0) - top_p[0]
                        R_1[0] = T.float32(9.9999999999999995e-08)
                        find_pivot[0] = T.bool(False)
                    T.tvm_storage_sync("shared")
                    L_local[0] = L[0]
                    R_local[0] = R_1[0]
                    for i in T.unroll(3):
                        pivot[i] = init_pivots[b, i]
                    find_pivot_local[0] = T.bool(False)
                    if L_local[0] - R_local[0] <= T.float32(9.9999999999999995e-08):
                        if tx == 0:
                            final_lsum[b] = T.float32(1.0)
                            final_pivot[b] = T.float32(0.0)
                        find_pivot_local[0] = T.bool(True)
                    while T.tvm_thread_invariant(L_local[0] - R_local[0] > T.float32(9.9999999999999995e-08) and not find_pivot_local[0]):
                        T.tvm_storage_sync("shared")
                        for pidx in T.unroll(3):
                            lsum[pidx] = T.float32(0.0)
                            lmin[pidx] = T.float32(340282346638528859811704183484516925440.0)
                            cmin[pidx] = 0
                        total_sum[0] = T.float32(0.0)
                        it[0] = 0
                        es_local[0] = T.bool(False)
                        while it[0] < (N + 1024 - 1) // 1024 and not es_local[0]:
                            q[0] = T.if_then_else(it[0] * 1024 + tx < N, prob[b, it[0] * 1024 + tx], T.float32(0.0))
                            total_sum[0] = total_sum[0] + q[0]
                            for pidx in T.unroll(3):
                                if q[0] >= pivot[pidx]:
                                    lsum[pidx] = lsum[pidx] + q[0]
                                    if lmin[pidx] > q[0]:
                                        lmin[pidx] = q[0]
                                        cmin[pidx] = 1
                                    else:
                                        if lmin[pidx] == q[0]:
                                            cmin[pidx] = cmin[pidx] + 1
                            it[0] = it[0] + 1
                            if it[0] % 32 == 0:
                                with T.block("block_cross_thread"):
                                    T.reads(total_sum[0])
                                    T.writes(total_sum_reduce[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), total_sum[0], T.bool(True), total_sum_reduce[0], tx)
                                if tx == 0:
                                    es[0] = T.float32(1.0) - total_sum_reduce[0] < pivot[2]
                                T.tvm_storage_sync("shared")
                                es_local[0] = es[0]
                        T.tvm_storage_sync("shared")
                        for pidx in range(3):
                            with T.block("block_cross_thread"):
                                T.reads(lsum[pidx])
                                T.writes(lsum_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lsum[pidx], T.bool(True), lsum_reduce[0], tx)
                            with T.block("block_cross_thread"):
                                T.reads(lmin[pidx])
                                T.writes(lmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: T.min(x0, y0), [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lmin[pidx], T.bool(True), lmin_reduce[0], tx)
                            if tx == 0:
                                lmin_broadcast[0] = lmin_reduce[0]
                            T.tvm_storage_sync("shared")
                            lmin_broadcast_local[0] = lmin_broadcast[0]
                            if lmin[pidx] > lmin_broadcast_local[0]:
                                cmin[pidx] = 0
                            if tx == 0:
                                lsum[pidx] = lsum_reduce[0]
                                lmin[pidx] = lmin_reduce[0]
                            with T.block("block_cross_thread"):
                                T.reads(cmin[pidx])
                                T.writes(cmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [0]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), cmin[pidx], T.bool(True), cmin_reduce[0], tx)
                            if tx == 0:
                                cmin[pidx] = cmin_reduce[0]
                        T.tvm_storage_sync("shared")
                        if tx == 0:
                            it[0] = 0
                            while it[0] < 3 and not find_pivot_local[0]:
                                if lsum[it[0]] >= top_p[0] and top_p[0] > lsum[it[0]] - T.Cast("float32", cmin[it[0]]) * lmin[it[0]]:
                                    find_pivot[0] = T.bool(True)
                                    find_pivot_local[0] = T.bool(True)
                                    final_pivot[b] = pivot[it[0]]
                                    final_lsum[b] = lsum[it[0]]
                                else:
                                    if lsum[it[0]] - lmin[it[0]] * T.Cast("float32", cmin[it[0]]) >= top_p[0]:
                                        R_1[0] = pivot[it[0]]
                                        final_lsum[b] = lsum[it[0]]
                                    else:
                                        if lsum[it[0]] < top_p[0]:
                                            L[0] = pivot[it[0]]
                                it[0] = it[0] + 1
                        T.tvm_storage_sync("shared")
                        L_local[0] = L[0]
                        R_local[0] = R_1[0]
                        find_pivot_local[0] = find_pivot[0]
                        for pidx in T.unroll(3):
                            pivot[pidx] = L[0] - T.Cast("float32", pidx + 1) * (L_local[0] - R_local[0]) / T.float32(4.0)
                    if tx == 0:
                        if not find_pivot_local[0]:
                            final_pivot[b] = R_local[0]
                            if R_local[0] == T.float32(9.9999999999999995e-08):
                                final_lsum[b] = lsum[2]

    @T.prim_func(private=True)
    def top_p_renorm_after_cutoff(var_prob: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle, var_renorm_prob: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        renorm_prob = T.match_buffer(var_renorm_prob, (B, N))
        # with T.block("root"):
        pivot = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((1,), scope="local")
        for _by in T.thread_binding(B, thread="blockIdx.y"):
            for _bx in T.thread_binding(511 // B + 1, thread="blockIdx.x"):
                for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("CTA"):
                        by, bx, tx = T.axis.remap("SSS", [_by, _bx, _tx])
                        T.reads(final_pivot[by], final_lsum[by], prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)], pivot[0], lsum[0])
                        T.writes(pivot[0], lsum[0], renorm_prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)])
                        pivot[0] = final_pivot[by]
                        lsum[0] = final_lsum[by]
                        for i in range((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024)):
                            if i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx < N:
                                renorm_prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] = T.if_then_else(prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] >= pivot[0], prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] / lsum[0], T.float32(0.0))

    @T.prim_func
    def tree_attn_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, tree_order_indptr_handle: T.handle, tree_order_handle: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        tree_order_indptr = T.match_buffer(tree_order_indptr_handle, (batch_size + 1,), "int32", offset_factor=1)
        total_tree_order_len = T.int32(is_size_var=True)
        tree_order = T.match_buffer(tree_order_handle, (total_tree_order_len, 2), "int32", offset_factor=1)
        # with T.block("root"):
        assert rotary_mode == 0, "Inline rotary mode is not supported in tree attention."
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                K_smem[i, j] = pages[page_no, 0, by, page_offset, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @R.function
    def alloc_embedding_tensor() -> R.Tensor((8192, 4096), dtype="float16"):
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        gv: R.Tensor((8192, 4096), dtype="float16") = R.builtin.alloc_tensor(R.shape([8192, 4096]), R.dtype("float16"), R.prim_value(0), R.str("global"))
        return gv

    @R.function
    def argsort_probs(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32")) -> R.Tuple(R.Tensor(("batch_size", "vocab_size"), dtype="float32"), R.Tensor(("batch_size", "vocab_size"), dtype="int32")):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv1 = R.call_tir(cls.argsort, (probs,), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="int32"))
            lv2 = R.call_tir(cls.take_sorted_probs, (probs, lv1), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv1: R.Tuple(R.Tensor((batch_size, vocab_size), dtype="float32"), R.Tensor((batch_size, vocab_size), dtype="int32")) = lv2, lv1
            R.output(gv1)
        return gv1

    @R.function
    def batch_decode(input_embeds: R.Tensor(("batch_size", 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight7: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale7: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm325 = R.call_tir(cls.rms_norm, (input_embeds, model_layers_0_input_layernorm_weight7), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_0_self_attn_qkv_proj_q_weight7, model_layers_0_self_attn_qkv_proj_q_scale7, rms_norm325), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape640 = R.call_tir(cls.reshape, (lv,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape641 = R.call_tir(cls.reshape1, (reshape640,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv807 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape641), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape642 = R.call_tir(cls.reshape2, (lv807,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape643 = R.call_tir(cls.reshape3, (reshape642,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv1 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_0_self_attn_o_proj_q_weight7, model_layers_0_self_attn_o_proj_q_scale7, reshape643), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv_1 = R.call_tir(cls.fuse_add_norm_decode, (lv1, input_embeds, model_layers_0_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv1_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv_1[1]
            rms_norm326: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv_1[0]
            lv2 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_0_mlp_gate_up_proj_q_weight7, model_layers_0_mlp_gate_up_proj_q_scale7, rms_norm326), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv_2 = R.call_tir(cls.fused_split_silu_multiply, (lv2,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv3 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_0_mlp_down_proj_q_weight7, model_layers_0_mlp_down_proj_q_scale7, lv_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv2_1 = R.call_tir(cls.fuse_add_norm_decode, (lv3, lv1_1, model_layers_1_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv3_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv2_1[1]
            rms_norm327: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv2_1[0]
            lv4 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_1_self_attn_qkv_proj_q_weight7, model_layers_1_self_attn_qkv_proj_q_scale7, rms_norm327), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape644 = R.call_tir(cls.reshape, (lv4,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape645 = R.call_tir(cls.reshape1, (reshape644,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv812 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape645), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape646 = R.call_tir(cls.reshape2, (lv812,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape647 = R.call_tir(cls.reshape3, (reshape646,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv5 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_1_self_attn_o_proj_q_weight7, model_layers_1_self_attn_o_proj_q_scale7, reshape647), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv4_1 = R.call_tir(cls.fuse_add_norm_decode, (lv5, lv3_1, model_layers_1_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv5_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv4_1[1]
            rms_norm328: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv4_1[0]
            lv6 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_1_mlp_gate_up_proj_q_weight7, model_layers_1_mlp_gate_up_proj_q_scale7, rms_norm328), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv1_2 = R.call_tir(cls.fused_split_silu_multiply, (lv6,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv7 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_1_mlp_down_proj_q_weight7, model_layers_1_mlp_down_proj_q_scale7, lv1_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv6_1 = R.call_tir(cls.fuse_add_norm_decode, (lv7, lv5_1, model_layers_2_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv7_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv6_1[1]
            rms_norm329: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv6_1[0]
            lv8 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_2_self_attn_qkv_proj_q_weight7, model_layers_2_self_attn_qkv_proj_q_scale7, rms_norm329), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape648 = R.call_tir(cls.reshape, (lv8,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape649 = R.call_tir(cls.reshape1, (reshape648,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv817 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape649), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape650 = R.call_tir(cls.reshape2, (lv817,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape651 = R.call_tir(cls.reshape3, (reshape650,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv9 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_2_self_attn_o_proj_q_weight7, model_layers_2_self_attn_o_proj_q_scale7, reshape651), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv8_1 = R.call_tir(cls.fuse_add_norm_decode, (lv9, lv7_1, model_layers_2_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv9_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv8_1[1]
            rms_norm330: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv8_1[0]
            lv10 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_2_mlp_gate_up_proj_q_weight7, model_layers_2_mlp_gate_up_proj_q_scale7, rms_norm330), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv2_2 = R.call_tir(cls.fused_split_silu_multiply, (lv10,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv11 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_2_mlp_down_proj_q_weight7, model_layers_2_mlp_down_proj_q_scale7, lv2_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv10_1 = R.call_tir(cls.fuse_add_norm_decode, (lv11, lv9_1, model_layers_3_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv11_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv10_1[1]
            rms_norm331: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv10_1[0]
            lv12 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_3_self_attn_qkv_proj_q_weight7, model_layers_3_self_attn_qkv_proj_q_scale7, rms_norm331), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape652 = R.call_tir(cls.reshape, (lv12,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape653 = R.call_tir(cls.reshape1, (reshape652,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv822 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape653), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape654 = R.call_tir(cls.reshape2, (lv822,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape655 = R.call_tir(cls.reshape3, (reshape654,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv13 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_3_self_attn_o_proj_q_weight7, model_layers_3_self_attn_o_proj_q_scale7, reshape655), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv12_1 = R.call_tir(cls.fuse_add_norm_decode, (lv13, lv11_1, model_layers_3_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv13_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv12_1[1]
            rms_norm332: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv12_1[0]
            lv14 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_3_mlp_gate_up_proj_q_weight7, model_layers_3_mlp_gate_up_proj_q_scale7, rms_norm332), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv3_2 = R.call_tir(cls.fused_split_silu_multiply, (lv14,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv15 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_3_mlp_down_proj_q_weight7, model_layers_3_mlp_down_proj_q_scale7, lv3_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv14_1 = R.call_tir(cls.fuse_add_norm_decode, (lv15, lv13_1, model_layers_4_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv15_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv14_1[1]
            rms_norm333: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv14_1[0]
            lv16 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_4_self_attn_qkv_proj_q_weight7, model_layers_4_self_attn_qkv_proj_q_scale7, rms_norm333), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape656 = R.call_tir(cls.reshape, (lv16,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape657 = R.call_tir(cls.reshape1, (reshape656,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv827 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape657), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape658 = R.call_tir(cls.reshape2, (lv827,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape659 = R.call_tir(cls.reshape3, (reshape658,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv17 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_4_self_attn_o_proj_q_weight7, model_layers_4_self_attn_o_proj_q_scale7, reshape659), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv16_1 = R.call_tir(cls.fuse_add_norm_decode, (lv17, lv15_1, model_layers_4_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv17_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv16_1[1]
            rms_norm334: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv16_1[0]
            lv18 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_4_mlp_gate_up_proj_q_weight7, model_layers_4_mlp_gate_up_proj_q_scale7, rms_norm334), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv4_2 = R.call_tir(cls.fused_split_silu_multiply, (lv18,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv19 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_4_mlp_down_proj_q_weight7, model_layers_4_mlp_down_proj_q_scale7, lv4_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv18_1 = R.call_tir(cls.fuse_add_norm_decode, (lv19, lv17_1, model_layers_5_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv19_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv18_1[1]
            rms_norm335: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv18_1[0]
            lv20 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_5_self_attn_qkv_proj_q_weight7, model_layers_5_self_attn_qkv_proj_q_scale7, rms_norm335), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape660 = R.call_tir(cls.reshape, (lv20,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape661 = R.call_tir(cls.reshape1, (reshape660,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv832 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape661), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape662 = R.call_tir(cls.reshape2, (lv832,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape663 = R.call_tir(cls.reshape3, (reshape662,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv21 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_5_self_attn_o_proj_q_weight7, model_layers_5_self_attn_o_proj_q_scale7, reshape663), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv20_1 = R.call_tir(cls.fuse_add_norm_decode, (lv21, lv19_1, model_layers_5_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv21_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv20_1[1]
            rms_norm336: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv20_1[0]
            lv22 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_5_mlp_gate_up_proj_q_weight7, model_layers_5_mlp_gate_up_proj_q_scale7, rms_norm336), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv5_2 = R.call_tir(cls.fused_split_silu_multiply, (lv22,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv23 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_5_mlp_down_proj_q_weight7, model_layers_5_mlp_down_proj_q_scale7, lv5_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv22_1 = R.call_tir(cls.fuse_add_norm_decode, (lv23, lv21_1, model_layers_6_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv23_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv22_1[1]
            rms_norm337: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv22_1[0]
            lv24 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_6_self_attn_qkv_proj_q_weight7, model_layers_6_self_attn_qkv_proj_q_scale7, rms_norm337), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape664 = R.call_tir(cls.reshape, (lv24,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape665 = R.call_tir(cls.reshape1, (reshape664,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv837 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape665), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape666 = R.call_tir(cls.reshape2, (lv837,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape667 = R.call_tir(cls.reshape3, (reshape666,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv25 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_6_self_attn_o_proj_q_weight7, model_layers_6_self_attn_o_proj_q_scale7, reshape667), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv24_1 = R.call_tir(cls.fuse_add_norm_decode, (lv25, lv23_1, model_layers_6_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv25_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv24_1[1]
            rms_norm338: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv24_1[0]
            lv26 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_6_mlp_gate_up_proj_q_weight7, model_layers_6_mlp_gate_up_proj_q_scale7, rms_norm338), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv6_2 = R.call_tir(cls.fused_split_silu_multiply, (lv26,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv27 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_6_mlp_down_proj_q_weight7, model_layers_6_mlp_down_proj_q_scale7, lv6_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv26_1 = R.call_tir(cls.fuse_add_norm_decode, (lv27, lv25_1, model_layers_7_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv27_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv26_1[1]
            rms_norm339: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv26_1[0]
            lv28 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_7_self_attn_qkv_proj_q_weight7, model_layers_7_self_attn_qkv_proj_q_scale7, rms_norm339), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape668 = R.call_tir(cls.reshape, (lv28,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape669 = R.call_tir(cls.reshape1, (reshape668,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv842 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape669), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape670 = R.call_tir(cls.reshape2, (lv842,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape671 = R.call_tir(cls.reshape3, (reshape670,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv29 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_7_self_attn_o_proj_q_weight7, model_layers_7_self_attn_o_proj_q_scale7, reshape671), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv28_1 = R.call_tir(cls.fuse_add_norm_decode, (lv29, lv27_1, model_layers_7_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv29_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv28_1[1]
            rms_norm340: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv28_1[0]
            lv30 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_7_mlp_gate_up_proj_q_weight7, model_layers_7_mlp_gate_up_proj_q_scale7, rms_norm340), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv7_2 = R.call_tir(cls.fused_split_silu_multiply, (lv30,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv31 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_7_mlp_down_proj_q_weight7, model_layers_7_mlp_down_proj_q_scale7, lv7_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv30_1 = R.call_tir(cls.fuse_add_norm_decode, (lv31, lv29_1, model_layers_8_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv31_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv30_1[1]
            rms_norm341: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv30_1[0]
            lv32 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_8_self_attn_qkv_proj_q_weight7, model_layers_8_self_attn_qkv_proj_q_scale7, rms_norm341), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape672 = R.call_tir(cls.reshape, (lv32,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape673 = R.call_tir(cls.reshape1, (reshape672,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv847 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape673), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape674 = R.call_tir(cls.reshape2, (lv847,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape675 = R.call_tir(cls.reshape3, (reshape674,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv33 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_8_self_attn_o_proj_q_weight7, model_layers_8_self_attn_o_proj_q_scale7, reshape675), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv32_1 = R.call_tir(cls.fuse_add_norm_decode, (lv33, lv31_1, model_layers_8_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv33_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv32_1[1]
            rms_norm342: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv32_1[0]
            lv34 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_8_mlp_gate_up_proj_q_weight7, model_layers_8_mlp_gate_up_proj_q_scale7, rms_norm342), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv8_2 = R.call_tir(cls.fused_split_silu_multiply, (lv34,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv35 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_8_mlp_down_proj_q_weight7, model_layers_8_mlp_down_proj_q_scale7, lv8_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv34_1 = R.call_tir(cls.fuse_add_norm_decode, (lv35, lv33_1, model_layers_9_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv35_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv34_1[1]
            rms_norm343: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv34_1[0]
            lv36 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_9_self_attn_qkv_proj_q_weight7, model_layers_9_self_attn_qkv_proj_q_scale7, rms_norm343), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape676 = R.call_tir(cls.reshape, (lv36,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape677 = R.call_tir(cls.reshape1, (reshape676,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv852 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape677), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape678 = R.call_tir(cls.reshape2, (lv852,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape679 = R.call_tir(cls.reshape3, (reshape678,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv37 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_9_self_attn_o_proj_q_weight7, model_layers_9_self_attn_o_proj_q_scale7, reshape679), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv36_1 = R.call_tir(cls.fuse_add_norm_decode, (lv37, lv35_1, model_layers_9_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv37_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv36_1[1]
            rms_norm344: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv36_1[0]
            lv38 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_9_mlp_gate_up_proj_q_weight7, model_layers_9_mlp_gate_up_proj_q_scale7, rms_norm344), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv9_2 = R.call_tir(cls.fused_split_silu_multiply, (lv38,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv39 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_9_mlp_down_proj_q_weight7, model_layers_9_mlp_down_proj_q_scale7, lv9_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv38_1 = R.call_tir(cls.fuse_add_norm_decode, (lv39, lv37_1, model_layers_10_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv39_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv38_1[1]
            rms_norm345: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv38_1[0]
            lv40 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_10_self_attn_qkv_proj_q_weight7, model_layers_10_self_attn_qkv_proj_q_scale7, rms_norm345), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape680 = R.call_tir(cls.reshape, (lv40,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape681 = R.call_tir(cls.reshape1, (reshape680,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv857 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape681), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape682 = R.call_tir(cls.reshape2, (lv857,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape683 = R.call_tir(cls.reshape3, (reshape682,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv41 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_10_self_attn_o_proj_q_weight7, model_layers_10_self_attn_o_proj_q_scale7, reshape683), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv40_1 = R.call_tir(cls.fuse_add_norm_decode, (lv41, lv39_1, model_layers_10_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv41_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv40_1[1]
            rms_norm346: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv40_1[0]
            lv42 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_10_mlp_gate_up_proj_q_weight7, model_layers_10_mlp_gate_up_proj_q_scale7, rms_norm346), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv10_2 = R.call_tir(cls.fused_split_silu_multiply, (lv42,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv43 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_10_mlp_down_proj_q_weight7, model_layers_10_mlp_down_proj_q_scale7, lv10_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv42_1 = R.call_tir(cls.fuse_add_norm_decode, (lv43, lv41_1, model_layers_11_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv43_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv42_1[1]
            rms_norm347: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv42_1[0]
            lv44 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_11_self_attn_qkv_proj_q_weight7, model_layers_11_self_attn_qkv_proj_q_scale7, rms_norm347), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape684 = R.call_tir(cls.reshape, (lv44,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape685 = R.call_tir(cls.reshape1, (reshape684,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv862 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape685), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape686 = R.call_tir(cls.reshape2, (lv862,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape687 = R.call_tir(cls.reshape3, (reshape686,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv45 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_11_self_attn_o_proj_q_weight7, model_layers_11_self_attn_o_proj_q_scale7, reshape687), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv44_1 = R.call_tir(cls.fuse_add_norm_decode, (lv45, lv43_1, model_layers_11_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv45_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv44_1[1]
            rms_norm348: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv44_1[0]
            lv46 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_11_mlp_gate_up_proj_q_weight7, model_layers_11_mlp_gate_up_proj_q_scale7, rms_norm348), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv11_2 = R.call_tir(cls.fused_split_silu_multiply, (lv46,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv47 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_11_mlp_down_proj_q_weight7, model_layers_11_mlp_down_proj_q_scale7, lv11_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv46_1 = R.call_tir(cls.fuse_add_norm_decode, (lv47, lv45_1, model_layers_12_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv47_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv46_1[1]
            rms_norm349: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv46_1[0]
            lv48 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_12_self_attn_qkv_proj_q_weight7, model_layers_12_self_attn_qkv_proj_q_scale7, rms_norm349), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape688 = R.call_tir(cls.reshape, (lv48,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape689 = R.call_tir(cls.reshape1, (reshape688,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv867 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape689), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape690 = R.call_tir(cls.reshape2, (lv867,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape691 = R.call_tir(cls.reshape3, (reshape690,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv49 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_12_self_attn_o_proj_q_weight7, model_layers_12_self_attn_o_proj_q_scale7, reshape691), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv48_1 = R.call_tir(cls.fuse_add_norm_decode, (lv49, lv47_1, model_layers_12_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv49_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv48_1[1]
            rms_norm350: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv48_1[0]
            lv50 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_12_mlp_gate_up_proj_q_weight7, model_layers_12_mlp_gate_up_proj_q_scale7, rms_norm350), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv12_2 = R.call_tir(cls.fused_split_silu_multiply, (lv50,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv51 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_12_mlp_down_proj_q_weight7, model_layers_12_mlp_down_proj_q_scale7, lv12_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv50_1 = R.call_tir(cls.fuse_add_norm_decode, (lv51, lv49_1, model_layers_13_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv51_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv50_1[1]
            rms_norm351: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv50_1[0]
            lv52 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_13_self_attn_qkv_proj_q_weight7, model_layers_13_self_attn_qkv_proj_q_scale7, rms_norm351), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape692 = R.call_tir(cls.reshape, (lv52,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape693 = R.call_tir(cls.reshape1, (reshape692,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv872 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape693), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape694 = R.call_tir(cls.reshape2, (lv872,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape695 = R.call_tir(cls.reshape3, (reshape694,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv53 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_13_self_attn_o_proj_q_weight7, model_layers_13_self_attn_o_proj_q_scale7, reshape695), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv52_1 = R.call_tir(cls.fuse_add_norm_decode, (lv53, lv51_1, model_layers_13_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv53_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv52_1[1]
            rms_norm352: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv52_1[0]
            lv54 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_13_mlp_gate_up_proj_q_weight7, model_layers_13_mlp_gate_up_proj_q_scale7, rms_norm352), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv13_2 = R.call_tir(cls.fused_split_silu_multiply, (lv54,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv55 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_13_mlp_down_proj_q_weight7, model_layers_13_mlp_down_proj_q_scale7, lv13_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv54_1 = R.call_tir(cls.fuse_add_norm_decode, (lv55, lv53_1, model_layers_14_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv55_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv54_1[1]
            rms_norm353: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv54_1[0]
            lv56 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_14_self_attn_qkv_proj_q_weight7, model_layers_14_self_attn_qkv_proj_q_scale7, rms_norm353), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape696 = R.call_tir(cls.reshape, (lv56,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape697 = R.call_tir(cls.reshape1, (reshape696,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv877 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape697), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape698 = R.call_tir(cls.reshape2, (lv877,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape699 = R.call_tir(cls.reshape3, (reshape698,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv57 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_14_self_attn_o_proj_q_weight7, model_layers_14_self_attn_o_proj_q_scale7, reshape699), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv56_1 = R.call_tir(cls.fuse_add_norm_decode, (lv57, lv55_1, model_layers_14_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv57_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv56_1[1]
            rms_norm354: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv56_1[0]
            lv58 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_14_mlp_gate_up_proj_q_weight7, model_layers_14_mlp_gate_up_proj_q_scale7, rms_norm354), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv14_2 = R.call_tir(cls.fused_split_silu_multiply, (lv58,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv59 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_14_mlp_down_proj_q_weight7, model_layers_14_mlp_down_proj_q_scale7, lv14_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv58_1 = R.call_tir(cls.fuse_add_norm_decode, (lv59, lv57_1, model_layers_15_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv59_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv58_1[1]
            rms_norm355: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv58_1[0]
            lv60 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_15_self_attn_qkv_proj_q_weight7, model_layers_15_self_attn_qkv_proj_q_scale7, rms_norm355), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape700 = R.call_tir(cls.reshape, (lv60,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape701 = R.call_tir(cls.reshape1, (reshape700,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv882 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape701), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape702 = R.call_tir(cls.reshape2, (lv882,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape703 = R.call_tir(cls.reshape3, (reshape702,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv61 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_15_self_attn_o_proj_q_weight7, model_layers_15_self_attn_o_proj_q_scale7, reshape703), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv60_1 = R.call_tir(cls.fuse_add_norm_decode, (lv61, lv59_1, model_layers_15_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv61_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv60_1[1]
            rms_norm356: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv60_1[0]
            lv62 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_15_mlp_gate_up_proj_q_weight7, model_layers_15_mlp_gate_up_proj_q_scale7, rms_norm356), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv15_2 = R.call_tir(cls.fused_split_silu_multiply, (lv62,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv63 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_15_mlp_down_proj_q_weight7, model_layers_15_mlp_down_proj_q_scale7, lv15_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv62_1 = R.call_tir(cls.fuse_add_norm_decode, (lv63, lv61_1, model_layers_16_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv63_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv62_1[1]
            rms_norm357: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv62_1[0]
            lv64 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_16_self_attn_qkv_proj_q_weight7, model_layers_16_self_attn_qkv_proj_q_scale7, rms_norm357), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape704 = R.call_tir(cls.reshape, (lv64,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape705 = R.call_tir(cls.reshape1, (reshape704,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv887 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape705), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape706 = R.call_tir(cls.reshape2, (lv887,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape707 = R.call_tir(cls.reshape3, (reshape706,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv65 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_16_self_attn_o_proj_q_weight7, model_layers_16_self_attn_o_proj_q_scale7, reshape707), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv64_1 = R.call_tir(cls.fuse_add_norm_decode, (lv65, lv63_1, model_layers_16_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv65_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv64_1[1]
            rms_norm358: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv64_1[0]
            lv66 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_16_mlp_gate_up_proj_q_weight7, model_layers_16_mlp_gate_up_proj_q_scale7, rms_norm358), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv16_2 = R.call_tir(cls.fused_split_silu_multiply, (lv66,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv67 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_16_mlp_down_proj_q_weight7, model_layers_16_mlp_down_proj_q_scale7, lv16_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv66_1 = R.call_tir(cls.fuse_add_norm_decode, (lv67, lv65_1, model_layers_17_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv67_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv66_1[1]
            rms_norm359: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv66_1[0]
            lv68 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_17_self_attn_qkv_proj_q_weight7, model_layers_17_self_attn_qkv_proj_q_scale7, rms_norm359), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape708 = R.call_tir(cls.reshape, (lv68,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape709 = R.call_tir(cls.reshape1, (reshape708,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv892 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape709), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape710 = R.call_tir(cls.reshape2, (lv892,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape711 = R.call_tir(cls.reshape3, (reshape710,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv69 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_17_self_attn_o_proj_q_weight7, model_layers_17_self_attn_o_proj_q_scale7, reshape711), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv68_1 = R.call_tir(cls.fuse_add_norm_decode, (lv69, lv67_1, model_layers_17_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv69_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv68_1[1]
            rms_norm360: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv68_1[0]
            lv70 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_17_mlp_gate_up_proj_q_weight7, model_layers_17_mlp_gate_up_proj_q_scale7, rms_norm360), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv17_2 = R.call_tir(cls.fused_split_silu_multiply, (lv70,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv71 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_17_mlp_down_proj_q_weight7, model_layers_17_mlp_down_proj_q_scale7, lv17_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv70_1 = R.call_tir(cls.fuse_add_norm_decode, (lv71, lv69_1, model_layers_18_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv71_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv70_1[1]
            rms_norm361: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv70_1[0]
            lv72 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_18_self_attn_qkv_proj_q_weight7, model_layers_18_self_attn_qkv_proj_q_scale7, rms_norm361), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape712 = R.call_tir(cls.reshape, (lv72,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape713 = R.call_tir(cls.reshape1, (reshape712,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv897 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape713), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape714 = R.call_tir(cls.reshape2, (lv897,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape715 = R.call_tir(cls.reshape3, (reshape714,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv73 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_18_self_attn_o_proj_q_weight7, model_layers_18_self_attn_o_proj_q_scale7, reshape715), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv72_1 = R.call_tir(cls.fuse_add_norm_decode, (lv73, lv71_1, model_layers_18_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv73_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv72_1[1]
            rms_norm362: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv72_1[0]
            lv74 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_18_mlp_gate_up_proj_q_weight7, model_layers_18_mlp_gate_up_proj_q_scale7, rms_norm362), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv18_2 = R.call_tir(cls.fused_split_silu_multiply, (lv74,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv75 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_18_mlp_down_proj_q_weight7, model_layers_18_mlp_down_proj_q_scale7, lv18_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv74_1 = R.call_tir(cls.fuse_add_norm_decode, (lv75, lv73_1, model_layers_19_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv75_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv74_1[1]
            rms_norm363: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv74_1[0]
            lv76 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_19_self_attn_qkv_proj_q_weight7, model_layers_19_self_attn_qkv_proj_q_scale7, rms_norm363), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape716 = R.call_tir(cls.reshape, (lv76,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape717 = R.call_tir(cls.reshape1, (reshape716,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv902 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape717), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape718 = R.call_tir(cls.reshape2, (lv902,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape719 = R.call_tir(cls.reshape3, (reshape718,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv77 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_19_self_attn_o_proj_q_weight7, model_layers_19_self_attn_o_proj_q_scale7, reshape719), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv76_1 = R.call_tir(cls.fuse_add_norm_decode, (lv77, lv75_1, model_layers_19_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv77_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv76_1[1]
            rms_norm364: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv76_1[0]
            lv78 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_19_mlp_gate_up_proj_q_weight7, model_layers_19_mlp_gate_up_proj_q_scale7, rms_norm364), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv19_2 = R.call_tir(cls.fused_split_silu_multiply, (lv78,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv79 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_19_mlp_down_proj_q_weight7, model_layers_19_mlp_down_proj_q_scale7, lv19_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv78_1 = R.call_tir(cls.fuse_add_norm_decode, (lv79, lv77_1, model_layers_20_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv79_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv78_1[1]
            rms_norm365: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv78_1[0]
            lv80 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_20_self_attn_qkv_proj_q_weight7, model_layers_20_self_attn_qkv_proj_q_scale7, rms_norm365), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape720 = R.call_tir(cls.reshape, (lv80,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape721 = R.call_tir(cls.reshape1, (reshape720,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv907 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape721), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape722 = R.call_tir(cls.reshape2, (lv907,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape723 = R.call_tir(cls.reshape3, (reshape722,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv81 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_20_self_attn_o_proj_q_weight7, model_layers_20_self_attn_o_proj_q_scale7, reshape723), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv80_1 = R.call_tir(cls.fuse_add_norm_decode, (lv81, lv79_1, model_layers_20_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv81_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv80_1[1]
            rms_norm366: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv80_1[0]
            lv82 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_20_mlp_gate_up_proj_q_weight7, model_layers_20_mlp_gate_up_proj_q_scale7, rms_norm366), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv20_2 = R.call_tir(cls.fused_split_silu_multiply, (lv82,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv83 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_20_mlp_down_proj_q_weight7, model_layers_20_mlp_down_proj_q_scale7, lv20_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv82_1 = R.call_tir(cls.fuse_add_norm_decode, (lv83, lv81_1, model_layers_21_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv83_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv82_1[1]
            rms_norm367: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv82_1[0]
            lv84 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_21_self_attn_qkv_proj_q_weight7, model_layers_21_self_attn_qkv_proj_q_scale7, rms_norm367), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape724 = R.call_tir(cls.reshape, (lv84,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape725 = R.call_tir(cls.reshape1, (reshape724,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv912 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape725), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape726 = R.call_tir(cls.reshape2, (lv912,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape727 = R.call_tir(cls.reshape3, (reshape726,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv85 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_21_self_attn_o_proj_q_weight7, model_layers_21_self_attn_o_proj_q_scale7, reshape727), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv84_1 = R.call_tir(cls.fuse_add_norm_decode, (lv85, lv83_1, model_layers_21_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv85_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv84_1[1]
            rms_norm368: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv84_1[0]
            lv86 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_21_mlp_gate_up_proj_q_weight7, model_layers_21_mlp_gate_up_proj_q_scale7, rms_norm368), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv21_2 = R.call_tir(cls.fused_split_silu_multiply, (lv86,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv87 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_21_mlp_down_proj_q_weight7, model_layers_21_mlp_down_proj_q_scale7, lv21_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv86_1 = R.call_tir(cls.fuse_add_norm_decode, (lv87, lv85_1, model_layers_22_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv87_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv86_1[1]
            rms_norm369: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv86_1[0]
            lv88 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_22_self_attn_qkv_proj_q_weight7, model_layers_22_self_attn_qkv_proj_q_scale7, rms_norm369), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape728 = R.call_tir(cls.reshape, (lv88,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape729 = R.call_tir(cls.reshape1, (reshape728,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv917 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape729), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape730 = R.call_tir(cls.reshape2, (lv917,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape731 = R.call_tir(cls.reshape3, (reshape730,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv89 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_22_self_attn_o_proj_q_weight7, model_layers_22_self_attn_o_proj_q_scale7, reshape731), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv88_1 = R.call_tir(cls.fuse_add_norm_decode, (lv89, lv87_1, model_layers_22_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv89_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv88_1[1]
            rms_norm370: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv88_1[0]
            lv90 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_22_mlp_gate_up_proj_q_weight7, model_layers_22_mlp_gate_up_proj_q_scale7, rms_norm370), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv22_2 = R.call_tir(cls.fused_split_silu_multiply, (lv90,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv91 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_22_mlp_down_proj_q_weight7, model_layers_22_mlp_down_proj_q_scale7, lv22_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv90_1 = R.call_tir(cls.fuse_add_norm_decode, (lv91, lv89_1, model_layers_23_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv91_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv90_1[1]
            rms_norm371: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv90_1[0]
            lv92 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_23_self_attn_qkv_proj_q_weight7, model_layers_23_self_attn_qkv_proj_q_scale7, rms_norm371), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape732 = R.call_tir(cls.reshape, (lv92,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape733 = R.call_tir(cls.reshape1, (reshape732,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv922 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape733), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape734 = R.call_tir(cls.reshape2, (lv922,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape735 = R.call_tir(cls.reshape3, (reshape734,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv93 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_23_self_attn_o_proj_q_weight7, model_layers_23_self_attn_o_proj_q_scale7, reshape735), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv92_1 = R.call_tir(cls.fuse_add_norm_decode, (lv93, lv91_1, model_layers_23_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv93_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv92_1[1]
            rms_norm372: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv92_1[0]
            lv94 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_23_mlp_gate_up_proj_q_weight7, model_layers_23_mlp_gate_up_proj_q_scale7, rms_norm372), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv23_2 = R.call_tir(cls.fused_split_silu_multiply, (lv94,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv95 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_23_mlp_down_proj_q_weight7, model_layers_23_mlp_down_proj_q_scale7, lv23_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv94_1 = R.call_tir(cls.fuse_add_norm_decode, (lv95, lv93_1, model_layers_24_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv95_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv94_1[1]
            rms_norm373: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv94_1[0]
            lv96 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_24_self_attn_qkv_proj_q_weight7, model_layers_24_self_attn_qkv_proj_q_scale7, rms_norm373), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape736 = R.call_tir(cls.reshape, (lv96,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape737 = R.call_tir(cls.reshape1, (reshape736,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv927 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape737), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape738 = R.call_tir(cls.reshape2, (lv927,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape739 = R.call_tir(cls.reshape3, (reshape738,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv97 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_24_self_attn_o_proj_q_weight7, model_layers_24_self_attn_o_proj_q_scale7, reshape739), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv96_1 = R.call_tir(cls.fuse_add_norm_decode, (lv97, lv95_1, model_layers_24_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv97_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv96_1[1]
            rms_norm374: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv96_1[0]
            lv98 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_24_mlp_gate_up_proj_q_weight7, model_layers_24_mlp_gate_up_proj_q_scale7, rms_norm374), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv24_2 = R.call_tir(cls.fused_split_silu_multiply, (lv98,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv99 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_24_mlp_down_proj_q_weight7, model_layers_24_mlp_down_proj_q_scale7, lv24_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv98_1 = R.call_tir(cls.fuse_add_norm_decode, (lv99, lv97_1, model_layers_25_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv99_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv98_1[1]
            rms_norm375: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv98_1[0]
            lv100 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_25_self_attn_qkv_proj_q_weight7, model_layers_25_self_attn_qkv_proj_q_scale7, rms_norm375), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape740 = R.call_tir(cls.reshape, (lv100,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape741 = R.call_tir(cls.reshape1, (reshape740,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv932 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape741), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape742 = R.call_tir(cls.reshape2, (lv932,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape743 = R.call_tir(cls.reshape3, (reshape742,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv101 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_25_self_attn_o_proj_q_weight7, model_layers_25_self_attn_o_proj_q_scale7, reshape743), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv100_1 = R.call_tir(cls.fuse_add_norm_decode, (lv101, lv99_1, model_layers_25_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv101_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv100_1[1]
            rms_norm376: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv100_1[0]
            lv102 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_25_mlp_gate_up_proj_q_weight7, model_layers_25_mlp_gate_up_proj_q_scale7, rms_norm376), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv25_2 = R.call_tir(cls.fused_split_silu_multiply, (lv102,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv103 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_25_mlp_down_proj_q_weight7, model_layers_25_mlp_down_proj_q_scale7, lv25_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv102_1 = R.call_tir(cls.fuse_add_norm_decode, (lv103, lv101_1, model_layers_26_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv103_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv102_1[1]
            rms_norm377: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv102_1[0]
            lv104 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_26_self_attn_qkv_proj_q_weight7, model_layers_26_self_attn_qkv_proj_q_scale7, rms_norm377), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape744 = R.call_tir(cls.reshape, (lv104,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape745 = R.call_tir(cls.reshape1, (reshape744,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv937 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape745), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape746 = R.call_tir(cls.reshape2, (lv937,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape747 = R.call_tir(cls.reshape3, (reshape746,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv105 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_26_self_attn_o_proj_q_weight7, model_layers_26_self_attn_o_proj_q_scale7, reshape747), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv104_1 = R.call_tir(cls.fuse_add_norm_decode, (lv105, lv103_1, model_layers_26_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv105_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv104_1[1]
            rms_norm378: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv104_1[0]
            lv106 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_26_mlp_gate_up_proj_q_weight7, model_layers_26_mlp_gate_up_proj_q_scale7, rms_norm378), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv26_2 = R.call_tir(cls.fused_split_silu_multiply, (lv106,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv107 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_26_mlp_down_proj_q_weight7, model_layers_26_mlp_down_proj_q_scale7, lv26_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv106_1 = R.call_tir(cls.fuse_add_norm_decode, (lv107, lv105_1, model_layers_27_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv107_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv106_1[1]
            rms_norm379: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv106_1[0]
            lv108 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_27_self_attn_qkv_proj_q_weight7, model_layers_27_self_attn_qkv_proj_q_scale7, rms_norm379), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape748 = R.call_tir(cls.reshape, (lv108,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape749 = R.call_tir(cls.reshape1, (reshape748,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv942 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape749), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape750 = R.call_tir(cls.reshape2, (lv942,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape751 = R.call_tir(cls.reshape3, (reshape750,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv109 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_27_self_attn_o_proj_q_weight7, model_layers_27_self_attn_o_proj_q_scale7, reshape751), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv108_1 = R.call_tir(cls.fuse_add_norm_decode, (lv109, lv107_1, model_layers_27_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv109_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv108_1[1]
            rms_norm380: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv108_1[0]
            lv110 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_27_mlp_gate_up_proj_q_weight7, model_layers_27_mlp_gate_up_proj_q_scale7, rms_norm380), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv27_2 = R.call_tir(cls.fused_split_silu_multiply, (lv110,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv111 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_27_mlp_down_proj_q_weight7, model_layers_27_mlp_down_proj_q_scale7, lv27_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv110_1 = R.call_tir(cls.fuse_add_norm_decode, (lv111, lv109_1, model_layers_28_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv111_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv110_1[1]
            rms_norm381: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv110_1[0]
            lv112 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_28_self_attn_qkv_proj_q_weight7, model_layers_28_self_attn_qkv_proj_q_scale7, rms_norm381), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape752 = R.call_tir(cls.reshape, (lv112,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape753 = R.call_tir(cls.reshape1, (reshape752,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv947 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape753), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape754 = R.call_tir(cls.reshape2, (lv947,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape755 = R.call_tir(cls.reshape3, (reshape754,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv113 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_28_self_attn_o_proj_q_weight7, model_layers_28_self_attn_o_proj_q_scale7, reshape755), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv112_1 = R.call_tir(cls.fuse_add_norm_decode, (lv113, lv111_1, model_layers_28_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv113_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv112_1[1]
            rms_norm382: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv112_1[0]
            lv114 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_28_mlp_gate_up_proj_q_weight7, model_layers_28_mlp_gate_up_proj_q_scale7, rms_norm382), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv28_2 = R.call_tir(cls.fused_split_silu_multiply, (lv114,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv115 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_28_mlp_down_proj_q_weight7, model_layers_28_mlp_down_proj_q_scale7, lv28_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv114_1 = R.call_tir(cls.fuse_add_norm_decode, (lv115, lv113_1, model_layers_29_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv115_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv114_1[1]
            rms_norm383: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv114_1[0]
            lv116 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_29_self_attn_qkv_proj_q_weight7, model_layers_29_self_attn_qkv_proj_q_scale7, rms_norm383), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape756 = R.call_tir(cls.reshape, (lv116,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape757 = R.call_tir(cls.reshape1, (reshape756,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv952 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape757), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape758 = R.call_tir(cls.reshape2, (lv952,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape759 = R.call_tir(cls.reshape3, (reshape758,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv117 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_29_self_attn_o_proj_q_weight7, model_layers_29_self_attn_o_proj_q_scale7, reshape759), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv116_1 = R.call_tir(cls.fuse_add_norm_decode, (lv117, lv115_1, model_layers_29_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv117_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv116_1[1]
            rms_norm384: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv116_1[0]
            lv118 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_29_mlp_gate_up_proj_q_weight7, model_layers_29_mlp_gate_up_proj_q_scale7, rms_norm384), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv29_2 = R.call_tir(cls.fused_split_silu_multiply, (lv118,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv119 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_29_mlp_down_proj_q_weight7, model_layers_29_mlp_down_proj_q_scale7, lv29_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv118_1 = R.call_tir(cls.fuse_add_norm_decode, (lv119, lv117_1, model_layers_30_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv119_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv118_1[1]
            rms_norm385: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv118_1[0]
            lv120 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_30_self_attn_qkv_proj_q_weight7, model_layers_30_self_attn_qkv_proj_q_scale7, rms_norm385), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape760 = R.call_tir(cls.reshape, (lv120,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape761 = R.call_tir(cls.reshape1, (reshape760,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv957 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape761), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape762 = R.call_tir(cls.reshape2, (lv957,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape763 = R.call_tir(cls.reshape3, (reshape762,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv121 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_30_self_attn_o_proj_q_weight7, model_layers_30_self_attn_o_proj_q_scale7, reshape763), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv120_1 = R.call_tir(cls.fuse_add_norm_decode, (lv121, lv119_1, model_layers_30_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv121_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv120_1[1]
            rms_norm386: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv120_1[0]
            lv122 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_30_mlp_gate_up_proj_q_weight7, model_layers_30_mlp_gate_up_proj_q_scale7, rms_norm386), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv30_2 = R.call_tir(cls.fused_split_silu_multiply, (lv122,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv123 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_30_mlp_down_proj_q_weight7, model_layers_30_mlp_down_proj_q_scale7, lv30_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv122_1 = R.call_tir(cls.fuse_add_norm_decode, (lv123, lv121_1, model_layers_31_input_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv123_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv122_1[1]
            rms_norm387: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv122_1[0]
            lv124 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_31_self_attn_qkv_proj_q_weight7, model_layers_31_self_attn_qkv_proj_q_scale7, rms_norm387), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape764 = R.call_tir(cls.reshape, (lv124,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape765 = R.call_tir(cls.reshape1, (reshape764,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv962 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape765), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape766 = R.call_tir(cls.reshape2, (lv962,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape767 = R.call_tir(cls.reshape3, (reshape766,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv125 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_31_self_attn_o_proj_q_weight7, model_layers_31_self_attn_o_proj_q_scale7, reshape767), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv124_1 = R.call_tir(cls.fuse_add_norm_decode, (lv125, lv123_1, model_layers_31_post_attention_layernorm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv125_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv124_1[1]
            rms_norm388: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv124_1[0]
            lv126 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_31_mlp_gate_up_proj_q_weight7, model_layers_31_mlp_gate_up_proj_q_scale7, rms_norm388), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv31_2 = R.call_tir(cls.fused_split_silu_multiply, (lv126,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv127 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_31_mlp_down_proj_q_weight7, model_layers_31_mlp_down_proj_q_scale7, lv31_2), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv126_1 = R.call_tir(cls.fuse_add_norm_decode, (lv127, lv125_1, model_norm_weight7), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            rms_norm389: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv126_1[0]
            lv128 = R.call_tir(cls.fused_dequantize_fused_NT_matmul4_cast, (lm_head_q_weight7, lm_head_q_scale7, rms_norm389), out_sinfo=R.Tensor((batch_size, 1, vocab_size), dtype="float32"))
            gv8: R.Tuple(R.Tensor((batch_size, 1, vocab_size), dtype="float32"), R.Object) = lv128, paged_kv_cache
            R.output(gv8)
        return gv8

    @R.function
    def batch_decode_to_last_hidden_states(input_embeds: R.Tensor(("batch_size", 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor(("batch_size", 1, 4096), dtype="float16"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm520 = R.call_tir(cls.rms_norm, (input_embeds, model_layers_0_input_layernorm_weight10), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv129 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_0_self_attn_qkv_proj_q_weight10, model_layers_0_self_attn_qkv_proj_q_scale10, rms_norm520), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1024 = R.call_tir(cls.reshape, (lv129,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1025 = R.call_tir(cls.reshape1, (reshape1024,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1289 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1025), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1026 = R.call_tir(cls.reshape2, (lv1289,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1027 = R.call_tir(cls.reshape3, (reshape1026,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv130 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_0_self_attn_o_proj_q_weight10, model_layers_0_self_attn_o_proj_q_scale10, reshape1027), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv128 = R.call_tir(cls.fuse_add_norm_decode, (lv130, input_embeds, model_layers_0_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv129_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv128[1]
            rms_norm521: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv128[0]
            lv131 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_0_mlp_gate_up_proj_q_weight10, model_layers_0_mlp_gate_up_proj_q_scale10, rms_norm521), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv33 = R.call_tir(cls.fused_split_silu_multiply, (lv131,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv132 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_0_mlp_down_proj_q_weight10, model_layers_0_mlp_down_proj_q_scale10, lv33), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv130_1 = R.call_tir(cls.fuse_add_norm_decode, (lv132, lv129_1, model_layers_1_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv131_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv130_1[1]
            rms_norm522: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv130_1[0]
            lv133 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_1_self_attn_qkv_proj_q_weight10, model_layers_1_self_attn_qkv_proj_q_scale10, rms_norm522), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1028 = R.call_tir(cls.reshape, (lv133,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1029 = R.call_tir(cls.reshape1, (reshape1028,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1294 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape1029), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1030 = R.call_tir(cls.reshape2, (lv1294,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1031 = R.call_tir(cls.reshape3, (reshape1030,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv134 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_1_self_attn_o_proj_q_weight10, model_layers_1_self_attn_o_proj_q_scale10, reshape1031), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv132_1 = R.call_tir(cls.fuse_add_norm_decode, (lv134, lv131_1, model_layers_1_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv133_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv132_1[1]
            rms_norm523: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv132_1[0]
            lv135 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_1_mlp_gate_up_proj_q_weight10, model_layers_1_mlp_gate_up_proj_q_scale10, rms_norm523), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv34 = R.call_tir(cls.fused_split_silu_multiply, (lv135,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv136 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_1_mlp_down_proj_q_weight10, model_layers_1_mlp_down_proj_q_scale10, lv34), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv134_1 = R.call_tir(cls.fuse_add_norm_decode, (lv136, lv133_1, model_layers_2_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv135_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv134_1[1]
            rms_norm524: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv134_1[0]
            lv137 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_2_self_attn_qkv_proj_q_weight10, model_layers_2_self_attn_qkv_proj_q_scale10, rms_norm524), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1032 = R.call_tir(cls.reshape, (lv137,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1033 = R.call_tir(cls.reshape1, (reshape1032,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1299 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape1033), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1034 = R.call_tir(cls.reshape2, (lv1299,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1035 = R.call_tir(cls.reshape3, (reshape1034,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv138 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_2_self_attn_o_proj_q_weight10, model_layers_2_self_attn_o_proj_q_scale10, reshape1035), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv136_1 = R.call_tir(cls.fuse_add_norm_decode, (lv138, lv135_1, model_layers_2_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv137_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv136_1[1]
            rms_norm525: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv136_1[0]
            lv139 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_2_mlp_gate_up_proj_q_weight10, model_layers_2_mlp_gate_up_proj_q_scale10, rms_norm525), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv35 = R.call_tir(cls.fused_split_silu_multiply, (lv139,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv140 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_2_mlp_down_proj_q_weight10, model_layers_2_mlp_down_proj_q_scale10, lv35), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv138_1 = R.call_tir(cls.fuse_add_norm_decode, (lv140, lv137_1, model_layers_3_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv139_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv138_1[1]
            rms_norm526: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv138_1[0]
            lv141 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_3_self_attn_qkv_proj_q_weight10, model_layers_3_self_attn_qkv_proj_q_scale10, rms_norm526), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1036 = R.call_tir(cls.reshape, (lv141,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1037 = R.call_tir(cls.reshape1, (reshape1036,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1304 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape1037), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1038 = R.call_tir(cls.reshape2, (lv1304,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1039 = R.call_tir(cls.reshape3, (reshape1038,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv142 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_3_self_attn_o_proj_q_weight10, model_layers_3_self_attn_o_proj_q_scale10, reshape1039), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv140_1 = R.call_tir(cls.fuse_add_norm_decode, (lv142, lv139_1, model_layers_3_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv141_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv140_1[1]
            rms_norm527: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv140_1[0]
            lv143 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_3_mlp_gate_up_proj_q_weight10, model_layers_3_mlp_gate_up_proj_q_scale10, rms_norm527), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv36 = R.call_tir(cls.fused_split_silu_multiply, (lv143,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv144 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_3_mlp_down_proj_q_weight10, model_layers_3_mlp_down_proj_q_scale10, lv36), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv142_1 = R.call_tir(cls.fuse_add_norm_decode, (lv144, lv141_1, model_layers_4_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv143_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv142_1[1]
            rms_norm528: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv142_1[0]
            lv145 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_4_self_attn_qkv_proj_q_weight10, model_layers_4_self_attn_qkv_proj_q_scale10, rms_norm528), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1040 = R.call_tir(cls.reshape, (lv145,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1041 = R.call_tir(cls.reshape1, (reshape1040,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1309 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape1041), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1042 = R.call_tir(cls.reshape2, (lv1309,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1043 = R.call_tir(cls.reshape3, (reshape1042,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv146 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_4_self_attn_o_proj_q_weight10, model_layers_4_self_attn_o_proj_q_scale10, reshape1043), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv144_1 = R.call_tir(cls.fuse_add_norm_decode, (lv146, lv143_1, model_layers_4_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv145_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv144_1[1]
            rms_norm529: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv144_1[0]
            lv147 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_4_mlp_gate_up_proj_q_weight10, model_layers_4_mlp_gate_up_proj_q_scale10, rms_norm529), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv37 = R.call_tir(cls.fused_split_silu_multiply, (lv147,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv148 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_4_mlp_down_proj_q_weight10, model_layers_4_mlp_down_proj_q_scale10, lv37), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv146_1 = R.call_tir(cls.fuse_add_norm_decode, (lv148, lv145_1, model_layers_5_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv147_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv146_1[1]
            rms_norm530: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv146_1[0]
            lv149 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_5_self_attn_qkv_proj_q_weight10, model_layers_5_self_attn_qkv_proj_q_scale10, rms_norm530), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1044 = R.call_tir(cls.reshape, (lv149,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1045 = R.call_tir(cls.reshape1, (reshape1044,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1314 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape1045), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1046 = R.call_tir(cls.reshape2, (lv1314,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1047 = R.call_tir(cls.reshape3, (reshape1046,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv150 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_5_self_attn_o_proj_q_weight10, model_layers_5_self_attn_o_proj_q_scale10, reshape1047), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv148_1 = R.call_tir(cls.fuse_add_norm_decode, (lv150, lv147_1, model_layers_5_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv149_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv148_1[1]
            rms_norm531: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv148_1[0]
            lv151 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_5_mlp_gate_up_proj_q_weight10, model_layers_5_mlp_gate_up_proj_q_scale10, rms_norm531), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv38 = R.call_tir(cls.fused_split_silu_multiply, (lv151,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv152 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_5_mlp_down_proj_q_weight10, model_layers_5_mlp_down_proj_q_scale10, lv38), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv150_1 = R.call_tir(cls.fuse_add_norm_decode, (lv152, lv149_1, model_layers_6_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv151_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv150_1[1]
            rms_norm532: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv150_1[0]
            lv153 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_6_self_attn_qkv_proj_q_weight10, model_layers_6_self_attn_qkv_proj_q_scale10, rms_norm532), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1048 = R.call_tir(cls.reshape, (lv153,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1049 = R.call_tir(cls.reshape1, (reshape1048,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1319 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape1049), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1050 = R.call_tir(cls.reshape2, (lv1319,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1051 = R.call_tir(cls.reshape3, (reshape1050,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv154 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_6_self_attn_o_proj_q_weight10, model_layers_6_self_attn_o_proj_q_scale10, reshape1051), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv152_1 = R.call_tir(cls.fuse_add_norm_decode, (lv154, lv151_1, model_layers_6_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv153_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv152_1[1]
            rms_norm533: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv152_1[0]
            lv155 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_6_mlp_gate_up_proj_q_weight10, model_layers_6_mlp_gate_up_proj_q_scale10, rms_norm533), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv39 = R.call_tir(cls.fused_split_silu_multiply, (lv155,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv156 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_6_mlp_down_proj_q_weight10, model_layers_6_mlp_down_proj_q_scale10, lv39), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv154_1 = R.call_tir(cls.fuse_add_norm_decode, (lv156, lv153_1, model_layers_7_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv155_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv154_1[1]
            rms_norm534: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv154_1[0]
            lv157 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_7_self_attn_qkv_proj_q_weight10, model_layers_7_self_attn_qkv_proj_q_scale10, rms_norm534), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1052 = R.call_tir(cls.reshape, (lv157,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1053 = R.call_tir(cls.reshape1, (reshape1052,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1324 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape1053), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1054 = R.call_tir(cls.reshape2, (lv1324,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1055 = R.call_tir(cls.reshape3, (reshape1054,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv158 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_7_self_attn_o_proj_q_weight10, model_layers_7_self_attn_o_proj_q_scale10, reshape1055), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv156_1 = R.call_tir(cls.fuse_add_norm_decode, (lv158, lv155_1, model_layers_7_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv157_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv156_1[1]
            rms_norm535: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv156_1[0]
            lv159 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_7_mlp_gate_up_proj_q_weight10, model_layers_7_mlp_gate_up_proj_q_scale10, rms_norm535), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv40 = R.call_tir(cls.fused_split_silu_multiply, (lv159,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv160 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_7_mlp_down_proj_q_weight10, model_layers_7_mlp_down_proj_q_scale10, lv40), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv158_1 = R.call_tir(cls.fuse_add_norm_decode, (lv160, lv157_1, model_layers_8_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv159_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv158_1[1]
            rms_norm536: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv158_1[0]
            lv161 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_8_self_attn_qkv_proj_q_weight10, model_layers_8_self_attn_qkv_proj_q_scale10, rms_norm536), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1056 = R.call_tir(cls.reshape, (lv161,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1057 = R.call_tir(cls.reshape1, (reshape1056,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1329 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape1057), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1058 = R.call_tir(cls.reshape2, (lv1329,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1059 = R.call_tir(cls.reshape3, (reshape1058,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv162 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_8_self_attn_o_proj_q_weight10, model_layers_8_self_attn_o_proj_q_scale10, reshape1059), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv160_1 = R.call_tir(cls.fuse_add_norm_decode, (lv162, lv159_1, model_layers_8_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv161_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv160_1[1]
            rms_norm537: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv160_1[0]
            lv163 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_8_mlp_gate_up_proj_q_weight10, model_layers_8_mlp_gate_up_proj_q_scale10, rms_norm537), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv41 = R.call_tir(cls.fused_split_silu_multiply, (lv163,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv164 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_8_mlp_down_proj_q_weight10, model_layers_8_mlp_down_proj_q_scale10, lv41), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv162_1 = R.call_tir(cls.fuse_add_norm_decode, (lv164, lv161_1, model_layers_9_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv163_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv162_1[1]
            rms_norm538: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv162_1[0]
            lv165 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_9_self_attn_qkv_proj_q_weight10, model_layers_9_self_attn_qkv_proj_q_scale10, rms_norm538), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1060 = R.call_tir(cls.reshape, (lv165,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1061 = R.call_tir(cls.reshape1, (reshape1060,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1334 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape1061), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1062 = R.call_tir(cls.reshape2, (lv1334,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1063 = R.call_tir(cls.reshape3, (reshape1062,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv166 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_9_self_attn_o_proj_q_weight10, model_layers_9_self_attn_o_proj_q_scale10, reshape1063), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv164_1 = R.call_tir(cls.fuse_add_norm_decode, (lv166, lv163_1, model_layers_9_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv165_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv164_1[1]
            rms_norm539: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv164_1[0]
            lv167 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_9_mlp_gate_up_proj_q_weight10, model_layers_9_mlp_gate_up_proj_q_scale10, rms_norm539), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv42 = R.call_tir(cls.fused_split_silu_multiply, (lv167,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv168 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_9_mlp_down_proj_q_weight10, model_layers_9_mlp_down_proj_q_scale10, lv42), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv166_1 = R.call_tir(cls.fuse_add_norm_decode, (lv168, lv165_1, model_layers_10_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv167_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv166_1[1]
            rms_norm540: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv166_1[0]
            lv169 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_10_self_attn_qkv_proj_q_weight10, model_layers_10_self_attn_qkv_proj_q_scale10, rms_norm540), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1064 = R.call_tir(cls.reshape, (lv169,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1065 = R.call_tir(cls.reshape1, (reshape1064,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1339 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape1065), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1066 = R.call_tir(cls.reshape2, (lv1339,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1067 = R.call_tir(cls.reshape3, (reshape1066,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv170 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_10_self_attn_o_proj_q_weight10, model_layers_10_self_attn_o_proj_q_scale10, reshape1067), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv168_1 = R.call_tir(cls.fuse_add_norm_decode, (lv170, lv167_1, model_layers_10_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv169_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv168_1[1]
            rms_norm541: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv168_1[0]
            lv171 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_10_mlp_gate_up_proj_q_weight10, model_layers_10_mlp_gate_up_proj_q_scale10, rms_norm541), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv43 = R.call_tir(cls.fused_split_silu_multiply, (lv171,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv172 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_10_mlp_down_proj_q_weight10, model_layers_10_mlp_down_proj_q_scale10, lv43), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv170_1 = R.call_tir(cls.fuse_add_norm_decode, (lv172, lv169_1, model_layers_11_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv171_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv170_1[1]
            rms_norm542: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv170_1[0]
            lv173 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_11_self_attn_qkv_proj_q_weight10, model_layers_11_self_attn_qkv_proj_q_scale10, rms_norm542), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1068 = R.call_tir(cls.reshape, (lv173,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1069 = R.call_tir(cls.reshape1, (reshape1068,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1344 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape1069), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1070 = R.call_tir(cls.reshape2, (lv1344,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1071 = R.call_tir(cls.reshape3, (reshape1070,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv174 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_11_self_attn_o_proj_q_weight10, model_layers_11_self_attn_o_proj_q_scale10, reshape1071), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv172_1 = R.call_tir(cls.fuse_add_norm_decode, (lv174, lv171_1, model_layers_11_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv173_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv172_1[1]
            rms_norm543: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv172_1[0]
            lv175 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_11_mlp_gate_up_proj_q_weight10, model_layers_11_mlp_gate_up_proj_q_scale10, rms_norm543), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv44 = R.call_tir(cls.fused_split_silu_multiply, (lv175,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv176 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_11_mlp_down_proj_q_weight10, model_layers_11_mlp_down_proj_q_scale10, lv44), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv174_1 = R.call_tir(cls.fuse_add_norm_decode, (lv176, lv173_1, model_layers_12_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv175_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv174_1[1]
            rms_norm544: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv174_1[0]
            lv177 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_12_self_attn_qkv_proj_q_weight10, model_layers_12_self_attn_qkv_proj_q_scale10, rms_norm544), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1072 = R.call_tir(cls.reshape, (lv177,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1073 = R.call_tir(cls.reshape1, (reshape1072,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1349 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape1073), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1074 = R.call_tir(cls.reshape2, (lv1349,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1075 = R.call_tir(cls.reshape3, (reshape1074,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv178 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_12_self_attn_o_proj_q_weight10, model_layers_12_self_attn_o_proj_q_scale10, reshape1075), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv176_1 = R.call_tir(cls.fuse_add_norm_decode, (lv178, lv175_1, model_layers_12_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv177_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv176_1[1]
            rms_norm545: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv176_1[0]
            lv179 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_12_mlp_gate_up_proj_q_weight10, model_layers_12_mlp_gate_up_proj_q_scale10, rms_norm545), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv45 = R.call_tir(cls.fused_split_silu_multiply, (lv179,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv180 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_12_mlp_down_proj_q_weight10, model_layers_12_mlp_down_proj_q_scale10, lv45), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv178_1 = R.call_tir(cls.fuse_add_norm_decode, (lv180, lv177_1, model_layers_13_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv179_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv178_1[1]
            rms_norm546: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv178_1[0]
            lv181 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_13_self_attn_qkv_proj_q_weight10, model_layers_13_self_attn_qkv_proj_q_scale10, rms_norm546), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1076 = R.call_tir(cls.reshape, (lv181,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1077 = R.call_tir(cls.reshape1, (reshape1076,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1354 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape1077), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1078 = R.call_tir(cls.reshape2, (lv1354,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1079 = R.call_tir(cls.reshape3, (reshape1078,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv182 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_13_self_attn_o_proj_q_weight10, model_layers_13_self_attn_o_proj_q_scale10, reshape1079), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv180_1 = R.call_tir(cls.fuse_add_norm_decode, (lv182, lv179_1, model_layers_13_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv181_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv180_1[1]
            rms_norm547: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv180_1[0]
            lv183 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_13_mlp_gate_up_proj_q_weight10, model_layers_13_mlp_gate_up_proj_q_scale10, rms_norm547), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv46 = R.call_tir(cls.fused_split_silu_multiply, (lv183,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv184 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_13_mlp_down_proj_q_weight10, model_layers_13_mlp_down_proj_q_scale10, lv46), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv182_1 = R.call_tir(cls.fuse_add_norm_decode, (lv184, lv181_1, model_layers_14_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv183_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv182_1[1]
            rms_norm548: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv182_1[0]
            lv185 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_14_self_attn_qkv_proj_q_weight10, model_layers_14_self_attn_qkv_proj_q_scale10, rms_norm548), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1080 = R.call_tir(cls.reshape, (lv185,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1081 = R.call_tir(cls.reshape1, (reshape1080,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1359 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape1081), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1082 = R.call_tir(cls.reshape2, (lv1359,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1083 = R.call_tir(cls.reshape3, (reshape1082,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv186 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_14_self_attn_o_proj_q_weight10, model_layers_14_self_attn_o_proj_q_scale10, reshape1083), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv184_1 = R.call_tir(cls.fuse_add_norm_decode, (lv186, lv183_1, model_layers_14_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv185_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv184_1[1]
            rms_norm549: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv184_1[0]
            lv187 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_14_mlp_gate_up_proj_q_weight10, model_layers_14_mlp_gate_up_proj_q_scale10, rms_norm549), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv47 = R.call_tir(cls.fused_split_silu_multiply, (lv187,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv188 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_14_mlp_down_proj_q_weight10, model_layers_14_mlp_down_proj_q_scale10, lv47), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv186_1 = R.call_tir(cls.fuse_add_norm_decode, (lv188, lv185_1, model_layers_15_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv187_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv186_1[1]
            rms_norm550: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv186_1[0]
            lv189 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_15_self_attn_qkv_proj_q_weight10, model_layers_15_self_attn_qkv_proj_q_scale10, rms_norm550), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1084 = R.call_tir(cls.reshape, (lv189,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1085 = R.call_tir(cls.reshape1, (reshape1084,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1364 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape1085), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1086 = R.call_tir(cls.reshape2, (lv1364,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1087 = R.call_tir(cls.reshape3, (reshape1086,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv190 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_15_self_attn_o_proj_q_weight10, model_layers_15_self_attn_o_proj_q_scale10, reshape1087), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv188_1 = R.call_tir(cls.fuse_add_norm_decode, (lv190, lv187_1, model_layers_15_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv189_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv188_1[1]
            rms_norm551: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv188_1[0]
            lv191 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_15_mlp_gate_up_proj_q_weight10, model_layers_15_mlp_gate_up_proj_q_scale10, rms_norm551), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv48 = R.call_tir(cls.fused_split_silu_multiply, (lv191,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv192 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_15_mlp_down_proj_q_weight10, model_layers_15_mlp_down_proj_q_scale10, lv48), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv190_1 = R.call_tir(cls.fuse_add_norm_decode, (lv192, lv189_1, model_layers_16_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv191_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv190_1[1]
            rms_norm552: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv190_1[0]
            lv193 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_16_self_attn_qkv_proj_q_weight10, model_layers_16_self_attn_qkv_proj_q_scale10, rms_norm552), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1088 = R.call_tir(cls.reshape, (lv193,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1089 = R.call_tir(cls.reshape1, (reshape1088,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1369 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape1089), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1090 = R.call_tir(cls.reshape2, (lv1369,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1091 = R.call_tir(cls.reshape3, (reshape1090,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv194 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_16_self_attn_o_proj_q_weight10, model_layers_16_self_attn_o_proj_q_scale10, reshape1091), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv192_1 = R.call_tir(cls.fuse_add_norm_decode, (lv194, lv191_1, model_layers_16_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv193_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv192_1[1]
            rms_norm553: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv192_1[0]
            lv195 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_16_mlp_gate_up_proj_q_weight10, model_layers_16_mlp_gate_up_proj_q_scale10, rms_norm553), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv49 = R.call_tir(cls.fused_split_silu_multiply, (lv195,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv196 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_16_mlp_down_proj_q_weight10, model_layers_16_mlp_down_proj_q_scale10, lv49), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv194_1 = R.call_tir(cls.fuse_add_norm_decode, (lv196, lv193_1, model_layers_17_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv195_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv194_1[1]
            rms_norm554: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv194_1[0]
            lv197 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_17_self_attn_qkv_proj_q_weight10, model_layers_17_self_attn_qkv_proj_q_scale10, rms_norm554), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1092 = R.call_tir(cls.reshape, (lv197,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1093 = R.call_tir(cls.reshape1, (reshape1092,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1374 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape1093), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1094 = R.call_tir(cls.reshape2, (lv1374,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1095 = R.call_tir(cls.reshape3, (reshape1094,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv198 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_17_self_attn_o_proj_q_weight10, model_layers_17_self_attn_o_proj_q_scale10, reshape1095), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv196_1 = R.call_tir(cls.fuse_add_norm_decode, (lv198, lv195_1, model_layers_17_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv197_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv196_1[1]
            rms_norm555: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv196_1[0]
            lv199 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_17_mlp_gate_up_proj_q_weight10, model_layers_17_mlp_gate_up_proj_q_scale10, rms_norm555), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv50 = R.call_tir(cls.fused_split_silu_multiply, (lv199,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv200 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_17_mlp_down_proj_q_weight10, model_layers_17_mlp_down_proj_q_scale10, lv50), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv198_1 = R.call_tir(cls.fuse_add_norm_decode, (lv200, lv197_1, model_layers_18_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv199_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv198_1[1]
            rms_norm556: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv198_1[0]
            lv201 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_18_self_attn_qkv_proj_q_weight10, model_layers_18_self_attn_qkv_proj_q_scale10, rms_norm556), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1096 = R.call_tir(cls.reshape, (lv201,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1097 = R.call_tir(cls.reshape1, (reshape1096,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1379 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape1097), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1098 = R.call_tir(cls.reshape2, (lv1379,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1099 = R.call_tir(cls.reshape3, (reshape1098,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv202 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_18_self_attn_o_proj_q_weight10, model_layers_18_self_attn_o_proj_q_scale10, reshape1099), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv200_1 = R.call_tir(cls.fuse_add_norm_decode, (lv202, lv199_1, model_layers_18_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv201_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv200_1[1]
            rms_norm557: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv200_1[0]
            lv203 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_18_mlp_gate_up_proj_q_weight10, model_layers_18_mlp_gate_up_proj_q_scale10, rms_norm557), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv51 = R.call_tir(cls.fused_split_silu_multiply, (lv203,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv204 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_18_mlp_down_proj_q_weight10, model_layers_18_mlp_down_proj_q_scale10, lv51), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv202_1 = R.call_tir(cls.fuse_add_norm_decode, (lv204, lv201_1, model_layers_19_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv203_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv202_1[1]
            rms_norm558: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv202_1[0]
            lv205 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_19_self_attn_qkv_proj_q_weight10, model_layers_19_self_attn_qkv_proj_q_scale10, rms_norm558), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1100 = R.call_tir(cls.reshape, (lv205,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1101 = R.call_tir(cls.reshape1, (reshape1100,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1384 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape1101), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1102 = R.call_tir(cls.reshape2, (lv1384,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1103 = R.call_tir(cls.reshape3, (reshape1102,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv206 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_19_self_attn_o_proj_q_weight10, model_layers_19_self_attn_o_proj_q_scale10, reshape1103), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv204_1 = R.call_tir(cls.fuse_add_norm_decode, (lv206, lv203_1, model_layers_19_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv205_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv204_1[1]
            rms_norm559: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv204_1[0]
            lv207 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_19_mlp_gate_up_proj_q_weight10, model_layers_19_mlp_gate_up_proj_q_scale10, rms_norm559), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv52 = R.call_tir(cls.fused_split_silu_multiply, (lv207,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv208 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_19_mlp_down_proj_q_weight10, model_layers_19_mlp_down_proj_q_scale10, lv52), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv206_1 = R.call_tir(cls.fuse_add_norm_decode, (lv208, lv205_1, model_layers_20_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv207_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv206_1[1]
            rms_norm560: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv206_1[0]
            lv209 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_20_self_attn_qkv_proj_q_weight10, model_layers_20_self_attn_qkv_proj_q_scale10, rms_norm560), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1104 = R.call_tir(cls.reshape, (lv209,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1105 = R.call_tir(cls.reshape1, (reshape1104,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1389 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape1105), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1106 = R.call_tir(cls.reshape2, (lv1389,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1107 = R.call_tir(cls.reshape3, (reshape1106,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv210 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_20_self_attn_o_proj_q_weight10, model_layers_20_self_attn_o_proj_q_scale10, reshape1107), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv208_1 = R.call_tir(cls.fuse_add_norm_decode, (lv210, lv207_1, model_layers_20_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv209_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv208_1[1]
            rms_norm561: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv208_1[0]
            lv211 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_20_mlp_gate_up_proj_q_weight10, model_layers_20_mlp_gate_up_proj_q_scale10, rms_norm561), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv53 = R.call_tir(cls.fused_split_silu_multiply, (lv211,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv212 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_20_mlp_down_proj_q_weight10, model_layers_20_mlp_down_proj_q_scale10, lv53), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv210_1 = R.call_tir(cls.fuse_add_norm_decode, (lv212, lv209_1, model_layers_21_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv211_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv210_1[1]
            rms_norm562: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv210_1[0]
            lv213 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_21_self_attn_qkv_proj_q_weight10, model_layers_21_self_attn_qkv_proj_q_scale10, rms_norm562), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1108 = R.call_tir(cls.reshape, (lv213,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1109 = R.call_tir(cls.reshape1, (reshape1108,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1394 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape1109), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1110 = R.call_tir(cls.reshape2, (lv1394,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1111 = R.call_tir(cls.reshape3, (reshape1110,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv214 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_21_self_attn_o_proj_q_weight10, model_layers_21_self_attn_o_proj_q_scale10, reshape1111), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv212_1 = R.call_tir(cls.fuse_add_norm_decode, (lv214, lv211_1, model_layers_21_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv213_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv212_1[1]
            rms_norm563: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv212_1[0]
            lv215 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_21_mlp_gate_up_proj_q_weight10, model_layers_21_mlp_gate_up_proj_q_scale10, rms_norm563), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv54 = R.call_tir(cls.fused_split_silu_multiply, (lv215,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv216 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_21_mlp_down_proj_q_weight10, model_layers_21_mlp_down_proj_q_scale10, lv54), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv214_1 = R.call_tir(cls.fuse_add_norm_decode, (lv216, lv213_1, model_layers_22_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv215_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv214_1[1]
            rms_norm564: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv214_1[0]
            lv217 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_22_self_attn_qkv_proj_q_weight10, model_layers_22_self_attn_qkv_proj_q_scale10, rms_norm564), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1112 = R.call_tir(cls.reshape, (lv217,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1113 = R.call_tir(cls.reshape1, (reshape1112,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1399 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape1113), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1114 = R.call_tir(cls.reshape2, (lv1399,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1115 = R.call_tir(cls.reshape3, (reshape1114,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv218 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_22_self_attn_o_proj_q_weight10, model_layers_22_self_attn_o_proj_q_scale10, reshape1115), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv216_1 = R.call_tir(cls.fuse_add_norm_decode, (lv218, lv215_1, model_layers_22_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv217_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv216_1[1]
            rms_norm565: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv216_1[0]
            lv219 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_22_mlp_gate_up_proj_q_weight10, model_layers_22_mlp_gate_up_proj_q_scale10, rms_norm565), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv55 = R.call_tir(cls.fused_split_silu_multiply, (lv219,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv220 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_22_mlp_down_proj_q_weight10, model_layers_22_mlp_down_proj_q_scale10, lv55), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv218_1 = R.call_tir(cls.fuse_add_norm_decode, (lv220, lv217_1, model_layers_23_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv219_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv218_1[1]
            rms_norm566: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv218_1[0]
            lv221 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_23_self_attn_qkv_proj_q_weight10, model_layers_23_self_attn_qkv_proj_q_scale10, rms_norm566), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1116 = R.call_tir(cls.reshape, (lv221,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1117 = R.call_tir(cls.reshape1, (reshape1116,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1404 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape1117), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1118 = R.call_tir(cls.reshape2, (lv1404,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1119 = R.call_tir(cls.reshape3, (reshape1118,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv222 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_23_self_attn_o_proj_q_weight10, model_layers_23_self_attn_o_proj_q_scale10, reshape1119), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv220_1 = R.call_tir(cls.fuse_add_norm_decode, (lv222, lv219_1, model_layers_23_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv221_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv220_1[1]
            rms_norm567: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv220_1[0]
            lv223 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_23_mlp_gate_up_proj_q_weight10, model_layers_23_mlp_gate_up_proj_q_scale10, rms_norm567), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv56 = R.call_tir(cls.fused_split_silu_multiply, (lv223,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv224 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_23_mlp_down_proj_q_weight10, model_layers_23_mlp_down_proj_q_scale10, lv56), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv222_1 = R.call_tir(cls.fuse_add_norm_decode, (lv224, lv221_1, model_layers_24_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv223_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv222_1[1]
            rms_norm568: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv222_1[0]
            lv225 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_24_self_attn_qkv_proj_q_weight10, model_layers_24_self_attn_qkv_proj_q_scale10, rms_norm568), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1120 = R.call_tir(cls.reshape, (lv225,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1121 = R.call_tir(cls.reshape1, (reshape1120,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1409 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape1121), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1122 = R.call_tir(cls.reshape2, (lv1409,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1123 = R.call_tir(cls.reshape3, (reshape1122,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv226 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_24_self_attn_o_proj_q_weight10, model_layers_24_self_attn_o_proj_q_scale10, reshape1123), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv224_1 = R.call_tir(cls.fuse_add_norm_decode, (lv226, lv223_1, model_layers_24_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv225_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv224_1[1]
            rms_norm569: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv224_1[0]
            lv227 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_24_mlp_gate_up_proj_q_weight10, model_layers_24_mlp_gate_up_proj_q_scale10, rms_norm569), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv57 = R.call_tir(cls.fused_split_silu_multiply, (lv227,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv228 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_24_mlp_down_proj_q_weight10, model_layers_24_mlp_down_proj_q_scale10, lv57), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv226_1 = R.call_tir(cls.fuse_add_norm_decode, (lv228, lv225_1, model_layers_25_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv227_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv226_1[1]
            rms_norm570: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv226_1[0]
            lv229 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_25_self_attn_qkv_proj_q_weight10, model_layers_25_self_attn_qkv_proj_q_scale10, rms_norm570), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1124 = R.call_tir(cls.reshape, (lv229,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1125 = R.call_tir(cls.reshape1, (reshape1124,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1414 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape1125), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1126 = R.call_tir(cls.reshape2, (lv1414,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1127 = R.call_tir(cls.reshape3, (reshape1126,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv230 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_25_self_attn_o_proj_q_weight10, model_layers_25_self_attn_o_proj_q_scale10, reshape1127), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv228_1 = R.call_tir(cls.fuse_add_norm_decode, (lv230, lv227_1, model_layers_25_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv229_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv228_1[1]
            rms_norm571: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv228_1[0]
            lv231 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_25_mlp_gate_up_proj_q_weight10, model_layers_25_mlp_gate_up_proj_q_scale10, rms_norm571), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv58 = R.call_tir(cls.fused_split_silu_multiply, (lv231,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv232 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_25_mlp_down_proj_q_weight10, model_layers_25_mlp_down_proj_q_scale10, lv58), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv230_1 = R.call_tir(cls.fuse_add_norm_decode, (lv232, lv229_1, model_layers_26_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv231_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv230_1[1]
            rms_norm572: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv230_1[0]
            lv233 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_26_self_attn_qkv_proj_q_weight10, model_layers_26_self_attn_qkv_proj_q_scale10, rms_norm572), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1128 = R.call_tir(cls.reshape, (lv233,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1129 = R.call_tir(cls.reshape1, (reshape1128,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1419 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1129), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1130 = R.call_tir(cls.reshape2, (lv1419,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1131 = R.call_tir(cls.reshape3, (reshape1130,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv234 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_26_self_attn_o_proj_q_weight10, model_layers_26_self_attn_o_proj_q_scale10, reshape1131), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv232_1 = R.call_tir(cls.fuse_add_norm_decode, (lv234, lv231_1, model_layers_26_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv233_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv232_1[1]
            rms_norm573: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv232_1[0]
            lv235 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_26_mlp_gate_up_proj_q_weight10, model_layers_26_mlp_gate_up_proj_q_scale10, rms_norm573), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv59 = R.call_tir(cls.fused_split_silu_multiply, (lv235,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv236 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_26_mlp_down_proj_q_weight10, model_layers_26_mlp_down_proj_q_scale10, lv59), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv234_1 = R.call_tir(cls.fuse_add_norm_decode, (lv236, lv233_1, model_layers_27_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv235_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv234_1[1]
            rms_norm574: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv234_1[0]
            lv237 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_27_self_attn_qkv_proj_q_weight10, model_layers_27_self_attn_qkv_proj_q_scale10, rms_norm574), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1132 = R.call_tir(cls.reshape, (lv237,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1133 = R.call_tir(cls.reshape1, (reshape1132,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1424 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1133), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1134 = R.call_tir(cls.reshape2, (lv1424,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1135 = R.call_tir(cls.reshape3, (reshape1134,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv238 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_27_self_attn_o_proj_q_weight10, model_layers_27_self_attn_o_proj_q_scale10, reshape1135), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv236_1 = R.call_tir(cls.fuse_add_norm_decode, (lv238, lv235_1, model_layers_27_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv237_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv236_1[1]
            rms_norm575: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv236_1[0]
            lv239 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_27_mlp_gate_up_proj_q_weight10, model_layers_27_mlp_gate_up_proj_q_scale10, rms_norm575), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv60 = R.call_tir(cls.fused_split_silu_multiply, (lv239,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv240 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_27_mlp_down_proj_q_weight10, model_layers_27_mlp_down_proj_q_scale10, lv60), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv238_1 = R.call_tir(cls.fuse_add_norm_decode, (lv240, lv237_1, model_layers_28_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv239_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv238_1[1]
            rms_norm576: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv238_1[0]
            lv241 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_28_self_attn_qkv_proj_q_weight10, model_layers_28_self_attn_qkv_proj_q_scale10, rms_norm576), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1136 = R.call_tir(cls.reshape, (lv241,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1137 = R.call_tir(cls.reshape1, (reshape1136,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1429 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1137), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1138 = R.call_tir(cls.reshape2, (lv1429,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1139 = R.call_tir(cls.reshape3, (reshape1138,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv242 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_28_self_attn_o_proj_q_weight10, model_layers_28_self_attn_o_proj_q_scale10, reshape1139), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv240_1 = R.call_tir(cls.fuse_add_norm_decode, (lv242, lv239_1, model_layers_28_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv241_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv240_1[1]
            rms_norm577: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv240_1[0]
            lv243 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_28_mlp_gate_up_proj_q_weight10, model_layers_28_mlp_gate_up_proj_q_scale10, rms_norm577), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv61 = R.call_tir(cls.fused_split_silu_multiply, (lv243,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv244 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_28_mlp_down_proj_q_weight10, model_layers_28_mlp_down_proj_q_scale10, lv61), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv242_1 = R.call_tir(cls.fuse_add_norm_decode, (lv244, lv241_1, model_layers_29_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv243_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv242_1[1]
            rms_norm578: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv242_1[0]
            lv245 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_29_self_attn_qkv_proj_q_weight10, model_layers_29_self_attn_qkv_proj_q_scale10, rms_norm578), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1140 = R.call_tir(cls.reshape, (lv245,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1141 = R.call_tir(cls.reshape1, (reshape1140,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1434 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1141), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1142 = R.call_tir(cls.reshape2, (lv1434,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1143 = R.call_tir(cls.reshape3, (reshape1142,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv246 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_29_self_attn_o_proj_q_weight10, model_layers_29_self_attn_o_proj_q_scale10, reshape1143), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv244_1 = R.call_tir(cls.fuse_add_norm_decode, (lv246, lv243_1, model_layers_29_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv245_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv244_1[1]
            rms_norm579: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv244_1[0]
            lv247 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_29_mlp_gate_up_proj_q_weight10, model_layers_29_mlp_gate_up_proj_q_scale10, rms_norm579), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv62 = R.call_tir(cls.fused_split_silu_multiply, (lv247,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv248 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_29_mlp_down_proj_q_weight10, model_layers_29_mlp_down_proj_q_scale10, lv62), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv246_1 = R.call_tir(cls.fuse_add_norm_decode, (lv248, lv245_1, model_layers_30_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv247_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv246_1[1]
            rms_norm580: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv246_1[0]
            lv249 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_30_self_attn_qkv_proj_q_weight10, model_layers_30_self_attn_qkv_proj_q_scale10, rms_norm580), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1144 = R.call_tir(cls.reshape, (lv249,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1145 = R.call_tir(cls.reshape1, (reshape1144,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1439 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1145), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1146 = R.call_tir(cls.reshape2, (lv1439,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1147 = R.call_tir(cls.reshape3, (reshape1146,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv250 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_30_self_attn_o_proj_q_weight10, model_layers_30_self_attn_o_proj_q_scale10, reshape1147), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv248_1 = R.call_tir(cls.fuse_add_norm_decode, (lv250, lv247_1, model_layers_30_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv249_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv248_1[1]
            rms_norm581: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv248_1[0]
            lv251 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_30_mlp_gate_up_proj_q_weight10, model_layers_30_mlp_gate_up_proj_q_scale10, rms_norm581), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv63 = R.call_tir(cls.fused_split_silu_multiply, (lv251,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv252 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_30_mlp_down_proj_q_weight10, model_layers_30_mlp_down_proj_q_scale10, lv63), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv250_1 = R.call_tir(cls.fuse_add_norm_decode, (lv252, lv249_1, model_layers_31_input_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv251_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv250_1[1]
            rms_norm582: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv250_1[0]
            lv253 = R.call_tir(cls.fused_dequantize1_NT_matmul, (model_layers_31_self_attn_qkv_proj_q_weight10, model_layers_31_self_attn_qkv_proj_q_scale10, rms_norm582), out_sinfo=R.Tensor((batch_size, 1, 6144), dtype="float16"))
            reshape1148 = R.call_tir(cls.reshape, (lv253,), out_sinfo=R.Tensor((batch_size, 1, 48, 128), dtype="float16"))
            reshape1149 = R.call_tir(cls.reshape1, (reshape1148,), out_sinfo=R.Tensor((batch_size, 48, 128), dtype="float16"))
            lv1444 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1149), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1150 = R.call_tir(cls.reshape2, (lv1444,), out_sinfo=R.Tensor((batch_size, 1, 32, 128), dtype="float16"))
            reshape1151 = R.call_tir(cls.reshape3, (reshape1150,), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv254 = R.call_tir(cls.fused_dequantize2_NT_matmul1, (model_layers_31_self_attn_o_proj_q_weight10, model_layers_31_self_attn_o_proj_q_scale10, reshape1151), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv252_1 = R.call_tir(cls.fuse_add_norm_decode, (lv254, lv251_1, model_layers_31_post_attention_layernorm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            lv253_1: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv252_1[1]
            rms_norm583: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv252_1[0]
            lv255 = R.call_tir(cls.fused_dequantize3_NT_matmul2, (model_layers_31_mlp_gate_up_proj_q_weight10, model_layers_31_mlp_gate_up_proj_q_scale10, rms_norm583), out_sinfo=R.Tensor((batch_size, 1, 28672), dtype="float16"))
            lv64 = R.call_tir(cls.fused_split_silu_multiply, (lv255,), out_sinfo=R.Tensor((batch_size, 1, 14336), dtype="float16"))
            lv256 = R.call_tir(cls.fused_dequantize4_NT_matmul3, (model_layers_31_mlp_down_proj_q_weight10, model_layers_31_mlp_down_proj_q_scale10, lv64), out_sinfo=R.Tensor((batch_size, 1, 4096), dtype="float16"))
            lv254_1 = R.call_tir(cls.fuse_add_norm_decode, (lv256, lv253_1, model_norm_weight10), out_sinfo=[R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Tensor((batch_size, 1, 4096), dtype="float16")])
            rms_norm584: R.Tensor((batch_size, 1, 4096), dtype="float16") = lv254_1[0]
            gv11: R.Tuple(R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Object) = rms_norm584, paged_kv_cache
            R.output(gv11)
        return gv11

    @R.function
    def batch_prefill(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), logit_positions: R.Tensor(("batch_size",), dtype="int32"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "batch_size", "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 3, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight6: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale6: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm260 = R.call_tir(cls.rms_norm1, (input_embeds, model_layers_0_input_layernorm_weight6), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv257 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_0_self_attn_qkv_proj_q_weight6, model_layers_0_self_attn_qkv_proj_q_scale6, rms_norm260), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape512 = R.call_tir(cls.reshape4, (lv257,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape513 = R.call_tir(cls.reshape5, (reshape512,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv646 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape513), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape514 = R.call_tir(cls.reshape6, (lv646,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape515 = R.call_tir(cls.reshape7, (reshape514,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv258 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_0_self_attn_o_proj_q_weight6, model_layers_0_self_attn_o_proj_q_scale6, reshape515), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv256 = R.call_tir(cls.fuse_add_norm_prefill, (lv258, input_embeds, model_layers_0_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv257_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv256[1]
            rms_norm261: R.Tensor((1, seq_len, 4096), dtype="float16") = lv256[0]
            lv259 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_0_mlp_gate_up_proj_q_weight6, model_layers_0_mlp_gate_up_proj_q_scale6, rms_norm261), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv65 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv259,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv260 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_0_mlp_down_proj_q_weight6, model_layers_0_mlp_down_proj_q_scale6, lv65), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv258_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv260, lv257_1, model_layers_1_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv259_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv258_1[1]
            rms_norm262: R.Tensor((1, seq_len, 4096), dtype="float16") = lv258_1[0]
            lv261 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_1_self_attn_qkv_proj_q_weight6, model_layers_1_self_attn_qkv_proj_q_scale6, rms_norm262), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape516 = R.call_tir(cls.reshape4, (lv261,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape517 = R.call_tir(cls.reshape5, (reshape516,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv651 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape517), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape518 = R.call_tir(cls.reshape6, (lv651,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape519 = R.call_tir(cls.reshape7, (reshape518,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv262 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_1_self_attn_o_proj_q_weight6, model_layers_1_self_attn_o_proj_q_scale6, reshape519), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv260_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv262, lv259_1, model_layers_1_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv261_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv260_1[1]
            rms_norm263: R.Tensor((1, seq_len, 4096), dtype="float16") = lv260_1[0]
            lv263 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_1_mlp_gate_up_proj_q_weight6, model_layers_1_mlp_gate_up_proj_q_scale6, rms_norm263), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv66 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv263,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv264 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_1_mlp_down_proj_q_weight6, model_layers_1_mlp_down_proj_q_scale6, lv66), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv262_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv264, lv261_1, model_layers_2_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv263_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv262_1[1]
            rms_norm264: R.Tensor((1, seq_len, 4096), dtype="float16") = lv262_1[0]
            lv265 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_2_self_attn_qkv_proj_q_weight6, model_layers_2_self_attn_qkv_proj_q_scale6, rms_norm264), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape520 = R.call_tir(cls.reshape4, (lv265,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape521 = R.call_tir(cls.reshape5, (reshape520,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv656 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape521), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape522 = R.call_tir(cls.reshape6, (lv656,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape523 = R.call_tir(cls.reshape7, (reshape522,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv266 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_2_self_attn_o_proj_q_weight6, model_layers_2_self_attn_o_proj_q_scale6, reshape523), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv264_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv266, lv263_1, model_layers_2_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv265_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv264_1[1]
            rms_norm265: R.Tensor((1, seq_len, 4096), dtype="float16") = lv264_1[0]
            lv267 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_2_mlp_gate_up_proj_q_weight6, model_layers_2_mlp_gate_up_proj_q_scale6, rms_norm265), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv67 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv267,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv268 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_2_mlp_down_proj_q_weight6, model_layers_2_mlp_down_proj_q_scale6, lv67), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv266_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv268, lv265_1, model_layers_3_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv267_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv266_1[1]
            rms_norm266: R.Tensor((1, seq_len, 4096), dtype="float16") = lv266_1[0]
            lv269 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_3_self_attn_qkv_proj_q_weight6, model_layers_3_self_attn_qkv_proj_q_scale6, rms_norm266), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape524 = R.call_tir(cls.reshape4, (lv269,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape525 = R.call_tir(cls.reshape5, (reshape524,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv661 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape525), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape526 = R.call_tir(cls.reshape6, (lv661,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape527 = R.call_tir(cls.reshape7, (reshape526,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv270 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_3_self_attn_o_proj_q_weight6, model_layers_3_self_attn_o_proj_q_scale6, reshape527), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv268_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv270, lv267_1, model_layers_3_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv269_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv268_1[1]
            rms_norm267: R.Tensor((1, seq_len, 4096), dtype="float16") = lv268_1[0]
            lv271 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_3_mlp_gate_up_proj_q_weight6, model_layers_3_mlp_gate_up_proj_q_scale6, rms_norm267), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv68 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv271,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv272 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_3_mlp_down_proj_q_weight6, model_layers_3_mlp_down_proj_q_scale6, lv68), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv270_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv272, lv269_1, model_layers_4_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv271_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv270_1[1]
            rms_norm268: R.Tensor((1, seq_len, 4096), dtype="float16") = lv270_1[0]
            lv273 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_4_self_attn_qkv_proj_q_weight6, model_layers_4_self_attn_qkv_proj_q_scale6, rms_norm268), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape528 = R.call_tir(cls.reshape4, (lv273,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape529 = R.call_tir(cls.reshape5, (reshape528,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv666 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape529), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape530 = R.call_tir(cls.reshape6, (lv666,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape531 = R.call_tir(cls.reshape7, (reshape530,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv274 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_4_self_attn_o_proj_q_weight6, model_layers_4_self_attn_o_proj_q_scale6, reshape531), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv272_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv274, lv271_1, model_layers_4_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv273_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv272_1[1]
            rms_norm269: R.Tensor((1, seq_len, 4096), dtype="float16") = lv272_1[0]
            lv275 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_4_mlp_gate_up_proj_q_weight6, model_layers_4_mlp_gate_up_proj_q_scale6, rms_norm269), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv69 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv275,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv276 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_4_mlp_down_proj_q_weight6, model_layers_4_mlp_down_proj_q_scale6, lv69), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv274_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv276, lv273_1, model_layers_5_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv275_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv274_1[1]
            rms_norm270: R.Tensor((1, seq_len, 4096), dtype="float16") = lv274_1[0]
            lv277 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_5_self_attn_qkv_proj_q_weight6, model_layers_5_self_attn_qkv_proj_q_scale6, rms_norm270), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape532 = R.call_tir(cls.reshape4, (lv277,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape533 = R.call_tir(cls.reshape5, (reshape532,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv671 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape533), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape534 = R.call_tir(cls.reshape6, (lv671,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape535 = R.call_tir(cls.reshape7, (reshape534,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv278 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_5_self_attn_o_proj_q_weight6, model_layers_5_self_attn_o_proj_q_scale6, reshape535), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv276_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv278, lv275_1, model_layers_5_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv277_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv276_1[1]
            rms_norm271: R.Tensor((1, seq_len, 4096), dtype="float16") = lv276_1[0]
            lv279 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_5_mlp_gate_up_proj_q_weight6, model_layers_5_mlp_gate_up_proj_q_scale6, rms_norm271), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv70 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv279,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv280 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_5_mlp_down_proj_q_weight6, model_layers_5_mlp_down_proj_q_scale6, lv70), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv278_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv280, lv277_1, model_layers_6_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv279_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv278_1[1]
            rms_norm272: R.Tensor((1, seq_len, 4096), dtype="float16") = lv278_1[0]
            lv281 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_6_self_attn_qkv_proj_q_weight6, model_layers_6_self_attn_qkv_proj_q_scale6, rms_norm272), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape536 = R.call_tir(cls.reshape4, (lv281,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape537 = R.call_tir(cls.reshape5, (reshape536,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv676 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape537), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape538 = R.call_tir(cls.reshape6, (lv676,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape539 = R.call_tir(cls.reshape7, (reshape538,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv282 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_6_self_attn_o_proj_q_weight6, model_layers_6_self_attn_o_proj_q_scale6, reshape539), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv280_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv282, lv279_1, model_layers_6_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv281_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv280_1[1]
            rms_norm273: R.Tensor((1, seq_len, 4096), dtype="float16") = lv280_1[0]
            lv283 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_6_mlp_gate_up_proj_q_weight6, model_layers_6_mlp_gate_up_proj_q_scale6, rms_norm273), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv71 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv283,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv284 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_6_mlp_down_proj_q_weight6, model_layers_6_mlp_down_proj_q_scale6, lv71), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv282_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv284, lv281_1, model_layers_7_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv283_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv282_1[1]
            rms_norm274: R.Tensor((1, seq_len, 4096), dtype="float16") = lv282_1[0]
            lv285 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_7_self_attn_qkv_proj_q_weight6, model_layers_7_self_attn_qkv_proj_q_scale6, rms_norm274), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape540 = R.call_tir(cls.reshape4, (lv285,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape541 = R.call_tir(cls.reshape5, (reshape540,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv681 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape541), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape542 = R.call_tir(cls.reshape6, (lv681,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape543 = R.call_tir(cls.reshape7, (reshape542,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv286 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_7_self_attn_o_proj_q_weight6, model_layers_7_self_attn_o_proj_q_scale6, reshape543), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv284_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv286, lv283_1, model_layers_7_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv285_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv284_1[1]
            rms_norm275: R.Tensor((1, seq_len, 4096), dtype="float16") = lv284_1[0]
            lv287 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_7_mlp_gate_up_proj_q_weight6, model_layers_7_mlp_gate_up_proj_q_scale6, rms_norm275), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv72 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv287,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv288 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_7_mlp_down_proj_q_weight6, model_layers_7_mlp_down_proj_q_scale6, lv72), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv286_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv288, lv285_1, model_layers_8_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv287_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv286_1[1]
            rms_norm276: R.Tensor((1, seq_len, 4096), dtype="float16") = lv286_1[0]
            lv289 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_8_self_attn_qkv_proj_q_weight6, model_layers_8_self_attn_qkv_proj_q_scale6, rms_norm276), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape544 = R.call_tir(cls.reshape4, (lv289,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape545 = R.call_tir(cls.reshape5, (reshape544,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv686 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape545), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape546 = R.call_tir(cls.reshape6, (lv686,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape547 = R.call_tir(cls.reshape7, (reshape546,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv290 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_8_self_attn_o_proj_q_weight6, model_layers_8_self_attn_o_proj_q_scale6, reshape547), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv288_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv290, lv287_1, model_layers_8_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv289_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv288_1[1]
            rms_norm277: R.Tensor((1, seq_len, 4096), dtype="float16") = lv288_1[0]
            lv291 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_8_mlp_gate_up_proj_q_weight6, model_layers_8_mlp_gate_up_proj_q_scale6, rms_norm277), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv73 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv291,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv292 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_8_mlp_down_proj_q_weight6, model_layers_8_mlp_down_proj_q_scale6, lv73), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv290_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv292, lv289_1, model_layers_9_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv291_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv290_1[1]
            rms_norm278: R.Tensor((1, seq_len, 4096), dtype="float16") = lv290_1[0]
            lv293 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_9_self_attn_qkv_proj_q_weight6, model_layers_9_self_attn_qkv_proj_q_scale6, rms_norm278), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape548 = R.call_tir(cls.reshape4, (lv293,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape549 = R.call_tir(cls.reshape5, (reshape548,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv691 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape549), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape550 = R.call_tir(cls.reshape6, (lv691,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape551 = R.call_tir(cls.reshape7, (reshape550,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv294 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_9_self_attn_o_proj_q_weight6, model_layers_9_self_attn_o_proj_q_scale6, reshape551), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv292_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv294, lv291_1, model_layers_9_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv293_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv292_1[1]
            rms_norm279: R.Tensor((1, seq_len, 4096), dtype="float16") = lv292_1[0]
            lv295 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_9_mlp_gate_up_proj_q_weight6, model_layers_9_mlp_gate_up_proj_q_scale6, rms_norm279), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv74 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv295,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv296 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_9_mlp_down_proj_q_weight6, model_layers_9_mlp_down_proj_q_scale6, lv74), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv294_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv296, lv293_1, model_layers_10_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv295_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv294_1[1]
            rms_norm280: R.Tensor((1, seq_len, 4096), dtype="float16") = lv294_1[0]
            lv297 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_10_self_attn_qkv_proj_q_weight6, model_layers_10_self_attn_qkv_proj_q_scale6, rms_norm280), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape552 = R.call_tir(cls.reshape4, (lv297,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape553 = R.call_tir(cls.reshape5, (reshape552,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv696 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape553), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape554 = R.call_tir(cls.reshape6, (lv696,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape555 = R.call_tir(cls.reshape7, (reshape554,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv298 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_10_self_attn_o_proj_q_weight6, model_layers_10_self_attn_o_proj_q_scale6, reshape555), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv296_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv298, lv295_1, model_layers_10_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv297_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv296_1[1]
            rms_norm281: R.Tensor((1, seq_len, 4096), dtype="float16") = lv296_1[0]
            lv299 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_10_mlp_gate_up_proj_q_weight6, model_layers_10_mlp_gate_up_proj_q_scale6, rms_norm281), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv75 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv299,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv300 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_10_mlp_down_proj_q_weight6, model_layers_10_mlp_down_proj_q_scale6, lv75), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv298_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv300, lv297_1, model_layers_11_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv299_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv298_1[1]
            rms_norm282: R.Tensor((1, seq_len, 4096), dtype="float16") = lv298_1[0]
            lv301 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_11_self_attn_qkv_proj_q_weight6, model_layers_11_self_attn_qkv_proj_q_scale6, rms_norm282), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape556 = R.call_tir(cls.reshape4, (lv301,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape557 = R.call_tir(cls.reshape5, (reshape556,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv701 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape557), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape558 = R.call_tir(cls.reshape6, (lv701,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape559 = R.call_tir(cls.reshape7, (reshape558,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv302 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_11_self_attn_o_proj_q_weight6, model_layers_11_self_attn_o_proj_q_scale6, reshape559), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv300_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv302, lv299_1, model_layers_11_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv301_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv300_1[1]
            rms_norm283: R.Tensor((1, seq_len, 4096), dtype="float16") = lv300_1[0]
            lv303 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_11_mlp_gate_up_proj_q_weight6, model_layers_11_mlp_gate_up_proj_q_scale6, rms_norm283), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv76 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv303,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv304 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_11_mlp_down_proj_q_weight6, model_layers_11_mlp_down_proj_q_scale6, lv76), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv302_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv304, lv301_1, model_layers_12_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv303_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv302_1[1]
            rms_norm284: R.Tensor((1, seq_len, 4096), dtype="float16") = lv302_1[0]
            lv305 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_12_self_attn_qkv_proj_q_weight6, model_layers_12_self_attn_qkv_proj_q_scale6, rms_norm284), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape560 = R.call_tir(cls.reshape4, (lv305,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape561 = R.call_tir(cls.reshape5, (reshape560,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv706 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape561), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape562 = R.call_tir(cls.reshape6, (lv706,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape563 = R.call_tir(cls.reshape7, (reshape562,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv306 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_12_self_attn_o_proj_q_weight6, model_layers_12_self_attn_o_proj_q_scale6, reshape563), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv304_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv306, lv303_1, model_layers_12_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv305_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv304_1[1]
            rms_norm285: R.Tensor((1, seq_len, 4096), dtype="float16") = lv304_1[0]
            lv307 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_12_mlp_gate_up_proj_q_weight6, model_layers_12_mlp_gate_up_proj_q_scale6, rms_norm285), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv77 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv307,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv308 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_12_mlp_down_proj_q_weight6, model_layers_12_mlp_down_proj_q_scale6, lv77), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv306_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv308, lv305_1, model_layers_13_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv307_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv306_1[1]
            rms_norm286: R.Tensor((1, seq_len, 4096), dtype="float16") = lv306_1[0]
            lv309 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_13_self_attn_qkv_proj_q_weight6, model_layers_13_self_attn_qkv_proj_q_scale6, rms_norm286), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape564 = R.call_tir(cls.reshape4, (lv309,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape565 = R.call_tir(cls.reshape5, (reshape564,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv711 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape565), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape566 = R.call_tir(cls.reshape6, (lv711,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape567 = R.call_tir(cls.reshape7, (reshape566,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv310 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_13_self_attn_o_proj_q_weight6, model_layers_13_self_attn_o_proj_q_scale6, reshape567), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv308_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv310, lv307_1, model_layers_13_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv309_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv308_1[1]
            rms_norm287: R.Tensor((1, seq_len, 4096), dtype="float16") = lv308_1[0]
            lv311 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_13_mlp_gate_up_proj_q_weight6, model_layers_13_mlp_gate_up_proj_q_scale6, rms_norm287), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv78 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv311,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv312 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_13_mlp_down_proj_q_weight6, model_layers_13_mlp_down_proj_q_scale6, lv78), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv310_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv312, lv309_1, model_layers_14_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv311_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv310_1[1]
            rms_norm288: R.Tensor((1, seq_len, 4096), dtype="float16") = lv310_1[0]
            lv313 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_14_self_attn_qkv_proj_q_weight6, model_layers_14_self_attn_qkv_proj_q_scale6, rms_norm288), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape568 = R.call_tir(cls.reshape4, (lv313,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape569 = R.call_tir(cls.reshape5, (reshape568,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv716 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape569), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape570 = R.call_tir(cls.reshape6, (lv716,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape571 = R.call_tir(cls.reshape7, (reshape570,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv314 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_14_self_attn_o_proj_q_weight6, model_layers_14_self_attn_o_proj_q_scale6, reshape571), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv312_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv314, lv311_1, model_layers_14_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv313_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv312_1[1]
            rms_norm289: R.Tensor((1, seq_len, 4096), dtype="float16") = lv312_1[0]
            lv315 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_14_mlp_gate_up_proj_q_weight6, model_layers_14_mlp_gate_up_proj_q_scale6, rms_norm289), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv79 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv315,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv316 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_14_mlp_down_proj_q_weight6, model_layers_14_mlp_down_proj_q_scale6, lv79), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv314_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv316, lv313_1, model_layers_15_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv315_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv314_1[1]
            rms_norm290: R.Tensor((1, seq_len, 4096), dtype="float16") = lv314_1[0]
            lv317 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_15_self_attn_qkv_proj_q_weight6, model_layers_15_self_attn_qkv_proj_q_scale6, rms_norm290), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape572 = R.call_tir(cls.reshape4, (lv317,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape573 = R.call_tir(cls.reshape5, (reshape572,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv721 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape573), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape574 = R.call_tir(cls.reshape6, (lv721,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape575 = R.call_tir(cls.reshape7, (reshape574,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv318 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_15_self_attn_o_proj_q_weight6, model_layers_15_self_attn_o_proj_q_scale6, reshape575), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv316_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv318, lv315_1, model_layers_15_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv317_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv316_1[1]
            rms_norm291: R.Tensor((1, seq_len, 4096), dtype="float16") = lv316_1[0]
            lv319 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_15_mlp_gate_up_proj_q_weight6, model_layers_15_mlp_gate_up_proj_q_scale6, rms_norm291), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv80 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv319,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv320 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_15_mlp_down_proj_q_weight6, model_layers_15_mlp_down_proj_q_scale6, lv80), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv318_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv320, lv317_1, model_layers_16_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv319_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv318_1[1]
            rms_norm292: R.Tensor((1, seq_len, 4096), dtype="float16") = lv318_1[0]
            lv321 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_16_self_attn_qkv_proj_q_weight6, model_layers_16_self_attn_qkv_proj_q_scale6, rms_norm292), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape576 = R.call_tir(cls.reshape4, (lv321,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape577 = R.call_tir(cls.reshape5, (reshape576,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv726 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape577), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape578 = R.call_tir(cls.reshape6, (lv726,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape579 = R.call_tir(cls.reshape7, (reshape578,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv322 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_16_self_attn_o_proj_q_weight6, model_layers_16_self_attn_o_proj_q_scale6, reshape579), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv320_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv322, lv319_1, model_layers_16_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv321_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv320_1[1]
            rms_norm293: R.Tensor((1, seq_len, 4096), dtype="float16") = lv320_1[0]
            lv323 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_16_mlp_gate_up_proj_q_weight6, model_layers_16_mlp_gate_up_proj_q_scale6, rms_norm293), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv81 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv323,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv324 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_16_mlp_down_proj_q_weight6, model_layers_16_mlp_down_proj_q_scale6, lv81), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv322_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv324, lv321_1, model_layers_17_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv323_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv322_1[1]
            rms_norm294: R.Tensor((1, seq_len, 4096), dtype="float16") = lv322_1[0]
            lv325 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_17_self_attn_qkv_proj_q_weight6, model_layers_17_self_attn_qkv_proj_q_scale6, rms_norm294), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape580 = R.call_tir(cls.reshape4, (lv325,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape581 = R.call_tir(cls.reshape5, (reshape580,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv731 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape581), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape582 = R.call_tir(cls.reshape6, (lv731,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape583 = R.call_tir(cls.reshape7, (reshape582,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv326 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_17_self_attn_o_proj_q_weight6, model_layers_17_self_attn_o_proj_q_scale6, reshape583), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv324_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv326, lv323_1, model_layers_17_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv325_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv324_1[1]
            rms_norm295: R.Tensor((1, seq_len, 4096), dtype="float16") = lv324_1[0]
            lv327 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_17_mlp_gate_up_proj_q_weight6, model_layers_17_mlp_gate_up_proj_q_scale6, rms_norm295), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv82 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv327,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv328 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_17_mlp_down_proj_q_weight6, model_layers_17_mlp_down_proj_q_scale6, lv82), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv326_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv328, lv325_1, model_layers_18_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv327_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv326_1[1]
            rms_norm296: R.Tensor((1, seq_len, 4096), dtype="float16") = lv326_1[0]
            lv329 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_18_self_attn_qkv_proj_q_weight6, model_layers_18_self_attn_qkv_proj_q_scale6, rms_norm296), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape584 = R.call_tir(cls.reshape4, (lv329,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape585 = R.call_tir(cls.reshape5, (reshape584,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv736 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape585), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape586 = R.call_tir(cls.reshape6, (lv736,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape587 = R.call_tir(cls.reshape7, (reshape586,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv330 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_18_self_attn_o_proj_q_weight6, model_layers_18_self_attn_o_proj_q_scale6, reshape587), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv328_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv330, lv327_1, model_layers_18_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv329_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv328_1[1]
            rms_norm297: R.Tensor((1, seq_len, 4096), dtype="float16") = lv328_1[0]
            lv331 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_18_mlp_gate_up_proj_q_weight6, model_layers_18_mlp_gate_up_proj_q_scale6, rms_norm297), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv83 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv331,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv332 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_18_mlp_down_proj_q_weight6, model_layers_18_mlp_down_proj_q_scale6, lv83), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv330_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv332, lv329_1, model_layers_19_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv331_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv330_1[1]
            rms_norm298: R.Tensor((1, seq_len, 4096), dtype="float16") = lv330_1[0]
            lv333 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_19_self_attn_qkv_proj_q_weight6, model_layers_19_self_attn_qkv_proj_q_scale6, rms_norm298), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape588 = R.call_tir(cls.reshape4, (lv333,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape589 = R.call_tir(cls.reshape5, (reshape588,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv741 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape589), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape590 = R.call_tir(cls.reshape6, (lv741,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape591 = R.call_tir(cls.reshape7, (reshape590,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv334 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_19_self_attn_o_proj_q_weight6, model_layers_19_self_attn_o_proj_q_scale6, reshape591), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv332_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv334, lv331_1, model_layers_19_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv333_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv332_1[1]
            rms_norm299: R.Tensor((1, seq_len, 4096), dtype="float16") = lv332_1[0]
            lv335 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_19_mlp_gate_up_proj_q_weight6, model_layers_19_mlp_gate_up_proj_q_scale6, rms_norm299), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv84 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv335,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv336 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_19_mlp_down_proj_q_weight6, model_layers_19_mlp_down_proj_q_scale6, lv84), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv334_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv336, lv333_1, model_layers_20_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv335_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv334_1[1]
            rms_norm300: R.Tensor((1, seq_len, 4096), dtype="float16") = lv334_1[0]
            lv337 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_20_self_attn_qkv_proj_q_weight6, model_layers_20_self_attn_qkv_proj_q_scale6, rms_norm300), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape592 = R.call_tir(cls.reshape4, (lv337,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape593 = R.call_tir(cls.reshape5, (reshape592,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv746 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape593), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape594 = R.call_tir(cls.reshape6, (lv746,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape595 = R.call_tir(cls.reshape7, (reshape594,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv338 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_20_self_attn_o_proj_q_weight6, model_layers_20_self_attn_o_proj_q_scale6, reshape595), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv336_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv338, lv335_1, model_layers_20_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv337_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv336_1[1]
            rms_norm301: R.Tensor((1, seq_len, 4096), dtype="float16") = lv336_1[0]
            lv339 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_20_mlp_gate_up_proj_q_weight6, model_layers_20_mlp_gate_up_proj_q_scale6, rms_norm301), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv85 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv339,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv340 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_20_mlp_down_proj_q_weight6, model_layers_20_mlp_down_proj_q_scale6, lv85), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv338_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv340, lv337_1, model_layers_21_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv339_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv338_1[1]
            rms_norm302: R.Tensor((1, seq_len, 4096), dtype="float16") = lv338_1[0]
            lv341 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_21_self_attn_qkv_proj_q_weight6, model_layers_21_self_attn_qkv_proj_q_scale6, rms_norm302), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape596 = R.call_tir(cls.reshape4, (lv341,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape597 = R.call_tir(cls.reshape5, (reshape596,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv751 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape597), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape598 = R.call_tir(cls.reshape6, (lv751,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape599 = R.call_tir(cls.reshape7, (reshape598,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv342 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_21_self_attn_o_proj_q_weight6, model_layers_21_self_attn_o_proj_q_scale6, reshape599), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv340_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv342, lv339_1, model_layers_21_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv341_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv340_1[1]
            rms_norm303: R.Tensor((1, seq_len, 4096), dtype="float16") = lv340_1[0]
            lv343 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_21_mlp_gate_up_proj_q_weight6, model_layers_21_mlp_gate_up_proj_q_scale6, rms_norm303), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv86 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv343,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv344 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_21_mlp_down_proj_q_weight6, model_layers_21_mlp_down_proj_q_scale6, lv86), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv342_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv344, lv341_1, model_layers_22_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv343_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv342_1[1]
            rms_norm304: R.Tensor((1, seq_len, 4096), dtype="float16") = lv342_1[0]
            lv345 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_22_self_attn_qkv_proj_q_weight6, model_layers_22_self_attn_qkv_proj_q_scale6, rms_norm304), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape600 = R.call_tir(cls.reshape4, (lv345,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape601 = R.call_tir(cls.reshape5, (reshape600,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv756 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape601), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape602 = R.call_tir(cls.reshape6, (lv756,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape603 = R.call_tir(cls.reshape7, (reshape602,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv346 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_22_self_attn_o_proj_q_weight6, model_layers_22_self_attn_o_proj_q_scale6, reshape603), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv344_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv346, lv343_1, model_layers_22_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv345_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv344_1[1]
            rms_norm305: R.Tensor((1, seq_len, 4096), dtype="float16") = lv344_1[0]
            lv347 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_22_mlp_gate_up_proj_q_weight6, model_layers_22_mlp_gate_up_proj_q_scale6, rms_norm305), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv87 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv347,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv348 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_22_mlp_down_proj_q_weight6, model_layers_22_mlp_down_proj_q_scale6, lv87), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv346_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv348, lv345_1, model_layers_23_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv347_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv346_1[1]
            rms_norm306: R.Tensor((1, seq_len, 4096), dtype="float16") = lv346_1[0]
            lv349 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_23_self_attn_qkv_proj_q_weight6, model_layers_23_self_attn_qkv_proj_q_scale6, rms_norm306), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape604 = R.call_tir(cls.reshape4, (lv349,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape605 = R.call_tir(cls.reshape5, (reshape604,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv761 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape605), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape606 = R.call_tir(cls.reshape6, (lv761,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape607 = R.call_tir(cls.reshape7, (reshape606,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv350 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_23_self_attn_o_proj_q_weight6, model_layers_23_self_attn_o_proj_q_scale6, reshape607), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv348_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv350, lv347_1, model_layers_23_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv349_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv348_1[1]
            rms_norm307: R.Tensor((1, seq_len, 4096), dtype="float16") = lv348_1[0]
            lv351 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_23_mlp_gate_up_proj_q_weight6, model_layers_23_mlp_gate_up_proj_q_scale6, rms_norm307), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv88 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv351,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv352 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_23_mlp_down_proj_q_weight6, model_layers_23_mlp_down_proj_q_scale6, lv88), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv350_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv352, lv349_1, model_layers_24_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv351_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv350_1[1]
            rms_norm308: R.Tensor((1, seq_len, 4096), dtype="float16") = lv350_1[0]
            lv353 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_24_self_attn_qkv_proj_q_weight6, model_layers_24_self_attn_qkv_proj_q_scale6, rms_norm308), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape608 = R.call_tir(cls.reshape4, (lv353,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape609 = R.call_tir(cls.reshape5, (reshape608,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv766 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape609), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape610 = R.call_tir(cls.reshape6, (lv766,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape611 = R.call_tir(cls.reshape7, (reshape610,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv354 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_24_self_attn_o_proj_q_weight6, model_layers_24_self_attn_o_proj_q_scale6, reshape611), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv352_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv354, lv351_1, model_layers_24_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv353_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv352_1[1]
            rms_norm309: R.Tensor((1, seq_len, 4096), dtype="float16") = lv352_1[0]
            lv355 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_24_mlp_gate_up_proj_q_weight6, model_layers_24_mlp_gate_up_proj_q_scale6, rms_norm309), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv89 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv355,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv356 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_24_mlp_down_proj_q_weight6, model_layers_24_mlp_down_proj_q_scale6, lv89), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv354_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv356, lv353_1, model_layers_25_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv355_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv354_1[1]
            rms_norm310: R.Tensor((1, seq_len, 4096), dtype="float16") = lv354_1[0]
            lv357 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_25_self_attn_qkv_proj_q_weight6, model_layers_25_self_attn_qkv_proj_q_scale6, rms_norm310), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape612 = R.call_tir(cls.reshape4, (lv357,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape613 = R.call_tir(cls.reshape5, (reshape612,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv771 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape613), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape614 = R.call_tir(cls.reshape6, (lv771,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape615 = R.call_tir(cls.reshape7, (reshape614,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv358 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_25_self_attn_o_proj_q_weight6, model_layers_25_self_attn_o_proj_q_scale6, reshape615), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv356_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv358, lv355_1, model_layers_25_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv357_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv356_1[1]
            rms_norm311: R.Tensor((1, seq_len, 4096), dtype="float16") = lv356_1[0]
            lv359 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_25_mlp_gate_up_proj_q_weight6, model_layers_25_mlp_gate_up_proj_q_scale6, rms_norm311), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv90 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv359,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv360 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_25_mlp_down_proj_q_weight6, model_layers_25_mlp_down_proj_q_scale6, lv90), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv358_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv360, lv357_1, model_layers_26_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv359_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv358_1[1]
            rms_norm312: R.Tensor((1, seq_len, 4096), dtype="float16") = lv358_1[0]
            lv361 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_26_self_attn_qkv_proj_q_weight6, model_layers_26_self_attn_qkv_proj_q_scale6, rms_norm312), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape616 = R.call_tir(cls.reshape4, (lv361,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape617 = R.call_tir(cls.reshape5, (reshape616,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv776 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape617), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape618 = R.call_tir(cls.reshape6, (lv776,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape619 = R.call_tir(cls.reshape7, (reshape618,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv362 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_26_self_attn_o_proj_q_weight6, model_layers_26_self_attn_o_proj_q_scale6, reshape619), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv360_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv362, lv359_1, model_layers_26_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv361_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv360_1[1]
            rms_norm313: R.Tensor((1, seq_len, 4096), dtype="float16") = lv360_1[0]
            lv363 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_26_mlp_gate_up_proj_q_weight6, model_layers_26_mlp_gate_up_proj_q_scale6, rms_norm313), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv91 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv363,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv364 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_26_mlp_down_proj_q_weight6, model_layers_26_mlp_down_proj_q_scale6, lv91), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv362_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv364, lv361_1, model_layers_27_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv363_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv362_1[1]
            rms_norm314: R.Tensor((1, seq_len, 4096), dtype="float16") = lv362_1[0]
            lv365 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_27_self_attn_qkv_proj_q_weight6, model_layers_27_self_attn_qkv_proj_q_scale6, rms_norm314), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape620 = R.call_tir(cls.reshape4, (lv365,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape621 = R.call_tir(cls.reshape5, (reshape620,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv781 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape621), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape622 = R.call_tir(cls.reshape6, (lv781,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape623 = R.call_tir(cls.reshape7, (reshape622,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv366 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_27_self_attn_o_proj_q_weight6, model_layers_27_self_attn_o_proj_q_scale6, reshape623), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv364_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv366, lv363_1, model_layers_27_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv365_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv364_1[1]
            rms_norm315: R.Tensor((1, seq_len, 4096), dtype="float16") = lv364_1[0]
            lv367 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_27_mlp_gate_up_proj_q_weight6, model_layers_27_mlp_gate_up_proj_q_scale6, rms_norm315), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv92 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv367,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv368 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_27_mlp_down_proj_q_weight6, model_layers_27_mlp_down_proj_q_scale6, lv92), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv366_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv368, lv365_1, model_layers_28_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv367_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv366_1[1]
            rms_norm316: R.Tensor((1, seq_len, 4096), dtype="float16") = lv366_1[0]
            lv369 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_28_self_attn_qkv_proj_q_weight6, model_layers_28_self_attn_qkv_proj_q_scale6, rms_norm316), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape624 = R.call_tir(cls.reshape4, (lv369,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape625 = R.call_tir(cls.reshape5, (reshape624,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv786 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape625), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape626 = R.call_tir(cls.reshape6, (lv786,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape627 = R.call_tir(cls.reshape7, (reshape626,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv370 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_28_self_attn_o_proj_q_weight6, model_layers_28_self_attn_o_proj_q_scale6, reshape627), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv368_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv370, lv367_1, model_layers_28_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv369_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv368_1[1]
            rms_norm317: R.Tensor((1, seq_len, 4096), dtype="float16") = lv368_1[0]
            lv371 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_28_mlp_gate_up_proj_q_weight6, model_layers_28_mlp_gate_up_proj_q_scale6, rms_norm317), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv93 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv371,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv372 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_28_mlp_down_proj_q_weight6, model_layers_28_mlp_down_proj_q_scale6, lv93), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv370_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv372, lv369_1, model_layers_29_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv371_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv370_1[1]
            rms_norm318: R.Tensor((1, seq_len, 4096), dtype="float16") = lv370_1[0]
            lv373 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_29_self_attn_qkv_proj_q_weight6, model_layers_29_self_attn_qkv_proj_q_scale6, rms_norm318), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape628 = R.call_tir(cls.reshape4, (lv373,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape629 = R.call_tir(cls.reshape5, (reshape628,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv791 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape629), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape630 = R.call_tir(cls.reshape6, (lv791,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape631 = R.call_tir(cls.reshape7, (reshape630,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv374 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_29_self_attn_o_proj_q_weight6, model_layers_29_self_attn_o_proj_q_scale6, reshape631), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv372_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv374, lv371_1, model_layers_29_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv373_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv372_1[1]
            rms_norm319: R.Tensor((1, seq_len, 4096), dtype="float16") = lv372_1[0]
            lv375 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_29_mlp_gate_up_proj_q_weight6, model_layers_29_mlp_gate_up_proj_q_scale6, rms_norm319), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv94 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv375,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv376 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_29_mlp_down_proj_q_weight6, model_layers_29_mlp_down_proj_q_scale6, lv94), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv374_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv376, lv373_1, model_layers_30_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv375_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv374_1[1]
            rms_norm320: R.Tensor((1, seq_len, 4096), dtype="float16") = lv374_1[0]
            lv377 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_30_self_attn_qkv_proj_q_weight6, model_layers_30_self_attn_qkv_proj_q_scale6, rms_norm320), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape632 = R.call_tir(cls.reshape4, (lv377,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape633 = R.call_tir(cls.reshape5, (reshape632,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv796 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape633), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape634 = R.call_tir(cls.reshape6, (lv796,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape635 = R.call_tir(cls.reshape7, (reshape634,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv378 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_30_self_attn_o_proj_q_weight6, model_layers_30_self_attn_o_proj_q_scale6, reshape635), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv376_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv378, lv375_1, model_layers_30_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv377_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv376_1[1]
            rms_norm321: R.Tensor((1, seq_len, 4096), dtype="float16") = lv376_1[0]
            lv379 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_30_mlp_gate_up_proj_q_weight6, model_layers_30_mlp_gate_up_proj_q_scale6, rms_norm321), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv95 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv379,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv380 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_30_mlp_down_proj_q_weight6, model_layers_30_mlp_down_proj_q_scale6, lv95), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv378_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv380, lv377_1, model_layers_31_input_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv379_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv378_1[1]
            rms_norm322: R.Tensor((1, seq_len, 4096), dtype="float16") = lv378_1[0]
            lv381 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_31_self_attn_qkv_proj_q_weight6, model_layers_31_self_attn_qkv_proj_q_scale6, rms_norm322), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape636 = R.call_tir(cls.reshape4, (lv381,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape637 = R.call_tir(cls.reshape5, (reshape636,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv801 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape637), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape638 = R.call_tir(cls.reshape6, (lv801,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape639 = R.call_tir(cls.reshape7, (reshape638,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv382 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_31_self_attn_o_proj_q_weight6, model_layers_31_self_attn_o_proj_q_scale6, reshape639), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv380_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv382, lv379_1, model_layers_31_post_attention_layernorm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv381_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv380_1[1]
            rms_norm323: R.Tensor((1, seq_len, 4096), dtype="float16") = lv380_1[0]
            lv383 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_31_mlp_gate_up_proj_q_weight6, model_layers_31_mlp_gate_up_proj_q_scale6, rms_norm323), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv96 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv383,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv384 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_31_mlp_down_proj_q_weight6, model_layers_31_mlp_down_proj_q_scale6, lv96), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv382_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv384, lv381_1, model_norm_weight6), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm324: R.Tensor((1, seq_len, 4096), dtype="float16") = lv382_1[0]
            take2 = R.call_tir(cls.take, (rms_norm324, logit_positions), out_sinfo=R.Tensor((1, batch_size, 4096), dtype="float16"))
            lv385 = R.call_tir(cls.fused_dequantize_fused_NT_matmul9_cast1, (lm_head_q_weight6, lm_head_q_scale6, take2), out_sinfo=R.Tensor((1, batch_size, vocab_size), dtype="float32"))
            gv7: R.Tuple(R.Tensor((1, batch_size, vocab_size), dtype="float32"), R.Object) = lv385, paged_kv_cache
            R.output(gv7)
        return gv7

    @R.function
    def batch_prefill_to_last_hidden_states(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm455 = R.call_tir(cls.rms_norm1, (input_embeds, model_layers_0_input_layernorm_weight9), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv386 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_0_self_attn_qkv_proj_q_weight9, model_layers_0_self_attn_qkv_proj_q_scale9, rms_norm455), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape896 = R.call_tir(cls.reshape4, (lv386,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape897 = R.call_tir(cls.reshape5, (reshape896,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1129 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape897), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape898 = R.call_tir(cls.reshape6, (lv1129,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape899 = R.call_tir(cls.reshape7, (reshape898,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv387 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_0_self_attn_o_proj_q_weight9, model_layers_0_self_attn_o_proj_q_scale9, reshape899), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv384 = R.call_tir(cls.fuse_add_norm_prefill, (lv387, input_embeds, model_layers_0_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv385: R.Tensor((1, seq_len, 4096), dtype="float16") = lv384[1]
            rms_norm456: R.Tensor((1, seq_len, 4096), dtype="float16") = lv384[0]
            lv388 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_0_mlp_gate_up_proj_q_weight9, model_layers_0_mlp_gate_up_proj_q_scale9, rms_norm456), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv98 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv388,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv389 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_0_mlp_down_proj_q_weight9, model_layers_0_mlp_down_proj_q_scale9, lv98), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv386_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv389, lv385, model_layers_1_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv387_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv386_1[1]
            rms_norm457: R.Tensor((1, seq_len, 4096), dtype="float16") = lv386_1[0]
            lv390 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_1_self_attn_qkv_proj_q_weight9, model_layers_1_self_attn_qkv_proj_q_scale9, rms_norm457), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape900 = R.call_tir(cls.reshape4, (lv390,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape901 = R.call_tir(cls.reshape5, (reshape900,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1134 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape901), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape902 = R.call_tir(cls.reshape6, (lv1134,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape903 = R.call_tir(cls.reshape7, (reshape902,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv391 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_1_self_attn_o_proj_q_weight9, model_layers_1_self_attn_o_proj_q_scale9, reshape903), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv388_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv391, lv387_1, model_layers_1_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv389_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv388_1[1]
            rms_norm458: R.Tensor((1, seq_len, 4096), dtype="float16") = lv388_1[0]
            lv392 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_1_mlp_gate_up_proj_q_weight9, model_layers_1_mlp_gate_up_proj_q_scale9, rms_norm458), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv99 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv392,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv393 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_1_mlp_down_proj_q_weight9, model_layers_1_mlp_down_proj_q_scale9, lv99), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv390_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv393, lv389_1, model_layers_2_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv391_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv390_1[1]
            rms_norm459: R.Tensor((1, seq_len, 4096), dtype="float16") = lv390_1[0]
            lv394 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_2_self_attn_qkv_proj_q_weight9, model_layers_2_self_attn_qkv_proj_q_scale9, rms_norm459), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape904 = R.call_tir(cls.reshape4, (lv394,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape905 = R.call_tir(cls.reshape5, (reshape904,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1139 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape905), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape906 = R.call_tir(cls.reshape6, (lv1139,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape907 = R.call_tir(cls.reshape7, (reshape906,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv395 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_2_self_attn_o_proj_q_weight9, model_layers_2_self_attn_o_proj_q_scale9, reshape907), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv392_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv395, lv391_1, model_layers_2_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv393_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv392_1[1]
            rms_norm460: R.Tensor((1, seq_len, 4096), dtype="float16") = lv392_1[0]
            lv396 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_2_mlp_gate_up_proj_q_weight9, model_layers_2_mlp_gate_up_proj_q_scale9, rms_norm460), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv100 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv396,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv397 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_2_mlp_down_proj_q_weight9, model_layers_2_mlp_down_proj_q_scale9, lv100), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv394_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv397, lv393_1, model_layers_3_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv395_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv394_1[1]
            rms_norm461: R.Tensor((1, seq_len, 4096), dtype="float16") = lv394_1[0]
            lv398 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_3_self_attn_qkv_proj_q_weight9, model_layers_3_self_attn_qkv_proj_q_scale9, rms_norm461), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape908 = R.call_tir(cls.reshape4, (lv398,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape909 = R.call_tir(cls.reshape5, (reshape908,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1144 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape909), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape910 = R.call_tir(cls.reshape6, (lv1144,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape911 = R.call_tir(cls.reshape7, (reshape910,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv399 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_3_self_attn_o_proj_q_weight9, model_layers_3_self_attn_o_proj_q_scale9, reshape911), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv396_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv399, lv395_1, model_layers_3_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv397_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv396_1[1]
            rms_norm462: R.Tensor((1, seq_len, 4096), dtype="float16") = lv396_1[0]
            lv400 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_3_mlp_gate_up_proj_q_weight9, model_layers_3_mlp_gate_up_proj_q_scale9, rms_norm462), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv101 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv400,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv401 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_3_mlp_down_proj_q_weight9, model_layers_3_mlp_down_proj_q_scale9, lv101), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv398_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv401, lv397_1, model_layers_4_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv399_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv398_1[1]
            rms_norm463: R.Tensor((1, seq_len, 4096), dtype="float16") = lv398_1[0]
            lv402 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_4_self_attn_qkv_proj_q_weight9, model_layers_4_self_attn_qkv_proj_q_scale9, rms_norm463), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape912 = R.call_tir(cls.reshape4, (lv402,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape913 = R.call_tir(cls.reshape5, (reshape912,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1149 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape913), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape914 = R.call_tir(cls.reshape6, (lv1149,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape915 = R.call_tir(cls.reshape7, (reshape914,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv403 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_4_self_attn_o_proj_q_weight9, model_layers_4_self_attn_o_proj_q_scale9, reshape915), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv400_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv403, lv399_1, model_layers_4_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv401_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv400_1[1]
            rms_norm464: R.Tensor((1, seq_len, 4096), dtype="float16") = lv400_1[0]
            lv404 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_4_mlp_gate_up_proj_q_weight9, model_layers_4_mlp_gate_up_proj_q_scale9, rms_norm464), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv102 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv404,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv405 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_4_mlp_down_proj_q_weight9, model_layers_4_mlp_down_proj_q_scale9, lv102), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv402_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv405, lv401_1, model_layers_5_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv403_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv402_1[1]
            rms_norm465: R.Tensor((1, seq_len, 4096), dtype="float16") = lv402_1[0]
            lv406 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_5_self_attn_qkv_proj_q_weight9, model_layers_5_self_attn_qkv_proj_q_scale9, rms_norm465), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape916 = R.call_tir(cls.reshape4, (lv406,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape917 = R.call_tir(cls.reshape5, (reshape916,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1154 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape917), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape918 = R.call_tir(cls.reshape6, (lv1154,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape919 = R.call_tir(cls.reshape7, (reshape918,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv407 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_5_self_attn_o_proj_q_weight9, model_layers_5_self_attn_o_proj_q_scale9, reshape919), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv404_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv407, lv403_1, model_layers_5_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv405_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv404_1[1]
            rms_norm466: R.Tensor((1, seq_len, 4096), dtype="float16") = lv404_1[0]
            lv408 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_5_mlp_gate_up_proj_q_weight9, model_layers_5_mlp_gate_up_proj_q_scale9, rms_norm466), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv103 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv408,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv409 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_5_mlp_down_proj_q_weight9, model_layers_5_mlp_down_proj_q_scale9, lv103), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv406_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv409, lv405_1, model_layers_6_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv407_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv406_1[1]
            rms_norm467: R.Tensor((1, seq_len, 4096), dtype="float16") = lv406_1[0]
            lv410 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_6_self_attn_qkv_proj_q_weight9, model_layers_6_self_attn_qkv_proj_q_scale9, rms_norm467), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape920 = R.call_tir(cls.reshape4, (lv410,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape921 = R.call_tir(cls.reshape5, (reshape920,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1159 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape921), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape922 = R.call_tir(cls.reshape6, (lv1159,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape923 = R.call_tir(cls.reshape7, (reshape922,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv411 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_6_self_attn_o_proj_q_weight9, model_layers_6_self_attn_o_proj_q_scale9, reshape923), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv408_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv411, lv407_1, model_layers_6_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv409_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv408_1[1]
            rms_norm468: R.Tensor((1, seq_len, 4096), dtype="float16") = lv408_1[0]
            lv412 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_6_mlp_gate_up_proj_q_weight9, model_layers_6_mlp_gate_up_proj_q_scale9, rms_norm468), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv104 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv412,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv413 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_6_mlp_down_proj_q_weight9, model_layers_6_mlp_down_proj_q_scale9, lv104), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv410_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv413, lv409_1, model_layers_7_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv411_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv410_1[1]
            rms_norm469: R.Tensor((1, seq_len, 4096), dtype="float16") = lv410_1[0]
            lv414 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_7_self_attn_qkv_proj_q_weight9, model_layers_7_self_attn_qkv_proj_q_scale9, rms_norm469), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape924 = R.call_tir(cls.reshape4, (lv414,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape925 = R.call_tir(cls.reshape5, (reshape924,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1164 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape925), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape926 = R.call_tir(cls.reshape6, (lv1164,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape927 = R.call_tir(cls.reshape7, (reshape926,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv415 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_7_self_attn_o_proj_q_weight9, model_layers_7_self_attn_o_proj_q_scale9, reshape927), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv412_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv415, lv411_1, model_layers_7_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv413_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv412_1[1]
            rms_norm470: R.Tensor((1, seq_len, 4096), dtype="float16") = lv412_1[0]
            lv416 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_7_mlp_gate_up_proj_q_weight9, model_layers_7_mlp_gate_up_proj_q_scale9, rms_norm470), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv105 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv416,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv417 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_7_mlp_down_proj_q_weight9, model_layers_7_mlp_down_proj_q_scale9, lv105), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv414_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv417, lv413_1, model_layers_8_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv415_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv414_1[1]
            rms_norm471: R.Tensor((1, seq_len, 4096), dtype="float16") = lv414_1[0]
            lv418 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_8_self_attn_qkv_proj_q_weight9, model_layers_8_self_attn_qkv_proj_q_scale9, rms_norm471), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape928 = R.call_tir(cls.reshape4, (lv418,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape929 = R.call_tir(cls.reshape5, (reshape928,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1169 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape929), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape930 = R.call_tir(cls.reshape6, (lv1169,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape931 = R.call_tir(cls.reshape7, (reshape930,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv419 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_8_self_attn_o_proj_q_weight9, model_layers_8_self_attn_o_proj_q_scale9, reshape931), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv416_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv419, lv415_1, model_layers_8_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv417_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv416_1[1]
            rms_norm472: R.Tensor((1, seq_len, 4096), dtype="float16") = lv416_1[0]
            lv420 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_8_mlp_gate_up_proj_q_weight9, model_layers_8_mlp_gate_up_proj_q_scale9, rms_norm472), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv106 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv420,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv421 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_8_mlp_down_proj_q_weight9, model_layers_8_mlp_down_proj_q_scale9, lv106), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv418_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv421, lv417_1, model_layers_9_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv419_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv418_1[1]
            rms_norm473: R.Tensor((1, seq_len, 4096), dtype="float16") = lv418_1[0]
            lv422 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_9_self_attn_qkv_proj_q_weight9, model_layers_9_self_attn_qkv_proj_q_scale9, rms_norm473), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape932 = R.call_tir(cls.reshape4, (lv422,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape933 = R.call_tir(cls.reshape5, (reshape932,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1174 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape933), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape934 = R.call_tir(cls.reshape6, (lv1174,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape935 = R.call_tir(cls.reshape7, (reshape934,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv423 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_9_self_attn_o_proj_q_weight9, model_layers_9_self_attn_o_proj_q_scale9, reshape935), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv420_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv423, lv419_1, model_layers_9_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv421_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv420_1[1]
            rms_norm474: R.Tensor((1, seq_len, 4096), dtype="float16") = lv420_1[0]
            lv424 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_9_mlp_gate_up_proj_q_weight9, model_layers_9_mlp_gate_up_proj_q_scale9, rms_norm474), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv107 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv424,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv425 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_9_mlp_down_proj_q_weight9, model_layers_9_mlp_down_proj_q_scale9, lv107), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv422_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv425, lv421_1, model_layers_10_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv423_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv422_1[1]
            rms_norm475: R.Tensor((1, seq_len, 4096), dtype="float16") = lv422_1[0]
            lv426 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_10_self_attn_qkv_proj_q_weight9, model_layers_10_self_attn_qkv_proj_q_scale9, rms_norm475), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape936 = R.call_tir(cls.reshape4, (lv426,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape937 = R.call_tir(cls.reshape5, (reshape936,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1179 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape937), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape938 = R.call_tir(cls.reshape6, (lv1179,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape939 = R.call_tir(cls.reshape7, (reshape938,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv427 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_10_self_attn_o_proj_q_weight9, model_layers_10_self_attn_o_proj_q_scale9, reshape939), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv424_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv427, lv423_1, model_layers_10_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv425_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv424_1[1]
            rms_norm476: R.Tensor((1, seq_len, 4096), dtype="float16") = lv424_1[0]
            lv428 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_10_mlp_gate_up_proj_q_weight9, model_layers_10_mlp_gate_up_proj_q_scale9, rms_norm476), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv108 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv428,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv429 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_10_mlp_down_proj_q_weight9, model_layers_10_mlp_down_proj_q_scale9, lv108), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv426_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv429, lv425_1, model_layers_11_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv427_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv426_1[1]
            rms_norm477: R.Tensor((1, seq_len, 4096), dtype="float16") = lv426_1[0]
            lv430 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_11_self_attn_qkv_proj_q_weight9, model_layers_11_self_attn_qkv_proj_q_scale9, rms_norm477), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape940 = R.call_tir(cls.reshape4, (lv430,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape941 = R.call_tir(cls.reshape5, (reshape940,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1184 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape941), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape942 = R.call_tir(cls.reshape6, (lv1184,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape943 = R.call_tir(cls.reshape7, (reshape942,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv431 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_11_self_attn_o_proj_q_weight9, model_layers_11_self_attn_o_proj_q_scale9, reshape943), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv428_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv431, lv427_1, model_layers_11_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv429_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv428_1[1]
            rms_norm478: R.Tensor((1, seq_len, 4096), dtype="float16") = lv428_1[0]
            lv432 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_11_mlp_gate_up_proj_q_weight9, model_layers_11_mlp_gate_up_proj_q_scale9, rms_norm478), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv109 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv432,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv433 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_11_mlp_down_proj_q_weight9, model_layers_11_mlp_down_proj_q_scale9, lv109), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv430_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv433, lv429_1, model_layers_12_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv431_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv430_1[1]
            rms_norm479: R.Tensor((1, seq_len, 4096), dtype="float16") = lv430_1[0]
            lv434 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_12_self_attn_qkv_proj_q_weight9, model_layers_12_self_attn_qkv_proj_q_scale9, rms_norm479), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape944 = R.call_tir(cls.reshape4, (lv434,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape945 = R.call_tir(cls.reshape5, (reshape944,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1189 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape945), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape946 = R.call_tir(cls.reshape6, (lv1189,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape947 = R.call_tir(cls.reshape7, (reshape946,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv435 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_12_self_attn_o_proj_q_weight9, model_layers_12_self_attn_o_proj_q_scale9, reshape947), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv432_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv435, lv431_1, model_layers_12_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv433_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv432_1[1]
            rms_norm480: R.Tensor((1, seq_len, 4096), dtype="float16") = lv432_1[0]
            lv436 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_12_mlp_gate_up_proj_q_weight9, model_layers_12_mlp_gate_up_proj_q_scale9, rms_norm480), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv110 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv436,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv437 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_12_mlp_down_proj_q_weight9, model_layers_12_mlp_down_proj_q_scale9, lv110), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv434_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv437, lv433_1, model_layers_13_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv435_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv434_1[1]
            rms_norm481: R.Tensor((1, seq_len, 4096), dtype="float16") = lv434_1[0]
            lv438 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_13_self_attn_qkv_proj_q_weight9, model_layers_13_self_attn_qkv_proj_q_scale9, rms_norm481), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape948 = R.call_tir(cls.reshape4, (lv438,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape949 = R.call_tir(cls.reshape5, (reshape948,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1194 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape949), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape950 = R.call_tir(cls.reshape6, (lv1194,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape951 = R.call_tir(cls.reshape7, (reshape950,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv439 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_13_self_attn_o_proj_q_weight9, model_layers_13_self_attn_o_proj_q_scale9, reshape951), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv436_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv439, lv435_1, model_layers_13_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv437_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv436_1[1]
            rms_norm482: R.Tensor((1, seq_len, 4096), dtype="float16") = lv436_1[0]
            lv440 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_13_mlp_gate_up_proj_q_weight9, model_layers_13_mlp_gate_up_proj_q_scale9, rms_norm482), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv111 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv440,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv441 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_13_mlp_down_proj_q_weight9, model_layers_13_mlp_down_proj_q_scale9, lv111), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv438_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv441, lv437_1, model_layers_14_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv439_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv438_1[1]
            rms_norm483: R.Tensor((1, seq_len, 4096), dtype="float16") = lv438_1[0]
            lv442 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_14_self_attn_qkv_proj_q_weight9, model_layers_14_self_attn_qkv_proj_q_scale9, rms_norm483), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape952 = R.call_tir(cls.reshape4, (lv442,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape953 = R.call_tir(cls.reshape5, (reshape952,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1199 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape953), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape954 = R.call_tir(cls.reshape6, (lv1199,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape955 = R.call_tir(cls.reshape7, (reshape954,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv443 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_14_self_attn_o_proj_q_weight9, model_layers_14_self_attn_o_proj_q_scale9, reshape955), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv440_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv443, lv439_1, model_layers_14_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv441_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv440_1[1]
            rms_norm484: R.Tensor((1, seq_len, 4096), dtype="float16") = lv440_1[0]
            lv444 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_14_mlp_gate_up_proj_q_weight9, model_layers_14_mlp_gate_up_proj_q_scale9, rms_norm484), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv112 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv444,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv445 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_14_mlp_down_proj_q_weight9, model_layers_14_mlp_down_proj_q_scale9, lv112), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv442_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv445, lv441_1, model_layers_15_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv443_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv442_1[1]
            rms_norm485: R.Tensor((1, seq_len, 4096), dtype="float16") = lv442_1[0]
            lv446 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_15_self_attn_qkv_proj_q_weight9, model_layers_15_self_attn_qkv_proj_q_scale9, rms_norm485), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape956 = R.call_tir(cls.reshape4, (lv446,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape957 = R.call_tir(cls.reshape5, (reshape956,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1204 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape957), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape958 = R.call_tir(cls.reshape6, (lv1204,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape959 = R.call_tir(cls.reshape7, (reshape958,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv447 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_15_self_attn_o_proj_q_weight9, model_layers_15_self_attn_o_proj_q_scale9, reshape959), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv444_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv447, lv443_1, model_layers_15_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv445_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv444_1[1]
            rms_norm486: R.Tensor((1, seq_len, 4096), dtype="float16") = lv444_1[0]
            lv448 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_15_mlp_gate_up_proj_q_weight9, model_layers_15_mlp_gate_up_proj_q_scale9, rms_norm486), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv113 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv448,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv449 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_15_mlp_down_proj_q_weight9, model_layers_15_mlp_down_proj_q_scale9, lv113), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv446_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv449, lv445_1, model_layers_16_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv447_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv446_1[1]
            rms_norm487: R.Tensor((1, seq_len, 4096), dtype="float16") = lv446_1[0]
            lv450 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_16_self_attn_qkv_proj_q_weight9, model_layers_16_self_attn_qkv_proj_q_scale9, rms_norm487), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape960 = R.call_tir(cls.reshape4, (lv450,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape961 = R.call_tir(cls.reshape5, (reshape960,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1209 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape961), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape962 = R.call_tir(cls.reshape6, (lv1209,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape963 = R.call_tir(cls.reshape7, (reshape962,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv451 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_16_self_attn_o_proj_q_weight9, model_layers_16_self_attn_o_proj_q_scale9, reshape963), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv448_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv451, lv447_1, model_layers_16_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv449_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv448_1[1]
            rms_norm488: R.Tensor((1, seq_len, 4096), dtype="float16") = lv448_1[0]
            lv452 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_16_mlp_gate_up_proj_q_weight9, model_layers_16_mlp_gate_up_proj_q_scale9, rms_norm488), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv114 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv452,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv453 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_16_mlp_down_proj_q_weight9, model_layers_16_mlp_down_proj_q_scale9, lv114), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv450_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv453, lv449_1, model_layers_17_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv451_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv450_1[1]
            rms_norm489: R.Tensor((1, seq_len, 4096), dtype="float16") = lv450_1[0]
            lv454 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_17_self_attn_qkv_proj_q_weight9, model_layers_17_self_attn_qkv_proj_q_scale9, rms_norm489), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape964 = R.call_tir(cls.reshape4, (lv454,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape965 = R.call_tir(cls.reshape5, (reshape964,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1214 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape965), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape966 = R.call_tir(cls.reshape6, (lv1214,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape967 = R.call_tir(cls.reshape7, (reshape966,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv455 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_17_self_attn_o_proj_q_weight9, model_layers_17_self_attn_o_proj_q_scale9, reshape967), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv452_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv455, lv451_1, model_layers_17_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv453_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv452_1[1]
            rms_norm490: R.Tensor((1, seq_len, 4096), dtype="float16") = lv452_1[0]
            lv456 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_17_mlp_gate_up_proj_q_weight9, model_layers_17_mlp_gate_up_proj_q_scale9, rms_norm490), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv115 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv456,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv457 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_17_mlp_down_proj_q_weight9, model_layers_17_mlp_down_proj_q_scale9, lv115), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv454_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv457, lv453_1, model_layers_18_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv455_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv454_1[1]
            rms_norm491: R.Tensor((1, seq_len, 4096), dtype="float16") = lv454_1[0]
            lv458 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_18_self_attn_qkv_proj_q_weight9, model_layers_18_self_attn_qkv_proj_q_scale9, rms_norm491), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape968 = R.call_tir(cls.reshape4, (lv458,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape969 = R.call_tir(cls.reshape5, (reshape968,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1219 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape969), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape970 = R.call_tir(cls.reshape6, (lv1219,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape971 = R.call_tir(cls.reshape7, (reshape970,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv459 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_18_self_attn_o_proj_q_weight9, model_layers_18_self_attn_o_proj_q_scale9, reshape971), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv456_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv459, lv455_1, model_layers_18_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv457_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv456_1[1]
            rms_norm492: R.Tensor((1, seq_len, 4096), dtype="float16") = lv456_1[0]
            lv460 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_18_mlp_gate_up_proj_q_weight9, model_layers_18_mlp_gate_up_proj_q_scale9, rms_norm492), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv116 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv460,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv461 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_18_mlp_down_proj_q_weight9, model_layers_18_mlp_down_proj_q_scale9, lv116), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv458_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv461, lv457_1, model_layers_19_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv459_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv458_1[1]
            rms_norm493: R.Tensor((1, seq_len, 4096), dtype="float16") = lv458_1[0]
            lv462 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_19_self_attn_qkv_proj_q_weight9, model_layers_19_self_attn_qkv_proj_q_scale9, rms_norm493), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape972 = R.call_tir(cls.reshape4, (lv462,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape973 = R.call_tir(cls.reshape5, (reshape972,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1224 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape973), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape974 = R.call_tir(cls.reshape6, (lv1224,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape975 = R.call_tir(cls.reshape7, (reshape974,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv463 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_19_self_attn_o_proj_q_weight9, model_layers_19_self_attn_o_proj_q_scale9, reshape975), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv460_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv463, lv459_1, model_layers_19_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv461_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv460_1[1]
            rms_norm494: R.Tensor((1, seq_len, 4096), dtype="float16") = lv460_1[0]
            lv464 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_19_mlp_gate_up_proj_q_weight9, model_layers_19_mlp_gate_up_proj_q_scale9, rms_norm494), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv117 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv464,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv465 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_19_mlp_down_proj_q_weight9, model_layers_19_mlp_down_proj_q_scale9, lv117), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv462_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv465, lv461_1, model_layers_20_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv463_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv462_1[1]
            rms_norm495: R.Tensor((1, seq_len, 4096), dtype="float16") = lv462_1[0]
            lv466 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_20_self_attn_qkv_proj_q_weight9, model_layers_20_self_attn_qkv_proj_q_scale9, rms_norm495), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape976 = R.call_tir(cls.reshape4, (lv466,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape977 = R.call_tir(cls.reshape5, (reshape976,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1229 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape977), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape978 = R.call_tir(cls.reshape6, (lv1229,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape979 = R.call_tir(cls.reshape7, (reshape978,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv467 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_20_self_attn_o_proj_q_weight9, model_layers_20_self_attn_o_proj_q_scale9, reshape979), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv464_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv467, lv463_1, model_layers_20_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv465_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv464_1[1]
            rms_norm496: R.Tensor((1, seq_len, 4096), dtype="float16") = lv464_1[0]
            lv468 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_20_mlp_gate_up_proj_q_weight9, model_layers_20_mlp_gate_up_proj_q_scale9, rms_norm496), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv118 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv468,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv469 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_20_mlp_down_proj_q_weight9, model_layers_20_mlp_down_proj_q_scale9, lv118), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv466_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv469, lv465_1, model_layers_21_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv467_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv466_1[1]
            rms_norm497: R.Tensor((1, seq_len, 4096), dtype="float16") = lv466_1[0]
            lv470 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_21_self_attn_qkv_proj_q_weight9, model_layers_21_self_attn_qkv_proj_q_scale9, rms_norm497), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape980 = R.call_tir(cls.reshape4, (lv470,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape981 = R.call_tir(cls.reshape5, (reshape980,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1234 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape981), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape982 = R.call_tir(cls.reshape6, (lv1234,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape983 = R.call_tir(cls.reshape7, (reshape982,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv471 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_21_self_attn_o_proj_q_weight9, model_layers_21_self_attn_o_proj_q_scale9, reshape983), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv468_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv471, lv467_1, model_layers_21_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv469_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv468_1[1]
            rms_norm498: R.Tensor((1, seq_len, 4096), dtype="float16") = lv468_1[0]
            lv472 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_21_mlp_gate_up_proj_q_weight9, model_layers_21_mlp_gate_up_proj_q_scale9, rms_norm498), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv119 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv472,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv473 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_21_mlp_down_proj_q_weight9, model_layers_21_mlp_down_proj_q_scale9, lv119), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv470_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv473, lv469_1, model_layers_22_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv471_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv470_1[1]
            rms_norm499: R.Tensor((1, seq_len, 4096), dtype="float16") = lv470_1[0]
            lv474 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_22_self_attn_qkv_proj_q_weight9, model_layers_22_self_attn_qkv_proj_q_scale9, rms_norm499), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape984 = R.call_tir(cls.reshape4, (lv474,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape985 = R.call_tir(cls.reshape5, (reshape984,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1239 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape985), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape986 = R.call_tir(cls.reshape6, (lv1239,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape987 = R.call_tir(cls.reshape7, (reshape986,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv475 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_22_self_attn_o_proj_q_weight9, model_layers_22_self_attn_o_proj_q_scale9, reshape987), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv472_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv475, lv471_1, model_layers_22_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv473_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv472_1[1]
            rms_norm500: R.Tensor((1, seq_len, 4096), dtype="float16") = lv472_1[0]
            lv476 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_22_mlp_gate_up_proj_q_weight9, model_layers_22_mlp_gate_up_proj_q_scale9, rms_norm500), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv120 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv476,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv477 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_22_mlp_down_proj_q_weight9, model_layers_22_mlp_down_proj_q_scale9, lv120), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv474_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv477, lv473_1, model_layers_23_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv475_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv474_1[1]
            rms_norm501: R.Tensor((1, seq_len, 4096), dtype="float16") = lv474_1[0]
            lv478 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_23_self_attn_qkv_proj_q_weight9, model_layers_23_self_attn_qkv_proj_q_scale9, rms_norm501), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape988 = R.call_tir(cls.reshape4, (lv478,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape989 = R.call_tir(cls.reshape5, (reshape988,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1244 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape989), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape990 = R.call_tir(cls.reshape6, (lv1244,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape991 = R.call_tir(cls.reshape7, (reshape990,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv479 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_23_self_attn_o_proj_q_weight9, model_layers_23_self_attn_o_proj_q_scale9, reshape991), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv476_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv479, lv475_1, model_layers_23_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv477_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv476_1[1]
            rms_norm502: R.Tensor((1, seq_len, 4096), dtype="float16") = lv476_1[0]
            lv480 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_23_mlp_gate_up_proj_q_weight9, model_layers_23_mlp_gate_up_proj_q_scale9, rms_norm502), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv121 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv480,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv481 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_23_mlp_down_proj_q_weight9, model_layers_23_mlp_down_proj_q_scale9, lv121), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv478_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv481, lv477_1, model_layers_24_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv479_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv478_1[1]
            rms_norm503: R.Tensor((1, seq_len, 4096), dtype="float16") = lv478_1[0]
            lv482 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_24_self_attn_qkv_proj_q_weight9, model_layers_24_self_attn_qkv_proj_q_scale9, rms_norm503), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape992 = R.call_tir(cls.reshape4, (lv482,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape993 = R.call_tir(cls.reshape5, (reshape992,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1249 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape993), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape994 = R.call_tir(cls.reshape6, (lv1249,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape995 = R.call_tir(cls.reshape7, (reshape994,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv483 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_24_self_attn_o_proj_q_weight9, model_layers_24_self_attn_o_proj_q_scale9, reshape995), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv480_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv483, lv479_1, model_layers_24_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv481_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv480_1[1]
            rms_norm504: R.Tensor((1, seq_len, 4096), dtype="float16") = lv480_1[0]
            lv484 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_24_mlp_gate_up_proj_q_weight9, model_layers_24_mlp_gate_up_proj_q_scale9, rms_norm504), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv122 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv484,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv485 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_24_mlp_down_proj_q_weight9, model_layers_24_mlp_down_proj_q_scale9, lv122), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv482_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv485, lv481_1, model_layers_25_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv483_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv482_1[1]
            rms_norm505: R.Tensor((1, seq_len, 4096), dtype="float16") = lv482_1[0]
            lv486 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_25_self_attn_qkv_proj_q_weight9, model_layers_25_self_attn_qkv_proj_q_scale9, rms_norm505), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape996 = R.call_tir(cls.reshape4, (lv486,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape997 = R.call_tir(cls.reshape5, (reshape996,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1254 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape997), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape998 = R.call_tir(cls.reshape6, (lv1254,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape999 = R.call_tir(cls.reshape7, (reshape998,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv487 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_25_self_attn_o_proj_q_weight9, model_layers_25_self_attn_o_proj_q_scale9, reshape999), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv484_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv487, lv483_1, model_layers_25_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv485_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv484_1[1]
            rms_norm506: R.Tensor((1, seq_len, 4096), dtype="float16") = lv484_1[0]
            lv488 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_25_mlp_gate_up_proj_q_weight9, model_layers_25_mlp_gate_up_proj_q_scale9, rms_norm506), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv123 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv488,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv489 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_25_mlp_down_proj_q_weight9, model_layers_25_mlp_down_proj_q_scale9, lv123), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv486_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv489, lv485_1, model_layers_26_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv487_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv486_1[1]
            rms_norm507: R.Tensor((1, seq_len, 4096), dtype="float16") = lv486_1[0]
            lv490 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_26_self_attn_qkv_proj_q_weight9, model_layers_26_self_attn_qkv_proj_q_scale9, rms_norm507), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1000 = R.call_tir(cls.reshape4, (lv490,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1001 = R.call_tir(cls.reshape5, (reshape1000,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1259 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1001), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1002 = R.call_tir(cls.reshape6, (lv1259,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1003 = R.call_tir(cls.reshape7, (reshape1002,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv491 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_26_self_attn_o_proj_q_weight9, model_layers_26_self_attn_o_proj_q_scale9, reshape1003), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv488_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv491, lv487_1, model_layers_26_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv489_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv488_1[1]
            rms_norm508: R.Tensor((1, seq_len, 4096), dtype="float16") = lv488_1[0]
            lv492 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_26_mlp_gate_up_proj_q_weight9, model_layers_26_mlp_gate_up_proj_q_scale9, rms_norm508), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv124 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv492,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv493 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_26_mlp_down_proj_q_weight9, model_layers_26_mlp_down_proj_q_scale9, lv124), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv490_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv493, lv489_1, model_layers_27_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv491_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv490_1[1]
            rms_norm509: R.Tensor((1, seq_len, 4096), dtype="float16") = lv490_1[0]
            lv494 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_27_self_attn_qkv_proj_q_weight9, model_layers_27_self_attn_qkv_proj_q_scale9, rms_norm509), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1004 = R.call_tir(cls.reshape4, (lv494,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1005 = R.call_tir(cls.reshape5, (reshape1004,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1264 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1005), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1006 = R.call_tir(cls.reshape6, (lv1264,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1007 = R.call_tir(cls.reshape7, (reshape1006,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv495 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_27_self_attn_o_proj_q_weight9, model_layers_27_self_attn_o_proj_q_scale9, reshape1007), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv492_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv495, lv491_1, model_layers_27_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv493_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv492_1[1]
            rms_norm510: R.Tensor((1, seq_len, 4096), dtype="float16") = lv492_1[0]
            lv496 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_27_mlp_gate_up_proj_q_weight9, model_layers_27_mlp_gate_up_proj_q_scale9, rms_norm510), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv125 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv496,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv497 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_27_mlp_down_proj_q_weight9, model_layers_27_mlp_down_proj_q_scale9, lv125), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv494_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv497, lv493_1, model_layers_28_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv495_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv494_1[1]
            rms_norm511: R.Tensor((1, seq_len, 4096), dtype="float16") = lv494_1[0]
            lv498 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_28_self_attn_qkv_proj_q_weight9, model_layers_28_self_attn_qkv_proj_q_scale9, rms_norm511), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1008 = R.call_tir(cls.reshape4, (lv498,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1009 = R.call_tir(cls.reshape5, (reshape1008,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1269 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1009), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1010 = R.call_tir(cls.reshape6, (lv1269,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1011 = R.call_tir(cls.reshape7, (reshape1010,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv499 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_28_self_attn_o_proj_q_weight9, model_layers_28_self_attn_o_proj_q_scale9, reshape1011), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv496_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv499, lv495_1, model_layers_28_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv497_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv496_1[1]
            rms_norm512: R.Tensor((1, seq_len, 4096), dtype="float16") = lv496_1[0]
            lv500 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_28_mlp_gate_up_proj_q_weight9, model_layers_28_mlp_gate_up_proj_q_scale9, rms_norm512), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv126 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv500,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv501 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_28_mlp_down_proj_q_weight9, model_layers_28_mlp_down_proj_q_scale9, lv126), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv498_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv501, lv497_1, model_layers_29_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv499_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv498_1[1]
            rms_norm513: R.Tensor((1, seq_len, 4096), dtype="float16") = lv498_1[0]
            lv502 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_29_self_attn_qkv_proj_q_weight9, model_layers_29_self_attn_qkv_proj_q_scale9, rms_norm513), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1012 = R.call_tir(cls.reshape4, (lv502,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1013 = R.call_tir(cls.reshape5, (reshape1012,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1274 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1013), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1014 = R.call_tir(cls.reshape6, (lv1274,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1015 = R.call_tir(cls.reshape7, (reshape1014,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv503 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_29_self_attn_o_proj_q_weight9, model_layers_29_self_attn_o_proj_q_scale9, reshape1015), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv500_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv503, lv499_1, model_layers_29_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv501_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv500_1[1]
            rms_norm514: R.Tensor((1, seq_len, 4096), dtype="float16") = lv500_1[0]
            lv504 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_29_mlp_gate_up_proj_q_weight9, model_layers_29_mlp_gate_up_proj_q_scale9, rms_norm514), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv127 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv504,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv505 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_29_mlp_down_proj_q_weight9, model_layers_29_mlp_down_proj_q_scale9, lv127), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv502_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv505, lv501_1, model_layers_30_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv503_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv502_1[1]
            rms_norm515: R.Tensor((1, seq_len, 4096), dtype="float16") = lv502_1[0]
            lv506 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_30_self_attn_qkv_proj_q_weight9, model_layers_30_self_attn_qkv_proj_q_scale9, rms_norm515), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1016 = R.call_tir(cls.reshape4, (lv506,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1017 = R.call_tir(cls.reshape5, (reshape1016,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1279 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1017), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1018 = R.call_tir(cls.reshape6, (lv1279,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1019 = R.call_tir(cls.reshape7, (reshape1018,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv507 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_30_self_attn_o_proj_q_weight9, model_layers_30_self_attn_o_proj_q_scale9, reshape1019), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv504_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv507, lv503_1, model_layers_30_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv505_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv504_1[1]
            rms_norm516: R.Tensor((1, seq_len, 4096), dtype="float16") = lv504_1[0]
            lv508 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_30_mlp_gate_up_proj_q_weight9, model_layers_30_mlp_gate_up_proj_q_scale9, rms_norm516), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv128 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv508,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv509 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_30_mlp_down_proj_q_weight9, model_layers_30_mlp_down_proj_q_scale9, lv128), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv506_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv509, lv505_1, model_layers_31_input_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv507_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv506_1[1]
            rms_norm517: R.Tensor((1, seq_len, 4096), dtype="float16") = lv506_1[0]
            lv510 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_31_self_attn_qkv_proj_q_weight9, model_layers_31_self_attn_qkv_proj_q_scale9, rms_norm517), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1020 = R.call_tir(cls.reshape4, (lv510,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1021 = R.call_tir(cls.reshape5, (reshape1020,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1284 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1021), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1022 = R.call_tir(cls.reshape6, (lv1284,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1023 = R.call_tir(cls.reshape7, (reshape1022,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv511 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_31_self_attn_o_proj_q_weight9, model_layers_31_self_attn_o_proj_q_scale9, reshape1023), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv508_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv511, lv507_1, model_layers_31_post_attention_layernorm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv509_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv508_1[1]
            rms_norm518: R.Tensor((1, seq_len, 4096), dtype="float16") = lv508_1[0]
            lv512 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_31_mlp_gate_up_proj_q_weight9, model_layers_31_mlp_gate_up_proj_q_scale9, rms_norm518), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv129 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv512,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv513 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_31_mlp_down_proj_q_weight9, model_layers_31_mlp_down_proj_q_scale9, lv129), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv510_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv513, lv509_1, model_norm_weight9), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm519: R.Tensor((1, seq_len, 4096), dtype="float16") = lv510_1[0]
            gv10: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm519, paged_kv_cache
            R.output(gv10)
        return gv10

    @R.function
    def batch_select_last_hidden_states(hidden_states: R.Tensor(("seq_len", 4096), dtype="float16"), logit_positions: R.Tensor(("batch_size",), dtype="int32")) -> R.Tensor(("batch_size", 4096), dtype="float16"):
        batch_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 2, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            gv2 = R.call_tir(cls.take1, (hidden_states, logit_positions), out_sinfo=R.Tensor((batch_size, 4096), dtype="float16"))
            R.output(gv2)
        return gv2

    @R.function
    def batch_verify(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", "vocab_size"), dtype="float32"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size", "seq_len"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight8: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale8: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm390 = R.call_tir(cls.rms_norm1, (input_embeds, model_layers_0_input_layernorm_weight8), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv514 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_0_self_attn_qkv_proj_q_weight8, model_layers_0_self_attn_qkv_proj_q_scale8, rms_norm390), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape768 = R.call_tir(cls.reshape4, (lv514,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape769 = R.call_tir(cls.reshape5, (reshape768,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv968 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape769), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape770 = R.call_tir(cls.reshape6, (lv968,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape771 = R.call_tir(cls.reshape7, (reshape770,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv515 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_0_self_attn_o_proj_q_weight8, model_layers_0_self_attn_o_proj_q_scale8, reshape771), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv512 = R.call_tir(cls.fuse_add_norm_prefill, (lv515, input_embeds, model_layers_0_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv513: R.Tensor((1, seq_len, 4096), dtype="float16") = lv512[1]
            rms_norm391: R.Tensor((1, seq_len, 4096), dtype="float16") = lv512[0]
            lv516 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_0_mlp_gate_up_proj_q_weight8, model_layers_0_mlp_gate_up_proj_q_scale8, rms_norm391), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv130 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv516,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv517 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_0_mlp_down_proj_q_weight8, model_layers_0_mlp_down_proj_q_scale8, lv130), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv514_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv517, lv513, model_layers_1_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv515_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv514_1[1]
            rms_norm392: R.Tensor((1, seq_len, 4096), dtype="float16") = lv514_1[0]
            lv518 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_1_self_attn_qkv_proj_q_weight8, model_layers_1_self_attn_qkv_proj_q_scale8, rms_norm392), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape772 = R.call_tir(cls.reshape4, (lv518,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape773 = R.call_tir(cls.reshape5, (reshape772,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv973 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape773), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape774 = R.call_tir(cls.reshape6, (lv973,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape775 = R.call_tir(cls.reshape7, (reshape774,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv519 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_1_self_attn_o_proj_q_weight8, model_layers_1_self_attn_o_proj_q_scale8, reshape775), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv516_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv519, lv515_1, model_layers_1_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv517_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv516_1[1]
            rms_norm393: R.Tensor((1, seq_len, 4096), dtype="float16") = lv516_1[0]
            lv520 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_1_mlp_gate_up_proj_q_weight8, model_layers_1_mlp_gate_up_proj_q_scale8, rms_norm393), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv131 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv520,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv521 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_1_mlp_down_proj_q_weight8, model_layers_1_mlp_down_proj_q_scale8, lv131), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv518_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv521, lv517_1, model_layers_2_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv519_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv518_1[1]
            rms_norm394: R.Tensor((1, seq_len, 4096), dtype="float16") = lv518_1[0]
            lv522 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_2_self_attn_qkv_proj_q_weight8, model_layers_2_self_attn_qkv_proj_q_scale8, rms_norm394), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape776 = R.call_tir(cls.reshape4, (lv522,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape777 = R.call_tir(cls.reshape5, (reshape776,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv978 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape777), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape778 = R.call_tir(cls.reshape6, (lv978,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape779 = R.call_tir(cls.reshape7, (reshape778,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv523 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_2_self_attn_o_proj_q_weight8, model_layers_2_self_attn_o_proj_q_scale8, reshape779), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv520_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv523, lv519_1, model_layers_2_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv521_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv520_1[1]
            rms_norm395: R.Tensor((1, seq_len, 4096), dtype="float16") = lv520_1[0]
            lv524 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_2_mlp_gate_up_proj_q_weight8, model_layers_2_mlp_gate_up_proj_q_scale8, rms_norm395), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv132 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv524,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv525 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_2_mlp_down_proj_q_weight8, model_layers_2_mlp_down_proj_q_scale8, lv132), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv522_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv525, lv521_1, model_layers_3_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv523_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv522_1[1]
            rms_norm396: R.Tensor((1, seq_len, 4096), dtype="float16") = lv522_1[0]
            lv526 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_3_self_attn_qkv_proj_q_weight8, model_layers_3_self_attn_qkv_proj_q_scale8, rms_norm396), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape780 = R.call_tir(cls.reshape4, (lv526,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape781 = R.call_tir(cls.reshape5, (reshape780,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv983 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape781), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape782 = R.call_tir(cls.reshape6, (lv983,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape783 = R.call_tir(cls.reshape7, (reshape782,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv527 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_3_self_attn_o_proj_q_weight8, model_layers_3_self_attn_o_proj_q_scale8, reshape783), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv524_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv527, lv523_1, model_layers_3_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv525_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv524_1[1]
            rms_norm397: R.Tensor((1, seq_len, 4096), dtype="float16") = lv524_1[0]
            lv528 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_3_mlp_gate_up_proj_q_weight8, model_layers_3_mlp_gate_up_proj_q_scale8, rms_norm397), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv133 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv528,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv529 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_3_mlp_down_proj_q_weight8, model_layers_3_mlp_down_proj_q_scale8, lv133), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv526_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv529, lv525_1, model_layers_4_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv527_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv526_1[1]
            rms_norm398: R.Tensor((1, seq_len, 4096), dtype="float16") = lv526_1[0]
            lv530 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_4_self_attn_qkv_proj_q_weight8, model_layers_4_self_attn_qkv_proj_q_scale8, rms_norm398), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape784 = R.call_tir(cls.reshape4, (lv530,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape785 = R.call_tir(cls.reshape5, (reshape784,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv988 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape785), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape786 = R.call_tir(cls.reshape6, (lv988,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape787 = R.call_tir(cls.reshape7, (reshape786,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv531 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_4_self_attn_o_proj_q_weight8, model_layers_4_self_attn_o_proj_q_scale8, reshape787), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv528_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv531, lv527_1, model_layers_4_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv529_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv528_1[1]
            rms_norm399: R.Tensor((1, seq_len, 4096), dtype="float16") = lv528_1[0]
            lv532 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_4_mlp_gate_up_proj_q_weight8, model_layers_4_mlp_gate_up_proj_q_scale8, rms_norm399), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv134 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv532,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv533 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_4_mlp_down_proj_q_weight8, model_layers_4_mlp_down_proj_q_scale8, lv134), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv530_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv533, lv529_1, model_layers_5_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv531_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv530_1[1]
            rms_norm400: R.Tensor((1, seq_len, 4096), dtype="float16") = lv530_1[0]
            lv534 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_5_self_attn_qkv_proj_q_weight8, model_layers_5_self_attn_qkv_proj_q_scale8, rms_norm400), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape788 = R.call_tir(cls.reshape4, (lv534,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape789 = R.call_tir(cls.reshape5, (reshape788,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv993 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape789), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape790 = R.call_tir(cls.reshape6, (lv993,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape791 = R.call_tir(cls.reshape7, (reshape790,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv535 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_5_self_attn_o_proj_q_weight8, model_layers_5_self_attn_o_proj_q_scale8, reshape791), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv532_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv535, lv531_1, model_layers_5_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv533_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv532_1[1]
            rms_norm401: R.Tensor((1, seq_len, 4096), dtype="float16") = lv532_1[0]
            lv536 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_5_mlp_gate_up_proj_q_weight8, model_layers_5_mlp_gate_up_proj_q_scale8, rms_norm401), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv135 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv536,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv537 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_5_mlp_down_proj_q_weight8, model_layers_5_mlp_down_proj_q_scale8, lv135), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv534_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv537, lv533_1, model_layers_6_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv535_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv534_1[1]
            rms_norm402: R.Tensor((1, seq_len, 4096), dtype="float16") = lv534_1[0]
            lv538 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_6_self_attn_qkv_proj_q_weight8, model_layers_6_self_attn_qkv_proj_q_scale8, rms_norm402), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape792 = R.call_tir(cls.reshape4, (lv538,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape793 = R.call_tir(cls.reshape5, (reshape792,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv998 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape793), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape794 = R.call_tir(cls.reshape6, (lv998,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape795 = R.call_tir(cls.reshape7, (reshape794,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv539 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_6_self_attn_o_proj_q_weight8, model_layers_6_self_attn_o_proj_q_scale8, reshape795), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv536_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv539, lv535_1, model_layers_6_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv537_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv536_1[1]
            rms_norm403: R.Tensor((1, seq_len, 4096), dtype="float16") = lv536_1[0]
            lv540 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_6_mlp_gate_up_proj_q_weight8, model_layers_6_mlp_gate_up_proj_q_scale8, rms_norm403), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv136 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv540,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv541 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_6_mlp_down_proj_q_weight8, model_layers_6_mlp_down_proj_q_scale8, lv136), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv538_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv541, lv537_1, model_layers_7_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv539_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv538_1[1]
            rms_norm404: R.Tensor((1, seq_len, 4096), dtype="float16") = lv538_1[0]
            lv542 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_7_self_attn_qkv_proj_q_weight8, model_layers_7_self_attn_qkv_proj_q_scale8, rms_norm404), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape796 = R.call_tir(cls.reshape4, (lv542,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape797 = R.call_tir(cls.reshape5, (reshape796,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1003 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape797), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape798 = R.call_tir(cls.reshape6, (lv1003,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape799 = R.call_tir(cls.reshape7, (reshape798,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv543 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_7_self_attn_o_proj_q_weight8, model_layers_7_self_attn_o_proj_q_scale8, reshape799), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv540_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv543, lv539_1, model_layers_7_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv541_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv540_1[1]
            rms_norm405: R.Tensor((1, seq_len, 4096), dtype="float16") = lv540_1[0]
            lv544 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_7_mlp_gate_up_proj_q_weight8, model_layers_7_mlp_gate_up_proj_q_scale8, rms_norm405), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv137 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv544,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv545 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_7_mlp_down_proj_q_weight8, model_layers_7_mlp_down_proj_q_scale8, lv137), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv542_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv545, lv541_1, model_layers_8_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv543_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv542_1[1]
            rms_norm406: R.Tensor((1, seq_len, 4096), dtype="float16") = lv542_1[0]
            lv546 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_8_self_attn_qkv_proj_q_weight8, model_layers_8_self_attn_qkv_proj_q_scale8, rms_norm406), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape800 = R.call_tir(cls.reshape4, (lv546,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape801 = R.call_tir(cls.reshape5, (reshape800,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1008 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape801), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape802 = R.call_tir(cls.reshape6, (lv1008,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape803 = R.call_tir(cls.reshape7, (reshape802,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv547 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_8_self_attn_o_proj_q_weight8, model_layers_8_self_attn_o_proj_q_scale8, reshape803), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv544_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv547, lv543_1, model_layers_8_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv545_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv544_1[1]
            rms_norm407: R.Tensor((1, seq_len, 4096), dtype="float16") = lv544_1[0]
            lv548 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_8_mlp_gate_up_proj_q_weight8, model_layers_8_mlp_gate_up_proj_q_scale8, rms_norm407), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv138 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv548,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv549 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_8_mlp_down_proj_q_weight8, model_layers_8_mlp_down_proj_q_scale8, lv138), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv546_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv549, lv545_1, model_layers_9_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv547_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv546_1[1]
            rms_norm408: R.Tensor((1, seq_len, 4096), dtype="float16") = lv546_1[0]
            lv550 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_9_self_attn_qkv_proj_q_weight8, model_layers_9_self_attn_qkv_proj_q_scale8, rms_norm408), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape804 = R.call_tir(cls.reshape4, (lv550,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape805 = R.call_tir(cls.reshape5, (reshape804,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1013 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape805), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape806 = R.call_tir(cls.reshape6, (lv1013,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape807 = R.call_tir(cls.reshape7, (reshape806,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv551 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_9_self_attn_o_proj_q_weight8, model_layers_9_self_attn_o_proj_q_scale8, reshape807), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv548_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv551, lv547_1, model_layers_9_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv549_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv548_1[1]
            rms_norm409: R.Tensor((1, seq_len, 4096), dtype="float16") = lv548_1[0]
            lv552 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_9_mlp_gate_up_proj_q_weight8, model_layers_9_mlp_gate_up_proj_q_scale8, rms_norm409), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv139 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv552,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv553 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_9_mlp_down_proj_q_weight8, model_layers_9_mlp_down_proj_q_scale8, lv139), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv550_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv553, lv549_1, model_layers_10_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv551_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv550_1[1]
            rms_norm410: R.Tensor((1, seq_len, 4096), dtype="float16") = lv550_1[0]
            lv554 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_10_self_attn_qkv_proj_q_weight8, model_layers_10_self_attn_qkv_proj_q_scale8, rms_norm410), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape808 = R.call_tir(cls.reshape4, (lv554,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape809 = R.call_tir(cls.reshape5, (reshape808,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1018 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape809), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape810 = R.call_tir(cls.reshape6, (lv1018,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape811 = R.call_tir(cls.reshape7, (reshape810,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv555 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_10_self_attn_o_proj_q_weight8, model_layers_10_self_attn_o_proj_q_scale8, reshape811), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv552_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv555, lv551_1, model_layers_10_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv553_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv552_1[1]
            rms_norm411: R.Tensor((1, seq_len, 4096), dtype="float16") = lv552_1[0]
            lv556 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_10_mlp_gate_up_proj_q_weight8, model_layers_10_mlp_gate_up_proj_q_scale8, rms_norm411), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv140 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv556,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv557 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_10_mlp_down_proj_q_weight8, model_layers_10_mlp_down_proj_q_scale8, lv140), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv554_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv557, lv553_1, model_layers_11_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv555_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv554_1[1]
            rms_norm412: R.Tensor((1, seq_len, 4096), dtype="float16") = lv554_1[0]
            lv558 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_11_self_attn_qkv_proj_q_weight8, model_layers_11_self_attn_qkv_proj_q_scale8, rms_norm412), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape812 = R.call_tir(cls.reshape4, (lv558,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape813 = R.call_tir(cls.reshape5, (reshape812,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1023 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape813), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape814 = R.call_tir(cls.reshape6, (lv1023,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape815 = R.call_tir(cls.reshape7, (reshape814,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv559 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_11_self_attn_o_proj_q_weight8, model_layers_11_self_attn_o_proj_q_scale8, reshape815), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv556_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv559, lv555_1, model_layers_11_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv557_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv556_1[1]
            rms_norm413: R.Tensor((1, seq_len, 4096), dtype="float16") = lv556_1[0]
            lv560 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_11_mlp_gate_up_proj_q_weight8, model_layers_11_mlp_gate_up_proj_q_scale8, rms_norm413), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv141 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv560,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv561 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_11_mlp_down_proj_q_weight8, model_layers_11_mlp_down_proj_q_scale8, lv141), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv558_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv561, lv557_1, model_layers_12_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv559_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv558_1[1]
            rms_norm414: R.Tensor((1, seq_len, 4096), dtype="float16") = lv558_1[0]
            lv562 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_12_self_attn_qkv_proj_q_weight8, model_layers_12_self_attn_qkv_proj_q_scale8, rms_norm414), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape816 = R.call_tir(cls.reshape4, (lv562,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape817 = R.call_tir(cls.reshape5, (reshape816,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1028 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape817), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape818 = R.call_tir(cls.reshape6, (lv1028,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape819 = R.call_tir(cls.reshape7, (reshape818,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv563 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_12_self_attn_o_proj_q_weight8, model_layers_12_self_attn_o_proj_q_scale8, reshape819), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv560_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv563, lv559_1, model_layers_12_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv561_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv560_1[1]
            rms_norm415: R.Tensor((1, seq_len, 4096), dtype="float16") = lv560_1[0]
            lv564 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_12_mlp_gate_up_proj_q_weight8, model_layers_12_mlp_gate_up_proj_q_scale8, rms_norm415), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv142 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv564,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv565 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_12_mlp_down_proj_q_weight8, model_layers_12_mlp_down_proj_q_scale8, lv142), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv562_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv565, lv561_1, model_layers_13_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv563_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv562_1[1]
            rms_norm416: R.Tensor((1, seq_len, 4096), dtype="float16") = lv562_1[0]
            lv566 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_13_self_attn_qkv_proj_q_weight8, model_layers_13_self_attn_qkv_proj_q_scale8, rms_norm416), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape820 = R.call_tir(cls.reshape4, (lv566,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape821 = R.call_tir(cls.reshape5, (reshape820,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1033 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape821), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape822 = R.call_tir(cls.reshape6, (lv1033,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape823 = R.call_tir(cls.reshape7, (reshape822,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv567 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_13_self_attn_o_proj_q_weight8, model_layers_13_self_attn_o_proj_q_scale8, reshape823), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv564_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv567, lv563_1, model_layers_13_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv565_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv564_1[1]
            rms_norm417: R.Tensor((1, seq_len, 4096), dtype="float16") = lv564_1[0]
            lv568 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_13_mlp_gate_up_proj_q_weight8, model_layers_13_mlp_gate_up_proj_q_scale8, rms_norm417), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv143 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv568,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv569 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_13_mlp_down_proj_q_weight8, model_layers_13_mlp_down_proj_q_scale8, lv143), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv566_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv569, lv565_1, model_layers_14_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv567_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv566_1[1]
            rms_norm418: R.Tensor((1, seq_len, 4096), dtype="float16") = lv566_1[0]
            lv570 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_14_self_attn_qkv_proj_q_weight8, model_layers_14_self_attn_qkv_proj_q_scale8, rms_norm418), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape824 = R.call_tir(cls.reshape4, (lv570,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape825 = R.call_tir(cls.reshape5, (reshape824,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1038 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape825), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape826 = R.call_tir(cls.reshape6, (lv1038,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape827 = R.call_tir(cls.reshape7, (reshape826,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv571 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_14_self_attn_o_proj_q_weight8, model_layers_14_self_attn_o_proj_q_scale8, reshape827), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv568_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv571, lv567_1, model_layers_14_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv569_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv568_1[1]
            rms_norm419: R.Tensor((1, seq_len, 4096), dtype="float16") = lv568_1[0]
            lv572 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_14_mlp_gate_up_proj_q_weight8, model_layers_14_mlp_gate_up_proj_q_scale8, rms_norm419), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv144 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv572,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv573 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_14_mlp_down_proj_q_weight8, model_layers_14_mlp_down_proj_q_scale8, lv144), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv570_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv573, lv569_1, model_layers_15_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv571_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv570_1[1]
            rms_norm420: R.Tensor((1, seq_len, 4096), dtype="float16") = lv570_1[0]
            lv574 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_15_self_attn_qkv_proj_q_weight8, model_layers_15_self_attn_qkv_proj_q_scale8, rms_norm420), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape828 = R.call_tir(cls.reshape4, (lv574,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape829 = R.call_tir(cls.reshape5, (reshape828,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1043 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape829), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape830 = R.call_tir(cls.reshape6, (lv1043,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape831 = R.call_tir(cls.reshape7, (reshape830,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv575 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_15_self_attn_o_proj_q_weight8, model_layers_15_self_attn_o_proj_q_scale8, reshape831), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv572_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv575, lv571_1, model_layers_15_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv573_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv572_1[1]
            rms_norm421: R.Tensor((1, seq_len, 4096), dtype="float16") = lv572_1[0]
            lv576 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_15_mlp_gate_up_proj_q_weight8, model_layers_15_mlp_gate_up_proj_q_scale8, rms_norm421), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv145 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv576,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv577 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_15_mlp_down_proj_q_weight8, model_layers_15_mlp_down_proj_q_scale8, lv145), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv574_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv577, lv573_1, model_layers_16_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv575_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv574_1[1]
            rms_norm422: R.Tensor((1, seq_len, 4096), dtype="float16") = lv574_1[0]
            lv578 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_16_self_attn_qkv_proj_q_weight8, model_layers_16_self_attn_qkv_proj_q_scale8, rms_norm422), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape832 = R.call_tir(cls.reshape4, (lv578,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape833 = R.call_tir(cls.reshape5, (reshape832,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1048 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape833), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape834 = R.call_tir(cls.reshape6, (lv1048,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape835 = R.call_tir(cls.reshape7, (reshape834,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv579 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_16_self_attn_o_proj_q_weight8, model_layers_16_self_attn_o_proj_q_scale8, reshape835), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv576_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv579, lv575_1, model_layers_16_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv577_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv576_1[1]
            rms_norm423: R.Tensor((1, seq_len, 4096), dtype="float16") = lv576_1[0]
            lv580 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_16_mlp_gate_up_proj_q_weight8, model_layers_16_mlp_gate_up_proj_q_scale8, rms_norm423), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv146 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv580,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv581 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_16_mlp_down_proj_q_weight8, model_layers_16_mlp_down_proj_q_scale8, lv146), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv578_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv581, lv577_1, model_layers_17_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv579_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv578_1[1]
            rms_norm424: R.Tensor((1, seq_len, 4096), dtype="float16") = lv578_1[0]
            lv582 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_17_self_attn_qkv_proj_q_weight8, model_layers_17_self_attn_qkv_proj_q_scale8, rms_norm424), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape836 = R.call_tir(cls.reshape4, (lv582,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape837 = R.call_tir(cls.reshape5, (reshape836,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1053 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape837), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape838 = R.call_tir(cls.reshape6, (lv1053,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape839 = R.call_tir(cls.reshape7, (reshape838,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv583 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_17_self_attn_o_proj_q_weight8, model_layers_17_self_attn_o_proj_q_scale8, reshape839), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv580_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv583, lv579_1, model_layers_17_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv581_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv580_1[1]
            rms_norm425: R.Tensor((1, seq_len, 4096), dtype="float16") = lv580_1[0]
            lv584 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_17_mlp_gate_up_proj_q_weight8, model_layers_17_mlp_gate_up_proj_q_scale8, rms_norm425), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv147 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv584,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv585 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_17_mlp_down_proj_q_weight8, model_layers_17_mlp_down_proj_q_scale8, lv147), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv582_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv585, lv581_1, model_layers_18_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv583_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv582_1[1]
            rms_norm426: R.Tensor((1, seq_len, 4096), dtype="float16") = lv582_1[0]
            lv586 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_18_self_attn_qkv_proj_q_weight8, model_layers_18_self_attn_qkv_proj_q_scale8, rms_norm426), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape840 = R.call_tir(cls.reshape4, (lv586,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape841 = R.call_tir(cls.reshape5, (reshape840,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1058 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape841), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape842 = R.call_tir(cls.reshape6, (lv1058,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape843 = R.call_tir(cls.reshape7, (reshape842,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv587 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_18_self_attn_o_proj_q_weight8, model_layers_18_self_attn_o_proj_q_scale8, reshape843), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv584_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv587, lv583_1, model_layers_18_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv585_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv584_1[1]
            rms_norm427: R.Tensor((1, seq_len, 4096), dtype="float16") = lv584_1[0]
            lv588 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_18_mlp_gate_up_proj_q_weight8, model_layers_18_mlp_gate_up_proj_q_scale8, rms_norm427), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv148 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv588,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv589 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_18_mlp_down_proj_q_weight8, model_layers_18_mlp_down_proj_q_scale8, lv148), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv586_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv589, lv585_1, model_layers_19_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv587_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv586_1[1]
            rms_norm428: R.Tensor((1, seq_len, 4096), dtype="float16") = lv586_1[0]
            lv590 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_19_self_attn_qkv_proj_q_weight8, model_layers_19_self_attn_qkv_proj_q_scale8, rms_norm428), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape844 = R.call_tir(cls.reshape4, (lv590,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape845 = R.call_tir(cls.reshape5, (reshape844,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1063 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape845), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape846 = R.call_tir(cls.reshape6, (lv1063,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape847 = R.call_tir(cls.reshape7, (reshape846,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv591 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_19_self_attn_o_proj_q_weight8, model_layers_19_self_attn_o_proj_q_scale8, reshape847), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv588_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv591, lv587_1, model_layers_19_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv589_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv588_1[1]
            rms_norm429: R.Tensor((1, seq_len, 4096), dtype="float16") = lv588_1[0]
            lv592 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_19_mlp_gate_up_proj_q_weight8, model_layers_19_mlp_gate_up_proj_q_scale8, rms_norm429), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv149 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv592,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv593 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_19_mlp_down_proj_q_weight8, model_layers_19_mlp_down_proj_q_scale8, lv149), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv590_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv593, lv589_1, model_layers_20_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv591_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv590_1[1]
            rms_norm430: R.Tensor((1, seq_len, 4096), dtype="float16") = lv590_1[0]
            lv594 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_20_self_attn_qkv_proj_q_weight8, model_layers_20_self_attn_qkv_proj_q_scale8, rms_norm430), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape848 = R.call_tir(cls.reshape4, (lv594,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape849 = R.call_tir(cls.reshape5, (reshape848,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1068 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape849), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape850 = R.call_tir(cls.reshape6, (lv1068,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape851 = R.call_tir(cls.reshape7, (reshape850,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv595 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_20_self_attn_o_proj_q_weight8, model_layers_20_self_attn_o_proj_q_scale8, reshape851), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv592_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv595, lv591_1, model_layers_20_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv593_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv592_1[1]
            rms_norm431: R.Tensor((1, seq_len, 4096), dtype="float16") = lv592_1[0]
            lv596 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_20_mlp_gate_up_proj_q_weight8, model_layers_20_mlp_gate_up_proj_q_scale8, rms_norm431), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv150 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv596,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv597 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_20_mlp_down_proj_q_weight8, model_layers_20_mlp_down_proj_q_scale8, lv150), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv594_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv597, lv593_1, model_layers_21_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv595_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv594_1[1]
            rms_norm432: R.Tensor((1, seq_len, 4096), dtype="float16") = lv594_1[0]
            lv598 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_21_self_attn_qkv_proj_q_weight8, model_layers_21_self_attn_qkv_proj_q_scale8, rms_norm432), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape852 = R.call_tir(cls.reshape4, (lv598,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape853 = R.call_tir(cls.reshape5, (reshape852,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1073 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape853), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape854 = R.call_tir(cls.reshape6, (lv1073,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape855 = R.call_tir(cls.reshape7, (reshape854,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv599 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_21_self_attn_o_proj_q_weight8, model_layers_21_self_attn_o_proj_q_scale8, reshape855), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv596_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv599, lv595_1, model_layers_21_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv597_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv596_1[1]
            rms_norm433: R.Tensor((1, seq_len, 4096), dtype="float16") = lv596_1[0]
            lv600 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_21_mlp_gate_up_proj_q_weight8, model_layers_21_mlp_gate_up_proj_q_scale8, rms_norm433), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv151 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv600,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv601 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_21_mlp_down_proj_q_weight8, model_layers_21_mlp_down_proj_q_scale8, lv151), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv598_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv601, lv597_1, model_layers_22_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv599_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv598_1[1]
            rms_norm434: R.Tensor((1, seq_len, 4096), dtype="float16") = lv598_1[0]
            lv602 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_22_self_attn_qkv_proj_q_weight8, model_layers_22_self_attn_qkv_proj_q_scale8, rms_norm434), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape856 = R.call_tir(cls.reshape4, (lv602,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape857 = R.call_tir(cls.reshape5, (reshape856,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1078 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape857), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape858 = R.call_tir(cls.reshape6, (lv1078,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape859 = R.call_tir(cls.reshape7, (reshape858,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv603 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_22_self_attn_o_proj_q_weight8, model_layers_22_self_attn_o_proj_q_scale8, reshape859), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv600_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv603, lv599_1, model_layers_22_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv601_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv600_1[1]
            rms_norm435: R.Tensor((1, seq_len, 4096), dtype="float16") = lv600_1[0]
            lv604 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_22_mlp_gate_up_proj_q_weight8, model_layers_22_mlp_gate_up_proj_q_scale8, rms_norm435), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv152 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv604,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv605 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_22_mlp_down_proj_q_weight8, model_layers_22_mlp_down_proj_q_scale8, lv152), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv602_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv605, lv601_1, model_layers_23_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv603_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv602_1[1]
            rms_norm436: R.Tensor((1, seq_len, 4096), dtype="float16") = lv602_1[0]
            lv606 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_23_self_attn_qkv_proj_q_weight8, model_layers_23_self_attn_qkv_proj_q_scale8, rms_norm436), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape860 = R.call_tir(cls.reshape4, (lv606,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape861 = R.call_tir(cls.reshape5, (reshape860,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1083 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape861), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape862 = R.call_tir(cls.reshape6, (lv1083,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape863 = R.call_tir(cls.reshape7, (reshape862,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv607 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_23_self_attn_o_proj_q_weight8, model_layers_23_self_attn_o_proj_q_scale8, reshape863), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv604_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv607, lv603_1, model_layers_23_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv605_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv604_1[1]
            rms_norm437: R.Tensor((1, seq_len, 4096), dtype="float16") = lv604_1[0]
            lv608 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_23_mlp_gate_up_proj_q_weight8, model_layers_23_mlp_gate_up_proj_q_scale8, rms_norm437), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv153 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv608,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv609 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_23_mlp_down_proj_q_weight8, model_layers_23_mlp_down_proj_q_scale8, lv153), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv606_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv609, lv605_1, model_layers_24_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv607_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv606_1[1]
            rms_norm438: R.Tensor((1, seq_len, 4096), dtype="float16") = lv606_1[0]
            lv610 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_24_self_attn_qkv_proj_q_weight8, model_layers_24_self_attn_qkv_proj_q_scale8, rms_norm438), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape864 = R.call_tir(cls.reshape4, (lv610,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape865 = R.call_tir(cls.reshape5, (reshape864,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1088 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape865), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape866 = R.call_tir(cls.reshape6, (lv1088,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape867 = R.call_tir(cls.reshape7, (reshape866,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv611 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_24_self_attn_o_proj_q_weight8, model_layers_24_self_attn_o_proj_q_scale8, reshape867), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv608_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv611, lv607_1, model_layers_24_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv609_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv608_1[1]
            rms_norm439: R.Tensor((1, seq_len, 4096), dtype="float16") = lv608_1[0]
            lv612 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_24_mlp_gate_up_proj_q_weight8, model_layers_24_mlp_gate_up_proj_q_scale8, rms_norm439), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv154 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv612,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv613 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_24_mlp_down_proj_q_weight8, model_layers_24_mlp_down_proj_q_scale8, lv154), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv610_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv613, lv609_1, model_layers_25_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv611_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv610_1[1]
            rms_norm440: R.Tensor((1, seq_len, 4096), dtype="float16") = lv610_1[0]
            lv614 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_25_self_attn_qkv_proj_q_weight8, model_layers_25_self_attn_qkv_proj_q_scale8, rms_norm440), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape868 = R.call_tir(cls.reshape4, (lv614,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape869 = R.call_tir(cls.reshape5, (reshape868,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1093 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape869), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape870 = R.call_tir(cls.reshape6, (lv1093,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape871 = R.call_tir(cls.reshape7, (reshape870,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv615 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_25_self_attn_o_proj_q_weight8, model_layers_25_self_attn_o_proj_q_scale8, reshape871), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv612_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv615, lv611_1, model_layers_25_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv613_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv612_1[1]
            rms_norm441: R.Tensor((1, seq_len, 4096), dtype="float16") = lv612_1[0]
            lv616 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_25_mlp_gate_up_proj_q_weight8, model_layers_25_mlp_gate_up_proj_q_scale8, rms_norm441), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv155 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv616,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv617 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_25_mlp_down_proj_q_weight8, model_layers_25_mlp_down_proj_q_scale8, lv155), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv614_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv617, lv613_1, model_layers_26_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv615_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv614_1[1]
            rms_norm442: R.Tensor((1, seq_len, 4096), dtype="float16") = lv614_1[0]
            lv618 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_26_self_attn_qkv_proj_q_weight8, model_layers_26_self_attn_qkv_proj_q_scale8, rms_norm442), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape872 = R.call_tir(cls.reshape4, (lv618,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape873 = R.call_tir(cls.reshape5, (reshape872,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1098 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape873), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape874 = R.call_tir(cls.reshape6, (lv1098,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape875 = R.call_tir(cls.reshape7, (reshape874,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv619 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_26_self_attn_o_proj_q_weight8, model_layers_26_self_attn_o_proj_q_scale8, reshape875), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv616_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv619, lv615_1, model_layers_26_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv617_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv616_1[1]
            rms_norm443: R.Tensor((1, seq_len, 4096), dtype="float16") = lv616_1[0]
            lv620 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_26_mlp_gate_up_proj_q_weight8, model_layers_26_mlp_gate_up_proj_q_scale8, rms_norm443), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv156 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv620,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv621 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_26_mlp_down_proj_q_weight8, model_layers_26_mlp_down_proj_q_scale8, lv156), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv618_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv621, lv617_1, model_layers_27_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv619_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv618_1[1]
            rms_norm444: R.Tensor((1, seq_len, 4096), dtype="float16") = lv618_1[0]
            lv622 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_27_self_attn_qkv_proj_q_weight8, model_layers_27_self_attn_qkv_proj_q_scale8, rms_norm444), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape876 = R.call_tir(cls.reshape4, (lv622,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape877 = R.call_tir(cls.reshape5, (reshape876,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1103 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape877), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape878 = R.call_tir(cls.reshape6, (lv1103,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape879 = R.call_tir(cls.reshape7, (reshape878,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv623 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_27_self_attn_o_proj_q_weight8, model_layers_27_self_attn_o_proj_q_scale8, reshape879), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv620_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv623, lv619_1, model_layers_27_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv621_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv620_1[1]
            rms_norm445: R.Tensor((1, seq_len, 4096), dtype="float16") = lv620_1[0]
            lv624 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_27_mlp_gate_up_proj_q_weight8, model_layers_27_mlp_gate_up_proj_q_scale8, rms_norm445), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv157 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv624,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv625 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_27_mlp_down_proj_q_weight8, model_layers_27_mlp_down_proj_q_scale8, lv157), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv622_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv625, lv621_1, model_layers_28_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv623_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv622_1[1]
            rms_norm446: R.Tensor((1, seq_len, 4096), dtype="float16") = lv622_1[0]
            lv626 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_28_self_attn_qkv_proj_q_weight8, model_layers_28_self_attn_qkv_proj_q_scale8, rms_norm446), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape880 = R.call_tir(cls.reshape4, (lv626,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape881 = R.call_tir(cls.reshape5, (reshape880,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1108 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape881), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape882 = R.call_tir(cls.reshape6, (lv1108,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape883 = R.call_tir(cls.reshape7, (reshape882,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv627 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_28_self_attn_o_proj_q_weight8, model_layers_28_self_attn_o_proj_q_scale8, reshape883), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv624_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv627, lv623_1, model_layers_28_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv625_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv624_1[1]
            rms_norm447: R.Tensor((1, seq_len, 4096), dtype="float16") = lv624_1[0]
            lv628 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_28_mlp_gate_up_proj_q_weight8, model_layers_28_mlp_gate_up_proj_q_scale8, rms_norm447), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv158 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv628,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv629 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_28_mlp_down_proj_q_weight8, model_layers_28_mlp_down_proj_q_scale8, lv158), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv626_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv629, lv625_1, model_layers_29_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv627_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv626_1[1]
            rms_norm448: R.Tensor((1, seq_len, 4096), dtype="float16") = lv626_1[0]
            lv630 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_29_self_attn_qkv_proj_q_weight8, model_layers_29_self_attn_qkv_proj_q_scale8, rms_norm448), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape884 = R.call_tir(cls.reshape4, (lv630,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape885 = R.call_tir(cls.reshape5, (reshape884,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1113 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape885), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape886 = R.call_tir(cls.reshape6, (lv1113,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape887 = R.call_tir(cls.reshape7, (reshape886,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv631 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_29_self_attn_o_proj_q_weight8, model_layers_29_self_attn_o_proj_q_scale8, reshape887), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv628_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv631, lv627_1, model_layers_29_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv629_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv628_1[1]
            rms_norm449: R.Tensor((1, seq_len, 4096), dtype="float16") = lv628_1[0]
            lv632 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_29_mlp_gate_up_proj_q_weight8, model_layers_29_mlp_gate_up_proj_q_scale8, rms_norm449), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv159 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv632,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv633 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_29_mlp_down_proj_q_weight8, model_layers_29_mlp_down_proj_q_scale8, lv159), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv630_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv633, lv629_1, model_layers_30_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv631_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv630_1[1]
            rms_norm450: R.Tensor((1, seq_len, 4096), dtype="float16") = lv630_1[0]
            lv634 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_30_self_attn_qkv_proj_q_weight8, model_layers_30_self_attn_qkv_proj_q_scale8, rms_norm450), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape888 = R.call_tir(cls.reshape4, (lv634,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape889 = R.call_tir(cls.reshape5, (reshape888,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1118 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape889), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape890 = R.call_tir(cls.reshape6, (lv1118,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape891 = R.call_tir(cls.reshape7, (reshape890,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv635 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_30_self_attn_o_proj_q_weight8, model_layers_30_self_attn_o_proj_q_scale8, reshape891), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv632_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv635, lv631_1, model_layers_30_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv633_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv632_1[1]
            rms_norm451: R.Tensor((1, seq_len, 4096), dtype="float16") = lv632_1[0]
            lv636 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_30_mlp_gate_up_proj_q_weight8, model_layers_30_mlp_gate_up_proj_q_scale8, rms_norm451), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv160 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv636,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv637 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_30_mlp_down_proj_q_weight8, model_layers_30_mlp_down_proj_q_scale8, lv160), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv634_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv637, lv633_1, model_layers_31_input_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv635_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv634_1[1]
            rms_norm452: R.Tensor((1, seq_len, 4096), dtype="float16") = lv634_1[0]
            lv638 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_31_self_attn_qkv_proj_q_weight8, model_layers_31_self_attn_qkv_proj_q_scale8, rms_norm452), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape892 = R.call_tir(cls.reshape4, (lv638,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape893 = R.call_tir(cls.reshape5, (reshape892,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1123 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape893), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape894 = R.call_tir(cls.reshape6, (lv1123,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape895 = R.call_tir(cls.reshape7, (reshape894,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv639 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_31_self_attn_o_proj_q_weight8, model_layers_31_self_attn_o_proj_q_scale8, reshape895), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv636_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv639, lv635_1, model_layers_31_post_attention_layernorm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv637_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv636_1[1]
            rms_norm453: R.Tensor((1, seq_len, 4096), dtype="float16") = lv636_1[0]
            lv640 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_31_mlp_gate_up_proj_q_weight8, model_layers_31_mlp_gate_up_proj_q_scale8, rms_norm453), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv161 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv640,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv641 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_31_mlp_down_proj_q_weight8, model_layers_31_mlp_down_proj_q_scale8, lv161), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv638_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv641, lv637_1, model_norm_weight8), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm454: R.Tensor((1, seq_len, 4096), dtype="float16") = lv638_1[0]
            lv642 = R.call_tir(cls.fused_dequantize_fused_NT_matmul9_cast1, (lm_head_q_weight8, lm_head_q_scale8, rms_norm454), out_sinfo=R.Tensor((1, seq_len, vocab_size), dtype="float32"))
            gv9: R.Tuple(R.Tensor((1, seq_len, vocab_size), dtype="float32"), R.Object) = lv642, paged_kv_cache
            R.output(gv9)
        return gv9

    @R.function
    def batch_verify_to_last_hidden_states(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size", "seq_len"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm585 = R.call_tir(cls.rms_norm1, (input_embeds, model_layers_0_input_layernorm_weight11), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv643 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_0_self_attn_qkv_proj_q_weight11, model_layers_0_self_attn_qkv_proj_q_scale11, rms_norm585), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1152 = R.call_tir(cls.reshape4, (lv643,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1153 = R.call_tir(cls.reshape5, (reshape1152,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1449 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1153), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1154 = R.call_tir(cls.reshape6, (lv1449,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1155 = R.call_tir(cls.reshape7, (reshape1154,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv644 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_0_self_attn_o_proj_q_weight11, model_layers_0_self_attn_o_proj_q_scale11, reshape1155), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv640 = R.call_tir(cls.fuse_add_norm_prefill, (lv644, input_embeds, model_layers_0_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv641: R.Tensor((1, seq_len, 4096), dtype="float16") = lv640[1]
            rms_norm586: R.Tensor((1, seq_len, 4096), dtype="float16") = lv640[0]
            lv645 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_0_mlp_gate_up_proj_q_weight11, model_layers_0_mlp_gate_up_proj_q_scale11, rms_norm586), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv163 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv645,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv646 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_0_mlp_down_proj_q_weight11, model_layers_0_mlp_down_proj_q_scale11, lv163), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv642 = R.call_tir(cls.fuse_add_norm_prefill, (lv646, lv641, model_layers_1_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv643_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv642[1]
            rms_norm587: R.Tensor((1, seq_len, 4096), dtype="float16") = lv642[0]
            lv647 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_1_self_attn_qkv_proj_q_weight11, model_layers_1_self_attn_qkv_proj_q_scale11, rms_norm587), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1156 = R.call_tir(cls.reshape4, (lv647,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1157 = R.call_tir(cls.reshape5, (reshape1156,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1454 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape1157), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1158 = R.call_tir(cls.reshape6, (lv1454,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1159 = R.call_tir(cls.reshape7, (reshape1158,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv648 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_1_self_attn_o_proj_q_weight11, model_layers_1_self_attn_o_proj_q_scale11, reshape1159), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv644_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv648, lv643_1, model_layers_1_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv645_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv644_1[1]
            rms_norm588: R.Tensor((1, seq_len, 4096), dtype="float16") = lv644_1[0]
            lv649 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_1_mlp_gate_up_proj_q_weight11, model_layers_1_mlp_gate_up_proj_q_scale11, rms_norm588), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv164 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv649,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv650 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_1_mlp_down_proj_q_weight11, model_layers_1_mlp_down_proj_q_scale11, lv164), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv646_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv650, lv645_1, model_layers_2_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv647_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv646_1[1]
            rms_norm589: R.Tensor((1, seq_len, 4096), dtype="float16") = lv646_1[0]
            lv651 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_2_self_attn_qkv_proj_q_weight11, model_layers_2_self_attn_qkv_proj_q_scale11, rms_norm589), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1160 = R.call_tir(cls.reshape4, (lv651,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1161 = R.call_tir(cls.reshape5, (reshape1160,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1459 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape1161), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1162 = R.call_tir(cls.reshape6, (lv1459,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1163 = R.call_tir(cls.reshape7, (reshape1162,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv652 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_2_self_attn_o_proj_q_weight11, model_layers_2_self_attn_o_proj_q_scale11, reshape1163), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv648_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv652, lv647_1, model_layers_2_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv649_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv648_1[1]
            rms_norm590: R.Tensor((1, seq_len, 4096), dtype="float16") = lv648_1[0]
            lv653 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_2_mlp_gate_up_proj_q_weight11, model_layers_2_mlp_gate_up_proj_q_scale11, rms_norm590), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv165 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv653,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv654 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_2_mlp_down_proj_q_weight11, model_layers_2_mlp_down_proj_q_scale11, lv165), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv650_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv654, lv649_1, model_layers_3_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv651_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv650_1[1]
            rms_norm591: R.Tensor((1, seq_len, 4096), dtype="float16") = lv650_1[0]
            lv655 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_3_self_attn_qkv_proj_q_weight11, model_layers_3_self_attn_qkv_proj_q_scale11, rms_norm591), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1164 = R.call_tir(cls.reshape4, (lv655,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1165 = R.call_tir(cls.reshape5, (reshape1164,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1464 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape1165), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1166 = R.call_tir(cls.reshape6, (lv1464,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1167 = R.call_tir(cls.reshape7, (reshape1166,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv656 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_3_self_attn_o_proj_q_weight11, model_layers_3_self_attn_o_proj_q_scale11, reshape1167), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv652_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv656, lv651_1, model_layers_3_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv653_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv652_1[1]
            rms_norm592: R.Tensor((1, seq_len, 4096), dtype="float16") = lv652_1[0]
            lv657 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_3_mlp_gate_up_proj_q_weight11, model_layers_3_mlp_gate_up_proj_q_scale11, rms_norm592), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv166 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv657,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv658 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_3_mlp_down_proj_q_weight11, model_layers_3_mlp_down_proj_q_scale11, lv166), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv654_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv658, lv653_1, model_layers_4_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv655_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv654_1[1]
            rms_norm593: R.Tensor((1, seq_len, 4096), dtype="float16") = lv654_1[0]
            lv659 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_4_self_attn_qkv_proj_q_weight11, model_layers_4_self_attn_qkv_proj_q_scale11, rms_norm593), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1168 = R.call_tir(cls.reshape4, (lv659,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1169 = R.call_tir(cls.reshape5, (reshape1168,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1469 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape1169), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1170 = R.call_tir(cls.reshape6, (lv1469,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1171 = R.call_tir(cls.reshape7, (reshape1170,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv660 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_4_self_attn_o_proj_q_weight11, model_layers_4_self_attn_o_proj_q_scale11, reshape1171), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv656_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv660, lv655_1, model_layers_4_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv657_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv656_1[1]
            rms_norm594: R.Tensor((1, seq_len, 4096), dtype="float16") = lv656_1[0]
            lv661 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_4_mlp_gate_up_proj_q_weight11, model_layers_4_mlp_gate_up_proj_q_scale11, rms_norm594), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv167 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv661,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv662 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_4_mlp_down_proj_q_weight11, model_layers_4_mlp_down_proj_q_scale11, lv167), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv658_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv662, lv657_1, model_layers_5_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv659_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv658_1[1]
            rms_norm595: R.Tensor((1, seq_len, 4096), dtype="float16") = lv658_1[0]
            lv663 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_5_self_attn_qkv_proj_q_weight11, model_layers_5_self_attn_qkv_proj_q_scale11, rms_norm595), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1172 = R.call_tir(cls.reshape4, (lv663,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1173 = R.call_tir(cls.reshape5, (reshape1172,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1474 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape1173), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1174 = R.call_tir(cls.reshape6, (lv1474,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1175 = R.call_tir(cls.reshape7, (reshape1174,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv664 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_5_self_attn_o_proj_q_weight11, model_layers_5_self_attn_o_proj_q_scale11, reshape1175), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv660_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv664, lv659_1, model_layers_5_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv661_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv660_1[1]
            rms_norm596: R.Tensor((1, seq_len, 4096), dtype="float16") = lv660_1[0]
            lv665 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_5_mlp_gate_up_proj_q_weight11, model_layers_5_mlp_gate_up_proj_q_scale11, rms_norm596), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv168 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv665,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv666 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_5_mlp_down_proj_q_weight11, model_layers_5_mlp_down_proj_q_scale11, lv168), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv662_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv666, lv661_1, model_layers_6_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv663_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv662_1[1]
            rms_norm597: R.Tensor((1, seq_len, 4096), dtype="float16") = lv662_1[0]
            lv667 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_6_self_attn_qkv_proj_q_weight11, model_layers_6_self_attn_qkv_proj_q_scale11, rms_norm597), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1176 = R.call_tir(cls.reshape4, (lv667,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1177 = R.call_tir(cls.reshape5, (reshape1176,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1479 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape1177), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1178 = R.call_tir(cls.reshape6, (lv1479,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1179 = R.call_tir(cls.reshape7, (reshape1178,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv668 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_6_self_attn_o_proj_q_weight11, model_layers_6_self_attn_o_proj_q_scale11, reshape1179), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv664_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv668, lv663_1, model_layers_6_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv665_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv664_1[1]
            rms_norm598: R.Tensor((1, seq_len, 4096), dtype="float16") = lv664_1[0]
            lv669 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_6_mlp_gate_up_proj_q_weight11, model_layers_6_mlp_gate_up_proj_q_scale11, rms_norm598), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv169 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv669,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv670 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_6_mlp_down_proj_q_weight11, model_layers_6_mlp_down_proj_q_scale11, lv169), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv666_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv670, lv665_1, model_layers_7_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv667_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv666_1[1]
            rms_norm599: R.Tensor((1, seq_len, 4096), dtype="float16") = lv666_1[0]
            lv671 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_7_self_attn_qkv_proj_q_weight11, model_layers_7_self_attn_qkv_proj_q_scale11, rms_norm599), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1180 = R.call_tir(cls.reshape4, (lv671,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1181 = R.call_tir(cls.reshape5, (reshape1180,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1484 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape1181), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1182 = R.call_tir(cls.reshape6, (lv1484,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1183 = R.call_tir(cls.reshape7, (reshape1182,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv672 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_7_self_attn_o_proj_q_weight11, model_layers_7_self_attn_o_proj_q_scale11, reshape1183), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv668_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv672, lv667_1, model_layers_7_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv669_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv668_1[1]
            rms_norm600: R.Tensor((1, seq_len, 4096), dtype="float16") = lv668_1[0]
            lv673 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_7_mlp_gate_up_proj_q_weight11, model_layers_7_mlp_gate_up_proj_q_scale11, rms_norm600), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv170 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv673,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv674 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_7_mlp_down_proj_q_weight11, model_layers_7_mlp_down_proj_q_scale11, lv170), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv670_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv674, lv669_1, model_layers_8_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv671_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv670_1[1]
            rms_norm601: R.Tensor((1, seq_len, 4096), dtype="float16") = lv670_1[0]
            lv675 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_8_self_attn_qkv_proj_q_weight11, model_layers_8_self_attn_qkv_proj_q_scale11, rms_norm601), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1184 = R.call_tir(cls.reshape4, (lv675,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1185 = R.call_tir(cls.reshape5, (reshape1184,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1489 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape1185), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1186 = R.call_tir(cls.reshape6, (lv1489,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1187 = R.call_tir(cls.reshape7, (reshape1186,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv676 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_8_self_attn_o_proj_q_weight11, model_layers_8_self_attn_o_proj_q_scale11, reshape1187), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv672_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv676, lv671_1, model_layers_8_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv673_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv672_1[1]
            rms_norm602: R.Tensor((1, seq_len, 4096), dtype="float16") = lv672_1[0]
            lv677 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_8_mlp_gate_up_proj_q_weight11, model_layers_8_mlp_gate_up_proj_q_scale11, rms_norm602), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv171 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv677,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv678 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_8_mlp_down_proj_q_weight11, model_layers_8_mlp_down_proj_q_scale11, lv171), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv674_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv678, lv673_1, model_layers_9_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv675_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv674_1[1]
            rms_norm603: R.Tensor((1, seq_len, 4096), dtype="float16") = lv674_1[0]
            lv679 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_9_self_attn_qkv_proj_q_weight11, model_layers_9_self_attn_qkv_proj_q_scale11, rms_norm603), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1188 = R.call_tir(cls.reshape4, (lv679,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1189 = R.call_tir(cls.reshape5, (reshape1188,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1494 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape1189), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1190 = R.call_tir(cls.reshape6, (lv1494,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1191 = R.call_tir(cls.reshape7, (reshape1190,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv680 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_9_self_attn_o_proj_q_weight11, model_layers_9_self_attn_o_proj_q_scale11, reshape1191), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv676_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv680, lv675_1, model_layers_9_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv677_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv676_1[1]
            rms_norm604: R.Tensor((1, seq_len, 4096), dtype="float16") = lv676_1[0]
            lv681 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_9_mlp_gate_up_proj_q_weight11, model_layers_9_mlp_gate_up_proj_q_scale11, rms_norm604), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv172 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv681,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv682 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_9_mlp_down_proj_q_weight11, model_layers_9_mlp_down_proj_q_scale11, lv172), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv678_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv682, lv677_1, model_layers_10_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv679_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv678_1[1]
            rms_norm605: R.Tensor((1, seq_len, 4096), dtype="float16") = lv678_1[0]
            lv683 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_10_self_attn_qkv_proj_q_weight11, model_layers_10_self_attn_qkv_proj_q_scale11, rms_norm605), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1192 = R.call_tir(cls.reshape4, (lv683,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1193 = R.call_tir(cls.reshape5, (reshape1192,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1499 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape1193), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1194 = R.call_tir(cls.reshape6, (lv1499,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1195 = R.call_tir(cls.reshape7, (reshape1194,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv684 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_10_self_attn_o_proj_q_weight11, model_layers_10_self_attn_o_proj_q_scale11, reshape1195), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv680_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv684, lv679_1, model_layers_10_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv681_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv680_1[1]
            rms_norm606: R.Tensor((1, seq_len, 4096), dtype="float16") = lv680_1[0]
            lv685 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_10_mlp_gate_up_proj_q_weight11, model_layers_10_mlp_gate_up_proj_q_scale11, rms_norm606), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv173 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv685,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv686 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_10_mlp_down_proj_q_weight11, model_layers_10_mlp_down_proj_q_scale11, lv173), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv682_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv686, lv681_1, model_layers_11_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv683_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv682_1[1]
            rms_norm607: R.Tensor((1, seq_len, 4096), dtype="float16") = lv682_1[0]
            lv687 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_11_self_attn_qkv_proj_q_weight11, model_layers_11_self_attn_qkv_proj_q_scale11, rms_norm607), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1196 = R.call_tir(cls.reshape4, (lv687,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1197 = R.call_tir(cls.reshape5, (reshape1196,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1504 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape1197), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1198 = R.call_tir(cls.reshape6, (lv1504,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1199 = R.call_tir(cls.reshape7, (reshape1198,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv688 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_11_self_attn_o_proj_q_weight11, model_layers_11_self_attn_o_proj_q_scale11, reshape1199), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv684_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv688, lv683_1, model_layers_11_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv685_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv684_1[1]
            rms_norm608: R.Tensor((1, seq_len, 4096), dtype="float16") = lv684_1[0]
            lv689 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_11_mlp_gate_up_proj_q_weight11, model_layers_11_mlp_gate_up_proj_q_scale11, rms_norm608), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv174 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv689,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv690 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_11_mlp_down_proj_q_weight11, model_layers_11_mlp_down_proj_q_scale11, lv174), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv686_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv690, lv685_1, model_layers_12_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv687_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv686_1[1]
            rms_norm609: R.Tensor((1, seq_len, 4096), dtype="float16") = lv686_1[0]
            lv691 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_12_self_attn_qkv_proj_q_weight11, model_layers_12_self_attn_qkv_proj_q_scale11, rms_norm609), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1200 = R.call_tir(cls.reshape4, (lv691,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1201 = R.call_tir(cls.reshape5, (reshape1200,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1509 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape1201), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1202 = R.call_tir(cls.reshape6, (lv1509,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1203 = R.call_tir(cls.reshape7, (reshape1202,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv692 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_12_self_attn_o_proj_q_weight11, model_layers_12_self_attn_o_proj_q_scale11, reshape1203), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv688_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv692, lv687_1, model_layers_12_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv689_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv688_1[1]
            rms_norm610: R.Tensor((1, seq_len, 4096), dtype="float16") = lv688_1[0]
            lv693 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_12_mlp_gate_up_proj_q_weight11, model_layers_12_mlp_gate_up_proj_q_scale11, rms_norm610), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv175 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv693,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv694 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_12_mlp_down_proj_q_weight11, model_layers_12_mlp_down_proj_q_scale11, lv175), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv690_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv694, lv689_1, model_layers_13_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv691_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv690_1[1]
            rms_norm611: R.Tensor((1, seq_len, 4096), dtype="float16") = lv690_1[0]
            lv695 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_13_self_attn_qkv_proj_q_weight11, model_layers_13_self_attn_qkv_proj_q_scale11, rms_norm611), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1204 = R.call_tir(cls.reshape4, (lv695,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1205 = R.call_tir(cls.reshape5, (reshape1204,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1514 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape1205), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1206 = R.call_tir(cls.reshape6, (lv1514,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1207 = R.call_tir(cls.reshape7, (reshape1206,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv696 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_13_self_attn_o_proj_q_weight11, model_layers_13_self_attn_o_proj_q_scale11, reshape1207), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv692_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv696, lv691_1, model_layers_13_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv693_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv692_1[1]
            rms_norm612: R.Tensor((1, seq_len, 4096), dtype="float16") = lv692_1[0]
            lv697 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_13_mlp_gate_up_proj_q_weight11, model_layers_13_mlp_gate_up_proj_q_scale11, rms_norm612), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv176 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv697,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv698 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_13_mlp_down_proj_q_weight11, model_layers_13_mlp_down_proj_q_scale11, lv176), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv694_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv698, lv693_1, model_layers_14_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv695_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv694_1[1]
            rms_norm613: R.Tensor((1, seq_len, 4096), dtype="float16") = lv694_1[0]
            lv699 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_14_self_attn_qkv_proj_q_weight11, model_layers_14_self_attn_qkv_proj_q_scale11, rms_norm613), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1208 = R.call_tir(cls.reshape4, (lv699,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1209 = R.call_tir(cls.reshape5, (reshape1208,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1519 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape1209), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1210 = R.call_tir(cls.reshape6, (lv1519,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1211 = R.call_tir(cls.reshape7, (reshape1210,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv700 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_14_self_attn_o_proj_q_weight11, model_layers_14_self_attn_o_proj_q_scale11, reshape1211), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv696_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv700, lv695_1, model_layers_14_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv697_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv696_1[1]
            rms_norm614: R.Tensor((1, seq_len, 4096), dtype="float16") = lv696_1[0]
            lv701 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_14_mlp_gate_up_proj_q_weight11, model_layers_14_mlp_gate_up_proj_q_scale11, rms_norm614), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv177 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv701,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv702 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_14_mlp_down_proj_q_weight11, model_layers_14_mlp_down_proj_q_scale11, lv177), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv698_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv702, lv697_1, model_layers_15_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv699_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv698_1[1]
            rms_norm615: R.Tensor((1, seq_len, 4096), dtype="float16") = lv698_1[0]
            lv703 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_15_self_attn_qkv_proj_q_weight11, model_layers_15_self_attn_qkv_proj_q_scale11, rms_norm615), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1212 = R.call_tir(cls.reshape4, (lv703,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1213 = R.call_tir(cls.reshape5, (reshape1212,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1524 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape1213), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1214 = R.call_tir(cls.reshape6, (lv1524,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1215 = R.call_tir(cls.reshape7, (reshape1214,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv704 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_15_self_attn_o_proj_q_weight11, model_layers_15_self_attn_o_proj_q_scale11, reshape1215), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv700_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv704, lv699_1, model_layers_15_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv701_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv700_1[1]
            rms_norm616: R.Tensor((1, seq_len, 4096), dtype="float16") = lv700_1[0]
            lv705 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_15_mlp_gate_up_proj_q_weight11, model_layers_15_mlp_gate_up_proj_q_scale11, rms_norm616), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv178 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv705,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv706 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_15_mlp_down_proj_q_weight11, model_layers_15_mlp_down_proj_q_scale11, lv178), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv702_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv706, lv701_1, model_layers_16_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv703_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv702_1[1]
            rms_norm617: R.Tensor((1, seq_len, 4096), dtype="float16") = lv702_1[0]
            lv707 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_16_self_attn_qkv_proj_q_weight11, model_layers_16_self_attn_qkv_proj_q_scale11, rms_norm617), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1216 = R.call_tir(cls.reshape4, (lv707,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1217 = R.call_tir(cls.reshape5, (reshape1216,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1529 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape1217), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1218 = R.call_tir(cls.reshape6, (lv1529,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1219 = R.call_tir(cls.reshape7, (reshape1218,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv708 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_16_self_attn_o_proj_q_weight11, model_layers_16_self_attn_o_proj_q_scale11, reshape1219), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv704_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv708, lv703_1, model_layers_16_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv705_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv704_1[1]
            rms_norm618: R.Tensor((1, seq_len, 4096), dtype="float16") = lv704_1[0]
            lv709 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_16_mlp_gate_up_proj_q_weight11, model_layers_16_mlp_gate_up_proj_q_scale11, rms_norm618), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv179 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv709,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv710 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_16_mlp_down_proj_q_weight11, model_layers_16_mlp_down_proj_q_scale11, lv179), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv706_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv710, lv705_1, model_layers_17_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv707_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv706_1[1]
            rms_norm619: R.Tensor((1, seq_len, 4096), dtype="float16") = lv706_1[0]
            lv711 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_17_self_attn_qkv_proj_q_weight11, model_layers_17_self_attn_qkv_proj_q_scale11, rms_norm619), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1220 = R.call_tir(cls.reshape4, (lv711,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1221 = R.call_tir(cls.reshape5, (reshape1220,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1534 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape1221), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1222 = R.call_tir(cls.reshape6, (lv1534,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1223 = R.call_tir(cls.reshape7, (reshape1222,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv712 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_17_self_attn_o_proj_q_weight11, model_layers_17_self_attn_o_proj_q_scale11, reshape1223), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv708_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv712, lv707_1, model_layers_17_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv709_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv708_1[1]
            rms_norm620: R.Tensor((1, seq_len, 4096), dtype="float16") = lv708_1[0]
            lv713 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_17_mlp_gate_up_proj_q_weight11, model_layers_17_mlp_gate_up_proj_q_scale11, rms_norm620), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv180 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv713,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv714 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_17_mlp_down_proj_q_weight11, model_layers_17_mlp_down_proj_q_scale11, lv180), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv710_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv714, lv709_1, model_layers_18_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv711_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv710_1[1]
            rms_norm621: R.Tensor((1, seq_len, 4096), dtype="float16") = lv710_1[0]
            lv715 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_18_self_attn_qkv_proj_q_weight11, model_layers_18_self_attn_qkv_proj_q_scale11, rms_norm621), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1224 = R.call_tir(cls.reshape4, (lv715,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1225 = R.call_tir(cls.reshape5, (reshape1224,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1539 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape1225), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1226 = R.call_tir(cls.reshape6, (lv1539,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1227 = R.call_tir(cls.reshape7, (reshape1226,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv716 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_18_self_attn_o_proj_q_weight11, model_layers_18_self_attn_o_proj_q_scale11, reshape1227), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv712_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv716, lv711_1, model_layers_18_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv713_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv712_1[1]
            rms_norm622: R.Tensor((1, seq_len, 4096), dtype="float16") = lv712_1[0]
            lv717 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_18_mlp_gate_up_proj_q_weight11, model_layers_18_mlp_gate_up_proj_q_scale11, rms_norm622), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv181 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv717,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv718 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_18_mlp_down_proj_q_weight11, model_layers_18_mlp_down_proj_q_scale11, lv181), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv714_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv718, lv713_1, model_layers_19_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv715_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv714_1[1]
            rms_norm623: R.Tensor((1, seq_len, 4096), dtype="float16") = lv714_1[0]
            lv719 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_19_self_attn_qkv_proj_q_weight11, model_layers_19_self_attn_qkv_proj_q_scale11, rms_norm623), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1228 = R.call_tir(cls.reshape4, (lv719,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1229 = R.call_tir(cls.reshape5, (reshape1228,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1544 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape1229), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1230 = R.call_tir(cls.reshape6, (lv1544,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1231 = R.call_tir(cls.reshape7, (reshape1230,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv720 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_19_self_attn_o_proj_q_weight11, model_layers_19_self_attn_o_proj_q_scale11, reshape1231), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv716_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv720, lv715_1, model_layers_19_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv717_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv716_1[1]
            rms_norm624: R.Tensor((1, seq_len, 4096), dtype="float16") = lv716_1[0]
            lv721 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_19_mlp_gate_up_proj_q_weight11, model_layers_19_mlp_gate_up_proj_q_scale11, rms_norm624), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv182 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv721,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv722 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_19_mlp_down_proj_q_weight11, model_layers_19_mlp_down_proj_q_scale11, lv182), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv718_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv722, lv717_1, model_layers_20_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv719_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv718_1[1]
            rms_norm625: R.Tensor((1, seq_len, 4096), dtype="float16") = lv718_1[0]
            lv723 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_20_self_attn_qkv_proj_q_weight11, model_layers_20_self_attn_qkv_proj_q_scale11, rms_norm625), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1232 = R.call_tir(cls.reshape4, (lv723,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1233 = R.call_tir(cls.reshape5, (reshape1232,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1549 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape1233), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1234 = R.call_tir(cls.reshape6, (lv1549,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1235 = R.call_tir(cls.reshape7, (reshape1234,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv724 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_20_self_attn_o_proj_q_weight11, model_layers_20_self_attn_o_proj_q_scale11, reshape1235), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv720_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv724, lv719_1, model_layers_20_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv721_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv720_1[1]
            rms_norm626: R.Tensor((1, seq_len, 4096), dtype="float16") = lv720_1[0]
            lv725 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_20_mlp_gate_up_proj_q_weight11, model_layers_20_mlp_gate_up_proj_q_scale11, rms_norm626), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv183 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv725,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv726 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_20_mlp_down_proj_q_weight11, model_layers_20_mlp_down_proj_q_scale11, lv183), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv722_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv726, lv721_1, model_layers_21_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv723_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv722_1[1]
            rms_norm627: R.Tensor((1, seq_len, 4096), dtype="float16") = lv722_1[0]
            lv727 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_21_self_attn_qkv_proj_q_weight11, model_layers_21_self_attn_qkv_proj_q_scale11, rms_norm627), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1236 = R.call_tir(cls.reshape4, (lv727,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1237 = R.call_tir(cls.reshape5, (reshape1236,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1554 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape1237), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1238 = R.call_tir(cls.reshape6, (lv1554,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1239 = R.call_tir(cls.reshape7, (reshape1238,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv728 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_21_self_attn_o_proj_q_weight11, model_layers_21_self_attn_o_proj_q_scale11, reshape1239), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv724_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv728, lv723_1, model_layers_21_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv725_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv724_1[1]
            rms_norm628: R.Tensor((1, seq_len, 4096), dtype="float16") = lv724_1[0]
            lv729 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_21_mlp_gate_up_proj_q_weight11, model_layers_21_mlp_gate_up_proj_q_scale11, rms_norm628), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv184 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv729,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv730 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_21_mlp_down_proj_q_weight11, model_layers_21_mlp_down_proj_q_scale11, lv184), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv726_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv730, lv725_1, model_layers_22_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv727_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv726_1[1]
            rms_norm629: R.Tensor((1, seq_len, 4096), dtype="float16") = lv726_1[0]
            lv731 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_22_self_attn_qkv_proj_q_weight11, model_layers_22_self_attn_qkv_proj_q_scale11, rms_norm629), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1240 = R.call_tir(cls.reshape4, (lv731,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1241 = R.call_tir(cls.reshape5, (reshape1240,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1559 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape1241), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1242 = R.call_tir(cls.reshape6, (lv1559,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1243 = R.call_tir(cls.reshape7, (reshape1242,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv732 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_22_self_attn_o_proj_q_weight11, model_layers_22_self_attn_o_proj_q_scale11, reshape1243), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv728_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv732, lv727_1, model_layers_22_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv729_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv728_1[1]
            rms_norm630: R.Tensor((1, seq_len, 4096), dtype="float16") = lv728_1[0]
            lv733 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_22_mlp_gate_up_proj_q_weight11, model_layers_22_mlp_gate_up_proj_q_scale11, rms_norm630), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv185 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv733,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv734 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_22_mlp_down_proj_q_weight11, model_layers_22_mlp_down_proj_q_scale11, lv185), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv730_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv734, lv729_1, model_layers_23_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv731_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv730_1[1]
            rms_norm631: R.Tensor((1, seq_len, 4096), dtype="float16") = lv730_1[0]
            lv735 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_23_self_attn_qkv_proj_q_weight11, model_layers_23_self_attn_qkv_proj_q_scale11, rms_norm631), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1244 = R.call_tir(cls.reshape4, (lv735,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1245 = R.call_tir(cls.reshape5, (reshape1244,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1564 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape1245), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1246 = R.call_tir(cls.reshape6, (lv1564,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1247 = R.call_tir(cls.reshape7, (reshape1246,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv736 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_23_self_attn_o_proj_q_weight11, model_layers_23_self_attn_o_proj_q_scale11, reshape1247), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv732_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv736, lv731_1, model_layers_23_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv733_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv732_1[1]
            rms_norm632: R.Tensor((1, seq_len, 4096), dtype="float16") = lv732_1[0]
            lv737 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_23_mlp_gate_up_proj_q_weight11, model_layers_23_mlp_gate_up_proj_q_scale11, rms_norm632), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv186 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv737,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv738 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_23_mlp_down_proj_q_weight11, model_layers_23_mlp_down_proj_q_scale11, lv186), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv734_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv738, lv733_1, model_layers_24_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv735_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv734_1[1]
            rms_norm633: R.Tensor((1, seq_len, 4096), dtype="float16") = lv734_1[0]
            lv739 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_24_self_attn_qkv_proj_q_weight11, model_layers_24_self_attn_qkv_proj_q_scale11, rms_norm633), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1248 = R.call_tir(cls.reshape4, (lv739,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1249 = R.call_tir(cls.reshape5, (reshape1248,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1569 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape1249), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1250 = R.call_tir(cls.reshape6, (lv1569,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1251 = R.call_tir(cls.reshape7, (reshape1250,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv740 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_24_self_attn_o_proj_q_weight11, model_layers_24_self_attn_o_proj_q_scale11, reshape1251), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv736_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv740, lv735_1, model_layers_24_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv737_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv736_1[1]
            rms_norm634: R.Tensor((1, seq_len, 4096), dtype="float16") = lv736_1[0]
            lv741 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_24_mlp_gate_up_proj_q_weight11, model_layers_24_mlp_gate_up_proj_q_scale11, rms_norm634), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv187 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv741,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv742 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_24_mlp_down_proj_q_weight11, model_layers_24_mlp_down_proj_q_scale11, lv187), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv738_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv742, lv737_1, model_layers_25_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv739_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv738_1[1]
            rms_norm635: R.Tensor((1, seq_len, 4096), dtype="float16") = lv738_1[0]
            lv743 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_25_self_attn_qkv_proj_q_weight11, model_layers_25_self_attn_qkv_proj_q_scale11, rms_norm635), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1252 = R.call_tir(cls.reshape4, (lv743,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1253 = R.call_tir(cls.reshape5, (reshape1252,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1574 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape1253), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1254 = R.call_tir(cls.reshape6, (lv1574,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1255 = R.call_tir(cls.reshape7, (reshape1254,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv744 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_25_self_attn_o_proj_q_weight11, model_layers_25_self_attn_o_proj_q_scale11, reshape1255), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv740_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv744, lv739_1, model_layers_25_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv741_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv740_1[1]
            rms_norm636: R.Tensor((1, seq_len, 4096), dtype="float16") = lv740_1[0]
            lv745 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_25_mlp_gate_up_proj_q_weight11, model_layers_25_mlp_gate_up_proj_q_scale11, rms_norm636), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv188 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv745,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv746 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_25_mlp_down_proj_q_weight11, model_layers_25_mlp_down_proj_q_scale11, lv188), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv742_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv746, lv741_1, model_layers_26_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv743_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv742_1[1]
            rms_norm637: R.Tensor((1, seq_len, 4096), dtype="float16") = lv742_1[0]
            lv747 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_26_self_attn_qkv_proj_q_weight11, model_layers_26_self_attn_qkv_proj_q_scale11, rms_norm637), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1256 = R.call_tir(cls.reshape4, (lv747,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1257 = R.call_tir(cls.reshape5, (reshape1256,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1579 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1257), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1258 = R.call_tir(cls.reshape6, (lv1579,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1259 = R.call_tir(cls.reshape7, (reshape1258,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv748 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_26_self_attn_o_proj_q_weight11, model_layers_26_self_attn_o_proj_q_scale11, reshape1259), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv744_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv748, lv743_1, model_layers_26_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv745_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv744_1[1]
            rms_norm638: R.Tensor((1, seq_len, 4096), dtype="float16") = lv744_1[0]
            lv749 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_26_mlp_gate_up_proj_q_weight11, model_layers_26_mlp_gate_up_proj_q_scale11, rms_norm638), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv189 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv749,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv750 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_26_mlp_down_proj_q_weight11, model_layers_26_mlp_down_proj_q_scale11, lv189), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv746_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv750, lv745_1, model_layers_27_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv747_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv746_1[1]
            rms_norm639: R.Tensor((1, seq_len, 4096), dtype="float16") = lv746_1[0]
            lv751 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_27_self_attn_qkv_proj_q_weight11, model_layers_27_self_attn_qkv_proj_q_scale11, rms_norm639), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1260 = R.call_tir(cls.reshape4, (lv751,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1261 = R.call_tir(cls.reshape5, (reshape1260,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1584 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1261), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1262 = R.call_tir(cls.reshape6, (lv1584,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1263 = R.call_tir(cls.reshape7, (reshape1262,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv752 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_27_self_attn_o_proj_q_weight11, model_layers_27_self_attn_o_proj_q_scale11, reshape1263), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv748_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv752, lv747_1, model_layers_27_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv749_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv748_1[1]
            rms_norm640: R.Tensor((1, seq_len, 4096), dtype="float16") = lv748_1[0]
            lv753 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_27_mlp_gate_up_proj_q_weight11, model_layers_27_mlp_gate_up_proj_q_scale11, rms_norm640), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv190 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv753,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv754 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_27_mlp_down_proj_q_weight11, model_layers_27_mlp_down_proj_q_scale11, lv190), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv750_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv754, lv749_1, model_layers_28_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv751_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv750_1[1]
            rms_norm641: R.Tensor((1, seq_len, 4096), dtype="float16") = lv750_1[0]
            lv755 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_28_self_attn_qkv_proj_q_weight11, model_layers_28_self_attn_qkv_proj_q_scale11, rms_norm641), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1264 = R.call_tir(cls.reshape4, (lv755,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1265 = R.call_tir(cls.reshape5, (reshape1264,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1589 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1265), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1266 = R.call_tir(cls.reshape6, (lv1589,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1267 = R.call_tir(cls.reshape7, (reshape1266,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv756 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_28_self_attn_o_proj_q_weight11, model_layers_28_self_attn_o_proj_q_scale11, reshape1267), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv752_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv756, lv751_1, model_layers_28_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv753_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv752_1[1]
            rms_norm642: R.Tensor((1, seq_len, 4096), dtype="float16") = lv752_1[0]
            lv757 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_28_mlp_gate_up_proj_q_weight11, model_layers_28_mlp_gate_up_proj_q_scale11, rms_norm642), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv191 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv757,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv758 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_28_mlp_down_proj_q_weight11, model_layers_28_mlp_down_proj_q_scale11, lv191), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv754_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv758, lv753_1, model_layers_29_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv755_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv754_1[1]
            rms_norm643: R.Tensor((1, seq_len, 4096), dtype="float16") = lv754_1[0]
            lv759 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_29_self_attn_qkv_proj_q_weight11, model_layers_29_self_attn_qkv_proj_q_scale11, rms_norm643), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1268 = R.call_tir(cls.reshape4, (lv759,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1269 = R.call_tir(cls.reshape5, (reshape1268,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1594 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1269), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1270 = R.call_tir(cls.reshape6, (lv1594,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1271 = R.call_tir(cls.reshape7, (reshape1270,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv760 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_29_self_attn_o_proj_q_weight11, model_layers_29_self_attn_o_proj_q_scale11, reshape1271), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv756_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv760, lv755_1, model_layers_29_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv757_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv756_1[1]
            rms_norm644: R.Tensor((1, seq_len, 4096), dtype="float16") = lv756_1[0]
            lv761 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_29_mlp_gate_up_proj_q_weight11, model_layers_29_mlp_gate_up_proj_q_scale11, rms_norm644), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv192 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv761,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv762 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_29_mlp_down_proj_q_weight11, model_layers_29_mlp_down_proj_q_scale11, lv192), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv758_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv762, lv757_1, model_layers_30_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv759_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv758_1[1]
            rms_norm645: R.Tensor((1, seq_len, 4096), dtype="float16") = lv758_1[0]
            lv763 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_30_self_attn_qkv_proj_q_weight11, model_layers_30_self_attn_qkv_proj_q_scale11, rms_norm645), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1272 = R.call_tir(cls.reshape4, (lv763,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1273 = R.call_tir(cls.reshape5, (reshape1272,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1599 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1273), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1274 = R.call_tir(cls.reshape6, (lv1599,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1275 = R.call_tir(cls.reshape7, (reshape1274,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv764 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_30_self_attn_o_proj_q_weight11, model_layers_30_self_attn_o_proj_q_scale11, reshape1275), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv760_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv764, lv759_1, model_layers_30_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv761_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv760_1[1]
            rms_norm646: R.Tensor((1, seq_len, 4096), dtype="float16") = lv760_1[0]
            lv765 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_30_mlp_gate_up_proj_q_weight11, model_layers_30_mlp_gate_up_proj_q_scale11, rms_norm646), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv193 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv765,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv766 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_30_mlp_down_proj_q_weight11, model_layers_30_mlp_down_proj_q_scale11, lv193), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv762_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv766, lv761_1, model_layers_31_input_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv763_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv762_1[1]
            rms_norm647: R.Tensor((1, seq_len, 4096), dtype="float16") = lv762_1[0]
            lv767 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_31_self_attn_qkv_proj_q_weight11, model_layers_31_self_attn_qkv_proj_q_scale11, rms_norm647), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape1276 = R.call_tir(cls.reshape4, (lv767,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1277 = R.call_tir(cls.reshape5, (reshape1276,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv1604 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1277), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1278 = R.call_tir(cls.reshape6, (lv1604,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape1279 = R.call_tir(cls.reshape7, (reshape1278,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv768 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_31_self_attn_o_proj_q_weight11, model_layers_31_self_attn_o_proj_q_scale11, reshape1279), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv764_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv768, lv763_1, model_layers_31_post_attention_layernorm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv765_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv764_1[1]
            rms_norm648: R.Tensor((1, seq_len, 4096), dtype="float16") = lv764_1[0]
            lv769 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_31_mlp_gate_up_proj_q_weight11, model_layers_31_mlp_gate_up_proj_q_scale11, rms_norm648), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv194 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv769,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv770 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_31_mlp_down_proj_q_weight11, model_layers_31_mlp_down_proj_q_scale11, lv194), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv766_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv770, lv765_1, model_norm_weight11), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm649: R.Tensor((1, seq_len, 4096), dtype="float16") = lv766_1[0]
            gv12: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm649, paged_kv_cache
            R.output(gv12)
        return gv12

    @R.function
    def create_tir_paged_kv_cache(max_batch_size_: R.Shape(["max_batch_size"]), max_total_seq_len_: R.Shape(["max_total_seq_len"]), prefill_chunk_size_: R.Shape(["prefill_chunk_size"]), page_size_: R.Shape(["page_size"]), support_sliding_window_: R.Shape(["support_sliding_window"])) -> R.Object:
        max_batch_size = T.int64()
        max_total_seq_len = T.int64()
        prefill_chunk_size = T.int64()
        page_size = T.int64()
        support_sliding_window = T.int64()
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        paged_kv_cache: R.Object = R.call_pure_packed("vm.builtin.paged_attention_kv_cache_create_reduced", R.shape([max_batch_size, max_total_seq_len, prefill_chunk_size, page_size, support_sliding_window]), R.shape([0, 32]), R.prim_value(32), R.prim_value(8), R.prim_value(128), R.prim_value(1), R.prim_value(1), R.prim_value(T.float32(500000.0)), R.const(0.0, "float16"), cls.tir_kv_cache_transpose_append, cls.batch_prefill_paged_kv, cls.batch_decode_paged_kv, cls.batch_prefill_paged_kv_sliding_window, cls.batch_decode_paged_kv_sliding_window, cls.batch_prefill_ragged_kv, cls.merge_state_inplace, cls.fused_rope, cls.copy_single_page, cls.tir_kv_cache_debug_get_kv, cls.compact_kv_copy, cls.batch_tree_attn, cls.tree_attn_paged_kv, R.prim_value(0), sinfo_args=(R.Object,))
        return paged_kv_cache

    @R.function
    def decode(input_embed: R.Tensor((1, 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight3: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale3: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm65 = R.call_tir(cls.rms_norm2, (input_embed, model_layers_0_input_layernorm_weight3), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv771 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_0_self_attn_qkv_proj_q_weight3, model_layers_0_self_attn_qkv_proj_q_scale3, rms_norm65), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv195 = R.call_tir(cls.fused_reshape8_reshape9, (lv771,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv165 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), lv195), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv196 = R.call_tir(cls.fused_reshape10_reshape11, (lv165,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv772 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_0_self_attn_o_proj_q_weight3, model_layers_0_self_attn_o_proj_q_scale3, lv196), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv768 = R.call_tir(cls.fuse_add_norm_prefill, (lv772, input_embed, model_layers_0_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv769: R.Tensor((1, 1, 4096), dtype="float16") = lv768[1]
            rms_norm66: R.Tensor((1, 1, 4096), dtype="float16") = lv768[0]
            lv773 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_0_mlp_gate_up_proj_q_weight3, model_layers_0_mlp_gate_up_proj_q_scale3, rms_norm66), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv197 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv773,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv774 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_0_mlp_down_proj_q_weight3, model_layers_0_mlp_down_proj_q_scale3, lv197), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv770 = R.call_tir(cls.fuse_add_norm_prefill, (lv774, lv769, model_layers_1_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv771_1: R.Tensor((1, 1, 4096), dtype="float16") = lv770[1]
            rms_norm67: R.Tensor((1, 1, 4096), dtype="float16") = lv770[0]
            lv775 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_1_self_attn_qkv_proj_q_weight3, model_layers_1_self_attn_qkv_proj_q_scale3, rms_norm67), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv198 = R.call_tir(cls.fused_reshape8_reshape9, (lv775,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv170 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), lv198), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv199 = R.call_tir(cls.fused_reshape10_reshape11, (lv170,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv776 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_1_self_attn_o_proj_q_weight3, model_layers_1_self_attn_o_proj_q_scale3, lv199), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv772_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv776, lv771_1, model_layers_1_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv773_1: R.Tensor((1, 1, 4096), dtype="float16") = lv772_1[1]
            rms_norm68: R.Tensor((1, 1, 4096), dtype="float16") = lv772_1[0]
            lv777 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_1_mlp_gate_up_proj_q_weight3, model_layers_1_mlp_gate_up_proj_q_scale3, rms_norm68), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv200 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv777,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv778 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_1_mlp_down_proj_q_weight3, model_layers_1_mlp_down_proj_q_scale3, lv200), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv774_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv778, lv773_1, model_layers_2_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv775_1: R.Tensor((1, 1, 4096), dtype="float16") = lv774_1[1]
            rms_norm69: R.Tensor((1, 1, 4096), dtype="float16") = lv774_1[0]
            lv779 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_2_self_attn_qkv_proj_q_weight3, model_layers_2_self_attn_qkv_proj_q_scale3, rms_norm69), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv201 = R.call_tir(cls.fused_reshape8_reshape9, (lv779,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv175 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), lv201), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv202 = R.call_tir(cls.fused_reshape10_reshape11, (lv175,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv780 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_2_self_attn_o_proj_q_weight3, model_layers_2_self_attn_o_proj_q_scale3, lv202), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv776_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv780, lv775_1, model_layers_2_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv777_1: R.Tensor((1, 1, 4096), dtype="float16") = lv776_1[1]
            rms_norm70: R.Tensor((1, 1, 4096), dtype="float16") = lv776_1[0]
            lv781 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_2_mlp_gate_up_proj_q_weight3, model_layers_2_mlp_gate_up_proj_q_scale3, rms_norm70), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv203 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv781,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv782 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_2_mlp_down_proj_q_weight3, model_layers_2_mlp_down_proj_q_scale3, lv203), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv778_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv782, lv777_1, model_layers_3_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv779_1: R.Tensor((1, 1, 4096), dtype="float16") = lv778_1[1]
            rms_norm71: R.Tensor((1, 1, 4096), dtype="float16") = lv778_1[0]
            lv783 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_3_self_attn_qkv_proj_q_weight3, model_layers_3_self_attn_qkv_proj_q_scale3, rms_norm71), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv204 = R.call_tir(cls.fused_reshape8_reshape9, (lv783,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv180 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), lv204), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv205 = R.call_tir(cls.fused_reshape10_reshape11, (lv180,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv784 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_3_self_attn_o_proj_q_weight3, model_layers_3_self_attn_o_proj_q_scale3, lv205), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv780_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv784, lv779_1, model_layers_3_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv781_1: R.Tensor((1, 1, 4096), dtype="float16") = lv780_1[1]
            rms_norm72: R.Tensor((1, 1, 4096), dtype="float16") = lv780_1[0]
            lv785 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_3_mlp_gate_up_proj_q_weight3, model_layers_3_mlp_gate_up_proj_q_scale3, rms_norm72), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv206 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv785,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv786 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_3_mlp_down_proj_q_weight3, model_layers_3_mlp_down_proj_q_scale3, lv206), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv782_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv786, lv781_1, model_layers_4_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv783_1: R.Tensor((1, 1, 4096), dtype="float16") = lv782_1[1]
            rms_norm73: R.Tensor((1, 1, 4096), dtype="float16") = lv782_1[0]
            lv787 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_4_self_attn_qkv_proj_q_weight3, model_layers_4_self_attn_qkv_proj_q_scale3, rms_norm73), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv207 = R.call_tir(cls.fused_reshape8_reshape9, (lv787,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv185 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), lv207), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv208 = R.call_tir(cls.fused_reshape10_reshape11, (lv185,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv788 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_4_self_attn_o_proj_q_weight3, model_layers_4_self_attn_o_proj_q_scale3, lv208), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv784_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv788, lv783_1, model_layers_4_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv785_1: R.Tensor((1, 1, 4096), dtype="float16") = lv784_1[1]
            rms_norm74: R.Tensor((1, 1, 4096), dtype="float16") = lv784_1[0]
            lv789 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_4_mlp_gate_up_proj_q_weight3, model_layers_4_mlp_gate_up_proj_q_scale3, rms_norm74), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv209 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv789,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv790 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_4_mlp_down_proj_q_weight3, model_layers_4_mlp_down_proj_q_scale3, lv209), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv786_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv790, lv785_1, model_layers_5_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv787_1: R.Tensor((1, 1, 4096), dtype="float16") = lv786_1[1]
            rms_norm75: R.Tensor((1, 1, 4096), dtype="float16") = lv786_1[0]
            lv791 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_5_self_attn_qkv_proj_q_weight3, model_layers_5_self_attn_qkv_proj_q_scale3, rms_norm75), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv210 = R.call_tir(cls.fused_reshape8_reshape9, (lv791,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv190 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), lv210), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv211 = R.call_tir(cls.fused_reshape10_reshape11, (lv190,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv792 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_5_self_attn_o_proj_q_weight3, model_layers_5_self_attn_o_proj_q_scale3, lv211), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv788_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv792, lv787_1, model_layers_5_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv789_1: R.Tensor((1, 1, 4096), dtype="float16") = lv788_1[1]
            rms_norm76: R.Tensor((1, 1, 4096), dtype="float16") = lv788_1[0]
            lv793 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_5_mlp_gate_up_proj_q_weight3, model_layers_5_mlp_gate_up_proj_q_scale3, rms_norm76), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv212 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv793,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv794 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_5_mlp_down_proj_q_weight3, model_layers_5_mlp_down_proj_q_scale3, lv212), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv790_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv794, lv789_1, model_layers_6_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv791_1: R.Tensor((1, 1, 4096), dtype="float16") = lv790_1[1]
            rms_norm77: R.Tensor((1, 1, 4096), dtype="float16") = lv790_1[0]
            lv795 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_6_self_attn_qkv_proj_q_weight3, model_layers_6_self_attn_qkv_proj_q_scale3, rms_norm77), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv213 = R.call_tir(cls.fused_reshape8_reshape9, (lv795,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv195_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), lv213), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv214 = R.call_tir(cls.fused_reshape10_reshape11, (lv195_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv796 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_6_self_attn_o_proj_q_weight3, model_layers_6_self_attn_o_proj_q_scale3, lv214), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv792_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv796, lv791_1, model_layers_6_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv793_1: R.Tensor((1, 1, 4096), dtype="float16") = lv792_1[1]
            rms_norm78: R.Tensor((1, 1, 4096), dtype="float16") = lv792_1[0]
            lv797 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_6_mlp_gate_up_proj_q_weight3, model_layers_6_mlp_gate_up_proj_q_scale3, rms_norm78), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv215 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv797,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv798 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_6_mlp_down_proj_q_weight3, model_layers_6_mlp_down_proj_q_scale3, lv215), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv794_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv798, lv793_1, model_layers_7_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv795_1: R.Tensor((1, 1, 4096), dtype="float16") = lv794_1[1]
            rms_norm79: R.Tensor((1, 1, 4096), dtype="float16") = lv794_1[0]
            lv799 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_7_self_attn_qkv_proj_q_weight3, model_layers_7_self_attn_qkv_proj_q_scale3, rms_norm79), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv216 = R.call_tir(cls.fused_reshape8_reshape9, (lv799,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv200_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), lv216), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv217 = R.call_tir(cls.fused_reshape10_reshape11, (lv200_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv800 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_7_self_attn_o_proj_q_weight3, model_layers_7_self_attn_o_proj_q_scale3, lv217), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv796_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv800, lv795_1, model_layers_7_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv797_1: R.Tensor((1, 1, 4096), dtype="float16") = lv796_1[1]
            rms_norm80: R.Tensor((1, 1, 4096), dtype="float16") = lv796_1[0]
            lv801 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_7_mlp_gate_up_proj_q_weight3, model_layers_7_mlp_gate_up_proj_q_scale3, rms_norm80), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv218 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv801,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv802 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_7_mlp_down_proj_q_weight3, model_layers_7_mlp_down_proj_q_scale3, lv218), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv798_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv802, lv797_1, model_layers_8_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv799_1: R.Tensor((1, 1, 4096), dtype="float16") = lv798_1[1]
            rms_norm81: R.Tensor((1, 1, 4096), dtype="float16") = lv798_1[0]
            lv803 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_8_self_attn_qkv_proj_q_weight3, model_layers_8_self_attn_qkv_proj_q_scale3, rms_norm81), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv219 = R.call_tir(cls.fused_reshape8_reshape9, (lv803,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv205_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), lv219), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv220 = R.call_tir(cls.fused_reshape10_reshape11, (lv205_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv804 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_8_self_attn_o_proj_q_weight3, model_layers_8_self_attn_o_proj_q_scale3, lv220), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv800_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv804, lv799_1, model_layers_8_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv801_1: R.Tensor((1, 1, 4096), dtype="float16") = lv800_1[1]
            rms_norm82: R.Tensor((1, 1, 4096), dtype="float16") = lv800_1[0]
            lv805 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_8_mlp_gate_up_proj_q_weight3, model_layers_8_mlp_gate_up_proj_q_scale3, rms_norm82), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv221 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv805,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv806 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_8_mlp_down_proj_q_weight3, model_layers_8_mlp_down_proj_q_scale3, lv221), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv802_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv806, lv801_1, model_layers_9_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv803_1: R.Tensor((1, 1, 4096), dtype="float16") = lv802_1[1]
            rms_norm83: R.Tensor((1, 1, 4096), dtype="float16") = lv802_1[0]
            lv807 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_9_self_attn_qkv_proj_q_weight3, model_layers_9_self_attn_qkv_proj_q_scale3, rms_norm83), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv222 = R.call_tir(cls.fused_reshape8_reshape9, (lv807,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv210_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), lv222), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv223 = R.call_tir(cls.fused_reshape10_reshape11, (lv210_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv808 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_9_self_attn_o_proj_q_weight3, model_layers_9_self_attn_o_proj_q_scale3, lv223), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv804_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv808, lv803_1, model_layers_9_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv805_1: R.Tensor((1, 1, 4096), dtype="float16") = lv804_1[1]
            rms_norm84: R.Tensor((1, 1, 4096), dtype="float16") = lv804_1[0]
            lv809 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_9_mlp_gate_up_proj_q_weight3, model_layers_9_mlp_gate_up_proj_q_scale3, rms_norm84), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv224 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv809,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv810 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_9_mlp_down_proj_q_weight3, model_layers_9_mlp_down_proj_q_scale3, lv224), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv806_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv810, lv805_1, model_layers_10_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv807_1: R.Tensor((1, 1, 4096), dtype="float16") = lv806_1[1]
            rms_norm85: R.Tensor((1, 1, 4096), dtype="float16") = lv806_1[0]
            lv811 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_10_self_attn_qkv_proj_q_weight3, model_layers_10_self_attn_qkv_proj_q_scale3, rms_norm85), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv225 = R.call_tir(cls.fused_reshape8_reshape9, (lv811,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv215_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), lv225), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv226 = R.call_tir(cls.fused_reshape10_reshape11, (lv215_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv812 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_10_self_attn_o_proj_q_weight3, model_layers_10_self_attn_o_proj_q_scale3, lv226), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv808_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv812, lv807_1, model_layers_10_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv809_1: R.Tensor((1, 1, 4096), dtype="float16") = lv808_1[1]
            rms_norm86: R.Tensor((1, 1, 4096), dtype="float16") = lv808_1[0]
            lv813 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_10_mlp_gate_up_proj_q_weight3, model_layers_10_mlp_gate_up_proj_q_scale3, rms_norm86), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv227 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv813,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv814 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_10_mlp_down_proj_q_weight3, model_layers_10_mlp_down_proj_q_scale3, lv227), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv810_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv814, lv809_1, model_layers_11_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv811_1: R.Tensor((1, 1, 4096), dtype="float16") = lv810_1[1]
            rms_norm87: R.Tensor((1, 1, 4096), dtype="float16") = lv810_1[0]
            lv815 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_11_self_attn_qkv_proj_q_weight3, model_layers_11_self_attn_qkv_proj_q_scale3, rms_norm87), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv228 = R.call_tir(cls.fused_reshape8_reshape9, (lv815,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv220_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), lv228), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv229 = R.call_tir(cls.fused_reshape10_reshape11, (lv220_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv816 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_11_self_attn_o_proj_q_weight3, model_layers_11_self_attn_o_proj_q_scale3, lv229), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv812_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv816, lv811_1, model_layers_11_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv813_1: R.Tensor((1, 1, 4096), dtype="float16") = lv812_1[1]
            rms_norm88: R.Tensor((1, 1, 4096), dtype="float16") = lv812_1[0]
            lv817 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_11_mlp_gate_up_proj_q_weight3, model_layers_11_mlp_gate_up_proj_q_scale3, rms_norm88), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv230 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv817,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv818 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_11_mlp_down_proj_q_weight3, model_layers_11_mlp_down_proj_q_scale3, lv230), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv814_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv818, lv813_1, model_layers_12_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv815_1: R.Tensor((1, 1, 4096), dtype="float16") = lv814_1[1]
            rms_norm89: R.Tensor((1, 1, 4096), dtype="float16") = lv814_1[0]
            lv819 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_12_self_attn_qkv_proj_q_weight3, model_layers_12_self_attn_qkv_proj_q_scale3, rms_norm89), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv231 = R.call_tir(cls.fused_reshape8_reshape9, (lv819,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv225_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), lv231), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv232 = R.call_tir(cls.fused_reshape10_reshape11, (lv225_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv820 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_12_self_attn_o_proj_q_weight3, model_layers_12_self_attn_o_proj_q_scale3, lv232), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv816_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv820, lv815_1, model_layers_12_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv817_1: R.Tensor((1, 1, 4096), dtype="float16") = lv816_1[1]
            rms_norm90: R.Tensor((1, 1, 4096), dtype="float16") = lv816_1[0]
            lv821 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_12_mlp_gate_up_proj_q_weight3, model_layers_12_mlp_gate_up_proj_q_scale3, rms_norm90), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv233 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv821,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv822 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_12_mlp_down_proj_q_weight3, model_layers_12_mlp_down_proj_q_scale3, lv233), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv818_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv822, lv817_1, model_layers_13_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv819_1: R.Tensor((1, 1, 4096), dtype="float16") = lv818_1[1]
            rms_norm91: R.Tensor((1, 1, 4096), dtype="float16") = lv818_1[0]
            lv823 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_13_self_attn_qkv_proj_q_weight3, model_layers_13_self_attn_qkv_proj_q_scale3, rms_norm91), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv234 = R.call_tir(cls.fused_reshape8_reshape9, (lv823,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv230_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), lv234), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv235 = R.call_tir(cls.fused_reshape10_reshape11, (lv230_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv824 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_13_self_attn_o_proj_q_weight3, model_layers_13_self_attn_o_proj_q_scale3, lv235), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv820_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv824, lv819_1, model_layers_13_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv821_1: R.Tensor((1, 1, 4096), dtype="float16") = lv820_1[1]
            rms_norm92: R.Tensor((1, 1, 4096), dtype="float16") = lv820_1[0]
            lv825 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_13_mlp_gate_up_proj_q_weight3, model_layers_13_mlp_gate_up_proj_q_scale3, rms_norm92), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv236 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv825,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv826 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_13_mlp_down_proj_q_weight3, model_layers_13_mlp_down_proj_q_scale3, lv236), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv822_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv826, lv821_1, model_layers_14_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv823_1: R.Tensor((1, 1, 4096), dtype="float16") = lv822_1[1]
            rms_norm93: R.Tensor((1, 1, 4096), dtype="float16") = lv822_1[0]
            lv827 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_14_self_attn_qkv_proj_q_weight3, model_layers_14_self_attn_qkv_proj_q_scale3, rms_norm93), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv237 = R.call_tir(cls.fused_reshape8_reshape9, (lv827,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv235_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), lv237), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv238 = R.call_tir(cls.fused_reshape10_reshape11, (lv235_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv828 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_14_self_attn_o_proj_q_weight3, model_layers_14_self_attn_o_proj_q_scale3, lv238), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv824_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv828, lv823_1, model_layers_14_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv825_1: R.Tensor((1, 1, 4096), dtype="float16") = lv824_1[1]
            rms_norm94: R.Tensor((1, 1, 4096), dtype="float16") = lv824_1[0]
            lv829 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_14_mlp_gate_up_proj_q_weight3, model_layers_14_mlp_gate_up_proj_q_scale3, rms_norm94), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv239 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv829,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv830 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_14_mlp_down_proj_q_weight3, model_layers_14_mlp_down_proj_q_scale3, lv239), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv826_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv830, lv825_1, model_layers_15_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv827_1: R.Tensor((1, 1, 4096), dtype="float16") = lv826_1[1]
            rms_norm95: R.Tensor((1, 1, 4096), dtype="float16") = lv826_1[0]
            lv831 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_15_self_attn_qkv_proj_q_weight3, model_layers_15_self_attn_qkv_proj_q_scale3, rms_norm95), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv240 = R.call_tir(cls.fused_reshape8_reshape9, (lv831,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv240_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), lv240), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv241 = R.call_tir(cls.fused_reshape10_reshape11, (lv240_1,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv832 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_15_self_attn_o_proj_q_weight3, model_layers_15_self_attn_o_proj_q_scale3, lv241), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv828_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv832, lv827_1, model_layers_15_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv829_1: R.Tensor((1, 1, 4096), dtype="float16") = lv828_1[1]
            rms_norm96: R.Tensor((1, 1, 4096), dtype="float16") = lv828_1[0]
            lv833 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_15_mlp_gate_up_proj_q_weight3, model_layers_15_mlp_gate_up_proj_q_scale3, rms_norm96), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv242 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv833,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv834 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_15_mlp_down_proj_q_weight3, model_layers_15_mlp_down_proj_q_scale3, lv242), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv830_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv834, lv829_1, model_layers_16_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv831_1: R.Tensor((1, 1, 4096), dtype="float16") = lv830_1[1]
            rms_norm97: R.Tensor((1, 1, 4096), dtype="float16") = lv830_1[0]
            lv835 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_16_self_attn_qkv_proj_q_weight3, model_layers_16_self_attn_qkv_proj_q_scale3, rms_norm97), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv243 = R.call_tir(cls.fused_reshape8_reshape9, (lv835,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv245 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), lv243), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv244 = R.call_tir(cls.fused_reshape10_reshape11, (lv245,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv836 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_16_self_attn_o_proj_q_weight3, model_layers_16_self_attn_o_proj_q_scale3, lv244), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv832_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv836, lv831_1, model_layers_16_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv833_1: R.Tensor((1, 1, 4096), dtype="float16") = lv832_1[1]
            rms_norm98: R.Tensor((1, 1, 4096), dtype="float16") = lv832_1[0]
            lv837 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_16_mlp_gate_up_proj_q_weight3, model_layers_16_mlp_gate_up_proj_q_scale3, rms_norm98), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv245_1 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv837,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv838 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_16_mlp_down_proj_q_weight3, model_layers_16_mlp_down_proj_q_scale3, lv245_1), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv834_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv838, lv833_1, model_layers_17_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv835_1: R.Tensor((1, 1, 4096), dtype="float16") = lv834_1[1]
            rms_norm99: R.Tensor((1, 1, 4096), dtype="float16") = lv834_1[0]
            lv839 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_17_self_attn_qkv_proj_q_weight3, model_layers_17_self_attn_qkv_proj_q_scale3, rms_norm99), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv246 = R.call_tir(cls.fused_reshape8_reshape9, (lv839,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv250 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), lv246), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv247 = R.call_tir(cls.fused_reshape10_reshape11, (lv250,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv840 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_17_self_attn_o_proj_q_weight3, model_layers_17_self_attn_o_proj_q_scale3, lv247), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv836_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv840, lv835_1, model_layers_17_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv837_1: R.Tensor((1, 1, 4096), dtype="float16") = lv836_1[1]
            rms_norm100: R.Tensor((1, 1, 4096), dtype="float16") = lv836_1[0]
            lv841 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_17_mlp_gate_up_proj_q_weight3, model_layers_17_mlp_gate_up_proj_q_scale3, rms_norm100), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv248 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv841,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv842 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_17_mlp_down_proj_q_weight3, model_layers_17_mlp_down_proj_q_scale3, lv248), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv838_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv842, lv837_1, model_layers_18_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv839_1: R.Tensor((1, 1, 4096), dtype="float16") = lv838_1[1]
            rms_norm101: R.Tensor((1, 1, 4096), dtype="float16") = lv838_1[0]
            lv843 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_18_self_attn_qkv_proj_q_weight3, model_layers_18_self_attn_qkv_proj_q_scale3, rms_norm101), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv249 = R.call_tir(cls.fused_reshape8_reshape9, (lv843,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv255 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), lv249), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv250_1 = R.call_tir(cls.fused_reshape10_reshape11, (lv255,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv844 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_18_self_attn_o_proj_q_weight3, model_layers_18_self_attn_o_proj_q_scale3, lv250_1), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv840_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv844, lv839_1, model_layers_18_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv841_1: R.Tensor((1, 1, 4096), dtype="float16") = lv840_1[1]
            rms_norm102: R.Tensor((1, 1, 4096), dtype="float16") = lv840_1[0]
            lv845 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_18_mlp_gate_up_proj_q_weight3, model_layers_18_mlp_gate_up_proj_q_scale3, rms_norm102), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv251 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv845,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv846 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_18_mlp_down_proj_q_weight3, model_layers_18_mlp_down_proj_q_scale3, lv251), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv842_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv846, lv841_1, model_layers_19_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv843_1: R.Tensor((1, 1, 4096), dtype="float16") = lv842_1[1]
            rms_norm103: R.Tensor((1, 1, 4096), dtype="float16") = lv842_1[0]
            lv847 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_19_self_attn_qkv_proj_q_weight3, model_layers_19_self_attn_qkv_proj_q_scale3, rms_norm103), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv252 = R.call_tir(cls.fused_reshape8_reshape9, (lv847,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv260 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), lv252), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv253 = R.call_tir(cls.fused_reshape10_reshape11, (lv260,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv848 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_19_self_attn_o_proj_q_weight3, model_layers_19_self_attn_o_proj_q_scale3, lv253), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv844_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv848, lv843_1, model_layers_19_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv845_1: R.Tensor((1, 1, 4096), dtype="float16") = lv844_1[1]
            rms_norm104: R.Tensor((1, 1, 4096), dtype="float16") = lv844_1[0]
            lv849 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_19_mlp_gate_up_proj_q_weight3, model_layers_19_mlp_gate_up_proj_q_scale3, rms_norm104), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv254 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv849,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv850 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_19_mlp_down_proj_q_weight3, model_layers_19_mlp_down_proj_q_scale3, lv254), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv846_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv850, lv845_1, model_layers_20_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv847_1: R.Tensor((1, 1, 4096), dtype="float16") = lv846_1[1]
            rms_norm105: R.Tensor((1, 1, 4096), dtype="float16") = lv846_1[0]
            lv851 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_20_self_attn_qkv_proj_q_weight3, model_layers_20_self_attn_qkv_proj_q_scale3, rms_norm105), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv255_1 = R.call_tir(cls.fused_reshape8_reshape9, (lv851,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv265 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), lv255_1), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv256 = R.call_tir(cls.fused_reshape10_reshape11, (lv265,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv852 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_20_self_attn_o_proj_q_weight3, model_layers_20_self_attn_o_proj_q_scale3, lv256), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv848_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv852, lv847_1, model_layers_20_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv849_1: R.Tensor((1, 1, 4096), dtype="float16") = lv848_1[1]
            rms_norm106: R.Tensor((1, 1, 4096), dtype="float16") = lv848_1[0]
            lv853 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_20_mlp_gate_up_proj_q_weight3, model_layers_20_mlp_gate_up_proj_q_scale3, rms_norm106), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv257 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv853,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv854 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_20_mlp_down_proj_q_weight3, model_layers_20_mlp_down_proj_q_scale3, lv257), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv850_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv854, lv849_1, model_layers_21_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv851_1: R.Tensor((1, 1, 4096), dtype="float16") = lv850_1[1]
            rms_norm107: R.Tensor((1, 1, 4096), dtype="float16") = lv850_1[0]
            lv855 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_21_self_attn_qkv_proj_q_weight3, model_layers_21_self_attn_qkv_proj_q_scale3, rms_norm107), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv258 = R.call_tir(cls.fused_reshape8_reshape9, (lv855,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv270 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), lv258), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv259 = R.call_tir(cls.fused_reshape10_reshape11, (lv270,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv856 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_21_self_attn_o_proj_q_weight3, model_layers_21_self_attn_o_proj_q_scale3, lv259), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv852_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv856, lv851_1, model_layers_21_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv853_1: R.Tensor((1, 1, 4096), dtype="float16") = lv852_1[1]
            rms_norm108: R.Tensor((1, 1, 4096), dtype="float16") = lv852_1[0]
            lv857 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_21_mlp_gate_up_proj_q_weight3, model_layers_21_mlp_gate_up_proj_q_scale3, rms_norm108), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv260_1 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv857,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv858 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_21_mlp_down_proj_q_weight3, model_layers_21_mlp_down_proj_q_scale3, lv260_1), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv854_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv858, lv853_1, model_layers_22_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv855_1: R.Tensor((1, 1, 4096), dtype="float16") = lv854_1[1]
            rms_norm109: R.Tensor((1, 1, 4096), dtype="float16") = lv854_1[0]
            lv859 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_22_self_attn_qkv_proj_q_weight3, model_layers_22_self_attn_qkv_proj_q_scale3, rms_norm109), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv261 = R.call_tir(cls.fused_reshape8_reshape9, (lv859,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv275 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), lv261), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv262 = R.call_tir(cls.fused_reshape10_reshape11, (lv275,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv860 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_22_self_attn_o_proj_q_weight3, model_layers_22_self_attn_o_proj_q_scale3, lv262), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv856_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv860, lv855_1, model_layers_22_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv857_1: R.Tensor((1, 1, 4096), dtype="float16") = lv856_1[1]
            rms_norm110: R.Tensor((1, 1, 4096), dtype="float16") = lv856_1[0]
            lv861 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_22_mlp_gate_up_proj_q_weight3, model_layers_22_mlp_gate_up_proj_q_scale3, rms_norm110), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv263 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv861,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv862 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_22_mlp_down_proj_q_weight3, model_layers_22_mlp_down_proj_q_scale3, lv263), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv858_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv862, lv857_1, model_layers_23_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv859_1: R.Tensor((1, 1, 4096), dtype="float16") = lv858_1[1]
            rms_norm111: R.Tensor((1, 1, 4096), dtype="float16") = lv858_1[0]
            lv863 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_23_self_attn_qkv_proj_q_weight3, model_layers_23_self_attn_qkv_proj_q_scale3, rms_norm111), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv264 = R.call_tir(cls.fused_reshape8_reshape9, (lv863,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv280 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), lv264), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv265_1 = R.call_tir(cls.fused_reshape10_reshape11, (lv280,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv864 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_23_self_attn_o_proj_q_weight3, model_layers_23_self_attn_o_proj_q_scale3, lv265_1), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv860_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv864, lv859_1, model_layers_23_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv861_1: R.Tensor((1, 1, 4096), dtype="float16") = lv860_1[1]
            rms_norm112: R.Tensor((1, 1, 4096), dtype="float16") = lv860_1[0]
            lv865 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_23_mlp_gate_up_proj_q_weight3, model_layers_23_mlp_gate_up_proj_q_scale3, rms_norm112), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv266 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv865,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv866 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_23_mlp_down_proj_q_weight3, model_layers_23_mlp_down_proj_q_scale3, lv266), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv862_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv866, lv861_1, model_layers_24_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv863_1: R.Tensor((1, 1, 4096), dtype="float16") = lv862_1[1]
            rms_norm113: R.Tensor((1, 1, 4096), dtype="float16") = lv862_1[0]
            lv867 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_24_self_attn_qkv_proj_q_weight3, model_layers_24_self_attn_qkv_proj_q_scale3, rms_norm113), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv267 = R.call_tir(cls.fused_reshape8_reshape9, (lv867,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv285 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), lv267), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv268 = R.call_tir(cls.fused_reshape10_reshape11, (lv285,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv868 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_24_self_attn_o_proj_q_weight3, model_layers_24_self_attn_o_proj_q_scale3, lv268), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv864_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv868, lv863_1, model_layers_24_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv865_1: R.Tensor((1, 1, 4096), dtype="float16") = lv864_1[1]
            rms_norm114: R.Tensor((1, 1, 4096), dtype="float16") = lv864_1[0]
            lv869 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_24_mlp_gate_up_proj_q_weight3, model_layers_24_mlp_gate_up_proj_q_scale3, rms_norm114), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv269 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv869,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv870 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_24_mlp_down_proj_q_weight3, model_layers_24_mlp_down_proj_q_scale3, lv269), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv866_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv870, lv865_1, model_layers_25_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv867_1: R.Tensor((1, 1, 4096), dtype="float16") = lv866_1[1]
            rms_norm115: R.Tensor((1, 1, 4096), dtype="float16") = lv866_1[0]
            lv871 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_25_self_attn_qkv_proj_q_weight3, model_layers_25_self_attn_qkv_proj_q_scale3, rms_norm115), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv270_1 = R.call_tir(cls.fused_reshape8_reshape9, (lv871,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv290 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), lv270_1), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv271 = R.call_tir(cls.fused_reshape10_reshape11, (lv290,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv872 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_25_self_attn_o_proj_q_weight3, model_layers_25_self_attn_o_proj_q_scale3, lv271), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv868_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv872, lv867_1, model_layers_25_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv869_1: R.Tensor((1, 1, 4096), dtype="float16") = lv868_1[1]
            rms_norm116: R.Tensor((1, 1, 4096), dtype="float16") = lv868_1[0]
            lv873 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_25_mlp_gate_up_proj_q_weight3, model_layers_25_mlp_gate_up_proj_q_scale3, rms_norm116), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv272 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv873,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv874 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_25_mlp_down_proj_q_weight3, model_layers_25_mlp_down_proj_q_scale3, lv272), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv870_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv874, lv869_1, model_layers_26_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv871_1: R.Tensor((1, 1, 4096), dtype="float16") = lv870_1[1]
            rms_norm117: R.Tensor((1, 1, 4096), dtype="float16") = lv870_1[0]
            lv875 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_26_self_attn_qkv_proj_q_weight3, model_layers_26_self_attn_qkv_proj_q_scale3, rms_norm117), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv273 = R.call_tir(cls.fused_reshape8_reshape9, (lv875,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv295 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), lv273), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv274 = R.call_tir(cls.fused_reshape10_reshape11, (lv295,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv876 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_26_self_attn_o_proj_q_weight3, model_layers_26_self_attn_o_proj_q_scale3, lv274), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv872_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv876, lv871_1, model_layers_26_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv873_1: R.Tensor((1, 1, 4096), dtype="float16") = lv872_1[1]
            rms_norm118: R.Tensor((1, 1, 4096), dtype="float16") = lv872_1[0]
            lv877 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_26_mlp_gate_up_proj_q_weight3, model_layers_26_mlp_gate_up_proj_q_scale3, rms_norm118), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv275_1 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv877,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv878 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_26_mlp_down_proj_q_weight3, model_layers_26_mlp_down_proj_q_scale3, lv275_1), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv874_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv878, lv873_1, model_layers_27_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv875_1: R.Tensor((1, 1, 4096), dtype="float16") = lv874_1[1]
            rms_norm119: R.Tensor((1, 1, 4096), dtype="float16") = lv874_1[0]
            lv879 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_27_self_attn_qkv_proj_q_weight3, model_layers_27_self_attn_qkv_proj_q_scale3, rms_norm119), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv276 = R.call_tir(cls.fused_reshape8_reshape9, (lv879,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv300 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), lv276), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv277 = R.call_tir(cls.fused_reshape10_reshape11, (lv300,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv880 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_27_self_attn_o_proj_q_weight3, model_layers_27_self_attn_o_proj_q_scale3, lv277), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv876_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv880, lv875_1, model_layers_27_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv877_1: R.Tensor((1, 1, 4096), dtype="float16") = lv876_1[1]
            rms_norm120: R.Tensor((1, 1, 4096), dtype="float16") = lv876_1[0]
            lv881 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_27_mlp_gate_up_proj_q_weight3, model_layers_27_mlp_gate_up_proj_q_scale3, rms_norm120), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv278 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv881,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv882 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_27_mlp_down_proj_q_weight3, model_layers_27_mlp_down_proj_q_scale3, lv278), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv878_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv882, lv877_1, model_layers_28_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv879_1: R.Tensor((1, 1, 4096), dtype="float16") = lv878_1[1]
            rms_norm121: R.Tensor((1, 1, 4096), dtype="float16") = lv878_1[0]
            lv883 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_28_self_attn_qkv_proj_q_weight3, model_layers_28_self_attn_qkv_proj_q_scale3, rms_norm121), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv279 = R.call_tir(cls.fused_reshape8_reshape9, (lv883,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv305 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), lv279), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv280_1 = R.call_tir(cls.fused_reshape10_reshape11, (lv305,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv884 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_28_self_attn_o_proj_q_weight3, model_layers_28_self_attn_o_proj_q_scale3, lv280_1), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv880_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv884, lv879_1, model_layers_28_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv881_1: R.Tensor((1, 1, 4096), dtype="float16") = lv880_1[1]
            rms_norm122: R.Tensor((1, 1, 4096), dtype="float16") = lv880_1[0]
            lv885 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_28_mlp_gate_up_proj_q_weight3, model_layers_28_mlp_gate_up_proj_q_scale3, rms_norm122), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv281 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv885,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv886 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_28_mlp_down_proj_q_weight3, model_layers_28_mlp_down_proj_q_scale3, lv281), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv882_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv886, lv881_1, model_layers_29_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv883_1: R.Tensor((1, 1, 4096), dtype="float16") = lv882_1[1]
            rms_norm123: R.Tensor((1, 1, 4096), dtype="float16") = lv882_1[0]
            lv887 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_29_self_attn_qkv_proj_q_weight3, model_layers_29_self_attn_qkv_proj_q_scale3, rms_norm123), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv282 = R.call_tir(cls.fused_reshape8_reshape9, (lv887,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv310 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), lv282), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv283 = R.call_tir(cls.fused_reshape10_reshape11, (lv310,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv888 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_29_self_attn_o_proj_q_weight3, model_layers_29_self_attn_o_proj_q_scale3, lv283), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv884_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv888, lv883_1, model_layers_29_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv885_1: R.Tensor((1, 1, 4096), dtype="float16") = lv884_1[1]
            rms_norm124: R.Tensor((1, 1, 4096), dtype="float16") = lv884_1[0]
            lv889 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_29_mlp_gate_up_proj_q_weight3, model_layers_29_mlp_gate_up_proj_q_scale3, rms_norm124), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv284 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv889,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv890 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_29_mlp_down_proj_q_weight3, model_layers_29_mlp_down_proj_q_scale3, lv284), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv886_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv890, lv885_1, model_layers_30_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv887_1: R.Tensor((1, 1, 4096), dtype="float16") = lv886_1[1]
            rms_norm125: R.Tensor((1, 1, 4096), dtype="float16") = lv886_1[0]
            lv891 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_30_self_attn_qkv_proj_q_weight3, model_layers_30_self_attn_qkv_proj_q_scale3, rms_norm125), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv285_1 = R.call_tir(cls.fused_reshape8_reshape9, (lv891,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv315 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), lv285_1), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv286 = R.call_tir(cls.fused_reshape10_reshape11, (lv315,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv892 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_30_self_attn_o_proj_q_weight3, model_layers_30_self_attn_o_proj_q_scale3, lv286), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv888_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv892, lv887_1, model_layers_30_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv889_1: R.Tensor((1, 1, 4096), dtype="float16") = lv888_1[1]
            rms_norm126: R.Tensor((1, 1, 4096), dtype="float16") = lv888_1[0]
            lv893 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_30_mlp_gate_up_proj_q_weight3, model_layers_30_mlp_gate_up_proj_q_scale3, rms_norm126), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv287 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv893,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv894 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_30_mlp_down_proj_q_weight3, model_layers_30_mlp_down_proj_q_scale3, lv287), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv890_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv894, lv889_1, model_layers_31_input_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv891_1: R.Tensor((1, 1, 4096), dtype="float16") = lv890_1[1]
            rms_norm127: R.Tensor((1, 1, 4096), dtype="float16") = lv890_1[0]
            lv895 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_31_self_attn_qkv_proj_q_weight3, model_layers_31_self_attn_qkv_proj_q_scale3, rms_norm127), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv288 = R.call_tir(cls.fused_reshape8_reshape9, (lv895,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv320 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), lv288), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv289 = R.call_tir(cls.fused_reshape10_reshape11, (lv320,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv896 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_31_self_attn_o_proj_q_weight3, model_layers_31_self_attn_o_proj_q_scale3, lv289), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv892_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv896, lv891_1, model_layers_31_post_attention_layernorm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv893_1: R.Tensor((1, 1, 4096), dtype="float16") = lv892_1[1]
            rms_norm128: R.Tensor((1, 1, 4096), dtype="float16") = lv892_1[0]
            lv897 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_31_mlp_gate_up_proj_q_weight3, model_layers_31_mlp_gate_up_proj_q_scale3, rms_norm128), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv290_1 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv897,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv898 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_31_mlp_down_proj_q_weight3, model_layers_31_mlp_down_proj_q_scale3, lv290_1), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv894_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv898, lv893_1, model_norm_weight3), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            rms_norm129: R.Tensor((1, 1, 4096), dtype="float16") = lv894_1[0]
            lv899 = R.call_tir(cls.fused_dequantize_fused_NT_matmul14_cast2, (lm_head_q_weight3, lm_head_q_scale3, rms_norm129), out_sinfo=R.Tensor((1, 1, vocab_size), dtype="float32"))
            gv4: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = lv899, paged_kv_cache
            R.output(gv4)
        return gv4

    @R.function
    def decode_to_last_hidden_states(input_embed: R.Tensor((1, 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, 4096), dtype="float16"), R.Object):
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm195 = R.call_tir(cls.rms_norm2, (input_embed, model_layers_0_input_layernorm_weight5), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv900 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_0_self_attn_qkv_proj_q_weight5, model_layers_0_self_attn_qkv_proj_q_scale5, rms_norm195), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv292 = R.call_tir(cls.fused_reshape8_reshape9, (lv900,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv486 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), lv292), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv293 = R.call_tir(cls.fused_reshape10_reshape11, (lv486,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv901 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_0_self_attn_o_proj_q_weight5, model_layers_0_self_attn_o_proj_q_scale5, lv293), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv896 = R.call_tir(cls.fuse_add_norm_prefill, (lv901, input_embed, model_layers_0_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv897: R.Tensor((1, 1, 4096), dtype="float16") = lv896[1]
            rms_norm196: R.Tensor((1, 1, 4096), dtype="float16") = lv896[0]
            lv902 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_0_mlp_gate_up_proj_q_weight5, model_layers_0_mlp_gate_up_proj_q_scale5, rms_norm196), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv294 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv902,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv903 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_0_mlp_down_proj_q_weight5, model_layers_0_mlp_down_proj_q_scale5, lv294), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv898 = R.call_tir(cls.fuse_add_norm_prefill, (lv903, lv897, model_layers_1_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv899: R.Tensor((1, 1, 4096), dtype="float16") = lv898[1]
            rms_norm197: R.Tensor((1, 1, 4096), dtype="float16") = lv898[0]
            lv904 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_1_self_attn_qkv_proj_q_weight5, model_layers_1_self_attn_qkv_proj_q_scale5, rms_norm197), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv295 = R.call_tir(cls.fused_reshape8_reshape9, (lv904,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv491 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), lv295), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv296 = R.call_tir(cls.fused_reshape10_reshape11, (lv491,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv905 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_1_self_attn_o_proj_q_weight5, model_layers_1_self_attn_o_proj_q_scale5, lv296), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv900_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv905, lv899, model_layers_1_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv901_1: R.Tensor((1, 1, 4096), dtype="float16") = lv900_1[1]
            rms_norm198: R.Tensor((1, 1, 4096), dtype="float16") = lv900_1[0]
            lv906 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_1_mlp_gate_up_proj_q_weight5, model_layers_1_mlp_gate_up_proj_q_scale5, rms_norm198), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv297 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv906,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv907 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_1_mlp_down_proj_q_weight5, model_layers_1_mlp_down_proj_q_scale5, lv297), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv902_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv907, lv901_1, model_layers_2_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv903_1: R.Tensor((1, 1, 4096), dtype="float16") = lv902_1[1]
            rms_norm199: R.Tensor((1, 1, 4096), dtype="float16") = lv902_1[0]
            lv908 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_2_self_attn_qkv_proj_q_weight5, model_layers_2_self_attn_qkv_proj_q_scale5, rms_norm199), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv298 = R.call_tir(cls.fused_reshape8_reshape9, (lv908,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv496 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), lv298), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv299 = R.call_tir(cls.fused_reshape10_reshape11, (lv496,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv909 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_2_self_attn_o_proj_q_weight5, model_layers_2_self_attn_o_proj_q_scale5, lv299), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv904_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv909, lv903_1, model_layers_2_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv905_1: R.Tensor((1, 1, 4096), dtype="float16") = lv904_1[1]
            rms_norm200: R.Tensor((1, 1, 4096), dtype="float16") = lv904_1[0]
            lv910 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_2_mlp_gate_up_proj_q_weight5, model_layers_2_mlp_gate_up_proj_q_scale5, rms_norm200), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv300 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv910,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv911 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_2_mlp_down_proj_q_weight5, model_layers_2_mlp_down_proj_q_scale5, lv300), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv906_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv911, lv905_1, model_layers_3_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv907_1: R.Tensor((1, 1, 4096), dtype="float16") = lv906_1[1]
            rms_norm201: R.Tensor((1, 1, 4096), dtype="float16") = lv906_1[0]
            lv912 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_3_self_attn_qkv_proj_q_weight5, model_layers_3_self_attn_qkv_proj_q_scale5, rms_norm201), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv301 = R.call_tir(cls.fused_reshape8_reshape9, (lv912,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv501 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), lv301), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv302 = R.call_tir(cls.fused_reshape10_reshape11, (lv501,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv913 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_3_self_attn_o_proj_q_weight5, model_layers_3_self_attn_o_proj_q_scale5, lv302), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv908_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv913, lv907_1, model_layers_3_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv909_1: R.Tensor((1, 1, 4096), dtype="float16") = lv908_1[1]
            rms_norm202: R.Tensor((1, 1, 4096), dtype="float16") = lv908_1[0]
            lv914 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_3_mlp_gate_up_proj_q_weight5, model_layers_3_mlp_gate_up_proj_q_scale5, rms_norm202), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv303 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv914,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv915 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_3_mlp_down_proj_q_weight5, model_layers_3_mlp_down_proj_q_scale5, lv303), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv910_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv915, lv909_1, model_layers_4_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv911_1: R.Tensor((1, 1, 4096), dtype="float16") = lv910_1[1]
            rms_norm203: R.Tensor((1, 1, 4096), dtype="float16") = lv910_1[0]
            lv916 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_4_self_attn_qkv_proj_q_weight5, model_layers_4_self_attn_qkv_proj_q_scale5, rms_norm203), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv304 = R.call_tir(cls.fused_reshape8_reshape9, (lv916,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv506 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), lv304), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv305 = R.call_tir(cls.fused_reshape10_reshape11, (lv506,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv917 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_4_self_attn_o_proj_q_weight5, model_layers_4_self_attn_o_proj_q_scale5, lv305), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv912_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv917, lv911_1, model_layers_4_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv913_1: R.Tensor((1, 1, 4096), dtype="float16") = lv912_1[1]
            rms_norm204: R.Tensor((1, 1, 4096), dtype="float16") = lv912_1[0]
            lv918 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_4_mlp_gate_up_proj_q_weight5, model_layers_4_mlp_gate_up_proj_q_scale5, rms_norm204), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv306 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv918,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv919 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_4_mlp_down_proj_q_weight5, model_layers_4_mlp_down_proj_q_scale5, lv306), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv914_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv919, lv913_1, model_layers_5_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv915_1: R.Tensor((1, 1, 4096), dtype="float16") = lv914_1[1]
            rms_norm205: R.Tensor((1, 1, 4096), dtype="float16") = lv914_1[0]
            lv920 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_5_self_attn_qkv_proj_q_weight5, model_layers_5_self_attn_qkv_proj_q_scale5, rms_norm205), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv307 = R.call_tir(cls.fused_reshape8_reshape9, (lv920,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv511 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), lv307), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv308 = R.call_tir(cls.fused_reshape10_reshape11, (lv511,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv921 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_5_self_attn_o_proj_q_weight5, model_layers_5_self_attn_o_proj_q_scale5, lv308), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv916_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv921, lv915_1, model_layers_5_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv917_1: R.Tensor((1, 1, 4096), dtype="float16") = lv916_1[1]
            rms_norm206: R.Tensor((1, 1, 4096), dtype="float16") = lv916_1[0]
            lv922 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_5_mlp_gate_up_proj_q_weight5, model_layers_5_mlp_gate_up_proj_q_scale5, rms_norm206), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv309 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv922,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv923 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_5_mlp_down_proj_q_weight5, model_layers_5_mlp_down_proj_q_scale5, lv309), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv918_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv923, lv917_1, model_layers_6_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv919_1: R.Tensor((1, 1, 4096), dtype="float16") = lv918_1[1]
            rms_norm207: R.Tensor((1, 1, 4096), dtype="float16") = lv918_1[0]
            lv924 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_6_self_attn_qkv_proj_q_weight5, model_layers_6_self_attn_qkv_proj_q_scale5, rms_norm207), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv310 = R.call_tir(cls.fused_reshape8_reshape9, (lv924,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv516 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), lv310), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv311 = R.call_tir(cls.fused_reshape10_reshape11, (lv516,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv925 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_6_self_attn_o_proj_q_weight5, model_layers_6_self_attn_o_proj_q_scale5, lv311), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv920_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv925, lv919_1, model_layers_6_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv921_1: R.Tensor((1, 1, 4096), dtype="float16") = lv920_1[1]
            rms_norm208: R.Tensor((1, 1, 4096), dtype="float16") = lv920_1[0]
            lv926 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_6_mlp_gate_up_proj_q_weight5, model_layers_6_mlp_gate_up_proj_q_scale5, rms_norm208), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv312 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv926,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv927 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_6_mlp_down_proj_q_weight5, model_layers_6_mlp_down_proj_q_scale5, lv312), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv922_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv927, lv921_1, model_layers_7_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv923_1: R.Tensor((1, 1, 4096), dtype="float16") = lv922_1[1]
            rms_norm209: R.Tensor((1, 1, 4096), dtype="float16") = lv922_1[0]
            lv928 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_7_self_attn_qkv_proj_q_weight5, model_layers_7_self_attn_qkv_proj_q_scale5, rms_norm209), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv313 = R.call_tir(cls.fused_reshape8_reshape9, (lv928,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv521 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), lv313), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv314 = R.call_tir(cls.fused_reshape10_reshape11, (lv521,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv929 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_7_self_attn_o_proj_q_weight5, model_layers_7_self_attn_o_proj_q_scale5, lv314), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv924_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv929, lv923_1, model_layers_7_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv925_1: R.Tensor((1, 1, 4096), dtype="float16") = lv924_1[1]
            rms_norm210: R.Tensor((1, 1, 4096), dtype="float16") = lv924_1[0]
            lv930 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_7_mlp_gate_up_proj_q_weight5, model_layers_7_mlp_gate_up_proj_q_scale5, rms_norm210), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv315 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv930,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv931 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_7_mlp_down_proj_q_weight5, model_layers_7_mlp_down_proj_q_scale5, lv315), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv926_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv931, lv925_1, model_layers_8_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv927_1: R.Tensor((1, 1, 4096), dtype="float16") = lv926_1[1]
            rms_norm211: R.Tensor((1, 1, 4096), dtype="float16") = lv926_1[0]
            lv932 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_8_self_attn_qkv_proj_q_weight5, model_layers_8_self_attn_qkv_proj_q_scale5, rms_norm211), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv316 = R.call_tir(cls.fused_reshape8_reshape9, (lv932,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv526 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), lv316), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv317 = R.call_tir(cls.fused_reshape10_reshape11, (lv526,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv933 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_8_self_attn_o_proj_q_weight5, model_layers_8_self_attn_o_proj_q_scale5, lv317), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv928_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv933, lv927_1, model_layers_8_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv929_1: R.Tensor((1, 1, 4096), dtype="float16") = lv928_1[1]
            rms_norm212: R.Tensor((1, 1, 4096), dtype="float16") = lv928_1[0]
            lv934 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_8_mlp_gate_up_proj_q_weight5, model_layers_8_mlp_gate_up_proj_q_scale5, rms_norm212), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv318 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv934,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv935 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_8_mlp_down_proj_q_weight5, model_layers_8_mlp_down_proj_q_scale5, lv318), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv930_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv935, lv929_1, model_layers_9_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv931_1: R.Tensor((1, 1, 4096), dtype="float16") = lv930_1[1]
            rms_norm213: R.Tensor((1, 1, 4096), dtype="float16") = lv930_1[0]
            lv936 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_9_self_attn_qkv_proj_q_weight5, model_layers_9_self_attn_qkv_proj_q_scale5, rms_norm213), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv319 = R.call_tir(cls.fused_reshape8_reshape9, (lv936,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv531 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), lv319), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv320 = R.call_tir(cls.fused_reshape10_reshape11, (lv531,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv937 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_9_self_attn_o_proj_q_weight5, model_layers_9_self_attn_o_proj_q_scale5, lv320), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv932_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv937, lv931_1, model_layers_9_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv933_1: R.Tensor((1, 1, 4096), dtype="float16") = lv932_1[1]
            rms_norm214: R.Tensor((1, 1, 4096), dtype="float16") = lv932_1[0]
            lv938 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_9_mlp_gate_up_proj_q_weight5, model_layers_9_mlp_gate_up_proj_q_scale5, rms_norm214), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv321 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv938,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv939 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_9_mlp_down_proj_q_weight5, model_layers_9_mlp_down_proj_q_scale5, lv321), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv934_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv939, lv933_1, model_layers_10_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv935_1: R.Tensor((1, 1, 4096), dtype="float16") = lv934_1[1]
            rms_norm215: R.Tensor((1, 1, 4096), dtype="float16") = lv934_1[0]
            lv940 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_10_self_attn_qkv_proj_q_weight5, model_layers_10_self_attn_qkv_proj_q_scale5, rms_norm215), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv322 = R.call_tir(cls.fused_reshape8_reshape9, (lv940,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv536 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), lv322), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv323 = R.call_tir(cls.fused_reshape10_reshape11, (lv536,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv941 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_10_self_attn_o_proj_q_weight5, model_layers_10_self_attn_o_proj_q_scale5, lv323), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv936_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv941, lv935_1, model_layers_10_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv937_1: R.Tensor((1, 1, 4096), dtype="float16") = lv936_1[1]
            rms_norm216: R.Tensor((1, 1, 4096), dtype="float16") = lv936_1[0]
            lv942 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_10_mlp_gate_up_proj_q_weight5, model_layers_10_mlp_gate_up_proj_q_scale5, rms_norm216), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv324 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv942,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv943 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_10_mlp_down_proj_q_weight5, model_layers_10_mlp_down_proj_q_scale5, lv324), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv938_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv943, lv937_1, model_layers_11_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv939_1: R.Tensor((1, 1, 4096), dtype="float16") = lv938_1[1]
            rms_norm217: R.Tensor((1, 1, 4096), dtype="float16") = lv938_1[0]
            lv944 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_11_self_attn_qkv_proj_q_weight5, model_layers_11_self_attn_qkv_proj_q_scale5, rms_norm217), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv325 = R.call_tir(cls.fused_reshape8_reshape9, (lv944,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv541 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), lv325), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv326 = R.call_tir(cls.fused_reshape10_reshape11, (lv541,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv945 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_11_self_attn_o_proj_q_weight5, model_layers_11_self_attn_o_proj_q_scale5, lv326), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv940_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv945, lv939_1, model_layers_11_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv941_1: R.Tensor((1, 1, 4096), dtype="float16") = lv940_1[1]
            rms_norm218: R.Tensor((1, 1, 4096), dtype="float16") = lv940_1[0]
            lv946 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_11_mlp_gate_up_proj_q_weight5, model_layers_11_mlp_gate_up_proj_q_scale5, rms_norm218), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv327 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv946,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv947 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_11_mlp_down_proj_q_weight5, model_layers_11_mlp_down_proj_q_scale5, lv327), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv942_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv947, lv941_1, model_layers_12_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv943_1: R.Tensor((1, 1, 4096), dtype="float16") = lv942_1[1]
            rms_norm219: R.Tensor((1, 1, 4096), dtype="float16") = lv942_1[0]
            lv948 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_12_self_attn_qkv_proj_q_weight5, model_layers_12_self_attn_qkv_proj_q_scale5, rms_norm219), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv328 = R.call_tir(cls.fused_reshape8_reshape9, (lv948,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv546 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), lv328), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv329 = R.call_tir(cls.fused_reshape10_reshape11, (lv546,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv949 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_12_self_attn_o_proj_q_weight5, model_layers_12_self_attn_o_proj_q_scale5, lv329), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv944_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv949, lv943_1, model_layers_12_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv945_1: R.Tensor((1, 1, 4096), dtype="float16") = lv944_1[1]
            rms_norm220: R.Tensor((1, 1, 4096), dtype="float16") = lv944_1[0]
            lv950 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_12_mlp_gate_up_proj_q_weight5, model_layers_12_mlp_gate_up_proj_q_scale5, rms_norm220), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv330 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv950,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv951 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_12_mlp_down_proj_q_weight5, model_layers_12_mlp_down_proj_q_scale5, lv330), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv946_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv951, lv945_1, model_layers_13_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv947_1: R.Tensor((1, 1, 4096), dtype="float16") = lv946_1[1]
            rms_norm221: R.Tensor((1, 1, 4096), dtype="float16") = lv946_1[0]
            lv952 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_13_self_attn_qkv_proj_q_weight5, model_layers_13_self_attn_qkv_proj_q_scale5, rms_norm221), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv331 = R.call_tir(cls.fused_reshape8_reshape9, (lv952,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv551 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), lv331), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv332 = R.call_tir(cls.fused_reshape10_reshape11, (lv551,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv953 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_13_self_attn_o_proj_q_weight5, model_layers_13_self_attn_o_proj_q_scale5, lv332), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv948_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv953, lv947_1, model_layers_13_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv949_1: R.Tensor((1, 1, 4096), dtype="float16") = lv948_1[1]
            rms_norm222: R.Tensor((1, 1, 4096), dtype="float16") = lv948_1[0]
            lv954 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_13_mlp_gate_up_proj_q_weight5, model_layers_13_mlp_gate_up_proj_q_scale5, rms_norm222), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv333 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv954,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv955 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_13_mlp_down_proj_q_weight5, model_layers_13_mlp_down_proj_q_scale5, lv333), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv950_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv955, lv949_1, model_layers_14_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv951_1: R.Tensor((1, 1, 4096), dtype="float16") = lv950_1[1]
            rms_norm223: R.Tensor((1, 1, 4096), dtype="float16") = lv950_1[0]
            lv956 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_14_self_attn_qkv_proj_q_weight5, model_layers_14_self_attn_qkv_proj_q_scale5, rms_norm223), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv334 = R.call_tir(cls.fused_reshape8_reshape9, (lv956,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv556 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), lv334), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv335 = R.call_tir(cls.fused_reshape10_reshape11, (lv556,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv957 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_14_self_attn_o_proj_q_weight5, model_layers_14_self_attn_o_proj_q_scale5, lv335), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv952_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv957, lv951_1, model_layers_14_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv953_1: R.Tensor((1, 1, 4096), dtype="float16") = lv952_1[1]
            rms_norm224: R.Tensor((1, 1, 4096), dtype="float16") = lv952_1[0]
            lv958 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_14_mlp_gate_up_proj_q_weight5, model_layers_14_mlp_gate_up_proj_q_scale5, rms_norm224), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv336 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv958,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv959 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_14_mlp_down_proj_q_weight5, model_layers_14_mlp_down_proj_q_scale5, lv336), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv954_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv959, lv953_1, model_layers_15_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv955_1: R.Tensor((1, 1, 4096), dtype="float16") = lv954_1[1]
            rms_norm225: R.Tensor((1, 1, 4096), dtype="float16") = lv954_1[0]
            lv960 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_15_self_attn_qkv_proj_q_weight5, model_layers_15_self_attn_qkv_proj_q_scale5, rms_norm225), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv337 = R.call_tir(cls.fused_reshape8_reshape9, (lv960,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv561 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), lv337), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv338 = R.call_tir(cls.fused_reshape10_reshape11, (lv561,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv961 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_15_self_attn_o_proj_q_weight5, model_layers_15_self_attn_o_proj_q_scale5, lv338), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv956_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv961, lv955_1, model_layers_15_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv957_1: R.Tensor((1, 1, 4096), dtype="float16") = lv956_1[1]
            rms_norm226: R.Tensor((1, 1, 4096), dtype="float16") = lv956_1[0]
            lv962 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_15_mlp_gate_up_proj_q_weight5, model_layers_15_mlp_gate_up_proj_q_scale5, rms_norm226), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv339 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv962,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv963 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_15_mlp_down_proj_q_weight5, model_layers_15_mlp_down_proj_q_scale5, lv339), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv958_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv963, lv957_1, model_layers_16_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv959_1: R.Tensor((1, 1, 4096), dtype="float16") = lv958_1[1]
            rms_norm227: R.Tensor((1, 1, 4096), dtype="float16") = lv958_1[0]
            lv964 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_16_self_attn_qkv_proj_q_weight5, model_layers_16_self_attn_qkv_proj_q_scale5, rms_norm227), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv340 = R.call_tir(cls.fused_reshape8_reshape9, (lv964,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv566 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), lv340), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv341 = R.call_tir(cls.fused_reshape10_reshape11, (lv566,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv965 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_16_self_attn_o_proj_q_weight5, model_layers_16_self_attn_o_proj_q_scale5, lv341), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv960_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv965, lv959_1, model_layers_16_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv961_1: R.Tensor((1, 1, 4096), dtype="float16") = lv960_1[1]
            rms_norm228: R.Tensor((1, 1, 4096), dtype="float16") = lv960_1[0]
            lv966 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_16_mlp_gate_up_proj_q_weight5, model_layers_16_mlp_gate_up_proj_q_scale5, rms_norm228), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv342 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv966,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv967 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_16_mlp_down_proj_q_weight5, model_layers_16_mlp_down_proj_q_scale5, lv342), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv962_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv967, lv961_1, model_layers_17_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv963_1: R.Tensor((1, 1, 4096), dtype="float16") = lv962_1[1]
            rms_norm229: R.Tensor((1, 1, 4096), dtype="float16") = lv962_1[0]
            lv968 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_17_self_attn_qkv_proj_q_weight5, model_layers_17_self_attn_qkv_proj_q_scale5, rms_norm229), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv343 = R.call_tir(cls.fused_reshape8_reshape9, (lv968,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv571 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), lv343), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv344 = R.call_tir(cls.fused_reshape10_reshape11, (lv571,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv969 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_17_self_attn_o_proj_q_weight5, model_layers_17_self_attn_o_proj_q_scale5, lv344), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv964_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv969, lv963_1, model_layers_17_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv965_1: R.Tensor((1, 1, 4096), dtype="float16") = lv964_1[1]
            rms_norm230: R.Tensor((1, 1, 4096), dtype="float16") = lv964_1[0]
            lv970 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_17_mlp_gate_up_proj_q_weight5, model_layers_17_mlp_gate_up_proj_q_scale5, rms_norm230), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv345 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv970,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv971 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_17_mlp_down_proj_q_weight5, model_layers_17_mlp_down_proj_q_scale5, lv345), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv966_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv971, lv965_1, model_layers_18_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv967_1: R.Tensor((1, 1, 4096), dtype="float16") = lv966_1[1]
            rms_norm231: R.Tensor((1, 1, 4096), dtype="float16") = lv966_1[0]
            lv972 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_18_self_attn_qkv_proj_q_weight5, model_layers_18_self_attn_qkv_proj_q_scale5, rms_norm231), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv346 = R.call_tir(cls.fused_reshape8_reshape9, (lv972,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv576 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), lv346), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv347 = R.call_tir(cls.fused_reshape10_reshape11, (lv576,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv973 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_18_self_attn_o_proj_q_weight5, model_layers_18_self_attn_o_proj_q_scale5, lv347), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv968_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv973, lv967_1, model_layers_18_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv969_1: R.Tensor((1, 1, 4096), dtype="float16") = lv968_1[1]
            rms_norm232: R.Tensor((1, 1, 4096), dtype="float16") = lv968_1[0]
            lv974 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_18_mlp_gate_up_proj_q_weight5, model_layers_18_mlp_gate_up_proj_q_scale5, rms_norm232), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv348 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv974,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv975 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_18_mlp_down_proj_q_weight5, model_layers_18_mlp_down_proj_q_scale5, lv348), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv970_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv975, lv969_1, model_layers_19_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv971_1: R.Tensor((1, 1, 4096), dtype="float16") = lv970_1[1]
            rms_norm233: R.Tensor((1, 1, 4096), dtype="float16") = lv970_1[0]
            lv976 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_19_self_attn_qkv_proj_q_weight5, model_layers_19_self_attn_qkv_proj_q_scale5, rms_norm233), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv349 = R.call_tir(cls.fused_reshape8_reshape9, (lv976,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv581 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), lv349), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv350 = R.call_tir(cls.fused_reshape10_reshape11, (lv581,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv977 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_19_self_attn_o_proj_q_weight5, model_layers_19_self_attn_o_proj_q_scale5, lv350), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv972_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv977, lv971_1, model_layers_19_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv973_1: R.Tensor((1, 1, 4096), dtype="float16") = lv972_1[1]
            rms_norm234: R.Tensor((1, 1, 4096), dtype="float16") = lv972_1[0]
            lv978 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_19_mlp_gate_up_proj_q_weight5, model_layers_19_mlp_gate_up_proj_q_scale5, rms_norm234), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv351 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv978,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv979 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_19_mlp_down_proj_q_weight5, model_layers_19_mlp_down_proj_q_scale5, lv351), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv974_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv979, lv973_1, model_layers_20_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv975_1: R.Tensor((1, 1, 4096), dtype="float16") = lv974_1[1]
            rms_norm235: R.Tensor((1, 1, 4096), dtype="float16") = lv974_1[0]
            lv980 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_20_self_attn_qkv_proj_q_weight5, model_layers_20_self_attn_qkv_proj_q_scale5, rms_norm235), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv352 = R.call_tir(cls.fused_reshape8_reshape9, (lv980,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv586 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), lv352), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv353 = R.call_tir(cls.fused_reshape10_reshape11, (lv586,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv981 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_20_self_attn_o_proj_q_weight5, model_layers_20_self_attn_o_proj_q_scale5, lv353), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv976_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv981, lv975_1, model_layers_20_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv977_1: R.Tensor((1, 1, 4096), dtype="float16") = lv976_1[1]
            rms_norm236: R.Tensor((1, 1, 4096), dtype="float16") = lv976_1[0]
            lv982 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_20_mlp_gate_up_proj_q_weight5, model_layers_20_mlp_gate_up_proj_q_scale5, rms_norm236), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv354 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv982,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv983 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_20_mlp_down_proj_q_weight5, model_layers_20_mlp_down_proj_q_scale5, lv354), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv978_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv983, lv977_1, model_layers_21_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv979_1: R.Tensor((1, 1, 4096), dtype="float16") = lv978_1[1]
            rms_norm237: R.Tensor((1, 1, 4096), dtype="float16") = lv978_1[0]
            lv984 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_21_self_attn_qkv_proj_q_weight5, model_layers_21_self_attn_qkv_proj_q_scale5, rms_norm237), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv355 = R.call_tir(cls.fused_reshape8_reshape9, (lv984,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv591 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), lv355), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv356 = R.call_tir(cls.fused_reshape10_reshape11, (lv591,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv985 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_21_self_attn_o_proj_q_weight5, model_layers_21_self_attn_o_proj_q_scale5, lv356), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv980_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv985, lv979_1, model_layers_21_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv981_1: R.Tensor((1, 1, 4096), dtype="float16") = lv980_1[1]
            rms_norm238: R.Tensor((1, 1, 4096), dtype="float16") = lv980_1[0]
            lv986 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_21_mlp_gate_up_proj_q_weight5, model_layers_21_mlp_gate_up_proj_q_scale5, rms_norm238), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv357 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv986,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv987 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_21_mlp_down_proj_q_weight5, model_layers_21_mlp_down_proj_q_scale5, lv357), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv982_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv987, lv981_1, model_layers_22_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv983_1: R.Tensor((1, 1, 4096), dtype="float16") = lv982_1[1]
            rms_norm239: R.Tensor((1, 1, 4096), dtype="float16") = lv982_1[0]
            lv988 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_22_self_attn_qkv_proj_q_weight5, model_layers_22_self_attn_qkv_proj_q_scale5, rms_norm239), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv358 = R.call_tir(cls.fused_reshape8_reshape9, (lv988,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv596 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), lv358), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv359 = R.call_tir(cls.fused_reshape10_reshape11, (lv596,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv989 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_22_self_attn_o_proj_q_weight5, model_layers_22_self_attn_o_proj_q_scale5, lv359), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv984_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv989, lv983_1, model_layers_22_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv985_1: R.Tensor((1, 1, 4096), dtype="float16") = lv984_1[1]
            rms_norm240: R.Tensor((1, 1, 4096), dtype="float16") = lv984_1[0]
            lv990 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_22_mlp_gate_up_proj_q_weight5, model_layers_22_mlp_gate_up_proj_q_scale5, rms_norm240), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv360 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv990,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv991 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_22_mlp_down_proj_q_weight5, model_layers_22_mlp_down_proj_q_scale5, lv360), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv986_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv991, lv985_1, model_layers_23_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv987_1: R.Tensor((1, 1, 4096), dtype="float16") = lv986_1[1]
            rms_norm241: R.Tensor((1, 1, 4096), dtype="float16") = lv986_1[0]
            lv992 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_23_self_attn_qkv_proj_q_weight5, model_layers_23_self_attn_qkv_proj_q_scale5, rms_norm241), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv361 = R.call_tir(cls.fused_reshape8_reshape9, (lv992,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv601 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), lv361), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv362 = R.call_tir(cls.fused_reshape10_reshape11, (lv601,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv993 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_23_self_attn_o_proj_q_weight5, model_layers_23_self_attn_o_proj_q_scale5, lv362), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv988_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv993, lv987_1, model_layers_23_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv989_1: R.Tensor((1, 1, 4096), dtype="float16") = lv988_1[1]
            rms_norm242: R.Tensor((1, 1, 4096), dtype="float16") = lv988_1[0]
            lv994 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_23_mlp_gate_up_proj_q_weight5, model_layers_23_mlp_gate_up_proj_q_scale5, rms_norm242), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv363 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv994,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv995 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_23_mlp_down_proj_q_weight5, model_layers_23_mlp_down_proj_q_scale5, lv363), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv990_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv995, lv989_1, model_layers_24_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv991_1: R.Tensor((1, 1, 4096), dtype="float16") = lv990_1[1]
            rms_norm243: R.Tensor((1, 1, 4096), dtype="float16") = lv990_1[0]
            lv996 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_24_self_attn_qkv_proj_q_weight5, model_layers_24_self_attn_qkv_proj_q_scale5, rms_norm243), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv364 = R.call_tir(cls.fused_reshape8_reshape9, (lv996,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv606 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), lv364), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv365 = R.call_tir(cls.fused_reshape10_reshape11, (lv606,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv997 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_24_self_attn_o_proj_q_weight5, model_layers_24_self_attn_o_proj_q_scale5, lv365), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv992_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv997, lv991_1, model_layers_24_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv993_1: R.Tensor((1, 1, 4096), dtype="float16") = lv992_1[1]
            rms_norm244: R.Tensor((1, 1, 4096), dtype="float16") = lv992_1[0]
            lv998 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_24_mlp_gate_up_proj_q_weight5, model_layers_24_mlp_gate_up_proj_q_scale5, rms_norm244), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv366 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv998,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv999 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_24_mlp_down_proj_q_weight5, model_layers_24_mlp_down_proj_q_scale5, lv366), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv994_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv999, lv993_1, model_layers_25_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv995_1: R.Tensor((1, 1, 4096), dtype="float16") = lv994_1[1]
            rms_norm245: R.Tensor((1, 1, 4096), dtype="float16") = lv994_1[0]
            lv1000 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_25_self_attn_qkv_proj_q_weight5, model_layers_25_self_attn_qkv_proj_q_scale5, rms_norm245), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv367 = R.call_tir(cls.fused_reshape8_reshape9, (lv1000,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv611 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), lv367), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv368 = R.call_tir(cls.fused_reshape10_reshape11, (lv611,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1001 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_25_self_attn_o_proj_q_weight5, model_layers_25_self_attn_o_proj_q_scale5, lv368), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv996_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1001, lv995_1, model_layers_25_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv997_1: R.Tensor((1, 1, 4096), dtype="float16") = lv996_1[1]
            rms_norm246: R.Tensor((1, 1, 4096), dtype="float16") = lv996_1[0]
            lv1002 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_25_mlp_gate_up_proj_q_weight5, model_layers_25_mlp_gate_up_proj_q_scale5, rms_norm246), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv369 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv1002,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv1003 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_25_mlp_down_proj_q_weight5, model_layers_25_mlp_down_proj_q_scale5, lv369), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv998_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1003, lv997_1, model_layers_26_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv999_1: R.Tensor((1, 1, 4096), dtype="float16") = lv998_1[1]
            rms_norm247: R.Tensor((1, 1, 4096), dtype="float16") = lv998_1[0]
            lv1004 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_26_self_attn_qkv_proj_q_weight5, model_layers_26_self_attn_qkv_proj_q_scale5, rms_norm247), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv370 = R.call_tir(cls.fused_reshape8_reshape9, (lv1004,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv616 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), lv370), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv371 = R.call_tir(cls.fused_reshape10_reshape11, (lv616,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1005 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_26_self_attn_o_proj_q_weight5, model_layers_26_self_attn_o_proj_q_scale5, lv371), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1000_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1005, lv999_1, model_layers_26_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1001_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1000_1[1]
            rms_norm248: R.Tensor((1, 1, 4096), dtype="float16") = lv1000_1[0]
            lv1006 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_26_mlp_gate_up_proj_q_weight5, model_layers_26_mlp_gate_up_proj_q_scale5, rms_norm248), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv372 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv1006,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv1007 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_26_mlp_down_proj_q_weight5, model_layers_26_mlp_down_proj_q_scale5, lv372), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1002_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1007, lv1001_1, model_layers_27_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1003_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1002_1[1]
            rms_norm249: R.Tensor((1, 1, 4096), dtype="float16") = lv1002_1[0]
            lv1008 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_27_self_attn_qkv_proj_q_weight5, model_layers_27_self_attn_qkv_proj_q_scale5, rms_norm249), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv373 = R.call_tir(cls.fused_reshape8_reshape9, (lv1008,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv621 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), lv373), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv374 = R.call_tir(cls.fused_reshape10_reshape11, (lv621,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1009 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_27_self_attn_o_proj_q_weight5, model_layers_27_self_attn_o_proj_q_scale5, lv374), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1004_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1009, lv1003_1, model_layers_27_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1005_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1004_1[1]
            rms_norm250: R.Tensor((1, 1, 4096), dtype="float16") = lv1004_1[0]
            lv1010 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_27_mlp_gate_up_proj_q_weight5, model_layers_27_mlp_gate_up_proj_q_scale5, rms_norm250), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv375 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv1010,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv1011 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_27_mlp_down_proj_q_weight5, model_layers_27_mlp_down_proj_q_scale5, lv375), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1006_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1011, lv1005_1, model_layers_28_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1007_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1006_1[1]
            rms_norm251: R.Tensor((1, 1, 4096), dtype="float16") = lv1006_1[0]
            lv1012 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_28_self_attn_qkv_proj_q_weight5, model_layers_28_self_attn_qkv_proj_q_scale5, rms_norm251), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv376 = R.call_tir(cls.fused_reshape8_reshape9, (lv1012,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv626 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), lv376), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv377 = R.call_tir(cls.fused_reshape10_reshape11, (lv626,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1013 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_28_self_attn_o_proj_q_weight5, model_layers_28_self_attn_o_proj_q_scale5, lv377), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1008_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1013, lv1007_1, model_layers_28_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1009_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1008_1[1]
            rms_norm252: R.Tensor((1, 1, 4096), dtype="float16") = lv1008_1[0]
            lv1014 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_28_mlp_gate_up_proj_q_weight5, model_layers_28_mlp_gate_up_proj_q_scale5, rms_norm252), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv378 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv1014,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv1015 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_28_mlp_down_proj_q_weight5, model_layers_28_mlp_down_proj_q_scale5, lv378), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1010_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1015, lv1009_1, model_layers_29_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1011_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1010_1[1]
            rms_norm253: R.Tensor((1, 1, 4096), dtype="float16") = lv1010_1[0]
            lv1016 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_29_self_attn_qkv_proj_q_weight5, model_layers_29_self_attn_qkv_proj_q_scale5, rms_norm253), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv379 = R.call_tir(cls.fused_reshape8_reshape9, (lv1016,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv631 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), lv379), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv380 = R.call_tir(cls.fused_reshape10_reshape11, (lv631,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1017 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_29_self_attn_o_proj_q_weight5, model_layers_29_self_attn_o_proj_q_scale5, lv380), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1012_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1017, lv1011_1, model_layers_29_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1013_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1012_1[1]
            rms_norm254: R.Tensor((1, 1, 4096), dtype="float16") = lv1012_1[0]
            lv1018 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_29_mlp_gate_up_proj_q_weight5, model_layers_29_mlp_gate_up_proj_q_scale5, rms_norm254), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv381 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv1018,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv1019 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_29_mlp_down_proj_q_weight5, model_layers_29_mlp_down_proj_q_scale5, lv381), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1014_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1019, lv1013_1, model_layers_30_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1015_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1014_1[1]
            rms_norm255: R.Tensor((1, 1, 4096), dtype="float16") = lv1014_1[0]
            lv1020 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_30_self_attn_qkv_proj_q_weight5, model_layers_30_self_attn_qkv_proj_q_scale5, rms_norm255), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv382 = R.call_tir(cls.fused_reshape8_reshape9, (lv1020,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv636 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), lv382), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv383 = R.call_tir(cls.fused_reshape10_reshape11, (lv636,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1021 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_30_self_attn_o_proj_q_weight5, model_layers_30_self_attn_o_proj_q_scale5, lv383), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1016_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1021, lv1015_1, model_layers_30_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1017_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1016_1[1]
            rms_norm256: R.Tensor((1, 1, 4096), dtype="float16") = lv1016_1[0]
            lv1022 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_30_mlp_gate_up_proj_q_weight5, model_layers_30_mlp_gate_up_proj_q_scale5, rms_norm256), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv384 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv1022,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv1023 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_30_mlp_down_proj_q_weight5, model_layers_30_mlp_down_proj_q_scale5, lv384), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1018_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1023, lv1017_1, model_layers_31_input_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1019_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1018_1[1]
            rms_norm257: R.Tensor((1, 1, 4096), dtype="float16") = lv1018_1[0]
            lv1024 = R.call_tir(cls.fused_dequantize1_NT_matmul10, (model_layers_31_self_attn_qkv_proj_q_weight5, model_layers_31_self_attn_qkv_proj_q_scale5, rms_norm257), out_sinfo=R.Tensor((1, 1, 6144), dtype="float16"))
            lv385 = R.call_tir(cls.fused_reshape8_reshape9, (lv1024,), out_sinfo=R.Tensor((1, 48, 128), dtype="float16"))
            lv641 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), lv385), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            lv386 = R.call_tir(cls.fused_reshape10_reshape11, (lv641,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1025 = R.call_tir(cls.fused_dequantize2_NT_matmul11, (model_layers_31_self_attn_o_proj_q_weight5, model_layers_31_self_attn_o_proj_q_scale5, lv386), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1020_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1025, lv1019_1, model_layers_31_post_attention_layernorm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            lv1021_1: R.Tensor((1, 1, 4096), dtype="float16") = lv1020_1[1]
            rms_norm258: R.Tensor((1, 1, 4096), dtype="float16") = lv1020_1[0]
            lv1026 = R.call_tir(cls.fused_dequantize3_NT_matmul12, (model_layers_31_mlp_gate_up_proj_q_weight5, model_layers_31_mlp_gate_up_proj_q_scale5, rms_norm258), out_sinfo=R.Tensor((1, 1, 28672), dtype="float16"))
            lv387 = R.call_tir(cls.fused_split2_silu2_multiply2, (lv1026,), out_sinfo=R.Tensor((1, 1, 14336), dtype="float16"))
            lv1027 = R.call_tir(cls.fused_dequantize4_NT_matmul13, (model_layers_31_mlp_down_proj_q_weight5, model_layers_31_mlp_down_proj_q_scale5, lv387), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1022_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1027, lv1021_1, model_norm_weight5), out_sinfo=[R.Tensor((1, 1, 4096), dtype="float16"), R.Tensor((1, 1, 4096), dtype="float16")])
            rms_norm259: R.Tensor((1, 1, 4096), dtype="float16") = lv1022_1[0]
            gv6: R.Tuple(R.Tensor((1, 1, 4096), dtype="float16"), R.Object) = rms_norm259, paged_kv_cache
            R.output(gv6)
        return gv6

    @R.function
    def embed(input_ids: R.Tensor(("seq_len",), dtype="int32"), packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tensor(("seq_len", 4096), dtype="float16"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            gv = R.call_tir(cls.fused_dequantize_take1, (model_embed_tokens_q_weight, model_embed_tokens_q_scale, input_ids), out_sinfo=R.Tensor((seq_len, 4096), dtype="float16"))
            R.output(gv)
        return gv

    @R.function
    def gather_hidden_states(src: R.Tensor(("m", "n"), dtype="float16"), indices: R.Tensor(("batch_size",), dtype="int32"), dst: R.Tensor(("batch_size", "n"), dtype="float16")) -> R.Tensor(("batch_size", "n"), dtype="float16"):
        batch_size = T.int64(is_size_var=True)
        n = T.int64(is_size_var=True)
        m = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        cls = Module
        with R.dataflow():
            gv: R.Tensor((batch_size, n), dtype="float16") = R.call_tir_inplace(cls._gather_hidden_states, (src, indices, dst), out_sinfo=R.Tensor((batch_size, n), dtype="float16"), inplace_indices=[2])
            R.output(gv)
        return gv

    @R.function
    def get_logits(hidden_states: R.Tensor(("seq_len", 4096), dtype="float16"), packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tensor(("seq_len", "vocab_size"), dtype="float32"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            lm_head_q_weight1: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale1: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            gv = R.call_tir(cls.fused_dequantize_fused_NT_matmul15_cast3, (lm_head_q_weight1, lm_head_q_scale1, hidden_states), out_sinfo=R.Tensor((seq_len, vocab_size), dtype="float32"))
            R.output(gv)
        return gv

    @R.function
    def multinomial_from_uniform(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            uniform_samples_1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices_1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            nn_multinomial_from_uniform = R.call_tir(cls.parallel_sampling_from_prob, (probs, uniform_samples_1, sample_indices_1), out_sinfo=R.Tensor((num_samples, 1), dtype="int32"))
            gv: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", nn_multinomial_from_uniform, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            R.output(gv)
        return gv

    @R.function
    def prefill(input_embed: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight2: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale2: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm = R.call_tir(cls.rms_norm1, (input_embed, model_layers_0_input_layernorm_weight2), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1028 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_0_self_attn_qkv_proj_q_weight2, model_layers_0_self_attn_qkv_proj_q_scale2, rms_norm), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape = R.call_tir(cls.reshape4, (lv1028,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape1 = R.call_tir(cls.reshape5, (reshape,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv3 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape2 = R.call_tir(cls.reshape6, (lv3,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape3 = R.call_tir(cls.reshape7, (reshape2,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1029 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_0_self_attn_o_proj_q_weight2, model_layers_0_self_attn_o_proj_q_scale2, reshape3), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1024 = R.call_tir(cls.fuse_add_norm_prefill, (lv1029, input_embed, model_layers_0_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1025: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1024[1]
            rms_norm1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1024[0]
            lv1030 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_0_mlp_gate_up_proj_q_weight2, model_layers_0_mlp_gate_up_proj_q_scale2, rms_norm1), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv389 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1030,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1031 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_0_mlp_down_proj_q_weight2, model_layers_0_mlp_down_proj_q_scale2, lv389), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1026 = R.call_tir(cls.fuse_add_norm_prefill, (lv1031, lv1025, model_layers_1_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1027: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1026[1]
            rms_norm2: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1026[0]
            lv1032 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_1_self_attn_qkv_proj_q_weight2, model_layers_1_self_attn_qkv_proj_q_scale2, rms_norm2), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape4 = R.call_tir(cls.reshape4, (lv1032,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape5 = R.call_tir(cls.reshape5, (reshape4,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv8 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape5), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape6 = R.call_tir(cls.reshape6, (lv8,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape7 = R.call_tir(cls.reshape7, (reshape6,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1033 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_1_self_attn_o_proj_q_weight2, model_layers_1_self_attn_o_proj_q_scale2, reshape7), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1028_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1033, lv1027, model_layers_1_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1029_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1028_1[1]
            rms_norm3: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1028_1[0]
            lv1034 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_1_mlp_gate_up_proj_q_weight2, model_layers_1_mlp_gate_up_proj_q_scale2, rms_norm3), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv390 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1034,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1035 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_1_mlp_down_proj_q_weight2, model_layers_1_mlp_down_proj_q_scale2, lv390), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1030_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1035, lv1029_1, model_layers_2_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1031_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1030_1[1]
            rms_norm4: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1030_1[0]
            lv1036 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_2_self_attn_qkv_proj_q_weight2, model_layers_2_self_attn_qkv_proj_q_scale2, rms_norm4), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape8 = R.call_tir(cls.reshape4, (lv1036,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape9 = R.call_tir(cls.reshape5, (reshape8,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv13 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape9), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape10 = R.call_tir(cls.reshape6, (lv13,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape11 = R.call_tir(cls.reshape7, (reshape10,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1037 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_2_self_attn_o_proj_q_weight2, model_layers_2_self_attn_o_proj_q_scale2, reshape11), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1032_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1037, lv1031_1, model_layers_2_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1033_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1032_1[1]
            rms_norm5: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1032_1[0]
            lv1038 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_2_mlp_gate_up_proj_q_weight2, model_layers_2_mlp_gate_up_proj_q_scale2, rms_norm5), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv391 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1038,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1039 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_2_mlp_down_proj_q_weight2, model_layers_2_mlp_down_proj_q_scale2, lv391), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1034_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1039, lv1033_1, model_layers_3_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1035_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1034_1[1]
            rms_norm6: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1034_1[0]
            lv1040 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_3_self_attn_qkv_proj_q_weight2, model_layers_3_self_attn_qkv_proj_q_scale2, rms_norm6), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape12 = R.call_tir(cls.reshape4, (lv1040,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape13 = R.call_tir(cls.reshape5, (reshape12,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv18 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape13), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape14 = R.call_tir(cls.reshape6, (lv18,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape15 = R.call_tir(cls.reshape7, (reshape14,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1041 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_3_self_attn_o_proj_q_weight2, model_layers_3_self_attn_o_proj_q_scale2, reshape15), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1036_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1041, lv1035_1, model_layers_3_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1037_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1036_1[1]
            rms_norm7: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1036_1[0]
            lv1042 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_3_mlp_gate_up_proj_q_weight2, model_layers_3_mlp_gate_up_proj_q_scale2, rms_norm7), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv392 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1042,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1043 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_3_mlp_down_proj_q_weight2, model_layers_3_mlp_down_proj_q_scale2, lv392), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1038_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1043, lv1037_1, model_layers_4_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1039_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1038_1[1]
            rms_norm8: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1038_1[0]
            lv1044 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_4_self_attn_qkv_proj_q_weight2, model_layers_4_self_attn_qkv_proj_q_scale2, rms_norm8), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape16 = R.call_tir(cls.reshape4, (lv1044,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape17 = R.call_tir(cls.reshape5, (reshape16,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv23 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape17), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape18 = R.call_tir(cls.reshape6, (lv23,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape19 = R.call_tir(cls.reshape7, (reshape18,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1045 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_4_self_attn_o_proj_q_weight2, model_layers_4_self_attn_o_proj_q_scale2, reshape19), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1040_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1045, lv1039_1, model_layers_4_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1041_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1040_1[1]
            rms_norm9: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1040_1[0]
            lv1046 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_4_mlp_gate_up_proj_q_weight2, model_layers_4_mlp_gate_up_proj_q_scale2, rms_norm9), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv393 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1046,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1047 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_4_mlp_down_proj_q_weight2, model_layers_4_mlp_down_proj_q_scale2, lv393), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1042_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1047, lv1041_1, model_layers_5_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1043_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1042_1[1]
            rms_norm10: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1042_1[0]
            lv1048 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_5_self_attn_qkv_proj_q_weight2, model_layers_5_self_attn_qkv_proj_q_scale2, rms_norm10), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape20 = R.call_tir(cls.reshape4, (lv1048,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape21 = R.call_tir(cls.reshape5, (reshape20,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv28 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape21), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape22 = R.call_tir(cls.reshape6, (lv28,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape23 = R.call_tir(cls.reshape7, (reshape22,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1049 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_5_self_attn_o_proj_q_weight2, model_layers_5_self_attn_o_proj_q_scale2, reshape23), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1044_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1049, lv1043_1, model_layers_5_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1045_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1044_1[1]
            rms_norm11: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1044_1[0]
            lv1050 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_5_mlp_gate_up_proj_q_weight2, model_layers_5_mlp_gate_up_proj_q_scale2, rms_norm11), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv394 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1050,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1051 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_5_mlp_down_proj_q_weight2, model_layers_5_mlp_down_proj_q_scale2, lv394), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1046_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1051, lv1045_1, model_layers_6_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1047_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1046_1[1]
            rms_norm12: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1046_1[0]
            lv1052 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_6_self_attn_qkv_proj_q_weight2, model_layers_6_self_attn_qkv_proj_q_scale2, rms_norm12), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape24 = R.call_tir(cls.reshape4, (lv1052,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape25 = R.call_tir(cls.reshape5, (reshape24,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv33 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape25), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape26 = R.call_tir(cls.reshape6, (lv33,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape27 = R.call_tir(cls.reshape7, (reshape26,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1053 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_6_self_attn_o_proj_q_weight2, model_layers_6_self_attn_o_proj_q_scale2, reshape27), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1048_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1053, lv1047_1, model_layers_6_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1049_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1048_1[1]
            rms_norm13: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1048_1[0]
            lv1054 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_6_mlp_gate_up_proj_q_weight2, model_layers_6_mlp_gate_up_proj_q_scale2, rms_norm13), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv395 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1054,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1055 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_6_mlp_down_proj_q_weight2, model_layers_6_mlp_down_proj_q_scale2, lv395), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1050_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1055, lv1049_1, model_layers_7_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1051_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1050_1[1]
            rms_norm14: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1050_1[0]
            lv1056 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_7_self_attn_qkv_proj_q_weight2, model_layers_7_self_attn_qkv_proj_q_scale2, rms_norm14), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape28 = R.call_tir(cls.reshape4, (lv1056,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape29 = R.call_tir(cls.reshape5, (reshape28,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv38 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape29), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape30 = R.call_tir(cls.reshape6, (lv38,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape31 = R.call_tir(cls.reshape7, (reshape30,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1057 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_7_self_attn_o_proj_q_weight2, model_layers_7_self_attn_o_proj_q_scale2, reshape31), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1052_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1057, lv1051_1, model_layers_7_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1053_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1052_1[1]
            rms_norm15: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1052_1[0]
            lv1058 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_7_mlp_gate_up_proj_q_weight2, model_layers_7_mlp_gate_up_proj_q_scale2, rms_norm15), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv396 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1058,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1059 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_7_mlp_down_proj_q_weight2, model_layers_7_mlp_down_proj_q_scale2, lv396), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1054_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1059, lv1053_1, model_layers_8_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1055_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1054_1[1]
            rms_norm16: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1054_1[0]
            lv1060 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_8_self_attn_qkv_proj_q_weight2, model_layers_8_self_attn_qkv_proj_q_scale2, rms_norm16), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape32 = R.call_tir(cls.reshape4, (lv1060,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape33 = R.call_tir(cls.reshape5, (reshape32,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv43 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape33), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape34 = R.call_tir(cls.reshape6, (lv43,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape35 = R.call_tir(cls.reshape7, (reshape34,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1061 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_8_self_attn_o_proj_q_weight2, model_layers_8_self_attn_o_proj_q_scale2, reshape35), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1056_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1061, lv1055_1, model_layers_8_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1057_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1056_1[1]
            rms_norm17: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1056_1[0]
            lv1062 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_8_mlp_gate_up_proj_q_weight2, model_layers_8_mlp_gate_up_proj_q_scale2, rms_norm17), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv397 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1062,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1063 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_8_mlp_down_proj_q_weight2, model_layers_8_mlp_down_proj_q_scale2, lv397), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1058_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1063, lv1057_1, model_layers_9_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1059_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1058_1[1]
            rms_norm18: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1058_1[0]
            lv1064 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_9_self_attn_qkv_proj_q_weight2, model_layers_9_self_attn_qkv_proj_q_scale2, rms_norm18), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape36 = R.call_tir(cls.reshape4, (lv1064,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape37 = R.call_tir(cls.reshape5, (reshape36,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv48 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape37), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape38 = R.call_tir(cls.reshape6, (lv48,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape39 = R.call_tir(cls.reshape7, (reshape38,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1065 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_9_self_attn_o_proj_q_weight2, model_layers_9_self_attn_o_proj_q_scale2, reshape39), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1060_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1065, lv1059_1, model_layers_9_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1061_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1060_1[1]
            rms_norm19: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1060_1[0]
            lv1066 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_9_mlp_gate_up_proj_q_weight2, model_layers_9_mlp_gate_up_proj_q_scale2, rms_norm19), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv398 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1066,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1067 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_9_mlp_down_proj_q_weight2, model_layers_9_mlp_down_proj_q_scale2, lv398), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1062_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1067, lv1061_1, model_layers_10_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1063_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1062_1[1]
            rms_norm20: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1062_1[0]
            lv1068 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_10_self_attn_qkv_proj_q_weight2, model_layers_10_self_attn_qkv_proj_q_scale2, rms_norm20), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape40 = R.call_tir(cls.reshape4, (lv1068,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape41 = R.call_tir(cls.reshape5, (reshape40,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv53 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape41), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape42 = R.call_tir(cls.reshape6, (lv53,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape43 = R.call_tir(cls.reshape7, (reshape42,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1069 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_10_self_attn_o_proj_q_weight2, model_layers_10_self_attn_o_proj_q_scale2, reshape43), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1064_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1069, lv1063_1, model_layers_10_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1065_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1064_1[1]
            rms_norm21: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1064_1[0]
            lv1070 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_10_mlp_gate_up_proj_q_weight2, model_layers_10_mlp_gate_up_proj_q_scale2, rms_norm21), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv399 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1070,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1071 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_10_mlp_down_proj_q_weight2, model_layers_10_mlp_down_proj_q_scale2, lv399), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1066_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1071, lv1065_1, model_layers_11_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1067_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1066_1[1]
            rms_norm22: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1066_1[0]
            lv1072 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_11_self_attn_qkv_proj_q_weight2, model_layers_11_self_attn_qkv_proj_q_scale2, rms_norm22), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape44 = R.call_tir(cls.reshape4, (lv1072,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape45 = R.call_tir(cls.reshape5, (reshape44,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv58 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape45), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape46 = R.call_tir(cls.reshape6, (lv58,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape47 = R.call_tir(cls.reshape7, (reshape46,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1073 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_11_self_attn_o_proj_q_weight2, model_layers_11_self_attn_o_proj_q_scale2, reshape47), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1068_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1073, lv1067_1, model_layers_11_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1069_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1068_1[1]
            rms_norm23: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1068_1[0]
            lv1074 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_11_mlp_gate_up_proj_q_weight2, model_layers_11_mlp_gate_up_proj_q_scale2, rms_norm23), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv400 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1074,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1075 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_11_mlp_down_proj_q_weight2, model_layers_11_mlp_down_proj_q_scale2, lv400), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1070_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1075, lv1069_1, model_layers_12_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1071_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1070_1[1]
            rms_norm24: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1070_1[0]
            lv1076 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_12_self_attn_qkv_proj_q_weight2, model_layers_12_self_attn_qkv_proj_q_scale2, rms_norm24), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape48 = R.call_tir(cls.reshape4, (lv1076,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape49 = R.call_tir(cls.reshape5, (reshape48,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv63 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape49), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape50 = R.call_tir(cls.reshape6, (lv63,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape51 = R.call_tir(cls.reshape7, (reshape50,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1077 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_12_self_attn_o_proj_q_weight2, model_layers_12_self_attn_o_proj_q_scale2, reshape51), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1072_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1077, lv1071_1, model_layers_12_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1073_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1072_1[1]
            rms_norm25: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1072_1[0]
            lv1078 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_12_mlp_gate_up_proj_q_weight2, model_layers_12_mlp_gate_up_proj_q_scale2, rms_norm25), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv401 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1078,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1079 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_12_mlp_down_proj_q_weight2, model_layers_12_mlp_down_proj_q_scale2, lv401), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1074_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1079, lv1073_1, model_layers_13_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1075_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1074_1[1]
            rms_norm26: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1074_1[0]
            lv1080 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_13_self_attn_qkv_proj_q_weight2, model_layers_13_self_attn_qkv_proj_q_scale2, rms_norm26), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape52 = R.call_tir(cls.reshape4, (lv1080,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape53 = R.call_tir(cls.reshape5, (reshape52,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv68 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape53), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape54 = R.call_tir(cls.reshape6, (lv68,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape55 = R.call_tir(cls.reshape7, (reshape54,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1081 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_13_self_attn_o_proj_q_weight2, model_layers_13_self_attn_o_proj_q_scale2, reshape55), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1076_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1081, lv1075_1, model_layers_13_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1077_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1076_1[1]
            rms_norm27: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1076_1[0]
            lv1082 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_13_mlp_gate_up_proj_q_weight2, model_layers_13_mlp_gate_up_proj_q_scale2, rms_norm27), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv402 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1082,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1083 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_13_mlp_down_proj_q_weight2, model_layers_13_mlp_down_proj_q_scale2, lv402), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1078_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1083, lv1077_1, model_layers_14_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1079_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1078_1[1]
            rms_norm28: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1078_1[0]
            lv1084 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_14_self_attn_qkv_proj_q_weight2, model_layers_14_self_attn_qkv_proj_q_scale2, rms_norm28), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape56 = R.call_tir(cls.reshape4, (lv1084,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape57 = R.call_tir(cls.reshape5, (reshape56,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv73 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape57), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape58 = R.call_tir(cls.reshape6, (lv73,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape59 = R.call_tir(cls.reshape7, (reshape58,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1085 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_14_self_attn_o_proj_q_weight2, model_layers_14_self_attn_o_proj_q_scale2, reshape59), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1080_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1085, lv1079_1, model_layers_14_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1081_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1080_1[1]
            rms_norm29: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1080_1[0]
            lv1086 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_14_mlp_gate_up_proj_q_weight2, model_layers_14_mlp_gate_up_proj_q_scale2, rms_norm29), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv403 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1086,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1087 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_14_mlp_down_proj_q_weight2, model_layers_14_mlp_down_proj_q_scale2, lv403), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1082_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1087, lv1081_1, model_layers_15_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1083_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1082_1[1]
            rms_norm30: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1082_1[0]
            lv1088 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_15_self_attn_qkv_proj_q_weight2, model_layers_15_self_attn_qkv_proj_q_scale2, rms_norm30), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape60 = R.call_tir(cls.reshape4, (lv1088,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape61 = R.call_tir(cls.reshape5, (reshape60,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv78 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape61), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape62 = R.call_tir(cls.reshape6, (lv78,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape63 = R.call_tir(cls.reshape7, (reshape62,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1089 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_15_self_attn_o_proj_q_weight2, model_layers_15_self_attn_o_proj_q_scale2, reshape63), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1084_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1089, lv1083_1, model_layers_15_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1085_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1084_1[1]
            rms_norm31: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1084_1[0]
            lv1090 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_15_mlp_gate_up_proj_q_weight2, model_layers_15_mlp_gate_up_proj_q_scale2, rms_norm31), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv404 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1090,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1091 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_15_mlp_down_proj_q_weight2, model_layers_15_mlp_down_proj_q_scale2, lv404), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1086_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1091, lv1085_1, model_layers_16_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1087_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1086_1[1]
            rms_norm32: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1086_1[0]
            lv1092 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_16_self_attn_qkv_proj_q_weight2, model_layers_16_self_attn_qkv_proj_q_scale2, rms_norm32), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape64 = R.call_tir(cls.reshape4, (lv1092,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape65 = R.call_tir(cls.reshape5, (reshape64,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv83 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape65), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape66 = R.call_tir(cls.reshape6, (lv83,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape67 = R.call_tir(cls.reshape7, (reshape66,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1093 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_16_self_attn_o_proj_q_weight2, model_layers_16_self_attn_o_proj_q_scale2, reshape67), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1088_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1093, lv1087_1, model_layers_16_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1089_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1088_1[1]
            rms_norm33: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1088_1[0]
            lv1094 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_16_mlp_gate_up_proj_q_weight2, model_layers_16_mlp_gate_up_proj_q_scale2, rms_norm33), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv405 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1094,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1095 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_16_mlp_down_proj_q_weight2, model_layers_16_mlp_down_proj_q_scale2, lv405), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1090_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1095, lv1089_1, model_layers_17_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1091_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1090_1[1]
            rms_norm34: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1090_1[0]
            lv1096 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_17_self_attn_qkv_proj_q_weight2, model_layers_17_self_attn_qkv_proj_q_scale2, rms_norm34), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape68 = R.call_tir(cls.reshape4, (lv1096,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape69 = R.call_tir(cls.reshape5, (reshape68,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv88 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape69), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape70 = R.call_tir(cls.reshape6, (lv88,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape71 = R.call_tir(cls.reshape7, (reshape70,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1097 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_17_self_attn_o_proj_q_weight2, model_layers_17_self_attn_o_proj_q_scale2, reshape71), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1092_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1097, lv1091_1, model_layers_17_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1093_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1092_1[1]
            rms_norm35: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1092_1[0]
            lv1098 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_17_mlp_gate_up_proj_q_weight2, model_layers_17_mlp_gate_up_proj_q_scale2, rms_norm35), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv406 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1098,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1099 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_17_mlp_down_proj_q_weight2, model_layers_17_mlp_down_proj_q_scale2, lv406), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1094_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1099, lv1093_1, model_layers_18_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1095_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1094_1[1]
            rms_norm36: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1094_1[0]
            lv1100 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_18_self_attn_qkv_proj_q_weight2, model_layers_18_self_attn_qkv_proj_q_scale2, rms_norm36), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape72 = R.call_tir(cls.reshape4, (lv1100,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape73 = R.call_tir(cls.reshape5, (reshape72,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv93 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape73), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape74 = R.call_tir(cls.reshape6, (lv93,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape75 = R.call_tir(cls.reshape7, (reshape74,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1101 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_18_self_attn_o_proj_q_weight2, model_layers_18_self_attn_o_proj_q_scale2, reshape75), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1096_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1101, lv1095_1, model_layers_18_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1097_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1096_1[1]
            rms_norm37: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1096_1[0]
            lv1102 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_18_mlp_gate_up_proj_q_weight2, model_layers_18_mlp_gate_up_proj_q_scale2, rms_norm37), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv407 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1102,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1103 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_18_mlp_down_proj_q_weight2, model_layers_18_mlp_down_proj_q_scale2, lv407), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1098_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1103, lv1097_1, model_layers_19_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1099_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1098_1[1]
            rms_norm38: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1098_1[0]
            lv1104 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_19_self_attn_qkv_proj_q_weight2, model_layers_19_self_attn_qkv_proj_q_scale2, rms_norm38), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape76 = R.call_tir(cls.reshape4, (lv1104,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape77 = R.call_tir(cls.reshape5, (reshape76,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv98 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape77), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape78 = R.call_tir(cls.reshape6, (lv98,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape79 = R.call_tir(cls.reshape7, (reshape78,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1105 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_19_self_attn_o_proj_q_weight2, model_layers_19_self_attn_o_proj_q_scale2, reshape79), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1100_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1105, lv1099_1, model_layers_19_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1101_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1100_1[1]
            rms_norm39: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1100_1[0]
            lv1106 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_19_mlp_gate_up_proj_q_weight2, model_layers_19_mlp_gate_up_proj_q_scale2, rms_norm39), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv408 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1106,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1107 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_19_mlp_down_proj_q_weight2, model_layers_19_mlp_down_proj_q_scale2, lv408), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1102_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1107, lv1101_1, model_layers_20_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1103_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1102_1[1]
            rms_norm40: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1102_1[0]
            lv1108 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_20_self_attn_qkv_proj_q_weight2, model_layers_20_self_attn_qkv_proj_q_scale2, rms_norm40), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape80 = R.call_tir(cls.reshape4, (lv1108,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape81 = R.call_tir(cls.reshape5, (reshape80,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv103 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape81), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape82 = R.call_tir(cls.reshape6, (lv103,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape83 = R.call_tir(cls.reshape7, (reshape82,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1109 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_20_self_attn_o_proj_q_weight2, model_layers_20_self_attn_o_proj_q_scale2, reshape83), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1104_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1109, lv1103_1, model_layers_20_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1105_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1104_1[1]
            rms_norm41: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1104_1[0]
            lv1110 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_20_mlp_gate_up_proj_q_weight2, model_layers_20_mlp_gate_up_proj_q_scale2, rms_norm41), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv409 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1110,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1111 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_20_mlp_down_proj_q_weight2, model_layers_20_mlp_down_proj_q_scale2, lv409), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1106_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1111, lv1105_1, model_layers_21_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1107_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1106_1[1]
            rms_norm42: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1106_1[0]
            lv1112 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_21_self_attn_qkv_proj_q_weight2, model_layers_21_self_attn_qkv_proj_q_scale2, rms_norm42), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape84 = R.call_tir(cls.reshape4, (lv1112,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape85 = R.call_tir(cls.reshape5, (reshape84,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv108 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape85), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape86 = R.call_tir(cls.reshape6, (lv108,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape87 = R.call_tir(cls.reshape7, (reshape86,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1113 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_21_self_attn_o_proj_q_weight2, model_layers_21_self_attn_o_proj_q_scale2, reshape87), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1108_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1113, lv1107_1, model_layers_21_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1109_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1108_1[1]
            rms_norm43: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1108_1[0]
            lv1114 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_21_mlp_gate_up_proj_q_weight2, model_layers_21_mlp_gate_up_proj_q_scale2, rms_norm43), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv410 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1114,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1115 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_21_mlp_down_proj_q_weight2, model_layers_21_mlp_down_proj_q_scale2, lv410), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1110_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1115, lv1109_1, model_layers_22_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1111_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1110_1[1]
            rms_norm44: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1110_1[0]
            lv1116 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_22_self_attn_qkv_proj_q_weight2, model_layers_22_self_attn_qkv_proj_q_scale2, rms_norm44), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape88 = R.call_tir(cls.reshape4, (lv1116,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape89 = R.call_tir(cls.reshape5, (reshape88,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv113 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape89), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape90 = R.call_tir(cls.reshape6, (lv113,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape91 = R.call_tir(cls.reshape7, (reshape90,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1117 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_22_self_attn_o_proj_q_weight2, model_layers_22_self_attn_o_proj_q_scale2, reshape91), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1112_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1117, lv1111_1, model_layers_22_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1113_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1112_1[1]
            rms_norm45: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1112_1[0]
            lv1118 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_22_mlp_gate_up_proj_q_weight2, model_layers_22_mlp_gate_up_proj_q_scale2, rms_norm45), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv411 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1118,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1119 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_22_mlp_down_proj_q_weight2, model_layers_22_mlp_down_proj_q_scale2, lv411), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1114_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1119, lv1113_1, model_layers_23_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1115_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1114_1[1]
            rms_norm46: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1114_1[0]
            lv1120 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_23_self_attn_qkv_proj_q_weight2, model_layers_23_self_attn_qkv_proj_q_scale2, rms_norm46), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape92 = R.call_tir(cls.reshape4, (lv1120,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape93 = R.call_tir(cls.reshape5, (reshape92,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv118 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape93), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape94 = R.call_tir(cls.reshape6, (lv118,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape95 = R.call_tir(cls.reshape7, (reshape94,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1121 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_23_self_attn_o_proj_q_weight2, model_layers_23_self_attn_o_proj_q_scale2, reshape95), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1116_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1121, lv1115_1, model_layers_23_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1117_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1116_1[1]
            rms_norm47: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1116_1[0]
            lv1122 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_23_mlp_gate_up_proj_q_weight2, model_layers_23_mlp_gate_up_proj_q_scale2, rms_norm47), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv412 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1122,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1123 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_23_mlp_down_proj_q_weight2, model_layers_23_mlp_down_proj_q_scale2, lv412), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1118_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1123, lv1117_1, model_layers_24_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1119_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1118_1[1]
            rms_norm48: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1118_1[0]
            lv1124 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_24_self_attn_qkv_proj_q_weight2, model_layers_24_self_attn_qkv_proj_q_scale2, rms_norm48), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape96 = R.call_tir(cls.reshape4, (lv1124,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape97 = R.call_tir(cls.reshape5, (reshape96,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv123 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape97), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape98 = R.call_tir(cls.reshape6, (lv123,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape99 = R.call_tir(cls.reshape7, (reshape98,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1125 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_24_self_attn_o_proj_q_weight2, model_layers_24_self_attn_o_proj_q_scale2, reshape99), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1120_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1125, lv1119_1, model_layers_24_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1121_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1120_1[1]
            rms_norm49: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1120_1[0]
            lv1126 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_24_mlp_gate_up_proj_q_weight2, model_layers_24_mlp_gate_up_proj_q_scale2, rms_norm49), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv413 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1126,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1127 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_24_mlp_down_proj_q_weight2, model_layers_24_mlp_down_proj_q_scale2, lv413), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1122_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1127, lv1121_1, model_layers_25_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1123_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1122_1[1]
            rms_norm50: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1122_1[0]
            lv1128 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_25_self_attn_qkv_proj_q_weight2, model_layers_25_self_attn_qkv_proj_q_scale2, rms_norm50), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape100 = R.call_tir(cls.reshape4, (lv1128,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape101 = R.call_tir(cls.reshape5, (reshape100,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv128 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape101), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape102 = R.call_tir(cls.reshape6, (lv128,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape103 = R.call_tir(cls.reshape7, (reshape102,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1129 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_25_self_attn_o_proj_q_weight2, model_layers_25_self_attn_o_proj_q_scale2, reshape103), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1124_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1129, lv1123_1, model_layers_25_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1125_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1124_1[1]
            rms_norm51: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1124_1[0]
            lv1130 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_25_mlp_gate_up_proj_q_weight2, model_layers_25_mlp_gate_up_proj_q_scale2, rms_norm51), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv414 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1130,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1131 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_25_mlp_down_proj_q_weight2, model_layers_25_mlp_down_proj_q_scale2, lv414), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1126_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1131, lv1125_1, model_layers_26_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1127_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1126_1[1]
            rms_norm52: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1126_1[0]
            lv1132 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_26_self_attn_qkv_proj_q_weight2, model_layers_26_self_attn_qkv_proj_q_scale2, rms_norm52), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape104 = R.call_tir(cls.reshape4, (lv1132,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape105 = R.call_tir(cls.reshape5, (reshape104,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv133 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape105), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape106 = R.call_tir(cls.reshape6, (lv133,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape107 = R.call_tir(cls.reshape7, (reshape106,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1133 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_26_self_attn_o_proj_q_weight2, model_layers_26_self_attn_o_proj_q_scale2, reshape107), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1128_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1133, lv1127_1, model_layers_26_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1129_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1128_1[1]
            rms_norm53: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1128_1[0]
            lv1134 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_26_mlp_gate_up_proj_q_weight2, model_layers_26_mlp_gate_up_proj_q_scale2, rms_norm53), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv415 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1134,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1135 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_26_mlp_down_proj_q_weight2, model_layers_26_mlp_down_proj_q_scale2, lv415), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1130_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1135, lv1129_1, model_layers_27_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1131_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1130_1[1]
            rms_norm54: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1130_1[0]
            lv1136 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_27_self_attn_qkv_proj_q_weight2, model_layers_27_self_attn_qkv_proj_q_scale2, rms_norm54), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape108 = R.call_tir(cls.reshape4, (lv1136,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape109 = R.call_tir(cls.reshape5, (reshape108,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv138 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape109), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape110 = R.call_tir(cls.reshape6, (lv138,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape111 = R.call_tir(cls.reshape7, (reshape110,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1137 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_27_self_attn_o_proj_q_weight2, model_layers_27_self_attn_o_proj_q_scale2, reshape111), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1132_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1137, lv1131_1, model_layers_27_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1133_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1132_1[1]
            rms_norm55: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1132_1[0]
            lv1138 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_27_mlp_gate_up_proj_q_weight2, model_layers_27_mlp_gate_up_proj_q_scale2, rms_norm55), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv416 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1138,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1139 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_27_mlp_down_proj_q_weight2, model_layers_27_mlp_down_proj_q_scale2, lv416), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1134_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1139, lv1133_1, model_layers_28_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1135_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1134_1[1]
            rms_norm56: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1134_1[0]
            lv1140 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_28_self_attn_qkv_proj_q_weight2, model_layers_28_self_attn_qkv_proj_q_scale2, rms_norm56), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape112 = R.call_tir(cls.reshape4, (lv1140,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape113 = R.call_tir(cls.reshape5, (reshape112,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv143 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape113), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape114 = R.call_tir(cls.reshape6, (lv143,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape115 = R.call_tir(cls.reshape7, (reshape114,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1141 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_28_self_attn_o_proj_q_weight2, model_layers_28_self_attn_o_proj_q_scale2, reshape115), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1136_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1141, lv1135_1, model_layers_28_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1137_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1136_1[1]
            rms_norm57: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1136_1[0]
            lv1142 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_28_mlp_gate_up_proj_q_weight2, model_layers_28_mlp_gate_up_proj_q_scale2, rms_norm57), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv417 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1142,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1143 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_28_mlp_down_proj_q_weight2, model_layers_28_mlp_down_proj_q_scale2, lv417), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1138_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1143, lv1137_1, model_layers_29_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1139_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1138_1[1]
            rms_norm58: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1138_1[0]
            lv1144 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_29_self_attn_qkv_proj_q_weight2, model_layers_29_self_attn_qkv_proj_q_scale2, rms_norm58), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape116 = R.call_tir(cls.reshape4, (lv1144,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape117 = R.call_tir(cls.reshape5, (reshape116,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv148 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape117), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape118 = R.call_tir(cls.reshape6, (lv148,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape119 = R.call_tir(cls.reshape7, (reshape118,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1145 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_29_self_attn_o_proj_q_weight2, model_layers_29_self_attn_o_proj_q_scale2, reshape119), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1140_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1145, lv1139_1, model_layers_29_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1141_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1140_1[1]
            rms_norm59: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1140_1[0]
            lv1146 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_29_mlp_gate_up_proj_q_weight2, model_layers_29_mlp_gate_up_proj_q_scale2, rms_norm59), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv418 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1146,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1147 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_29_mlp_down_proj_q_weight2, model_layers_29_mlp_down_proj_q_scale2, lv418), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1142_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1147, lv1141_1, model_layers_30_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1143_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1142_1[1]
            rms_norm60: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1142_1[0]
            lv1148 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_30_self_attn_qkv_proj_q_weight2, model_layers_30_self_attn_qkv_proj_q_scale2, rms_norm60), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape120 = R.call_tir(cls.reshape4, (lv1148,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape121 = R.call_tir(cls.reshape5, (reshape120,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv153 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape121), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape122 = R.call_tir(cls.reshape6, (lv153,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape123 = R.call_tir(cls.reshape7, (reshape122,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1149 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_30_self_attn_o_proj_q_weight2, model_layers_30_self_attn_o_proj_q_scale2, reshape123), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1144_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1149, lv1143_1, model_layers_30_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1145_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1144_1[1]
            rms_norm61: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1144_1[0]
            lv1150 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_30_mlp_gate_up_proj_q_weight2, model_layers_30_mlp_gate_up_proj_q_scale2, rms_norm61), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv419 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1150,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1151 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_30_mlp_down_proj_q_weight2, model_layers_30_mlp_down_proj_q_scale2, lv419), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1146_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1151, lv1145_1, model_layers_31_input_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1147_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1146_1[1]
            rms_norm62: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1146_1[0]
            lv1152 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_31_self_attn_qkv_proj_q_weight2, model_layers_31_self_attn_qkv_proj_q_scale2, rms_norm62), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape124 = R.call_tir(cls.reshape4, (lv1152,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape125 = R.call_tir(cls.reshape5, (reshape124,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv158 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape125), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape126 = R.call_tir(cls.reshape6, (lv158,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape127 = R.call_tir(cls.reshape7, (reshape126,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1153 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_31_self_attn_o_proj_q_weight2, model_layers_31_self_attn_o_proj_q_scale2, reshape127), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1148_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1153, lv1147_1, model_layers_31_post_attention_layernorm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1149_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1148_1[1]
            rms_norm63: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1148_1[0]
            lv1154 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_31_mlp_gate_up_proj_q_weight2, model_layers_31_mlp_gate_up_proj_q_scale2, rms_norm63), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv420 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1154,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1155 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_31_mlp_down_proj_q_weight2, model_layers_31_mlp_down_proj_q_scale2, lv420), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1150_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1155, lv1149_1, model_norm_weight2), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm64: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1150_1[0]
            lv162 = R.call_tir(cls.index, (rms_norm64,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv1156 = R.call_tir(cls.fused_dequantize_fused_NT_matmul14_cast2, (lm_head_q_weight2, lm_head_q_scale2, lv162), out_sinfo=R.Tensor((1, 1, vocab_size), dtype="float32"))
            gv3: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = lv1156, paged_kv_cache
            R.output(gv3)
        return gv3

    @R.function
    def prefill_to_last_hidden_states(input_embed: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_layers_0_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[322]
            rms_norm130 = R.call_tir(cls.rms_norm1, (input_embed, model_layers_0_input_layernorm_weight4), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1157 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_0_self_attn_qkv_proj_q_weight4, model_layers_0_self_attn_qkv_proj_q_scale4, rms_norm130), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape256 = R.call_tir(cls.reshape4, (lv1157,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape257 = R.call_tir(cls.reshape5, (reshape256,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv326 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape257), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape258 = R.call_tir(cls.reshape6, (lv326,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape259 = R.call_tir(cls.reshape7, (reshape258,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1158 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_0_self_attn_o_proj_q_weight4, model_layers_0_self_attn_o_proj_q_scale4, reshape259), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1152 = R.call_tir(cls.fuse_add_norm_prefill, (lv1158, input_embed, model_layers_0_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1153: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1152[1]
            rms_norm131: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1152[0]
            lv1159 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_0_mlp_gate_up_proj_q_weight4, model_layers_0_mlp_gate_up_proj_q_scale4, rms_norm131), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv422 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1159,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1160 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_0_mlp_down_proj_q_weight4, model_layers_0_mlp_down_proj_q_scale4, lv422), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1154 = R.call_tir(cls.fuse_add_norm_prefill, (lv1160, lv1153, model_layers_1_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1155: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1154[1]
            rms_norm132: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1154[0]
            lv1161 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_1_self_attn_qkv_proj_q_weight4, model_layers_1_self_attn_qkv_proj_q_scale4, rms_norm132), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape260 = R.call_tir(cls.reshape4, (lv1161,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape261 = R.call_tir(cls.reshape5, (reshape260,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv331 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape261), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape262 = R.call_tir(cls.reshape6, (lv331,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape263 = R.call_tir(cls.reshape7, (reshape262,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1162 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_1_self_attn_o_proj_q_weight4, model_layers_1_self_attn_o_proj_q_scale4, reshape263), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1156 = R.call_tir(cls.fuse_add_norm_prefill, (lv1162, lv1155, model_layers_1_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1157_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1156[1]
            rms_norm133: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1156[0]
            lv1163 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_1_mlp_gate_up_proj_q_weight4, model_layers_1_mlp_gate_up_proj_q_scale4, rms_norm133), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv423 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1163,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1164 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_1_mlp_down_proj_q_weight4, model_layers_1_mlp_down_proj_q_scale4, lv423), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1158_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1164, lv1157_1, model_layers_2_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1159_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1158_1[1]
            rms_norm134: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1158_1[0]
            lv1165 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_2_self_attn_qkv_proj_q_weight4, model_layers_2_self_attn_qkv_proj_q_scale4, rms_norm134), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape264 = R.call_tir(cls.reshape4, (lv1165,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape265 = R.call_tir(cls.reshape5, (reshape264,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv336 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape265), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape266 = R.call_tir(cls.reshape6, (lv336,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape267 = R.call_tir(cls.reshape7, (reshape266,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1166 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_2_self_attn_o_proj_q_weight4, model_layers_2_self_attn_o_proj_q_scale4, reshape267), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1160_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1166, lv1159_1, model_layers_2_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1161_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1160_1[1]
            rms_norm135: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1160_1[0]
            lv1167 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_2_mlp_gate_up_proj_q_weight4, model_layers_2_mlp_gate_up_proj_q_scale4, rms_norm135), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv424 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1167,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1168 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_2_mlp_down_proj_q_weight4, model_layers_2_mlp_down_proj_q_scale4, lv424), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1162_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1168, lv1161_1, model_layers_3_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1163_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1162_1[1]
            rms_norm136: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1162_1[0]
            lv1169 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_3_self_attn_qkv_proj_q_weight4, model_layers_3_self_attn_qkv_proj_q_scale4, rms_norm136), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape268 = R.call_tir(cls.reshape4, (lv1169,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape269 = R.call_tir(cls.reshape5, (reshape268,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv341 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape269), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape270 = R.call_tir(cls.reshape6, (lv341,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape271 = R.call_tir(cls.reshape7, (reshape270,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1170 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_3_self_attn_o_proj_q_weight4, model_layers_3_self_attn_o_proj_q_scale4, reshape271), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1164_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1170, lv1163_1, model_layers_3_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1165_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1164_1[1]
            rms_norm137: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1164_1[0]
            lv1171 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_3_mlp_gate_up_proj_q_weight4, model_layers_3_mlp_gate_up_proj_q_scale4, rms_norm137), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv425 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1171,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1172 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_3_mlp_down_proj_q_weight4, model_layers_3_mlp_down_proj_q_scale4, lv425), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1166_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1172, lv1165_1, model_layers_4_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1167_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1166_1[1]
            rms_norm138: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1166_1[0]
            lv1173 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_4_self_attn_qkv_proj_q_weight4, model_layers_4_self_attn_qkv_proj_q_scale4, rms_norm138), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape272 = R.call_tir(cls.reshape4, (lv1173,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape273 = R.call_tir(cls.reshape5, (reshape272,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv346 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape273), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape274 = R.call_tir(cls.reshape6, (lv346,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape275 = R.call_tir(cls.reshape7, (reshape274,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1174 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_4_self_attn_o_proj_q_weight4, model_layers_4_self_attn_o_proj_q_scale4, reshape275), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1168_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1174, lv1167_1, model_layers_4_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1169_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1168_1[1]
            rms_norm139: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1168_1[0]
            lv1175 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_4_mlp_gate_up_proj_q_weight4, model_layers_4_mlp_gate_up_proj_q_scale4, rms_norm139), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv426 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1175,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1176 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_4_mlp_down_proj_q_weight4, model_layers_4_mlp_down_proj_q_scale4, lv426), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1170_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1176, lv1169_1, model_layers_5_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1171_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1170_1[1]
            rms_norm140: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1170_1[0]
            lv1177 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_5_self_attn_qkv_proj_q_weight4, model_layers_5_self_attn_qkv_proj_q_scale4, rms_norm140), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape276 = R.call_tir(cls.reshape4, (lv1177,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape277 = R.call_tir(cls.reshape5, (reshape276,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv351 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape277), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape278 = R.call_tir(cls.reshape6, (lv351,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape279 = R.call_tir(cls.reshape7, (reshape278,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1178 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_5_self_attn_o_proj_q_weight4, model_layers_5_self_attn_o_proj_q_scale4, reshape279), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1172_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1178, lv1171_1, model_layers_5_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1173_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1172_1[1]
            rms_norm141: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1172_1[0]
            lv1179 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_5_mlp_gate_up_proj_q_weight4, model_layers_5_mlp_gate_up_proj_q_scale4, rms_norm141), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv427 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1179,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1180 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_5_mlp_down_proj_q_weight4, model_layers_5_mlp_down_proj_q_scale4, lv427), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1174_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1180, lv1173_1, model_layers_6_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1175_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1174_1[1]
            rms_norm142: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1174_1[0]
            lv1181 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_6_self_attn_qkv_proj_q_weight4, model_layers_6_self_attn_qkv_proj_q_scale4, rms_norm142), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape280 = R.call_tir(cls.reshape4, (lv1181,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape281 = R.call_tir(cls.reshape5, (reshape280,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv356 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape281), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape282 = R.call_tir(cls.reshape6, (lv356,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape283 = R.call_tir(cls.reshape7, (reshape282,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1182 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_6_self_attn_o_proj_q_weight4, model_layers_6_self_attn_o_proj_q_scale4, reshape283), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1176_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1182, lv1175_1, model_layers_6_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1177_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1176_1[1]
            rms_norm143: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1176_1[0]
            lv1183 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_6_mlp_gate_up_proj_q_weight4, model_layers_6_mlp_gate_up_proj_q_scale4, rms_norm143), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv428 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1183,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1184 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_6_mlp_down_proj_q_weight4, model_layers_6_mlp_down_proj_q_scale4, lv428), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1178_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1184, lv1177_1, model_layers_7_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1179_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1178_1[1]
            rms_norm144: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1178_1[0]
            lv1185 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_7_self_attn_qkv_proj_q_weight4, model_layers_7_self_attn_qkv_proj_q_scale4, rms_norm144), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape284 = R.call_tir(cls.reshape4, (lv1185,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape285 = R.call_tir(cls.reshape5, (reshape284,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv361 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape285), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape286 = R.call_tir(cls.reshape6, (lv361,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape287 = R.call_tir(cls.reshape7, (reshape286,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1186 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_7_self_attn_o_proj_q_weight4, model_layers_7_self_attn_o_proj_q_scale4, reshape287), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1180_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1186, lv1179_1, model_layers_7_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1181_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1180_1[1]
            rms_norm145: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1180_1[0]
            lv1187 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_7_mlp_gate_up_proj_q_weight4, model_layers_7_mlp_gate_up_proj_q_scale4, rms_norm145), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv429 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1187,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1188 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_7_mlp_down_proj_q_weight4, model_layers_7_mlp_down_proj_q_scale4, lv429), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1182_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1188, lv1181_1, model_layers_8_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1183_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1182_1[1]
            rms_norm146: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1182_1[0]
            lv1189 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_8_self_attn_qkv_proj_q_weight4, model_layers_8_self_attn_qkv_proj_q_scale4, rms_norm146), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape288 = R.call_tir(cls.reshape4, (lv1189,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape289 = R.call_tir(cls.reshape5, (reshape288,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv366 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape289), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape290 = R.call_tir(cls.reshape6, (lv366,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape291 = R.call_tir(cls.reshape7, (reshape290,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1190 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_8_self_attn_o_proj_q_weight4, model_layers_8_self_attn_o_proj_q_scale4, reshape291), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1184_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1190, lv1183_1, model_layers_8_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1185_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1184_1[1]
            rms_norm147: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1184_1[0]
            lv1191 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_8_mlp_gate_up_proj_q_weight4, model_layers_8_mlp_gate_up_proj_q_scale4, rms_norm147), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv430 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1191,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1192 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_8_mlp_down_proj_q_weight4, model_layers_8_mlp_down_proj_q_scale4, lv430), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1186_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1192, lv1185_1, model_layers_9_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1187_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1186_1[1]
            rms_norm148: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1186_1[0]
            lv1193 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_9_self_attn_qkv_proj_q_weight4, model_layers_9_self_attn_qkv_proj_q_scale4, rms_norm148), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape292 = R.call_tir(cls.reshape4, (lv1193,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape293 = R.call_tir(cls.reshape5, (reshape292,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv371 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape293), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape294 = R.call_tir(cls.reshape6, (lv371,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape295 = R.call_tir(cls.reshape7, (reshape294,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1194 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_9_self_attn_o_proj_q_weight4, model_layers_9_self_attn_o_proj_q_scale4, reshape295), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1188_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1194, lv1187_1, model_layers_9_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1189_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1188_1[1]
            rms_norm149: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1188_1[0]
            lv1195 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_9_mlp_gate_up_proj_q_weight4, model_layers_9_mlp_gate_up_proj_q_scale4, rms_norm149), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv431 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1195,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1196 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_9_mlp_down_proj_q_weight4, model_layers_9_mlp_down_proj_q_scale4, lv431), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1190_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1196, lv1189_1, model_layers_10_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1191_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1190_1[1]
            rms_norm150: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1190_1[0]
            lv1197 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_10_self_attn_qkv_proj_q_weight4, model_layers_10_self_attn_qkv_proj_q_scale4, rms_norm150), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape296 = R.call_tir(cls.reshape4, (lv1197,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape297 = R.call_tir(cls.reshape5, (reshape296,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv376 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape297), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape298 = R.call_tir(cls.reshape6, (lv376,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape299 = R.call_tir(cls.reshape7, (reshape298,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1198 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_10_self_attn_o_proj_q_weight4, model_layers_10_self_attn_o_proj_q_scale4, reshape299), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1192_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1198, lv1191_1, model_layers_10_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1193_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1192_1[1]
            rms_norm151: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1192_1[0]
            lv1199 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_10_mlp_gate_up_proj_q_weight4, model_layers_10_mlp_gate_up_proj_q_scale4, rms_norm151), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv432 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1199,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1200 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_10_mlp_down_proj_q_weight4, model_layers_10_mlp_down_proj_q_scale4, lv432), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1194_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1200, lv1193_1, model_layers_11_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1195_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1194_1[1]
            rms_norm152: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1194_1[0]
            lv1201 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_11_self_attn_qkv_proj_q_weight4, model_layers_11_self_attn_qkv_proj_q_scale4, rms_norm152), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape300 = R.call_tir(cls.reshape4, (lv1201,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape301 = R.call_tir(cls.reshape5, (reshape300,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv381 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape301), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape302 = R.call_tir(cls.reshape6, (lv381,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape303 = R.call_tir(cls.reshape7, (reshape302,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1202 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_11_self_attn_o_proj_q_weight4, model_layers_11_self_attn_o_proj_q_scale4, reshape303), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1196_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1202, lv1195_1, model_layers_11_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1197_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1196_1[1]
            rms_norm153: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1196_1[0]
            lv1203 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_11_mlp_gate_up_proj_q_weight4, model_layers_11_mlp_gate_up_proj_q_scale4, rms_norm153), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv433 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1203,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1204 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_11_mlp_down_proj_q_weight4, model_layers_11_mlp_down_proj_q_scale4, lv433), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1198_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1204, lv1197_1, model_layers_12_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1199_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1198_1[1]
            rms_norm154: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1198_1[0]
            lv1205 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_12_self_attn_qkv_proj_q_weight4, model_layers_12_self_attn_qkv_proj_q_scale4, rms_norm154), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape304 = R.call_tir(cls.reshape4, (lv1205,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape305 = R.call_tir(cls.reshape5, (reshape304,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv386 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape305), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape306 = R.call_tir(cls.reshape6, (lv386,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape307 = R.call_tir(cls.reshape7, (reshape306,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1206 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_12_self_attn_o_proj_q_weight4, model_layers_12_self_attn_o_proj_q_scale4, reshape307), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1200_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1206, lv1199_1, model_layers_12_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1201_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1200_1[1]
            rms_norm155: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1200_1[0]
            lv1207 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_12_mlp_gate_up_proj_q_weight4, model_layers_12_mlp_gate_up_proj_q_scale4, rms_norm155), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv434 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1207,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1208 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_12_mlp_down_proj_q_weight4, model_layers_12_mlp_down_proj_q_scale4, lv434), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1202_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1208, lv1201_1, model_layers_13_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1203_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1202_1[1]
            rms_norm156: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1202_1[0]
            lv1209 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_13_self_attn_qkv_proj_q_weight4, model_layers_13_self_attn_qkv_proj_q_scale4, rms_norm156), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape308 = R.call_tir(cls.reshape4, (lv1209,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape309 = R.call_tir(cls.reshape5, (reshape308,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv391 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape309), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape310 = R.call_tir(cls.reshape6, (lv391,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape311 = R.call_tir(cls.reshape7, (reshape310,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1210 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_13_self_attn_o_proj_q_weight4, model_layers_13_self_attn_o_proj_q_scale4, reshape311), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1204_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1210, lv1203_1, model_layers_13_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1205_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1204_1[1]
            rms_norm157: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1204_1[0]
            lv1211 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_13_mlp_gate_up_proj_q_weight4, model_layers_13_mlp_gate_up_proj_q_scale4, rms_norm157), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv435 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1211,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1212 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_13_mlp_down_proj_q_weight4, model_layers_13_mlp_down_proj_q_scale4, lv435), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1206_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1212, lv1205_1, model_layers_14_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1207_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1206_1[1]
            rms_norm158: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1206_1[0]
            lv1213 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_14_self_attn_qkv_proj_q_weight4, model_layers_14_self_attn_qkv_proj_q_scale4, rms_norm158), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape312 = R.call_tir(cls.reshape4, (lv1213,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape313 = R.call_tir(cls.reshape5, (reshape312,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv396 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape313), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape314 = R.call_tir(cls.reshape6, (lv396,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape315 = R.call_tir(cls.reshape7, (reshape314,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1214 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_14_self_attn_o_proj_q_weight4, model_layers_14_self_attn_o_proj_q_scale4, reshape315), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1208_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1214, lv1207_1, model_layers_14_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1209_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1208_1[1]
            rms_norm159: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1208_1[0]
            lv1215 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_14_mlp_gate_up_proj_q_weight4, model_layers_14_mlp_gate_up_proj_q_scale4, rms_norm159), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv436 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1215,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1216 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_14_mlp_down_proj_q_weight4, model_layers_14_mlp_down_proj_q_scale4, lv436), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1210_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1216, lv1209_1, model_layers_15_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1211_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1210_1[1]
            rms_norm160: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1210_1[0]
            lv1217 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_15_self_attn_qkv_proj_q_weight4, model_layers_15_self_attn_qkv_proj_q_scale4, rms_norm160), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape316 = R.call_tir(cls.reshape4, (lv1217,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape317 = R.call_tir(cls.reshape5, (reshape316,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv401 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape317), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape318 = R.call_tir(cls.reshape6, (lv401,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape319 = R.call_tir(cls.reshape7, (reshape318,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1218 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_15_self_attn_o_proj_q_weight4, model_layers_15_self_attn_o_proj_q_scale4, reshape319), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1212_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1218, lv1211_1, model_layers_15_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1213_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1212_1[1]
            rms_norm161: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1212_1[0]
            lv1219 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_15_mlp_gate_up_proj_q_weight4, model_layers_15_mlp_gate_up_proj_q_scale4, rms_norm161), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv437 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1219,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1220 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_15_mlp_down_proj_q_weight4, model_layers_15_mlp_down_proj_q_scale4, lv437), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1214_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1220, lv1213_1, model_layers_16_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1215_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1214_1[1]
            rms_norm162: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1214_1[0]
            lv1221 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_16_self_attn_qkv_proj_q_weight4, model_layers_16_self_attn_qkv_proj_q_scale4, rms_norm162), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape320 = R.call_tir(cls.reshape4, (lv1221,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape321 = R.call_tir(cls.reshape5, (reshape320,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv406 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape321), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape322 = R.call_tir(cls.reshape6, (lv406,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape323 = R.call_tir(cls.reshape7, (reshape322,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1222 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_16_self_attn_o_proj_q_weight4, model_layers_16_self_attn_o_proj_q_scale4, reshape323), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1216_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1222, lv1215_1, model_layers_16_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1217_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1216_1[1]
            rms_norm163: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1216_1[0]
            lv1223 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_16_mlp_gate_up_proj_q_weight4, model_layers_16_mlp_gate_up_proj_q_scale4, rms_norm163), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv438 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1223,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1224 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_16_mlp_down_proj_q_weight4, model_layers_16_mlp_down_proj_q_scale4, lv438), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1218_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1224, lv1217_1, model_layers_17_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1219_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1218_1[1]
            rms_norm164: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1218_1[0]
            lv1225 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_17_self_attn_qkv_proj_q_weight4, model_layers_17_self_attn_qkv_proj_q_scale4, rms_norm164), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape324 = R.call_tir(cls.reshape4, (lv1225,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape325 = R.call_tir(cls.reshape5, (reshape324,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv411 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape325), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape326 = R.call_tir(cls.reshape6, (lv411,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape327 = R.call_tir(cls.reshape7, (reshape326,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1226 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_17_self_attn_o_proj_q_weight4, model_layers_17_self_attn_o_proj_q_scale4, reshape327), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1220_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1226, lv1219_1, model_layers_17_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1221_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1220_1[1]
            rms_norm165: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1220_1[0]
            lv1227 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_17_mlp_gate_up_proj_q_weight4, model_layers_17_mlp_gate_up_proj_q_scale4, rms_norm165), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv439 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1227,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1228 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_17_mlp_down_proj_q_weight4, model_layers_17_mlp_down_proj_q_scale4, lv439), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1222_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1228, lv1221_1, model_layers_18_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1223_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1222_1[1]
            rms_norm166: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1222_1[0]
            lv1229 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_18_self_attn_qkv_proj_q_weight4, model_layers_18_self_attn_qkv_proj_q_scale4, rms_norm166), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape328 = R.call_tir(cls.reshape4, (lv1229,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape329 = R.call_tir(cls.reshape5, (reshape328,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv416 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape329), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape330 = R.call_tir(cls.reshape6, (lv416,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape331 = R.call_tir(cls.reshape7, (reshape330,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1230 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_18_self_attn_o_proj_q_weight4, model_layers_18_self_attn_o_proj_q_scale4, reshape331), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1224_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1230, lv1223_1, model_layers_18_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1225_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1224_1[1]
            rms_norm167: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1224_1[0]
            lv1231 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_18_mlp_gate_up_proj_q_weight4, model_layers_18_mlp_gate_up_proj_q_scale4, rms_norm167), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv440 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1231,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1232 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_18_mlp_down_proj_q_weight4, model_layers_18_mlp_down_proj_q_scale4, lv440), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1226_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1232, lv1225_1, model_layers_19_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1227_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1226_1[1]
            rms_norm168: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1226_1[0]
            lv1233 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_19_self_attn_qkv_proj_q_weight4, model_layers_19_self_attn_qkv_proj_q_scale4, rms_norm168), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape332 = R.call_tir(cls.reshape4, (lv1233,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape333 = R.call_tir(cls.reshape5, (reshape332,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv421 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape333), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape334 = R.call_tir(cls.reshape6, (lv421,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape335 = R.call_tir(cls.reshape7, (reshape334,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1234 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_19_self_attn_o_proj_q_weight4, model_layers_19_self_attn_o_proj_q_scale4, reshape335), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1228_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1234, lv1227_1, model_layers_19_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1229_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1228_1[1]
            rms_norm169: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1228_1[0]
            lv1235 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_19_mlp_gate_up_proj_q_weight4, model_layers_19_mlp_gate_up_proj_q_scale4, rms_norm169), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv441 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1235,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1236 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_19_mlp_down_proj_q_weight4, model_layers_19_mlp_down_proj_q_scale4, lv441), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1230_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1236, lv1229_1, model_layers_20_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1231_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1230_1[1]
            rms_norm170: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1230_1[0]
            lv1237 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_20_self_attn_qkv_proj_q_weight4, model_layers_20_self_attn_qkv_proj_q_scale4, rms_norm170), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape336 = R.call_tir(cls.reshape4, (lv1237,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape337 = R.call_tir(cls.reshape5, (reshape336,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv426_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape337), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape338 = R.call_tir(cls.reshape6, (lv426_1,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape339 = R.call_tir(cls.reshape7, (reshape338,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1238 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_20_self_attn_o_proj_q_weight4, model_layers_20_self_attn_o_proj_q_scale4, reshape339), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1232_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1238, lv1231_1, model_layers_20_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1233_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1232_1[1]
            rms_norm171: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1232_1[0]
            lv1239 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_20_mlp_gate_up_proj_q_weight4, model_layers_20_mlp_gate_up_proj_q_scale4, rms_norm171), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv442 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1239,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1240 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_20_mlp_down_proj_q_weight4, model_layers_20_mlp_down_proj_q_scale4, lv442), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1234_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1240, lv1233_1, model_layers_21_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1235_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1234_1[1]
            rms_norm172: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1234_1[0]
            lv1241 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_21_self_attn_qkv_proj_q_weight4, model_layers_21_self_attn_qkv_proj_q_scale4, rms_norm172), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape340 = R.call_tir(cls.reshape4, (lv1241,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape341 = R.call_tir(cls.reshape5, (reshape340,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv431_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape341), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape342 = R.call_tir(cls.reshape6, (lv431_1,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape343 = R.call_tir(cls.reshape7, (reshape342,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1242 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_21_self_attn_o_proj_q_weight4, model_layers_21_self_attn_o_proj_q_scale4, reshape343), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1236_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1242, lv1235_1, model_layers_21_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1237_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1236_1[1]
            rms_norm173: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1236_1[0]
            lv1243 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_21_mlp_gate_up_proj_q_weight4, model_layers_21_mlp_gate_up_proj_q_scale4, rms_norm173), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv443 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1243,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1244 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_21_mlp_down_proj_q_weight4, model_layers_21_mlp_down_proj_q_scale4, lv443), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1238_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1244, lv1237_1, model_layers_22_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1239_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1238_1[1]
            rms_norm174: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1238_1[0]
            lv1245 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_22_self_attn_qkv_proj_q_weight4, model_layers_22_self_attn_qkv_proj_q_scale4, rms_norm174), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape344 = R.call_tir(cls.reshape4, (lv1245,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape345 = R.call_tir(cls.reshape5, (reshape344,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv436_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape345), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape346 = R.call_tir(cls.reshape6, (lv436_1,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape347 = R.call_tir(cls.reshape7, (reshape346,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1246 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_22_self_attn_o_proj_q_weight4, model_layers_22_self_attn_o_proj_q_scale4, reshape347), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1240_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1246, lv1239_1, model_layers_22_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1241_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1240_1[1]
            rms_norm175: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1240_1[0]
            lv1247 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_22_mlp_gate_up_proj_q_weight4, model_layers_22_mlp_gate_up_proj_q_scale4, rms_norm175), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv444 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1247,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1248 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_22_mlp_down_proj_q_weight4, model_layers_22_mlp_down_proj_q_scale4, lv444), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1242_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1248, lv1241_1, model_layers_23_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1243_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1242_1[1]
            rms_norm176: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1242_1[0]
            lv1249 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_23_self_attn_qkv_proj_q_weight4, model_layers_23_self_attn_qkv_proj_q_scale4, rms_norm176), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape348 = R.call_tir(cls.reshape4, (lv1249,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape349 = R.call_tir(cls.reshape5, (reshape348,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv441_1 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape349), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape350 = R.call_tir(cls.reshape6, (lv441_1,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape351 = R.call_tir(cls.reshape7, (reshape350,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1250 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_23_self_attn_o_proj_q_weight4, model_layers_23_self_attn_o_proj_q_scale4, reshape351), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1244_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1250, lv1243_1, model_layers_23_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1245_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1244_1[1]
            rms_norm177: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1244_1[0]
            lv1251 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_23_mlp_gate_up_proj_q_weight4, model_layers_23_mlp_gate_up_proj_q_scale4, rms_norm177), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv445 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1251,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1252 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_23_mlp_down_proj_q_weight4, model_layers_23_mlp_down_proj_q_scale4, lv445), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1246_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1252, lv1245_1, model_layers_24_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1247_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1246_1[1]
            rms_norm178: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1246_1[0]
            lv1253 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_24_self_attn_qkv_proj_q_weight4, model_layers_24_self_attn_qkv_proj_q_scale4, rms_norm178), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape352 = R.call_tir(cls.reshape4, (lv1253,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape353 = R.call_tir(cls.reshape5, (reshape352,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv446 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape353), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape354 = R.call_tir(cls.reshape6, (lv446,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape355 = R.call_tir(cls.reshape7, (reshape354,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1254 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_24_self_attn_o_proj_q_weight4, model_layers_24_self_attn_o_proj_q_scale4, reshape355), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1248_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1254, lv1247_1, model_layers_24_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1249_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1248_1[1]
            rms_norm179: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1248_1[0]
            lv1255 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_24_mlp_gate_up_proj_q_weight4, model_layers_24_mlp_gate_up_proj_q_scale4, rms_norm179), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv446_1 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1255,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1256 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_24_mlp_down_proj_q_weight4, model_layers_24_mlp_down_proj_q_scale4, lv446_1), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1250_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1256, lv1249_1, model_layers_25_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1251_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1250_1[1]
            rms_norm180: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1250_1[0]
            lv1257 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_25_self_attn_qkv_proj_q_weight4, model_layers_25_self_attn_qkv_proj_q_scale4, rms_norm180), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape356 = R.call_tir(cls.reshape4, (lv1257,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape357 = R.call_tir(cls.reshape5, (reshape356,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv451 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape357), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape358 = R.call_tir(cls.reshape6, (lv451,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape359 = R.call_tir(cls.reshape7, (reshape358,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1258 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_25_self_attn_o_proj_q_weight4, model_layers_25_self_attn_o_proj_q_scale4, reshape359), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1252_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1258, lv1251_1, model_layers_25_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1253_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1252_1[1]
            rms_norm181: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1252_1[0]
            lv1259 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_25_mlp_gate_up_proj_q_weight4, model_layers_25_mlp_gate_up_proj_q_scale4, rms_norm181), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv447 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1259,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1260 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_25_mlp_down_proj_q_weight4, model_layers_25_mlp_down_proj_q_scale4, lv447), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1254_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1260, lv1253_1, model_layers_26_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1255_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1254_1[1]
            rms_norm182: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1254_1[0]
            lv1261 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_26_self_attn_qkv_proj_q_weight4, model_layers_26_self_attn_qkv_proj_q_scale4, rms_norm182), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape360 = R.call_tir(cls.reshape4, (lv1261,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape361 = R.call_tir(cls.reshape5, (reshape360,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv456 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape361), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape362 = R.call_tir(cls.reshape6, (lv456,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape363 = R.call_tir(cls.reshape7, (reshape362,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1262 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_26_self_attn_o_proj_q_weight4, model_layers_26_self_attn_o_proj_q_scale4, reshape363), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1256_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1262, lv1255_1, model_layers_26_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1257_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1256_1[1]
            rms_norm183: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1256_1[0]
            lv1263 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_26_mlp_gate_up_proj_q_weight4, model_layers_26_mlp_gate_up_proj_q_scale4, rms_norm183), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv448 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1263,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1264 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_26_mlp_down_proj_q_weight4, model_layers_26_mlp_down_proj_q_scale4, lv448), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1258_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1264, lv1257_1, model_layers_27_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1259_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1258_1[1]
            rms_norm184: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1258_1[0]
            lv1265 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_27_self_attn_qkv_proj_q_weight4, model_layers_27_self_attn_qkv_proj_q_scale4, rms_norm184), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape364 = R.call_tir(cls.reshape4, (lv1265,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape365 = R.call_tir(cls.reshape5, (reshape364,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv461 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape365), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape366 = R.call_tir(cls.reshape6, (lv461,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape367 = R.call_tir(cls.reshape7, (reshape366,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1266 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_27_self_attn_o_proj_q_weight4, model_layers_27_self_attn_o_proj_q_scale4, reshape367), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1260_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1266, lv1259_1, model_layers_27_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1261_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1260_1[1]
            rms_norm185: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1260_1[0]
            lv1267 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_27_mlp_gate_up_proj_q_weight4, model_layers_27_mlp_gate_up_proj_q_scale4, rms_norm185), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv449 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1267,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1268 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_27_mlp_down_proj_q_weight4, model_layers_27_mlp_down_proj_q_scale4, lv449), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1262_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1268, lv1261_1, model_layers_28_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1263_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1262_1[1]
            rms_norm186: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1262_1[0]
            lv1269 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_28_self_attn_qkv_proj_q_weight4, model_layers_28_self_attn_qkv_proj_q_scale4, rms_norm186), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape368 = R.call_tir(cls.reshape4, (lv1269,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape369 = R.call_tir(cls.reshape5, (reshape368,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv466 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape369), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape370 = R.call_tir(cls.reshape6, (lv466,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape371 = R.call_tir(cls.reshape7, (reshape370,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1270 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_28_self_attn_o_proj_q_weight4, model_layers_28_self_attn_o_proj_q_scale4, reshape371), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1264_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1270, lv1263_1, model_layers_28_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1265_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1264_1[1]
            rms_norm187: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1264_1[0]
            lv1271 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_28_mlp_gate_up_proj_q_weight4, model_layers_28_mlp_gate_up_proj_q_scale4, rms_norm187), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv450 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1271,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1272 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_28_mlp_down_proj_q_weight4, model_layers_28_mlp_down_proj_q_scale4, lv450), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1266_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1272, lv1265_1, model_layers_29_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1267_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1266_1[1]
            rms_norm188: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1266_1[0]
            lv1273 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_29_self_attn_qkv_proj_q_weight4, model_layers_29_self_attn_qkv_proj_q_scale4, rms_norm188), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape372 = R.call_tir(cls.reshape4, (lv1273,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape373 = R.call_tir(cls.reshape5, (reshape372,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv471 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape373), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape374 = R.call_tir(cls.reshape6, (lv471,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape375 = R.call_tir(cls.reshape7, (reshape374,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1274 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_29_self_attn_o_proj_q_weight4, model_layers_29_self_attn_o_proj_q_scale4, reshape375), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1268_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1274, lv1267_1, model_layers_29_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1269_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1268_1[1]
            rms_norm189: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1268_1[0]
            lv1275 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_29_mlp_gate_up_proj_q_weight4, model_layers_29_mlp_gate_up_proj_q_scale4, rms_norm189), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv451_1 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1275,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1276 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_29_mlp_down_proj_q_weight4, model_layers_29_mlp_down_proj_q_scale4, lv451_1), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1270_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1276, lv1269_1, model_layers_30_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1271_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1270_1[1]
            rms_norm190: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1270_1[0]
            lv1277 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_30_self_attn_qkv_proj_q_weight4, model_layers_30_self_attn_qkv_proj_q_scale4, rms_norm190), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape376 = R.call_tir(cls.reshape4, (lv1277,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape377 = R.call_tir(cls.reshape5, (reshape376,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv476 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape377), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape378 = R.call_tir(cls.reshape6, (lv476,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape379 = R.call_tir(cls.reshape7, (reshape378,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1278 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_30_self_attn_o_proj_q_weight4, model_layers_30_self_attn_o_proj_q_scale4, reshape379), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1272_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1278, lv1271_1, model_layers_30_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1273_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1272_1[1]
            rms_norm191: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1272_1[0]
            lv1279 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_30_mlp_gate_up_proj_q_weight4, model_layers_30_mlp_gate_up_proj_q_scale4, rms_norm191), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv452 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1279,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1280 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_30_mlp_down_proj_q_weight4, model_layers_30_mlp_down_proj_q_scale4, lv452), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1274_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1280, lv1273_1, model_layers_31_input_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1275_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1274_1[1]
            rms_norm192: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1274_1[0]
            lv1281 = R.call_tir(cls.fused_dequantize1_NT_matmul5, (model_layers_31_self_attn_qkv_proj_q_weight4, model_layers_31_self_attn_qkv_proj_q_scale4, rms_norm192), out_sinfo=R.Tensor((1, seq_len, 6144), dtype="float16"))
            reshape380 = R.call_tir(cls.reshape4, (lv1281,), out_sinfo=R.Tensor((1, seq_len, 48, 128), dtype="float16"))
            reshape381 = R.call_tir(cls.reshape5, (reshape380,), out_sinfo=R.Tensor((seq_len, 48, 128), dtype="float16"))
            lv481 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape381), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape382 = R.call_tir(cls.reshape6, (lv481,), out_sinfo=R.Tensor((1, seq_len, 32, 128), dtype="float16"))
            reshape383 = R.call_tir(cls.reshape7, (reshape382,), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1282 = R.call_tir(cls.fused_dequantize2_NT_matmul6, (model_layers_31_self_attn_o_proj_q_weight4, model_layers_31_self_attn_o_proj_q_scale4, reshape383), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1276_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1282, lv1275_1, model_layers_31_post_attention_layernorm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            lv1277_1: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1276_1[1]
            rms_norm193: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1276_1[0]
            lv1283 = R.call_tir(cls.fused_dequantize3_NT_matmul7, (model_layers_31_mlp_gate_up_proj_q_weight4, model_layers_31_mlp_gate_up_proj_q_scale4, rms_norm193), out_sinfo=R.Tensor((1, seq_len, 28672), dtype="float16"))
            lv453 = R.call_tir(cls.fused_split1_silu1_multiply1, (lv1283,), out_sinfo=R.Tensor((1, seq_len, 14336), dtype="float16"))
            lv1284 = R.call_tir(cls.fused_dequantize4_NT_matmul8, (model_layers_31_mlp_down_proj_q_weight4, model_layers_31_mlp_down_proj_q_scale4, lv453), out_sinfo=R.Tensor((1, seq_len, 4096), dtype="float16"))
            lv1278_1 = R.call_tir(cls.fuse_add_norm_prefill, (lv1284, lv1277_1, model_norm_weight4), out_sinfo=[R.Tensor((1, seq_len, 4096), dtype="float16"), R.Tensor((1, seq_len, 4096), dtype="float16")])
            rms_norm194: R.Tensor((1, seq_len, 4096), dtype="float16") = lv1278_1[0]
            gv5: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm194, paged_kv_cache
            R.output(gv5)
        return gv5

    @R.function
    def renormalize_by_top_p(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), top_p: R.Tensor(("batch_size",), dtype="float32"), init_pivots: R.Tensor(("batch_size", 3), dtype="float32")) -> R.Tensor(("batch_size", "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv6 = R.call_tir(cls.top_p_pivot_cutoff, (probs, top_p, init_pivots), out_sinfo=[R.Tensor((batch_size,), dtype="float32"), R.Tensor((batch_size,), dtype="float32")])
            lv7: R.Tensor((batch_size,), dtype="float32") = lv6[0]
            lv8: R.Tensor((batch_size,), dtype="float32") = lv6[1]
            gv5 = R.call_tir(cls.top_p_renorm_after_cutoff, (probs, lv7, lv8), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            R.output(gv5)
        return gv5

    @R.function
    def sample_with_top_p(sorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), top_p: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            uniform_samples1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            sample_indices2: R.Tensor((batch_size, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", top_p, R.shape([batch_size, 1]), sinfo_args=(R.Tensor((batch_size, 1), dtype="float32"),))
            lv3 = R.call_tir(cls.full, R.tuple(), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"), tir_vars=R.shape([vocab_size]))
            lv: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", sorted_probs, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv1 = R.call_tir(cls.gpu_2d_continuous_cumsum, (lv,), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            cumsum: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", lv1, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv4 = R.call_tir(cls.get_renorm_prob, (cumsum, sample_indices2, lv3), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv5 = R.call_tir(cls.get_index_from_sorted, (cumsum, sorted_indices, lv4, uniform_samples1, sample_indices1), out_sinfo=R.Tensor((num_samples, 1), dtype="int32"))
            gv2: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", lv5, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            R.output(gv2)
        return gv2

    @R.function
    def sampler_take_probs(unsorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), sampling_result: R.Tensor(("num_samples",), dtype="int32"), lobprob_offsets: R.Tensor(("num_positions",), dtype="int32")) -> R.Tuple(R.Tensor(("num_samples",), dtype="float32"), R.Tensor(("num_positions",), dtype="float32"), R.Tensor(("num_positions",), dtype="int32")):
        num_samples = T.int64(is_size_var=True)
        num_positions = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv3 = R.call_tir(cls.sampler_take_probs_tir, (unsorted_probs, sorted_indices, sample_indices, sampling_result, lobprob_offsets), out_sinfo=[R.Tensor((num_samples,), dtype="float32"), R.Tensor((num_positions,), dtype="float32"), R.Tensor((num_positions,), dtype="int32")])
            R.output(gv3)
        return gv3

    @R.function
    def sampler_verify_draft_tokens(draft_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), draft_tokens: R.Tensor(("num_nodes",), dtype="int32"), model_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), token_tree_first_child: R.Tensor(("num_nodes",), dtype="int32"), token_tree_next_sibling: R.Tensor(("num_nodes",), dtype="int32"), uniform_samples: R.Tensor(("num_nodes",), dtype="float32"), token_tree_parent_ptr: R.Tensor(("nbatch",), dtype="int32")) -> R.Tuple(R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), R.Tensor(("nbatch",), dtype="int32")):
        num_nodes = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        nbatch = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv4: R.Tuple(R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")) = R.call_tir_inplace(cls.batch_verify_on_gpu_single_kernel, (draft_probs, draft_tokens, model_probs, token_tree_first_child, token_tree_next_sibling, uniform_samples, token_tree_parent_ptr), out_sinfo=[R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")], inplace_indices=[2, 6])
            R.output(gv4)
        return gv4

    @R.function
    def scatter_hidden_states(src: R.Tensor(("batch_size", "n"), dtype="float16"), indices: R.Tensor(("batch_size",), dtype="int32"), dst: R.Tensor(("m", "n"), dtype="float16")) -> R.Tensor(("m", "n"), dtype="float16"):
        m = T.int64(is_size_var=True)
        n = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        cls = Module
        with R.dataflow():
            gv1: R.Tensor((m, n), dtype="float16") = R.call_tir_inplace(cls._scatter_hidden_states, (src, indices, dst), out_sinfo=R.Tensor((m, n), dtype="float16"), inplace_indices=[2])
            R.output(gv1)
        return gv1

    @R.function
    def softmax_with_temperature(logits: R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), temperature: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            lv: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", logits, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv1 = R.call_tir(cls.chunk_lse, (lv, temperature), out_sinfo=[R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32"), R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32")])
            lv2: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[0]
            lv3: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[1]
            lv4 = R.call_tir(cls.softmax_with_chunked_sum, (lv, temperature, lv2, lv3), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv: R.Tensor((batch_size, 1, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", lv4, R.shape([batch_size, 1, vocab_size]), sinfo_args=(R.Tensor((batch_size, 1, vocab_size), dtype="float32"),))
            R.output(gv)
        return gv