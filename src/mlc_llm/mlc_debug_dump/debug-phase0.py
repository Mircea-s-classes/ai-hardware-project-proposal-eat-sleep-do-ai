# from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func
    def _gather_hidden_states(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        m, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (m, n), "float16")
        batch_size = T.int32(is_size_var=True)
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        dst = T.match_buffer(var_dst, (batch_size, n), "float16")
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("gather_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[indices[vb], vj], indices[vb])
                T.writes(dst[vb, vj])
                dst[vb, vj] = src[indices[vb], vj]

    @T.prim_func
    def _scatter_hidden_states(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (batch_size, n), "float16")
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        m = T.int32(is_size_var=True)
        dst = T.match_buffer(var_dst, (m, n), "float16")
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("scatter_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[vb, vj], indices[vb])
                T.writes(dst[indices[vb], vj])
                dst[indices[vb], vj] = src[vb, vj]

    @T.prim_func
    def apply_bitmask_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_bitmask: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        bitmask = T.match_buffer(var_bitmask, (batch_size, (vocab_size + 31) // 32), "int32")
        # with T.block("root"):
        for fused_s_v_0 in T.thread_binding((num_seq * vocab_size + 1023) // 1024, thread="blockIdx.x"):
            for fused_s_v_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vs = T.axis.spatial(num_seq, (fused_s_v_0 * 1024 + fused_s_v_1) // vocab_size)
                    vv = T.axis.spatial(vocab_size, (fused_s_v_0 * 1024 + fused_s_v_1) % vocab_size)
                    T.where(fused_s_v_0 * 1024 + fused_s_v_1 < num_seq * vocab_size)
                    T.reads(bitmask[seq_ids[vs], vv // 32], seq_ids[vs], logits[seq_ids[vs], vv])
                    T.writes(logits[seq_ids[vs], vv])
                    logits[seq_ids[vs], vv] = T.if_then_else(T.bitwise_and(T.shift_right(bitmask[seq_ids[vs], vv // 32], vv % 32), 1) == 1, logits[seq_ids[vs], vv], T.float32(-340282346638528859811704183484516925440.0))

    @T.prim_func
    def apply_logit_bias_inplace(var_logits: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_logit_bias: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        logit_bias = T.match_buffer(var_logit_bias, (num_token,))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[pos2seq_id[vp], token_ids[vp]], pos2seq_id[vp], token_ids[vp], logit_bias[vp])
                    T.writes(logits[pos2seq_id[vp], token_ids[vp]])
                    logits[pos2seq_id[vp], token_ids[vp]] = logits[pos2seq_id[vp], token_ids[vp]] + logit_bias[vp]

    @T.prim_func
    def apply_penalty_inplace(var_logits: T.handle, var_seq_ids: T.handle, var_pos2seq_id: T.handle, var_token_ids: T.handle, var_token_cnt: T.handle, var_penalties: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": T.bool(True), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        logits = T.match_buffer(var_logits, (batch_size, vocab_size))
        num_seq = T.int32(is_size_var=True)
        seq_ids = T.match_buffer(var_seq_ids, (num_seq,), "int32")
        num_token = T.int32(is_size_var=True)
        pos2seq_id = T.match_buffer(var_pos2seq_id, (num_token,), "int32")
        token_ids = T.match_buffer(var_token_ids, (num_token,), "int32")
        token_cnt = T.match_buffer(var_token_cnt, (num_token,), "int32")
        penalties = T.match_buffer(var_penalties, (num_seq, 3))
        # with T.block("root"):
        for p0 in T.thread_binding((num_token + 1023) // 1024, thread="blockIdx.x"):
            for p1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("block"):
                    vp = T.axis.spatial(num_token, p0 * 1024 + p1)
                    T.where(p0 * 1024 + p1 < num_token)
                    T.reads(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]], seq_ids[pos2seq_id[vp]], pos2seq_id[vp], token_ids[vp], penalties[pos2seq_id[vp], 0:3], token_cnt[vp])
                    T.writes(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] - (penalties[pos2seq_id[vp], 0] + T.Cast("float32", token_cnt[vp]) * penalties[pos2seq_id[vp], 1])
                    logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] = T.if_then_else(logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] < T.float32(0.0), logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] * penalties[pos2seq_id[vp], 2], logits[seq_ids[pos2seq_id[vp]], token_ids[vp]] / penalties[pos2seq_id[vp], 2])

    @T.prim_func
    def batch_decode_paged_kv(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 128), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 8, 16, 128), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (B,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 128), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.12751743082459868)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(8, thread="blockIdx.y"):
                for ty in T.thread_binding(4, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        for tz in T.thread_binding(4, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[bx], q_rope_position[bx], Q[bx, fused_by_bz // 8 * 4 + fused_by_bz % 8 * 4 + ty, tx * 4 - 64:tx * 4 - 64 + 132])
                                T.writes(output[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((4, 4, 128), scope="shared")
                                md_allreduce = T.alloc_buffer((4, 4, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((4,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 8
                                bz: T.int32 = fused_by_bz // 8
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec + 64] * T.float16(-1.0), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 15) // 16):
                                    tile_start_s: T.int32 = tz * 4 + ty
                                    tile_start_g: T.int32 = (iterator * 4 + tz) * 4 + ty
                                    for j in range(1):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = row_g
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, pages[page_no, 0, by, page_offset, tx * 4 + vec + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 4 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 4 + tz) * 4 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(4):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 4 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(4):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by * 4 + bz * 4 + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by * 4 + bz * 4 + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_decode_paged_kv_sliding_window(_0: T.int32, Q_handle: T.handle, pages_handle: T.handle, page_table_indptr_handle: T.handle, page_table_values_handle: T.handle, var_length_info: T.handle, k_rope_pos_offset_handle: T.handle, q_rope_position_handle: T.handle, output_handle: T.handle, lse_handle: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        B = T.int32(is_size_var=True)
        Q = T.match_buffer(Q_handle, (B, 32, 128), "float16")
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(pages_handle, (max_num_pages, 2, 8, 16, 128), "float16")
        page_table_indptr = T.match_buffer(page_table_indptr_handle, (B + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_table_values = T.match_buffer(page_table_values_handle, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, B), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(k_rope_pos_offset_handle, (B,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(q_rope_position_handle, (B,), "int32", offset_factor=1)
        output = T.match_buffer(output_handle, (B, 32, 128), "float16")
        lse = T.match_buffer(lse_handle, (B, 32))
        # with T.block("root"):
        sm_scale: T.float32 = T.float32(0.12751743082459868)
        for bx in T.thread_binding(B, thread="blockIdx.x"):
            for fused_by_bz in T.thread_binding(8, thread="blockIdx.y"):
                for ty in T.thread_binding(4, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        for tz in T.thread_binding(4, thread="threadIdx.z"):
                            with T.block("attn"):
                                T.reads(page_table_indptr[bx:bx + 2], length_info[0:3, bx], q_rope_position[bx], Q[bx, fused_by_bz // 8 * 4 + fused_by_bz % 8 * 4 + ty, tx * 4 - 64:tx * 4 - 64 + 132])
                                T.writes(output[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty, tx * 4:tx * 4 + 4], lse[bx, fused_by_bz % 8 * 4 + fused_by_bz // 8 * 4 + ty])
                                Q_local = T.alloc_buffer((4,), "float16", scope="local")
                                kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                                K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                                O_allreduce = T.alloc_buffer((4, 4, 128), scope="shared")
                                md_allreduce = T.alloc_buffer((4, 4, 2), scope="shared")
                                S_reduce_local = T.alloc_buffer((1,), scope="local")
                                t0 = T.alloc_buffer((1,), scope="local")
                                S_local = T.alloc_buffer((4,), scope="local")
                                QK_local = T.alloc_buffer((4,), scope="local")
                                V_local = T.alloc_buffer((4,), "float16", scope="local")
                                m_prev = T.alloc_buffer((1,), scope="local")
                                d_prev = T.alloc_buffer((1,), scope="local")
                                other_m = T.alloc_buffer((1,), scope="local")
                                other_d = T.alloc_buffer((1,), scope="local")
                                exp_mprev = T.alloc_buffer((1,), scope="local")
                                exp_otherm = T.alloc_buffer((1,), scope="local")
                                other_o = T.alloc_buffer((4,), scope="local")
                                st_m = T.alloc_buffer((1,), scope="local")
                                st_d = T.alloc_buffer((1,), scope="local")
                                O_local = T.alloc_buffer((4,), scope="local")
                                by: T.int32 = fused_by_bz % 8
                                bz: T.int32 = fused_by_bz // 8
                                batch_idx: T.int32 = bx
                                cur_page_indptr_begin: T.int32 = page_table_indptr[batch_idx]
                                cur_page_indptr_end: T.int32 = page_table_indptr[batch_idx + 1]
                                kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, batch_idx] - length_info[1, batch_idx] + length_info[2, batch_idx], 0)
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for vec in T.vectorized(4):
                                    freq = T.float32()
                                    Q_local[vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec + 64] * T.float16(-1.0), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", q_rope_position[batch_idx]) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), Q[bx, by * 4 + bz * 4 + ty, tx * 4 + vec])
                                for iterator in range((kv_chunk_len[0] + 15) // 16):
                                    tile_start_s: T.int32 = tz * 4 + ty
                                    tile_start_g: T.int32 = (iterator * 4 + tz) * 4 + ty
                                    for j in range(1):
                                        with T.block("KV_load"):
                                            T.reads()
                                            T.writes()
                                            row_g: T.int32 = tile_start_g + j
                                            if row_g < kv_chunk_len[0]:
                                                seq_offset: T.int32 = T.if_then_else(row_g < length_info[2, batch_idx], row_g, row_g - length_info[2, batch_idx] + length_info[1, batch_idx])
                                                page_no: T.int32 = page_table_values[cur_page_indptr_begin + seq_offset // 16]
                                                page_offset: T.int32 = seq_offset % 16
                                                for vec in T.vectorized(4):
                                                    freq = T.float32()
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, tx * 4 + vec]) + T.sin(freq) * T.Cast("float32", T.if_then_else(tx * 4 + vec < 64, pages[page_no, 0, by, page_offset, tx * 4 + vec + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, tx * 4 + vec - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[batch_idx] + row_g) * rope_scale / T.pow(rope_theta, T.Cast("float32", (tx * 4 + vec) * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, tx * 4 + vec])
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = pages[page_no, 1, by, page_offset, tx * 4 + vec]
                                            else:
                                                for vec in T.vectorized(4):
                                                    K_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                                    V_smem[tile_start_s + j, tx * 4 + vec] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    m_prev[0] = st_m[0]
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            QK_local[vec] = T.Cast("float32", Q_local[vec]) * T.Cast("float32", K_smem[tz * 4 + j, tx * 4 + vec]) * attn_score_scaling_factor * sm_scale
                                        S_reduce_local[0] = T.float32(0.0)
                                        for vec in T.unroll(4):
                                            S_reduce_local[0] = S_reduce_local[0] + QK_local[vec]
                                        with T.block("block_cross_thread"):
                                            T.reads(S_reduce_local[0])
                                            T.writes(t0[0])
                                            T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                            T.tvm_thread_allreduce(T.uint32(1), S_reduce_local[0], T.bool(True), t0[0], tx)
                                        S_local[j] = T.float32(-50000.0)
                                        if (iterator * 4 + tz) * 4 + j < kv_chunk_len[0]:
                                            S_local[j] = t0[0]
                                        st_m[0] = T.max(st_m[0], S_local[j])
                                    o_scale: T.float32 = T.exp2(m_prev[0] - st_m[0])
                                    st_d[0] = st_d[0] * o_scale
                                    for j in range(4):
                                        S_local[j] = T.exp2(S_local[j] - st_m[0])
                                        st_d[0] = st_d[0] + S_local[j]
                                    for j in T.vectorized(4):
                                        O_local[j] = O_local[j] * o_scale
                                    for j in range(4):
                                        for vec in T.vectorized(4):
                                            V_local[vec] = V_smem[tz * 4 + j, tx * 4 + vec]
                                        for vec in T.vectorized(4):
                                            O_local[vec] = O_local[vec] + T.Cast("float32", V_local[vec]) * S_local[j]
                                for vec in T.vectorized(4):
                                    O_allreduce[tz, ty, tx * 4 + vec] = O_local[vec]
                                md_allreduce[tz, ty, 0] = st_m[0]
                                md_allreduce[tz, ty, 1] = st_d[0]
                                T.tvm_storage_sync("shared")
                                st_m[0] = T.float32(-50000.0)
                                st_d[0] = T.float32(1.0)
                                for vec in T.vectorized(4):
                                    O_local[vec] = T.float32(0.0)
                                for j in range(4):
                                    m_prev[0] = st_m[0]
                                    d_prev[0] = st_d[0]
                                    other_m[0] = md_allreduce[j, ty, 0]
                                    other_d[0] = md_allreduce[j, ty, 1]
                                    for vec in T.vectorized(4):
                                        other_o[vec] = O_allreduce[j, ty, tx * 4 + vec]
                                    st_m[0] = T.max(st_m[0], other_m[0])
                                    st_d[0] = d_prev[0] * T.exp2(m_prev[0] - st_m[0]) + other_d[0] * T.exp2(other_m[0] - st_m[0])
                                    exp_mprev[0] = T.exp2(m_prev[0] - st_m[0])
                                    exp_otherm[0] = T.exp2(other_m[0] - st_m[0])
                                    for vec in T.vectorized(4):
                                        O_local[vec] = O_local[vec] * exp_mprev[0] + other_o[vec] * exp_otherm[0]
                                for vec in T.vectorized(4):
                                    O_local[vec] = O_local[vec] / st_d[0]
                                for vec in T.vectorized(4):
                                    output[batch_idx, by * 4 + bz * 4 + ty, tx * 4 + vec] = T.Cast("float16", O_local[vec])
                                lse[batch_idx, by * 4 + bz * 4 + ty] = st_m[0] + T.log2(st_d[0])

    @T.prim_func
    def batch_prefill_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, pages[page_no, 0, by, page_offset, j + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_paged_kv_sliding_window(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (3, batch_size), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[0, b_idx] - length_info[1, b_idx] + length_info[2, b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", pages[page_no, 0, by, page_offset, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, pages[page_no, 0, by, page_offset, j + 64] * T.float16(-1.0), pages[page_no, 0, by, page_offset, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), pages[page_no, 0, by, page_offset, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = T.if_then_else(cur_L < length_info[2, b_idx], cur_L, cur_L - length_info[2, b_idx] + length_info[1, b_idx])
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_prefill_ragged_kv(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_k_rope_pos_offset: T.handle, var_output: T.handle, var_lse: T.handle, causal: T.int32, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (kv_len, 8, 128), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[L_kv_base + cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, k[L_kv_base + cur_L, by, j + 64] * T.float16(-1.0), k[L_kv_base + cur_L, by, j - 64]))), where={freq: T.Cast("float32", k_rope_pos_offset[b_idx] + cur_L) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), k[L_kv_base + cur_L, by, j])
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                V_smem[i, j] = v[L_kv_base + cur_L, by, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if T.if_then_else(causal > 0, L_kv_start + j < kv_chunk_len[0] - (q_indptr[b_idx + 1] - q_indptr[b_idx]) + row_ + 1, L_kv_start + j < kv_chunk_len[0]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func
    def batch_tree_attn(var_q: T.handle, var_q_indptr: T.handle, var_k: T.handle, var_v: T.handle, var_kv_indptr: T.handle, var_q_rope_position: T.handle, var_mn_indptr: T.handle, var_mask: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, batch_size: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        qo_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (qo_len, 32, 128), "float16")
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        kv_len = T.int32(is_size_var=True)
        k = T.match_buffer(var_k, (kv_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (kv_len, 8, 128), "float16")
        kv_indptr = T.match_buffer(var_kv_indptr, (batch_size + 1,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (qo_len,), "int32", offset_factor=1)
        mn_indptr = T.match_buffer(var_mn_indptr, (batch_size + 1,), "int32", offset_factor=1)
        tree_size = T.int32(is_size_var=True)
        mask = T.match_buffer(var_mask, (tree_size, 2), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (qo_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (qo_len, 32))
        # with T.block("root"):
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    kv_chunk_len[0] = kv_indptr[b_idx + 1] - kv_indptr[b_idx]
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        L_kv_base: T.int32 = kv_indptr[b_idx]
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("KV_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_base + L_kv_start + i
                                                            if L_kv_start + i < kv_chunk_len[0]:
                                                                freq = T.float32()
                                                                K_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", k[cur_L, by, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, k[cur_L, by, j + 64] * T.float16(-1.0), k[cur_L, by, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), k[cur_L, by, j])
                                                                V_smem[i, j] = v[cur_L, by, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k_1 = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k_1], K_smem[j, k_1])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k_1]) * T.Cast("float32", K_smem[j, k_1]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], mn_indptr[b_idx:b_idx + 2], mask[T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + mn_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + mn_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) or mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 0] and mask[mn_indptr[b_idx] + (row_ + (mn_indptr[b_idx + 1] - mn_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < mask[mn_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (mn_indptr[b_idx + 1] - mn_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k_1 = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k_1], V_smem[k_1, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k_1] * T.Cast("float32", V_smem[k_1, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @T.prim_func(private=True)
    def batch_verify_on_gpu_single_kernel(var_draft_probs: T.handle, var_draft_tokens: T.handle, var_model_probs: T.handle, var_token_tree_first_child: T.handle, var_token_tree_next_sibling: T.handle, var_uniform_samples: T.handle, var_token_tree_parent_ptr: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        num_nodes, vocab_size = T.int32(is_size_var=True), T.int64(is_size_var=True)
        draft_probs = T.match_buffer(var_draft_probs, (num_nodes, vocab_size))
        draft_tokens = T.match_buffer(var_draft_tokens, (num_nodes,), "int32")
        model_probs = T.match_buffer(var_model_probs, (num_nodes, vocab_size))
        token_tree_first_child = T.match_buffer(var_token_tree_first_child, (num_nodes,), "int32")
        token_tree_next_sibling = T.match_buffer(var_token_tree_next_sibling, (num_nodes,), "int32")
        uniform_samples = T.match_buffer(var_uniform_samples, (num_nodes,))
        nbatch = T.int32(is_size_var=True)
        token_tree_parent_ptr = T.match_buffer(var_token_tree_parent_ptr, (nbatch,), "int32")
        # with T.block("root"):
        child_ptr = T.alloc_buffer((1,), "int32", scope="local")
        parent_ptr = T.alloc_buffer((1,), "int32", scope="local")
        child_token = T.alloc_buffer((1,), "int32", scope="local")
        done = T.alloc_buffer((1,), "bool", scope="local")
        psum = T.alloc_buffer((1,), scope="local")
        t0 = T.alloc_buffer((1,), scope="local")
        model_prob_local = T.alloc_buffer((1,), scope="local")
        draft_prob_local = T.alloc_buffer((1,), scope="local")
        p_child = T.alloc_buffer((1,), scope="local")
        q_child = T.alloc_buffer((1,), scope="local")
        uniform_sample = T.alloc_buffer((1,), scope="local")
        pred_shared = T.alloc_buffer((1,), "bool", scope="shared")
        pred_local = T.alloc_buffer((1,), "bool", scope="local")
        for _bx in T.thread_binding(nbatch, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(token_tree_parent_ptr[b], token_tree_first_child[T.min(parent_ptr[0], child_ptr[0]):T.min(parent_ptr[0], child_ptr[0]) + (T.max(parent_ptr[0], child_ptr[0]) + 1 - T.min(parent_ptr[0], child_ptr[0]))], parent_ptr[0], done[0], child_ptr[0], draft_tokens[child_ptr[0]], model_probs[parent_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], child_token[0], draft_probs[child_ptr[0], T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)):T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)) + (T.max(T.Cast("int64", child_token[0]), (vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) + T.Cast("int64", tx) - T.int64(1024)) + T.int64(1) - T.min(T.Cast("int64", child_token[0]), T.Cast("int64", tx)))], uniform_samples[child_ptr[0]], p_child[0], uniform_sample[0], q_child[0], pred_shared[0], pred_local[0], model_prob_local[0], draft_prob_local[0], psum[0], t0[0], token_tree_next_sibling[child_ptr[0]])
                    T.writes(parent_ptr[0], child_ptr[0], done[0], child_token[0], p_child[0], q_child[0], uniform_sample[0], pred_shared[0], pred_local[0], psum[0], model_prob_local[0], draft_prob_local[0], t0[0], model_probs[parent_ptr[0], T.Cast("int64", tx):T.Cast("int64", tx) + ((vocab_size + T.int64(1023)) // T.int64(1024) * T.int64(1024) - T.int64(1023))], token_tree_parent_ptr[b])
                    parent_ptr[0] = token_tree_parent_ptr[b]
                    child_ptr[0] = token_tree_first_child[parent_ptr[0]]
                    done[0] = T.bool(False)
                    while not done[0]:
                        T.tvm_storage_sync("shared")
                        if child_ptr[0] == -1:
                            done[0] = T.bool(True)
                            T.tvm_storage_sync("shared")
                        else:
                            if tx == 0:
                                child_token[0] = draft_tokens[child_ptr[0]]
                                p_child[0] = model_probs[parent_ptr[0], child_token[0]]
                                q_child[0] = draft_probs[child_ptr[0], child_token[0]]
                                uniform_sample[0] = uniform_samples[child_ptr[0]]
                                pred_shared[0] = p_child[0] >= uniform_sample[0] * q_child[0]
                            T.tvm_storage_sync("shared")
                            pred_local[0] = pred_shared[0]
                            if pred_local[0]:
                                parent_ptr[0] = child_ptr[0]
                                child_ptr[0] = token_tree_first_child[child_ptr[0]]
                            else:
                                psum[0] = T.float32(0.0)
                                for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                    if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                        model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                        model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                        psum[0] = psum[0] + model_prob_local[0]
                                with T.block("block_cross_thread"):
                                    T.reads(psum[0])
                                    T.writes(t0[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), psum[0], T.bool(True), t0[0], tx)
                                if t0[0] < T.float32(9.9999999999999995e-08):
                                    parent_ptr[0] = child_ptr[0]
                                    child_ptr[0] = token_tree_first_child[child_ptr[0]]
                                else:
                                    for i in range((vocab_size + T.int64(1023)) // T.int64(1024)):
                                        if i * T.int64(1024) + T.Cast("int64", tx) < vocab_size:
                                            model_prob_local[0] = model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            draft_prob_local[0] = draft_probs[child_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)]
                                            model_prob_local[0] = T.max(model_prob_local[0] - draft_prob_local[0], T.float32(0.0))
                                            model_probs[parent_ptr[0], i * T.int64(1024) + T.Cast("int64", tx)] = model_prob_local[0] / t0[0]
                                    child_ptr[0] = token_tree_next_sibling[child_ptr[0]]
                    if tx == 0:
                        token_tree_parent_ptr[b] = parent_ptr[0]

    @T.prim_func
    def chunk_lse(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        # with T.block("root"):
        A_pad = T.alloc_buffer((batch_size, num_chunks, T.int64(4096)))
        temp_max = T.alloc_buffer((batch_size, num_chunks))
        temp_sum = T.alloc_buffer((batch_size, num_chunks))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("pad"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2])
                T.writes(A_pad[v0, v1, v2])
                A_pad[v0, v1, v2] = T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), A[v0, v1 * T.int64(4096) + v2] / temperature[v0], A[v0, v1 * T.int64(4096) + v2]), T.float32(-340282346638528859811704183484516925440.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("max"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(A_pad[v0, v1, v2])
                T.writes(temp_max[v0, v1])
                with T.init():
                    temp_max[v0, v1] = T.float32(-340282346638528859811704183484516925440.0)
                temp_max[v0, v1] = T.max(temp_max[v0, v1], A_pad[v0, v1, v2])
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(4096)):
            with T.block("sum_exp"):
                v0, v1, v2 = T.axis.remap("SSR", [l0, l1, l2])
                T.reads(temperature[v0], A_pad[v0, v1, v2], temp_max[v0, v1])
                T.writes(temp_sum[v0, v1])
                with T.init():
                    temp_sum[v0, v1] = T.float32(0.0)
                temp_sum[v0, v1] = temp_sum[v0, v1] + T.if_then_else(v1 * T.int64(4096) + v2 < vocab_size, T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A_pad[v0, v1, v2] - temp_max[v0, v1]), T.Cast("float32", A_pad[v0, v1, v2] == temp_max[v0, v1])), T.float32(0.0))
        for l0, l1, l2 in T.grid(batch_size, num_chunks, T.int64(1)):
            with T.block("log"):
                v0, v1, v2 = T.axis.remap("SSS", [l0, l1, l2])
                T.reads(temperature[v0], temp_sum[v0, v1], temp_max[v0, v1])
                T.writes(chunked_sum[v0, v1], chunked_max[v0, v1])
                chunked_sum[v0, v1] = T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.log(temp_sum[v0, v1]), temp_sum[v0, v1])
                chunked_max[v0, v1] = temp_max[v0, v1]

    @T.prim_func
    def compact_kv_copy(var_pages: T.handle, var_copy_length_indptr: T.handle, var_copy_src_dst_pos: T.handle, batch_size: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        num_pages = T.int32()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, 16, 128), "float16")
        copy_length_indptr = T.match_buffer(var_copy_length_indptr, (batch_size + 1,), "int32", offset_factor=1)
        total_copy_length = T.int32()
        copy_src_dst_pos = T.match_buffer(var_copy_src_dst_pos, (2, total_copy_length), "int32", offset_factor=1)
        with T.block("root"):
            T.reads()
            T.writes()
            for bhd_o in T.thread_binding(batch_size, thread="blockIdx.x"):
                for bhd_i in T.thread_binding(1024, thread="threadIdx.x"):
                    b: T.int32 = (bhd_o * 1024 + bhd_i) // 1024
                    h: T.int32 = (bhd_o * 1024 + bhd_i) // 128 % 8
                    d: T.int32 = (bhd_o * 1024 + bhd_i) % 128
                    if bhd_o * 1024 + bhd_i < batch_size * 8 * 128:
                        for i in range(copy_length_indptr[b + 1] - copy_length_indptr[b]):
                            src_pos: T.int32 = copy_src_dst_pos[0, copy_length_indptr[b] + i]
                            dst_pos: T.int32 = copy_src_dst_pos[1, copy_length_indptr[b] + i]
                            pages[dst_pos // 16, 0, h, dst_pos % 16, d] = pages[src_pos // 16, 0, h, src_pos % 16, d]
                            pages[dst_pos // 16, 1, h, dst_pos % 16, d] = pages[src_pos // 16, 1, h, src_pos % 16, d]

    @T.prim_func
    def copy_single_page(var_pages: T.handle, src_page_id: T.int64, tgt_page_id: T.int64, copy_length: T.int64):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        num_pages, page_size = T.int32(), T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, page_size, 128), "float16")
        # with T.block("root"):
        for b in T.thread_binding(copy_length, thread="blockIdx.x"):
            for t in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("copy"):
                    vh = T.axis.spatial(8, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) // (copy_length * T.int64(128))))
                    vp = T.axis.spatial(copy_length, (b * T.int64(1024) + T.Cast("int64", t)) % (copy_length * T.int64(128)) // T.int64(128))
                    vd = T.axis.spatial(128, T.Cast("int32", (b * T.int64(1024) + T.Cast("int64", t)) % T.int64(128)))
                    T.where(b * T.int64(1024) + T.Cast("int64", t) < copy_length * T.int64(8) * T.int64(128))
                    T.reads(pages[src_page_id, 0:2, vh, vp, vd])
                    T.writes(pages[tgt_page_id, 0:2, vh, vp, vd])
                    pages[tgt_page_id, 0, vh, vp, vd] = pages[src_page_id, 0, vh, vp, vd]
                    pages[tgt_page_id, 1, vh, vp, vd] = pages[src_page_id, 1, vh, vp, vd]

    @T.prim_func(private=True)
    def dequantize(var_model_embed_tokens_q_weight: T.handle, var_model_embed_tokens_q_scale: T.handle, var_dequantize: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        vocab_size = T.int64()
        model_embed_tokens_q_weight = T.match_buffer(var_model_embed_tokens_q_weight, (vocab_size, T.int64(412)), "uint32")
        model_embed_tokens_q_scale = T.match_buffer(var_model_embed_tokens_q_scale, (vocab_size, T.int64(103)), "float16")
        dequantize = T.match_buffer(var_dequantize, (vocab_size, T.int64(4096)), "float16")
        # with T.block("root"):
        compute = T.alloc_buffer((vocab_size, T.int64(4096)), "float16")
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_embed_tokens_q_weight[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_embed_tokens_q_weight[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(vocab_size, T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_embed_tokens_q_scale[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_embed_tokens_q_scale[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize1(model_layers_0_self_attn_qkv_proj_q_weight2: T.Buffer((T.int64(6144), T.int64(412)), "uint32"), model_layers_0_self_attn_qkv_proj_q_scale2: T.Buffer((T.int64(6144), T.int64(103)), "float16"), dequantize: T.Buffer((T.int64(6144), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(6144), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_qkv_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_qkv_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(6144), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_qkv_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_qkv_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize2(model_layers_0_self_attn_o_proj_q_weight2: T.Buffer((T.int64(4096), T.int64(412)), "uint32"), model_layers_0_self_attn_o_proj_q_scale2: T.Buffer((T.int64(4096), T.int64(103)), "float16"), dequantize: T.Buffer((T.int64(4096), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_self_attn_o_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_self_attn_o_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_self_attn_o_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_self_attn_o_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize3(model_layers_0_mlp_gate_up_proj_q_weight2: T.Buffer((T.int64(28672), T.int64(412)), "uint32"), model_layers_0_mlp_gate_up_proj_q_scale2: T.Buffer((T.int64(28672), T.int64(103)), "float16"), dequantize: T.Buffer((T.int64(28672), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(28672), T.int64(4096)), "float16")
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_gate_up_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_gate_up_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(28672), T.int64(4096)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_gate_up_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_gate_up_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func(private=True)
    def dequantize4(model_layers_0_mlp_down_proj_q_weight2: T.Buffer((T.int64(4096), T.int64(1436)), "uint32"), model_layers_0_mlp_down_proj_q_scale2: T.Buffer((T.int64(4096), T.int64(359)), "float16"), dequantize: T.Buffer((T.int64(4096), T.int64(14336)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        # with T.block("root"):
        compute = T.alloc_buffer((T.int64(4096), T.int64(14336)), "float16")
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("compute"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(model_layers_0_mlp_down_proj_q_weight2[v_i0, v_i1 // T.int64(10)])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.Cast("float16", T.bitwise_and(T.shift_right(model_layers_0_mlp_down_proj_q_weight2[v_i0, v_i1 // T.int64(10)], T.Cast("uint32", v_i1 % T.int64(10) * T.int64(3))), T.uint32(7)))
        for i0, i1 in T.grid(T.int64(4096), T.int64(14336)):
            with T.block("dequantize"):
                v_i0, v_i1 = T.axis.remap("SS", [i0, i1])
                T.reads(compute[v_i0, v_i1], model_layers_0_mlp_down_proj_q_scale2[v_i0, v_i1 // T.int64(40)])
                T.writes(dequantize[v_i0, v_i1])
                dequantize[v_i0, v_i1] = (compute[v_i0, v_i1] - T.float16(3.0)) * model_layers_0_mlp_down_proj_q_scale2[v_i0, v_i1 // T.int64(40)]

    @T.prim_func
    def full(var_result: T.handle, value: T.int32):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch_size = T.int32(is_size_var=True)
        result = T.match_buffer(var_result, (batch_size, 1), "int32")
        # with T.block("root"):
        for i in range(batch_size):
            with T.block("block"):
                vi = T.axis.spatial(batch_size, i)
                T.reads()
                T.writes(result[vi, 0])
                result[vi, 0] = value

    @T.prim_func
    def fused_rope(var_qkv: T.handle, var_position_map: T.handle, var_q: T.handle, var_k: T.handle, var_v: T.handle, apply_rope: T.int32):
        T.func_attr({"op_pattern": 8, "target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        seq_len = T.int32()
        qkv = T.match_buffer(var_qkv, (seq_len, 48, 128), "float16")
        position_map = T.match_buffer(var_position_map, (seq_len,), "int32", offset_factor=1)
        q = T.match_buffer(var_q, (seq_len, 32, 128), "float16")
        k = T.match_buffer(var_k, (seq_len, 8, 128), "float16")
        v = T.match_buffer(var_v, (seq_len, 8, 128), "float16")
        # with T.block("root"):
        for iters_0, iters_1, iters_2 in T.grid(seq_len, 48, 128):
            with T.block("llama_fused_rope"):
                s, h, d = T.axis.remap("SSS", [iters_0, iters_1, iters_2])
                T.reads(position_map[s], qkv[s, h, d - 64:d - 64 + 129])
                T.writes(q[s, h, d], k[s, h - 32, d], v[s, h - 40, d])
                if h < 32:
                    freq = T.float32()
                    q[s, h, d] = T.if_then_else(apply_rope > 0 and d < 128, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 64, qkv[s, h, d + 64] * T.float16(-1.0), qkv[s, h, d - 64]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(500000.0), T.Cast("float32", d * 2 % 128) / T.float32(128.0))}), qkv[s, h, d])
                else:
                    if h < 40:
                        freq = T.float32()
                        k[s, h - 32, d] = T.if_then_else(apply_rope > 0 and d < 128, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", qkv[s, h, d]) + T.sin(freq) * T.Cast("float32", T.if_then_else(d < 64, qkv[s, h, d + 64] * T.float16(-1.0), qkv[s, h, d - 64]))), where={freq: T.Cast("float32", position_map[s]) / T.pow(T.float32(500000.0), T.Cast("float32", d * 2 % 128) / T.float32(128.0))}), qkv[s, h, d])
                    else:
                        v[s, h - 40, d] = qkv[s, h, d]

    @T.prim_func
    def gather_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        m, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (m, n))
        batch_size = T.int32(is_size_var=True)
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        dst = T.match_buffer(var_dst, (batch_size, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("gather_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[indices[vb], vj], indices[vb])
                T.writes(dst[vb, vj])
                dst[vb, vj] = src[indices[vb], vj]

    @T.prim_func(private=True)
    def get_index_from_sorted(A: T.handle, B: T.handle, C: T.handle, D: T.handle, E: T.handle, F: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        indices = T.match_buffer(B, (batch, vocab_size), "int32")
        renorm_prob = T.match_buffer(C, (batch, 1))
        out_batch = T.int64(is_size_var=True)
        usample = T.match_buffer(D, (out_batch, 1))
        sample_indices = T.match_buffer(E, (out_batch, 1), "int32")
        output_index = T.match_buffer(F, (out_batch, 1), "int32")
        # with T.block("root"):
        for ax0, ax1 in T.grid(out_batch, vocab_size):
            with T.block("T_get_index_from_sorted"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(usample[v_ax0, T.int64(0)], cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1):v_ax1 - T.int64(1) + T.int64(2)], sample_indices[v_ax0, T.int64(0)], renorm_prob[sample_indices[v_ax0, T.int64(0)], 0], indices[sample_indices[v_ax0, T.int64(0)], T.min(T.int64(0), v_ax1):T.min(T.int64(0), v_ax1) + (T.max(T.int64(0), v_ax1) + T.int64(1) - T.min(T.int64(0), v_ax1))])
                T.writes(output_index[v_ax0, 0])
                if usample[v_ax0, T.int64(0)] < cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0] or v_ax1 + T.int64(1) == vocab_size:
                    if v_ax1 == T.int64(0):
                        output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], 0]
                    else:
                        if usample[v_ax0, T.int64(0)] >= cumsum_sorted[sample_indices[v_ax0, T.int64(0)], v_ax1 - T.int64(1)] / renorm_prob[sample_indices[v_ax0, T.int64(0)], 0]:
                            output_index[v_ax0, 0] = indices[sample_indices[v_ax0, T.int64(0)], v_ax1]

    @T.prim_func(private=True)
    def get_renorm_prob(A: T.handle, B: T.handle, C: T.handle, D: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        cumsum_sorted = T.match_buffer(A, (batch, vocab_size))
        top_p = T.match_buffer(B, (batch, 1))
        top_k = T.match_buffer(C, (batch, 1), "int32")
        renorm_prob = T.match_buffer(D, (batch, 1))
        # with T.block("root"):
        for ax0, ax1 in T.grid(batch, vocab_size):
            with T.block("T_get_renorm_prob"):
                v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                T.reads(cumsum_sorted[v_ax0, T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)):T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + (T.max(T.max(T.int64(0), v_ax1), v_ax1 + T.int64(1)) + T.int64(1) - T.min(T.min(T.int64(0), v_ax1), v_ax1 + T.int64(1)))], top_p[v_ax0, 0], top_k[v_ax0, 0])
                T.writes(renorm_prob[v_ax0, 0])
                if not (cumsum_sorted[v_ax0, 0] < top_p[v_ax0, 0] and top_k[v_ax0, 0] > 1):
                    renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, 0]
                else:
                    if cumsum_sorted[v_ax0, v_ax1] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0]):
                        if v_ax1 + T.int64(1) == vocab_size:
                            renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1]
                        else:
                            if not (cumsum_sorted[v_ax0, v_ax1 + T.int64(1)] < top_p[v_ax0, 0] and v_ax1 + T.int64(1) + T.int64(1) < T.Cast("int64", top_k[v_ax0, 0])):
                                renorm_prob[v_ax0, 0] = cumsum_sorted[v_ax0, v_ax1 + T.int64(1)]

    @T.prim_func(private=True)
    def index(var_rms_norm64: T.handle, index: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), "float16")):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        seq_len = T.int64()
        rms_norm64 = T.match_buffer(var_rms_norm64, (T.int64(1), seq_len, T.int64(4096)), "float16")
        # with T.block("root"):
        for i, _, k in T.grid(T.int64(1), T.int64(1), T.int64(4096)):
            with T.block("index"):
                v_i, v__, v_k = T.axis.remap("SSS", [i, _, k])
                T.reads(rms_norm64[v_i, seq_len - T.int64(1), v_k])
                T.writes(index[v_i, v__, v_k])
                index[v_i, v__, v_k] = rms_norm64[v_i, seq_len - T.int64(1), v_k]

    @T.prim_func
    def merge_state_inplace(v: T.handle, s: T.handle, v_other: T.handle, s_other: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        N, H, D = T.int32(is_size_var=True), T.int32(is_size_var=True), T.int32(is_size_var=True)
        V = T.match_buffer(v, (N, H, D), "float16")
        S = T.match_buffer(s, (N, H))
        V_other = T.match_buffer(v_other, (N, H, D), "float16")
        S_other = T.match_buffer(s_other, (N, H))
        # with T.block("root"):
        for bx in T.thread_binding(N, thread="blockIdx.x"):
            for by in T.thread_binding(1, thread="blockIdx.y"):
                for ty in T.thread_binding(32, thread="threadIdx.y"):
                    for tx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("merge"):
                            T.reads(S[bx, ty + by * 32], S_other[bx, ty + by * 32], V[bx, ty + by * 32, tx * 4:tx * 4 + 4], V_other[bx, ty + by * 32, tx * 4:tx * 4 + 4])
                            T.writes(V[bx, ty + by * 32, tx * 4:tx * 4 + 4], S[bx, ty + by * 32])
                            s_val = T.alloc_buffer((1,), scope="local")
                            s_other_val = T.alloc_buffer((1,), scope="local")
                            s_max = T.alloc_buffer((1,), scope="local")
                            scale = T.alloc_buffer((1,), scope="local")
                            other_scale = T.alloc_buffer((1,), scope="local")
                            v_vec = T.alloc_buffer((4,), "float16", scope="local")
                            v_other_vec = T.alloc_buffer((4,), "float16", scope="local")
                            s_val[0] = S[bx, ty + by * 32]
                            s_other_val[0] = S_other[bx, ty + by * 32]
                            s_max[0] = T.max(s_val[0], s_other_val[0])
                            s_val[0] = T.exp2(s_val[0] - s_max[0])
                            s_other_val[0] = T.exp2(s_other_val[0] - s_max[0])
                            scale[0] = s_val[0] / (s_val[0] + s_other_val[0])
                            other_scale[0] = s_other_val[0] / (s_val[0] + s_other_val[0])
                            for vec in T.vectorized(4):
                                v_vec[vec] = V[bx, ty + by * 32, tx * 4 + vec]
                            for vec in T.vectorized(4):
                                v_other_vec[vec] = V_other[bx, ty + by * 32, tx * 4 + vec]
                            for vec in range(4):
                                v_vec[vec] = T.Cast("float16", T.Cast("float32", v_vec[vec]) * scale[0] + T.Cast("float32", v_other_vec[vec]) * other_scale[0])
                            for vec in T.vectorized(4):
                                V[bx, ty + by * 32, tx * 4 + vec] = v_vec[vec]
                            S[bx, ty + by * 32] = T.log2(s_val[0] + s_other_val[0]) + s_max[0]

    @T.prim_func
    def sampler_take_probs_tir(var_unsorted_probs: T.handle, var_sorted_indices: T.handle, var_sample_indices: T.handle, var_sampling_results: T.handle, var_top_prob_offsets: T.handle, var_sampled_values: T.handle, var_top_prob_probs: T.handle, var_top_prob_indices: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1})})
        batch_size, vocab_size = T.int32(is_size_var=True), T.int32(is_size_var=True)
        unsorted_probs = T.match_buffer(var_unsorted_probs, (batch_size, vocab_size))
        sorted_indices = T.match_buffer(var_sorted_indices, (batch_size, vocab_size), "int32")
        num_samples = T.int32(is_size_var=True)
        sample_indices = T.match_buffer(var_sample_indices, (num_samples,), "int32")
        sampling_results = T.match_buffer(var_sampling_results, (num_samples,), "int32")
        num_positions = T.int32(is_size_var=True)
        top_prob_offsets = T.match_buffer(var_top_prob_offsets, (num_positions,), "int32")
        sampled_values = T.match_buffer(var_sampled_values, (num_samples,))
        top_prob_probs = T.match_buffer(var_top_prob_probs, (num_positions,))
        top_prob_indices = T.match_buffer(var_top_prob_indices, (num_positions,), "int32")
        # with T.block("root"):
        for i in range(num_positions + num_samples):
            with T.block("block"):
                vi = T.axis.spatial(num_positions + num_samples, i)
                T.reads(top_prob_offsets[vi], sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], unsorted_probs[T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]):T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + (T.max(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions]) + 1 - T.min(top_prob_offsets[vi] // vocab_size, sample_indices[vi - num_positions])), T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]):T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + (T.max(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]) + 1 - T.min(sorted_indices[top_prob_offsets[vi] // vocab_size, top_prob_offsets[vi] % vocab_size], sampling_results[vi - num_positions]))], sample_indices[vi - num_positions], sampling_results[vi - num_positions])
                T.writes(top_prob_indices[vi], top_prob_probs[vi], sampled_values[vi - num_positions])
                if vi < num_positions:
                    row: T.int32 = top_prob_offsets[vi] // vocab_size
                    col: T.int32 = top_prob_offsets[vi] % vocab_size
                    top_prob_indices[vi] = sorted_indices[row, col]
                    top_prob_probs[vi] = unsorted_probs[row, sorted_indices[row, col]]
                else:
                    vj: T.int32 = vi - num_positions
                    sampled_values[vj] = unsorted_probs[sample_indices[vj], sampling_results[vj]]

    @T.prim_func
    def scatter_probs(var_src: T.handle, var_indices: T.handle, var_dst: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, n = T.int32(is_size_var=True), T.int32(is_size_var=True)
        src = T.match_buffer(var_src, (batch_size, n))
        indices = T.match_buffer(var_indices, (batch_size,), "int32")
        m = T.int32(is_size_var=True)
        dst = T.match_buffer(var_dst, (m, n))
        # with T.block("root"):
        for b, j in T.grid(batch_size, n):
            with T.block("scatter_2d"):
                vb, vj = T.axis.remap("SS", [b, j])
                T.reads(src[vb, vj], indices[vb])
                T.writes(dst[indices[vb], vj])
                dst[indices[vb], vj] = src[vb, vj]

    @T.prim_func
    def softmax_with_chunked_sum(var_A: T.handle, var_temperature: T.handle, var_chunked_sum: T.handle, var_chunked_max: T.handle, var_softmax: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(is_size_var=True), T.int64(is_size_var=True)
        A = T.match_buffer(var_A, (batch_size, vocab_size))
        temperature = T.match_buffer(var_temperature, (batch_size,))
        num_chunks = T.int64(is_size_var=True)
        chunked_sum = T.match_buffer(var_chunked_sum, (batch_size, num_chunks))
        chunked_max = T.match_buffer(var_chunked_max, (batch_size, num_chunks))
        softmax = T.match_buffer(var_softmax, (batch_size, vocab_size))
        # with T.block("root"):
        temp_max_shared = T.alloc_buffer((batch_size,), scope="shared")
        temp_sum_shared = T.alloc_buffer((batch_size,), scope="shared")
        for l0_l1_fused in T.thread_binding(batch_size * num_chunks, thread="blockIdx.x"):
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("max"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(chunked_max[v0, v1])
                        T.writes(temp_max_shared[v0])
                        with T.init():
                            temp_max_shared[v0] = T.float32(-340282346638528859811704183484516925440.0)
                        temp_max_shared[v0] = T.max(temp_max_shared[v0], chunked_max[v0, v1])
            for ax0_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                for ax0_0 in T.serial((num_chunks + T.int64(31)) // T.int64(32), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("sum_exp"):
                        v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                        v1 = T.axis.reduce(num_chunks, ax0_0 * T.int64(32) + ax0_1)
                        T.where(ax0_0 * T.int64(32) + ax0_1 < num_chunks)
                        T.reads(temperature[v0], chunked_sum[v0, v1], chunked_max[v0, v1], temp_max_shared[v0])
                        T.writes(temp_sum_shared[v0])
                        with T.init():
                            temp_sum_shared[v0] = T.float32(0.0)
                        temp_sum_shared[v0] = temp_sum_shared[v0] + T.Select(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(chunked_sum[v0, v1] + chunked_max[v0, v1] - temp_max_shared[v0]), T.Cast("float32", chunked_max[v0, v1] == temp_max_shared[v0]) * chunked_sum[v0, v1])
            for l2_0 in T.serial(T.int64(4), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                for l2_1 in T.thread_binding(T.int64(32), thread="threadIdx.y"):
                    for l2_2 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                        with T.block("log_pad"):
                            v0 = T.axis.spatial(batch_size, l0_l1_fused % (num_chunks * batch_size) // num_chunks)
                            v1 = T.axis.spatial(num_chunks, l0_l1_fused % num_chunks)
                            v2 = T.axis.spatial(T.int64(4096), l2_0 * T.int64(1024) + l2_1 * T.int64(32) + l2_2)
                            T.reads(temperature[v0], A[v0, v1 * T.int64(4096) + v2], temp_sum_shared[v0], temp_max_shared[v0])
                            T.writes(softmax[v0, v1 * T.int64(4096) + v2])
                            if v1 * T.int64(4096) + v2 < vocab_size:
                                softmax[v0, v1 * T.int64(4096) + v2] = T.if_then_else(temperature[v0] > T.float32(1.0000000000000001e-05), T.exp(A[v0, v1 * T.int64(4096) + v2] / temperature[v0] - (T.log(temp_sum_shared[v0]) + temp_max_shared[v0])), T.Cast("float32", A[v0, v1 * T.int64(4096) + v2] == temp_max_shared[v0]) / temp_sum_shared[v0])

    @T.prim_func(private=True)
    def take_sorted_probs(var_probs: T.handle, var_lv1: T.handle, var_take_sorted_probs: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        batch_size, vocab_size = T.int64(), T.int64()
        probs = T.match_buffer(var_probs, (batch_size, vocab_size))
        lv1 = T.match_buffer(var_lv1, (batch_size, vocab_size), "int32")
        batch_size_1, vocab_size_1 = T.int64(is_size_var=True), T.int64(is_size_var=True)
        take_sorted_probs = T.match_buffer(var_take_sorted_probs, (batch_size_1, vocab_size_1))
        # with T.block("root"):
        for i, j in T.grid(batch_size_1, vocab_size_1):
            with T.block("take_sorted_probs"):
                v_i, v_j = T.axis.remap("SS", [i, j])
                T.reads(probs[v_i, lv1[v_i, v_j]], lv1[v_i, v_j])
                T.writes(take_sorted_probs[v_i, v_j])
                take_sorted_probs[v_i, v_j] = probs[v_i, lv1[v_i, v_j]]

    @T.prim_func
    def tir_kv_cache_debug_get_kv(var_pages: T.handle, var_position_map: T.handle, var_k_data: T.handle, var_v_data: T.handle, layer_id: T.int64):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        num_pages, page_size = T.int64(), T.int64(is_size_var=True)
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, page_size, 128), "float16")
        seqlen = T.int64(is_size_var=True)
        position_map = T.match_buffer(var_position_map, (seqlen,), "int32", offset_factor=1)
        k_data = T.match_buffer(var_k_data, (32, seqlen, 8, 128), "float16")
        v_data = T.match_buffer(var_v_data, (32, seqlen, 8, 128), "float16")
        # with T.block("root"):
        for p, h, d in T.grid(seqlen, 8, 128):
            with T.block("copy0"):
                vp, vh, vd = T.axis.remap("SSS", [p, h, d])
                T.reads(position_map[vp], pages[T.Cast("int64", position_map[vp]) // page_size, 0:2, vh, T.Cast("int64", position_map[vp]) % page_size, vd])
                T.writes(k_data[layer_id, vp, vh, vd], v_data[layer_id, vp, vh, vd])
                position: T.int32 = position_map[vp]
                k_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 0, vh, T.Cast("int64", position) % page_size, vd]
                v_data[layer_id, vp, vh, vd] = pages[T.Cast("int64", position) // page_size, 1, vh, T.Cast("int64", position) % page_size, vd]

    @T.prim_func
    def tir_kv_cache_transpose_append(var_pages: T.handle, var_k_data: T.handle, var_v_data: T.handle, var_position_map: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.noalias": T.bool(True)})
        num_pages = T.int64()
        pages = T.match_buffer(var_pages, (num_pages, 2, 8, 16, 128), "float16")
        ntoken = T.int64(is_size_var=True)
        k_data = T.match_buffer(var_k_data, (ntoken, 8, 128), "float16")
        v_data = T.match_buffer(var_v_data, (ntoken, 8, 128), "float16")
        position_map = T.match_buffer(var_position_map, (ntoken,), "int32", offset_factor=1)
        # with T.block("root"):
        for global_pos, h, f in T.grid(ntoken, 8, 128):
            if position_map[global_pos] != -1:
                with T.block("k_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], k_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 0, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 0, vh, position % 16, vf] = k_data[vgpos, vh, vf]
                with T.block("v_transpose_append"):
                    vgpos, vh, vf = T.axis.remap("SSS", [global_pos, h, f])
                    T.reads(position_map[vgpos], v_data[vgpos, vh, vf])
                    T.writes(pages[position_map[vgpos] // 16, 1, vh, position_map[vgpos] % 16, vf])
                    position: T.int32 = position_map[vgpos]
                    pages[position // 16, 1, vh, position % 16, vf] = v_data[vgpos, vh, vf]

    @T.prim_func(private=True)
    def top_p_pivot_cutoff(var_prob: T.handle, var_top_p_arr: T.handle, var_init_pivots: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        top_p_arr = T.match_buffer(var_top_p_arr, (B,))
        init_pivots = T.match_buffer(var_init_pivots, (B, 3))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        # with T.block("root"):
        pivot = T.alloc_buffer((3,), scope="local")
        top_p = T.alloc_buffer((1,), scope="local")
        L = T.alloc_buffer((1,), scope="shared")
        R_1 = T.alloc_buffer((1,), scope="shared")
        L_local = T.alloc_buffer((1,), scope="local")
        R_local = T.alloc_buffer((1,), scope="local")
        q = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((3,), scope="local")
        lmin_broadcast = T.alloc_buffer((1,), scope="shared")
        lmin_broadcast_local = T.alloc_buffer((1,), scope="local")
        lmin = T.alloc_buffer((3,), scope="local")
        cmin = T.alloc_buffer((3,), "int32", scope="local")
        total_sum = T.alloc_buffer((1,), scope="local")
        it = T.alloc_buffer((1,), "int32", scope="local")
        es_local = T.alloc_buffer((1,), "bool", scope="local")
        es = T.alloc_buffer((1,), "bool", scope="shared")
        find_pivot_local = T.alloc_buffer((1,), "bool", scope="local")
        find_pivot = T.alloc_buffer((1,), "bool", scope="shared")
        total_sum_reduce = T.alloc_buffer((1,), scope="local")
        lsum_reduce = T.alloc_buffer((1,), scope="local")
        lmin_reduce = T.alloc_buffer((1,), scope="local")
        cmin_reduce = T.alloc_buffer((1,), "int32", scope="local")
        for _bx in T.thread_binding(B, thread="blockIdx.x"):
            for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("CTA"):
                    b, tx = T.axis.remap("SS", [_bx, _tx])
                    T.reads(top_p_arr[b], top_p[0], L[0], R_1[0], init_pivots[b, 0:3], L_local[0], R_local[0], find_pivot_local[0], it[0], es_local[0], prob[b, it[0] * 1024 + tx], total_sum[0], q[0], pivot[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lsum[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], lmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], cmin[T.min(0, it[0]):T.min(0, it[0]) + (T.max(2, it[0]) + 1 - T.min(0, it[0]))], total_sum_reduce[0], es[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], lsum_reduce[0], cmin_reduce[0], find_pivot[0])
                    T.writes(top_p[0], L[0], R_1[0], find_pivot[0], L_local[0], R_local[0], pivot[0:3], find_pivot_local[0], final_lsum[b], final_pivot[b], lsum[0:3], lmin[0:3], cmin[0:3], total_sum[0], it[0], es_local[0], q[0], total_sum_reduce[0], es[0], lsum_reduce[0], lmin_reduce[0], lmin_broadcast[0], lmin_broadcast_local[0], cmin_reduce[0])
                    top_p[0] = top_p_arr[b]
                    if tx == 0:
                        L[0] = T.float32(1.0) - top_p[0]
                        R_1[0] = T.float32(9.9999999999999995e-08)
                        find_pivot[0] = T.bool(False)
                    T.tvm_storage_sync("shared")
                    L_local[0] = L[0]
                    R_local[0] = R_1[0]
                    for i in T.unroll(3):
                        pivot[i] = init_pivots[b, i]
                    find_pivot_local[0] = T.bool(False)
                    if L_local[0] - R_local[0] <= T.float32(9.9999999999999995e-08):
                        if tx == 0:
                            final_lsum[b] = T.float32(1.0)
                            final_pivot[b] = T.float32(0.0)
                        find_pivot_local[0] = T.bool(True)
                    while T.tvm_thread_invariant(L_local[0] - R_local[0] > T.float32(9.9999999999999995e-08) and not find_pivot_local[0]):
                        T.tvm_storage_sync("shared")
                        for pidx in T.unroll(3):
                            lsum[pidx] = T.float32(0.0)
                            lmin[pidx] = T.float32(340282346638528859811704183484516925440.0)
                            cmin[pidx] = 0
                        total_sum[0] = T.float32(0.0)
                        it[0] = 0
                        es_local[0] = T.bool(False)
                        while it[0] < (N + 1024 - 1) // 1024 and not es_local[0]:
                            q[0] = T.if_then_else(it[0] * 1024 + tx < N, prob[b, it[0] * 1024 + tx], T.float32(0.0))
                            total_sum[0] = total_sum[0] + q[0]
                            for pidx in T.unroll(3):
                                if q[0] >= pivot[pidx]:
                                    lsum[pidx] = lsum[pidx] + q[0]
                                    if lmin[pidx] > q[0]:
                                        lmin[pidx] = q[0]
                                        cmin[pidx] = 1
                                    else:
                                        if lmin[pidx] == q[0]:
                                            cmin[pidx] = cmin[pidx] + 1
                            it[0] = it[0] + 1
                            if it[0] % 32 == 0:
                                with T.block("block_cross_thread"):
                                    T.reads(total_sum[0])
                                    T.writes(total_sum_reduce[0])
                                    T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                    T.tvm_thread_allreduce(T.uint32(1), total_sum[0], T.bool(True), total_sum_reduce[0], tx)
                                if tx == 0:
                                    es[0] = T.float32(1.0) - total_sum_reduce[0] < pivot[2]
                                T.tvm_storage_sync("shared")
                                es_local[0] = es[0]
                        T.tvm_storage_sync("shared")
                        for pidx in range(3):
                            with T.block("block_cross_thread"):
                                T.reads(lsum[pidx])
                                T.writes(lsum_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lsum[pidx], T.bool(True), lsum_reduce[0], tx)
                            with T.block("block_cross_thread"):
                                T.reads(lmin[pidx])
                                T.writes(lmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: T.min(x0, y0), [T.float32(0.0)]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), lmin[pidx], T.bool(True), lmin_reduce[0], tx)
                            if tx == 0:
                                lmin_broadcast[0] = lmin_reduce[0]
                            T.tvm_storage_sync("shared")
                            lmin_broadcast_local[0] = lmin_broadcast[0]
                            if lmin[pidx] > lmin_broadcast_local[0]:
                                cmin[pidx] = 0
                            if tx == 0:
                                lsum[pidx] = lsum_reduce[0]
                                lmin[pidx] = lmin_reduce[0]
                            with T.block("block_cross_thread"):
                                T.reads(cmin[pidx])
                                T.writes(cmin_reduce[0])
                                T.attr(T.comm_reducer(lambda x0, y0: x0 + y0, [0]), "reduce_scope", T.reinterpret("handle", T.uint64(0)))
                                T.tvm_thread_allreduce(T.uint32(1), cmin[pidx], T.bool(True), cmin_reduce[0], tx)
                            if tx == 0:
                                cmin[pidx] = cmin_reduce[0]
                        T.tvm_storage_sync("shared")
                        if tx == 0:
                            it[0] = 0
                            while it[0] < 3 and not find_pivot_local[0]:
                                if lsum[it[0]] >= top_p[0] and top_p[0] > lsum[it[0]] - T.Cast("float32", cmin[it[0]]) * lmin[it[0]]:
                                    find_pivot[0] = T.bool(True)
                                    find_pivot_local[0] = T.bool(True)
                                    final_pivot[b] = pivot[it[0]]
                                    final_lsum[b] = lsum[it[0]]
                                else:
                                    if lsum[it[0]] - lmin[it[0]] * T.Cast("float32", cmin[it[0]]) >= top_p[0]:
                                        R_1[0] = pivot[it[0]]
                                        final_lsum[b] = lsum[it[0]]
                                    else:
                                        if lsum[it[0]] < top_p[0]:
                                            L[0] = pivot[it[0]]
                                it[0] = it[0] + 1
                        T.tvm_storage_sync("shared")
                        L_local[0] = L[0]
                        R_local[0] = R_1[0]
                        find_pivot_local[0] = find_pivot[0]
                        for pidx in T.unroll(3):
                            pivot[pidx] = L[0] - T.Cast("float32", pidx + 1) * (L_local[0] - R_local[0]) / T.float32(4.0)
                    if tx == 0:
                        if not find_pivot_local[0]:
                            final_pivot[b] = R_local[0]
                            if R_local[0] == T.float32(9.9999999999999995e-08):
                                final_lsum[b] = lsum[2]

    @T.prim_func(private=True)
    def top_p_renorm_after_cutoff(var_prob: T.handle, var_final_pivot: T.handle, var_final_lsum: T.handle, var_renorm_prob: T.handle):
        T.func_attr({"target": T.target({"keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        B, N = T.int32(is_size_var=True), T.int32(is_size_var=True)
        prob = T.match_buffer(var_prob, (B, N))
        final_pivot = T.match_buffer(var_final_pivot, (B,))
        final_lsum = T.match_buffer(var_final_lsum, (B,))
        renorm_prob = T.match_buffer(var_renorm_prob, (B, N))
        # with T.block("root"):
        pivot = T.alloc_buffer((1,), scope="local")
        lsum = T.alloc_buffer((1,), scope="local")
        for _by in T.thread_binding(B, thread="blockIdx.y"):
            for _bx in T.thread_binding(511 // B + 1, thread="blockIdx.x"):
                for _tx in T.thread_binding(1024, thread="threadIdx.x"):
                    with T.block("CTA"):
                        by, bx, tx = T.axis.remap("SSS", [_by, _bx, _tx])
                        T.reads(final_pivot[by], final_lsum[by], prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)], pivot[0], lsum[0])
                        T.writes(pivot[0], lsum[0], renorm_prob[by, bx * 1024 + tx:bx * 1024 + tx + (((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024) - 1) * (511 // B + 1) * 1024 + 1)])
                        pivot[0] = final_pivot[by]
                        lsum[0] = final_lsum[by]
                        for i in range((511 // B * 1024 + N + 1023) // (511 // B * 1024 + 1024)):
                            if i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx < N:
                                renorm_prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] = T.if_then_else(prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] >= pivot[0], prob[by, i * ((512 + B - 1) // B) * 1024 + bx * 1024 + tx] / lsum[0], T.float32(0.0))

    @T.prim_func
    def tree_attn_paged_kv(_0: T.int32, var_q: T.handle, var_q_indptr: T.handle, var_pages: T.handle, var_page_indptr: T.handle, var_page_values: T.handle, var_length_info: T.handle, var_k_rope_pos_offset: T.handle, var_q_rope_position: T.handle, var_output: T.handle, var_lse: T.handle, rotary_mode: T.int32, rope_scale: T.float32, rope_theta: T.float32, attn_score_scaling_factor: T.float32, tree_order_indptr_handle: T.handle, tree_order_handle: T.handle):
        T.func_attr({"target": T.target({"host": {"keys": ["cpu"], "kind": "llvm", "mcpu": "alderlake", "mtriple": "x86_64-redhat-linux-gnu", "tag": ""}, "keys": ["vulkan", "gpu"], "kind": "vulkan", "max_num_threads": 256, "max_shared_memory_per_block": 49152, "max_threads_per_block": 1024, "supports_16bit_buffer": True, "supports_8bit_buffer": True, "supports_float16": True, "supports_float32": True, "supports_int16": True, "supports_int32": True, "supports_int64": True, "supports_int8": True, "supports_storage_buffer_storage_class": True, "tag": "", "thread_warp_size": 1}), "tir.is_scheduled": 1})
        total_len = T.int32(is_size_var=True)
        q = T.match_buffer(var_q, (total_len, 32, 128), "float16")
        batch_size = T.int32(is_size_var=True)
        q_indptr = T.match_buffer(var_q_indptr, (batch_size + 1,), "int32", offset_factor=1)
        max_num_pages = T.int32(is_size_var=True)
        pages = T.match_buffer(var_pages, (max_num_pages, 2, 8, 16, 128), "float16")
        page_indptr = T.match_buffer(var_page_indptr, (batch_size + 1,), "int32", offset_factor=1)
        nnz_pages = T.int32(is_size_var=True)
        page_values = T.match_buffer(var_page_values, (nnz_pages,), "int32", offset_factor=1)
        length_info = T.match_buffer(var_length_info, (batch_size,), "int32", offset_factor=1)
        k_rope_pos_offset = T.match_buffer(var_k_rope_pos_offset, (batch_size,), "int32", offset_factor=1)
        q_rope_position = T.match_buffer(var_q_rope_position, (total_len,), "int32", offset_factor=1)
        output = T.match_buffer(var_output, (total_len, 32, 128), "float16")
        lse = T.match_buffer(var_lse, (total_len, 32))
        tree_order_indptr = T.match_buffer(tree_order_indptr_handle, (batch_size + 1,), "int32", offset_factor=1)
        total_tree_order_len = T.int32(is_size_var=True)
        tree_order = T.match_buffer(tree_order_handle, (total_tree_order_len, 2), "int32", offset_factor=1)
        # with T.block("root"):
        assert rotary_mode == 0, "Inline rotary mode is not supported in tree attention."
        for lbx in T.thread_binding(16, thread="blockIdx.x"):
            for lby in T.thread_binding(8, thread="blockIdx.y"):
                for lty in T.thread_binding(4, thread="threadIdx.y"):
                    for ltx in T.thread_binding(32, thread="threadIdx.x"):
                        with T.block("attn"):
                            bx, by, ty, tx = T.axis.remap("SSSS", [lbx, lby, lty, ltx])
                            T.reads()
                            T.writes()
                            tile_id = T.alloc_buffer((1,), "int32", scope="local")
                            batch_idx = T.alloc_buffer((1,), "int32", scope="local")
                            batch_tiles = T.alloc_buffer((1,), "int32", scope="local")
                            batch_rows = T.alloc_buffer((1,), "int32", scope="local")
                            iterator = T.alloc_buffer((1,), "int32", scope="local")
                            kv_chunk_len = T.alloc_buffer((1,), "int32", scope="local")
                            Q_smem = T.alloc_buffer((32, 128), "float16", scope="shared")
                            K_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            V_smem = T.alloc_buffer((16, 128), "float16", scope="shared")
                            S_smem = T.alloc_buffer((32, 16), scope="shared")
                            S_local = T.alloc_buffer((32, 16), scope="local")
                            O_local = T.alloc_buffer((32, 128), scope="local")
                            m_smem = T.alloc_buffer((32,), scope="shared")
                            m_prev_smem = T.alloc_buffer((32,), scope="shared")
                            d_smem = T.alloc_buffer((32,), scope="shared")
                            m_new = T.alloc_buffer((1,), scope="local")
                            m_prev = T.alloc_buffer((1,), scope="local")
                            d_new = T.alloc_buffer((1,), scope="local")
                            tile_id[0] = bx
                            batch_idx[0] = 0
                            batch_rows[0] = (q_indptr[1] - q_indptr[0]) * 4
                            batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                            while T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                while tile_id[0] >= batch_tiles[0] and batch_idx[0] < batch_size:
                                    tile_id[0] = tile_id[0] - batch_tiles[0]
                                    batch_idx[0] = batch_idx[0] + 1
                                    if batch_idx[0] < batch_size:
                                        b_idx: T.int32 = batch_idx[0]
                                        batch_rows[0] = (q_indptr[b_idx + 1] - q_indptr[b_idx]) * 4
                                        batch_tiles[0] = (batch_rows[0] + 32 - 1) // 32
                                if T.tvm_thread_invariant(batch_idx[0] < batch_size):
                                    b_idx: T.int32 = batch_idx[0]
                                    LH_start: T.int32 = tile_id[0] * 32
                                    q_indptr_val: T.int32 = q_indptr[b_idx]
                                    cur_page_indptr_begin: T.int32 = page_indptr[b_idx]
                                    cur_page_indptr_end: T.int32 = page_indptr[b_idx + 1]
                                    kv_chunk_len[0] = T.if_then_else(cur_page_indptr_begin != cur_page_indptr_end, (cur_page_indptr_end - cur_page_indptr_begin - 1) * 16 + length_info[b_idx], 0)
                                    T.tvm_storage_sync("shared")
                                    for i in range(1):
                                        row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                        if row < 32:
                                            m_smem[row] = T.float32(-50000.0)
                                            d_smem[row] = T.float32(1.0)
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_init"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads()
                                                    T.writes(O_local[i, j])
                                                    O_local[i, j] = T.float32(0.0)
                                    T.tvm_storage_sync("shared")
                                    for li_lj_fused_0 in range(8):
                                        for li_lj_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_lj_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_lj_fused_3 in T.vectorized(4):
                                                    with T.block("Q_load"):
                                                        i = T.axis.spatial(32, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) // 128)
                                                        j = T.axis.spatial(128, (li_lj_fused_0 * 512 + li_lj_fused_1 * 128 + li_lj_fused_2 * 4 + li_lj_fused_3) % 128)
                                                        T.reads()
                                                        T.writes()
                                                        cur_L: T.int32 = q_indptr_val + (LH_start + i) // 4
                                                        cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                        if cur_L < q_indptr[b_idx + 1]:
                                                            freq = T.float32()
                                                            Q_smem[i, j] = T.if_then_else(rotary_mode == 1, T.Let(T.Cast("float16", T.cos(freq) * T.Cast("float32", q[cur_L, cur_H_qo, j]) + T.sin(freq) * T.Cast("float32", T.if_then_else(j < 64, q[cur_L, cur_H_qo, j + 64] * T.float16(-1.0), q[cur_L, cur_H_qo, j - 64]))), where={freq: T.Cast("float32", q_rope_position[cur_L]) * rope_scale / T.pow(rope_theta, T.Cast("float32", j * 2 % 128) / T.float32(128.0))}), q[cur_L, cur_H_qo, j])
                                                        else:
                                                            Q_smem[i, j] = T.float16(0.0)
                                    T.tvm_storage_sync("shared")
                                    for iterator_1 in range((kv_chunk_len[0] + 15) // 16):
                                        L_kv_start: T.int32 = iterator_1 * 16
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("K_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                K_smem[i, j] = pages[page_no, 0, by, page_offset, j]
                                                            else:
                                                                K_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        for lz_ly_fused_0 in range(4):
                                            for lz_ly_fused_1 in T.thread_binding(4, thread="threadIdx.y"):
                                                for lz_ly_fused_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lz_ly_fused_3 in T.vectorized(4):
                                                        with T.block("V_load"):
                                                            i = T.axis.spatial(16, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) // 128)
                                                            j = T.axis.spatial(128, (lz_ly_fused_0 * 512 + lz_ly_fused_1 * 128 + lz_ly_fused_2 * 4 + lz_ly_fused_3) % 128)
                                                            T.reads()
                                                            T.writes()
                                                            cur_L: T.int32 = L_kv_start + i
                                                            if cur_L < kv_chunk_len[0]:
                                                                seq_offset: T.int32 = cur_L
                                                                page_no: T.int32 = page_values[cur_page_indptr_begin + seq_offset // 16]
                                                                page_offset: T.int32 = seq_offset % 16
                                                                V_smem[i, j] = pages[page_no, 1, by, page_offset, j]
                                                            else:
                                                                V_smem[i, j] = T.float16(0.0)
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(Q_smem[0:32, 0:128], K_smem[0:16, 0:128])
                                            T.writes(S_local[0:32, 0:16])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(2, 2):
                                                        with T.block("S_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 8 * 2 + li_1_init)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 8 * 2 + lj_1_init)
                                                            T.reads()
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = T.float32(0.0)
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, li_1, lj_1, lk_1 in T.grid(16, 2, 2, 8):
                                                        with T.block("S_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                            j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                            k = T.axis.reduce(128, lk_0 * 8 + lk_1)
                                                            T.reads(S_local[i, j], Q_smem[i, k], K_smem[j, k])
                                                            T.writes(S_local[i, j])
                                                            S_local[i, j] = S_local[i, j] + T.Cast("float32", Q_smem[i, k]) * T.Cast("float32", K_smem[j, k]) * attn_score_scaling_factor * T.float32(0.12751743082459868)
                                        T.tvm_storage_sync("shared")
                                        for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                for li_1, lj_1 in T.grid(2, 2):
                                                    with T.block("S_store"):
                                                        i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 8 * 2 + li_1)
                                                        j = T.axis.spatial(16, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 8 * 2 + lj_1)
                                                        T.reads(S_local[i, j])
                                                        T.writes(S_smem[i, j])
                                                        S_smem[i, j] = S_local[i, j]
                                        T.tvm_storage_sync("shared")
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update1"):
                                                    T.reads(m_smem[row], kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], m_new[i], S_smem[row, 0:16], d_smem[row], m_prev[i])
                                                    T.writes(m_prev[i], m_new[i], d_new[i])
                                                    m_prev[i] = m_smem[row]
                                                    m_new[i] = m_smem[row]
                                                    row_: T.int32 = (LH_start + row) // 4
                                                    for j in range(16):
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            m_new[i] = T.max(m_new[i], S_smem[row, j])
                                                    d_new[i] = d_smem[row] * T.exp2(m_prev[i] - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            with T.block("update"):
                                                T.reads(kv_chunk_len[0], tree_order_indptr[b_idx:b_idx + 2], tree_order[T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]):T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0]) + (T.max((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] + 15 - kv_chunk_len[0]) + 1 - T.min((LH_start + row) // 4 + tree_order_indptr[b_idx + 1] + q_indptr[b_idx] - q_indptr[b_idx + 1], L_kv_start + tree_order_indptr[b_idx + 1] - kv_chunk_len[0])), 0:2], q_indptr[b_idx:b_idx + 2], S_smem[row, 0:16], m_new[i])
                                                T.writes(S_smem[row, 0:16])
                                                for j in range(16):
                                                    if row < 32:
                                                        row_: T.int32 = (LH_start + row) // 4
                                                        if L_kv_start + j < kv_chunk_len[0] and (L_kv_start + j < kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) or tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] >= tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 0] and tree_order[tree_order_indptr[b_idx] + (row_ + (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]) - (q_indptr[b_idx + 1] - q_indptr[b_idx])), 0] < tree_order[tree_order_indptr[b_idx] + (L_kv_start + j - (kv_chunk_len[0] - (tree_order_indptr[b_idx + 1] - tree_order_indptr[b_idx]))), 1]):
                                                            S_smem[row, j] = T.exp2(S_smem[row, j] - m_new[i])
                                                        else:
                                                            S_smem[row, j] = T.exp2(T.float32(-50000.0) - m_new[i])
                                        for i in range(1):
                                            row: T.int32 = i * 32 * 4 + ty * 32 + tx
                                            if row < 32:
                                                with T.block("update"):
                                                    T.reads(d_new[i], S_smem[row, 0:16], m_new[i], m_prev[i])
                                                    T.writes(d_new[i], m_smem[row], d_smem[row], m_prev_smem[row])
                                                    for j in range(16):
                                                        d_new[i] = d_new[i] + S_smem[row, j]
                                                    m_smem[row] = m_new[i]
                                                    d_smem[row] = d_new[i]
                                                    m_prev_smem[row] = m_prev[i]
                                        T.tvm_storage_sync("shared")
                                        with T.block(""):
                                            T.reads(m_prev_smem[0:32], m_smem[0:32], S_smem[0:32, 0:16], V_smem[0:16, 0:128])
                                            T.writes(O_local[0:32, 0:128])
                                            for li_0_lj_0_fused_0_init in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1_init in T.thread_binding(32, thread="threadIdx.x"):
                                                    for li_1_init, lj_1_init in T.grid(4, 8):
                                                        with T.block("O_gemm_init"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) // 16 * 4 + li_1_init)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0_init * 32 + li_0_lj_0_fused_1_init) % 16 * 8 + lj_1_init)
                                                            T.reads()
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] * T.exp2(m_prev_smem[i] - m_smem[i])
                                            for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                                for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                                    for lk_0, lk_1, li_1, lj_1 in T.grid(2, 8, 4, 8):
                                                        with T.block("O_gemm_update"):
                                                            i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                            j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                            k = T.axis.reduce(16, lk_0 * 8 + lk_1)
                                                            T.reads(O_local[i, j], m_prev_smem[i], m_smem[i], S_smem[i, k], V_smem[k, j])
                                                            T.writes(O_local[i, j])
                                                            O_local[i, j] = O_local[i, j] + S_smem[i, k] * T.Cast("float32", V_smem[k, j])
                                    for li_0_lj_0_fused_0 in T.thread_binding(4, thread="threadIdx.y"):
                                        for li_0_lj_0_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                            for li_1, lj_1 in T.grid(4, 8):
                                                with T.block("O_store"):
                                                    i = T.axis.spatial(32, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) // 16 * 4 + li_1)
                                                    j = T.axis.spatial(128, (li_0_lj_0_fused_0 * 32 + li_0_lj_0_fused_1) % 16 * 8 + lj_1)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], O_local[i, j], d_smem[i])
                                                    T.writes(output[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4, j])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        output[cur_L, cur_H_qo, j] = T.Cast("float16", O_local[i, j] / d_smem[i])
                                    for li_0 in range(1):
                                        for li_1 in T.thread_binding(4, thread="threadIdx.y"):
                                            for li_2 in T.thread_binding(32, thread="threadIdx.x"):
                                                with T.block("lse_store"):
                                                    i = T.axis.spatial(32, li_0 * 128 + li_1 * 32 + li_2)
                                                    T.where((li_0 * 4 + li_1) * 32 + li_2 < 32)
                                                    T.reads(q_indptr[b_idx:b_idx + 2], m_smem[i], d_smem[i])
                                                    T.writes(lse[q_indptr[b_idx] + (LH_start + i) // 4, by * 4 + (LH_start + i) % 4])
                                                    cur_L: T.int32 = q_indptr[b_idx] + (LH_start + i) // 4
                                                    cur_H_qo: T.int32 = by * 4 + (LH_start + i) % 4
                                                    if cur_L < q_indptr[b_idx + 1]:
                                                        lse[cur_L, cur_H_qo] = m_smem[i] + T.log2(d_smem[i])
                                    tile_id[0] = tile_id[0] + 16

    @R.function
    def alloc_embedding_tensor() -> R.Tensor((8192, 4096), dtype="float16"):
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        gv: R.Tensor((8192, 4096), dtype="float16") = R.builtin.alloc_tensor(R.shape([8192, 4096]), R.dtype("float16"), R.prim_value(0), R.str("global"))
        return gv

    @R.function
    def argsort_probs(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32")) -> R.Tuple(R.Tensor(("batch_size", "vocab_size"), dtype="float32"), R.Tensor(("batch_size", "vocab_size"), dtype="int32")):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv1: R.Tensor((batch_size, vocab_size), dtype="int32") = R.argsort(probs, axis=-1, descending=True, dtype="int32")
            lv2 = R.call_tir(cls.take_sorted_probs, (probs, lv1), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv1: R.Tuple(R.Tensor((batch_size, vocab_size), dtype="float32"), R.Tensor((batch_size, vocab_size), dtype="int32")) = lv2, lv1
            R.output(gv1)
        return gv1

    @R.function
    def batch_decode(input_embeds: R.Tensor(("batch_size", 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight7: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale7: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight7: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale7: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight7: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale7: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight7: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale7: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight7: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale7: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight7: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight7: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale7: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm325: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv806 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight7, model_layers_0_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims644: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv806, axes=None)
            matmul644: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm325, permute_dims644, out_dtype="void")
            reshape640: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul644, R.shape([batch_size, 1, 48, 128]))
            reshape641: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape640, R.shape([batch_size, 48, 128]))
            lv807 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape641), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape642: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv807, R.shape([batch_size, 1, 32, 128]))
            reshape643: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape642, R.shape([batch_size, 1, 4096]))
            lv808 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight7, model_layers_0_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims645: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv808, axes=None)
            matmul645: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape643, permute_dims645, out_dtype="void")
            add320: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul645, input_embeds)
            rms_norm326: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add320, model_layers_0_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv809 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight7, model_layers_0_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims646: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv809, axes=None)
            matmul646: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm326, permute_dims646, out_dtype="void")
            split160: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul646, indices_or_sections=2, axis=-1)
            split_0160: R.Tensor((batch_size, 1, 14336), dtype="float16") = split160[0]
            split_1160: R.Tensor((batch_size, 1, 14336), dtype="float16") = split160[1]
            silu160: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0160)
            mul160: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu160, split_1160)
            lv810 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight7, model_layers_0_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims647: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv810, axes=None)
            matmul647: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul160, permute_dims647, out_dtype="void")
            add321: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul647, add320)
            rms_norm327: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add321, model_layers_1_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv811 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight7, model_layers_1_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims648: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv811, axes=None)
            matmul648: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm327, permute_dims648, out_dtype="void")
            reshape644: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul648, R.shape([batch_size, 1, 48, 128]))
            reshape645: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape644, R.shape([batch_size, 48, 128]))
            lv812 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape645), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape646: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv812, R.shape([batch_size, 1, 32, 128]))
            reshape647: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape646, R.shape([batch_size, 1, 4096]))
            lv813 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight7, model_layers_1_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims649: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv813, axes=None)
            matmul649: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape647, permute_dims649, out_dtype="void")
            add322: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul649, add321)
            rms_norm328: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add322, model_layers_1_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv814 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight7, model_layers_1_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims650: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv814, axes=None)
            matmul650: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm328, permute_dims650, out_dtype="void")
            split161: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul650, indices_or_sections=2, axis=-1)
            split_0161: R.Tensor((batch_size, 1, 14336), dtype="float16") = split161[0]
            split_1161: R.Tensor((batch_size, 1, 14336), dtype="float16") = split161[1]
            silu161: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0161)
            mul161: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu161, split_1161)
            lv815 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight7, model_layers_1_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims651: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv815, axes=None)
            matmul651: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul161, permute_dims651, out_dtype="void")
            add323: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul651, add322)
            rms_norm329: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add323, model_layers_2_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv816 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight7, model_layers_2_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims652: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv816, axes=None)
            matmul652: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm329, permute_dims652, out_dtype="void")
            reshape648: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul652, R.shape([batch_size, 1, 48, 128]))
            reshape649: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape648, R.shape([batch_size, 48, 128]))
            lv817 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape649), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape650: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv817, R.shape([batch_size, 1, 32, 128]))
            reshape651: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape650, R.shape([batch_size, 1, 4096]))
            lv818 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight7, model_layers_2_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims653: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv818, axes=None)
            matmul653: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape651, permute_dims653, out_dtype="void")
            add324: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul653, add323)
            rms_norm330: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add324, model_layers_2_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv819 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight7, model_layers_2_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims654: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv819, axes=None)
            matmul654: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm330, permute_dims654, out_dtype="void")
            split162: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul654, indices_or_sections=2, axis=-1)
            split_0162: R.Tensor((batch_size, 1, 14336), dtype="float16") = split162[0]
            split_1162: R.Tensor((batch_size, 1, 14336), dtype="float16") = split162[1]
            silu162: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0162)
            mul162: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu162, split_1162)
            lv820 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight7, model_layers_2_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims655: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv820, axes=None)
            matmul655: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul162, permute_dims655, out_dtype="void")
            add325: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul655, add324)
            rms_norm331: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add325, model_layers_3_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv821 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight7, model_layers_3_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims656: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv821, axes=None)
            matmul656: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm331, permute_dims656, out_dtype="void")
            reshape652: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul656, R.shape([batch_size, 1, 48, 128]))
            reshape653: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape652, R.shape([batch_size, 48, 128]))
            lv822 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape653), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape654: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv822, R.shape([batch_size, 1, 32, 128]))
            reshape655: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape654, R.shape([batch_size, 1, 4096]))
            lv823 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight7, model_layers_3_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims657: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv823, axes=None)
            matmul657: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape655, permute_dims657, out_dtype="void")
            add326: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul657, add325)
            rms_norm332: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add326, model_layers_3_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv824 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight7, model_layers_3_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims658: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv824, axes=None)
            matmul658: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm332, permute_dims658, out_dtype="void")
            split163: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul658, indices_or_sections=2, axis=-1)
            split_0163: R.Tensor((batch_size, 1, 14336), dtype="float16") = split163[0]
            split_1163: R.Tensor((batch_size, 1, 14336), dtype="float16") = split163[1]
            silu163: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0163)
            mul163: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu163, split_1163)
            lv825 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight7, model_layers_3_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims659: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv825, axes=None)
            matmul659: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul163, permute_dims659, out_dtype="void")
            add327: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul659, add326)
            rms_norm333: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add327, model_layers_4_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv826 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight7, model_layers_4_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims660: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv826, axes=None)
            matmul660: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm333, permute_dims660, out_dtype="void")
            reshape656: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul660, R.shape([batch_size, 1, 48, 128]))
            reshape657: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape656, R.shape([batch_size, 48, 128]))
            lv827 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape657), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape658: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv827, R.shape([batch_size, 1, 32, 128]))
            reshape659: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape658, R.shape([batch_size, 1, 4096]))
            lv828 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight7, model_layers_4_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims661: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv828, axes=None)
            matmul661: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape659, permute_dims661, out_dtype="void")
            add328: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul661, add327)
            rms_norm334: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add328, model_layers_4_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv829 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight7, model_layers_4_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims662: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv829, axes=None)
            matmul662: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm334, permute_dims662, out_dtype="void")
            split164: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul662, indices_or_sections=2, axis=-1)
            split_0164: R.Tensor((batch_size, 1, 14336), dtype="float16") = split164[0]
            split_1164: R.Tensor((batch_size, 1, 14336), dtype="float16") = split164[1]
            silu164: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0164)
            mul164: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu164, split_1164)
            lv830 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight7, model_layers_4_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims663: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv830, axes=None)
            matmul663: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul164, permute_dims663, out_dtype="void")
            add329: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul663, add328)
            rms_norm335: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add329, model_layers_5_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv831 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight7, model_layers_5_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims664: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv831, axes=None)
            matmul664: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm335, permute_dims664, out_dtype="void")
            reshape660: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul664, R.shape([batch_size, 1, 48, 128]))
            reshape661: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape660, R.shape([batch_size, 48, 128]))
            lv832 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape661), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape662: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv832, R.shape([batch_size, 1, 32, 128]))
            reshape663: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape662, R.shape([batch_size, 1, 4096]))
            lv833 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight7, model_layers_5_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims665: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv833, axes=None)
            matmul665: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape663, permute_dims665, out_dtype="void")
            add330: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul665, add329)
            rms_norm336: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add330, model_layers_5_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv834 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight7, model_layers_5_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims666: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv834, axes=None)
            matmul666: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm336, permute_dims666, out_dtype="void")
            split165: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul666, indices_or_sections=2, axis=-1)
            split_0165: R.Tensor((batch_size, 1, 14336), dtype="float16") = split165[0]
            split_1165: R.Tensor((batch_size, 1, 14336), dtype="float16") = split165[1]
            silu165: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0165)
            mul165: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu165, split_1165)
            lv835 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight7, model_layers_5_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims667: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv835, axes=None)
            matmul667: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul165, permute_dims667, out_dtype="void")
            add331: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul667, add330)
            rms_norm337: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add331, model_layers_6_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv836 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight7, model_layers_6_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims668: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv836, axes=None)
            matmul668: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm337, permute_dims668, out_dtype="void")
            reshape664: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul668, R.shape([batch_size, 1, 48, 128]))
            reshape665: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape664, R.shape([batch_size, 48, 128]))
            lv837 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape665), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape666: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv837, R.shape([batch_size, 1, 32, 128]))
            reshape667: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape666, R.shape([batch_size, 1, 4096]))
            lv838 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight7, model_layers_6_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims669: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv838, axes=None)
            matmul669: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape667, permute_dims669, out_dtype="void")
            add332: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul669, add331)
            rms_norm338: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add332, model_layers_6_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv839 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight7, model_layers_6_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims670: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv839, axes=None)
            matmul670: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm338, permute_dims670, out_dtype="void")
            split166: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul670, indices_or_sections=2, axis=-1)
            split_0166: R.Tensor((batch_size, 1, 14336), dtype="float16") = split166[0]
            split_1166: R.Tensor((batch_size, 1, 14336), dtype="float16") = split166[1]
            silu166: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0166)
            mul166: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu166, split_1166)
            lv840 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight7, model_layers_6_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims671: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv840, axes=None)
            matmul671: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul166, permute_dims671, out_dtype="void")
            add333: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul671, add332)
            rms_norm339: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add333, model_layers_7_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv841 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight7, model_layers_7_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims672: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv841, axes=None)
            matmul672: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm339, permute_dims672, out_dtype="void")
            reshape668: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul672, R.shape([batch_size, 1, 48, 128]))
            reshape669: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape668, R.shape([batch_size, 48, 128]))
            lv842 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape669), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape670: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv842, R.shape([batch_size, 1, 32, 128]))
            reshape671: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape670, R.shape([batch_size, 1, 4096]))
            lv843 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight7, model_layers_7_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims673: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv843, axes=None)
            matmul673: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape671, permute_dims673, out_dtype="void")
            add334: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul673, add333)
            rms_norm340: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add334, model_layers_7_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv844 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight7, model_layers_7_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims674: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv844, axes=None)
            matmul674: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm340, permute_dims674, out_dtype="void")
            split167: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul674, indices_or_sections=2, axis=-1)
            split_0167: R.Tensor((batch_size, 1, 14336), dtype="float16") = split167[0]
            split_1167: R.Tensor((batch_size, 1, 14336), dtype="float16") = split167[1]
            silu167: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0167)
            mul167: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu167, split_1167)
            lv845 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight7, model_layers_7_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims675: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv845, axes=None)
            matmul675: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul167, permute_dims675, out_dtype="void")
            add335: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul675, add334)
            rms_norm341: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add335, model_layers_8_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv846 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight7, model_layers_8_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims676: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv846, axes=None)
            matmul676: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm341, permute_dims676, out_dtype="void")
            reshape672: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul676, R.shape([batch_size, 1, 48, 128]))
            reshape673: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape672, R.shape([batch_size, 48, 128]))
            lv847 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape673), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape674: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv847, R.shape([batch_size, 1, 32, 128]))
            reshape675: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape674, R.shape([batch_size, 1, 4096]))
            lv848 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight7, model_layers_8_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims677: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv848, axes=None)
            matmul677: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape675, permute_dims677, out_dtype="void")
            add336: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul677, add335)
            rms_norm342: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add336, model_layers_8_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv849 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight7, model_layers_8_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims678: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv849, axes=None)
            matmul678: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm342, permute_dims678, out_dtype="void")
            split168: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul678, indices_or_sections=2, axis=-1)
            split_0168: R.Tensor((batch_size, 1, 14336), dtype="float16") = split168[0]
            split_1168: R.Tensor((batch_size, 1, 14336), dtype="float16") = split168[1]
            silu168: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0168)
            mul168: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu168, split_1168)
            lv850 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight7, model_layers_8_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims679: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv850, axes=None)
            matmul679: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul168, permute_dims679, out_dtype="void")
            add337: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul679, add336)
            rms_norm343: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add337, model_layers_9_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv851 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight7, model_layers_9_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims680: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv851, axes=None)
            matmul680: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm343, permute_dims680, out_dtype="void")
            reshape676: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul680, R.shape([batch_size, 1, 48, 128]))
            reshape677: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape676, R.shape([batch_size, 48, 128]))
            lv852 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape677), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape678: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv852, R.shape([batch_size, 1, 32, 128]))
            reshape679: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape678, R.shape([batch_size, 1, 4096]))
            lv853 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight7, model_layers_9_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims681: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv853, axes=None)
            matmul681: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape679, permute_dims681, out_dtype="void")
            add338: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul681, add337)
            rms_norm344: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add338, model_layers_9_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv854 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight7, model_layers_9_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims682: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv854, axes=None)
            matmul682: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm344, permute_dims682, out_dtype="void")
            split169: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul682, indices_or_sections=2, axis=-1)
            split_0169: R.Tensor((batch_size, 1, 14336), dtype="float16") = split169[0]
            split_1169: R.Tensor((batch_size, 1, 14336), dtype="float16") = split169[1]
            silu169: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0169)
            mul169: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu169, split_1169)
            lv855 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight7, model_layers_9_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims683: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv855, axes=None)
            matmul683: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul169, permute_dims683, out_dtype="void")
            add339: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul683, add338)
            rms_norm345: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add339, model_layers_10_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv856 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight7, model_layers_10_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims684: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv856, axes=None)
            matmul684: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm345, permute_dims684, out_dtype="void")
            reshape680: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul684, R.shape([batch_size, 1, 48, 128]))
            reshape681: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape680, R.shape([batch_size, 48, 128]))
            lv857 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape681), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape682: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv857, R.shape([batch_size, 1, 32, 128]))
            reshape683: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape682, R.shape([batch_size, 1, 4096]))
            lv858 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight7, model_layers_10_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims685: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv858, axes=None)
            matmul685: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape683, permute_dims685, out_dtype="void")
            add340: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul685, add339)
            rms_norm346: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add340, model_layers_10_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv859 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight7, model_layers_10_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims686: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv859, axes=None)
            matmul686: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm346, permute_dims686, out_dtype="void")
            split170: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul686, indices_or_sections=2, axis=-1)
            split_0170: R.Tensor((batch_size, 1, 14336), dtype="float16") = split170[0]
            split_1170: R.Tensor((batch_size, 1, 14336), dtype="float16") = split170[1]
            silu170: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0170)
            mul170: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu170, split_1170)
            lv860 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight7, model_layers_10_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims687: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv860, axes=None)
            matmul687: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul170, permute_dims687, out_dtype="void")
            add341: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul687, add340)
            rms_norm347: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add341, model_layers_11_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv861 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight7, model_layers_11_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims688: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv861, axes=None)
            matmul688: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm347, permute_dims688, out_dtype="void")
            reshape684: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul688, R.shape([batch_size, 1, 48, 128]))
            reshape685: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape684, R.shape([batch_size, 48, 128]))
            lv862 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape685), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape686: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv862, R.shape([batch_size, 1, 32, 128]))
            reshape687: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape686, R.shape([batch_size, 1, 4096]))
            lv863 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight7, model_layers_11_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims689: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv863, axes=None)
            matmul689: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape687, permute_dims689, out_dtype="void")
            add342: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul689, add341)
            rms_norm348: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add342, model_layers_11_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv864 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight7, model_layers_11_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims690: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv864, axes=None)
            matmul690: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm348, permute_dims690, out_dtype="void")
            split171: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul690, indices_or_sections=2, axis=-1)
            split_0171: R.Tensor((batch_size, 1, 14336), dtype="float16") = split171[0]
            split_1171: R.Tensor((batch_size, 1, 14336), dtype="float16") = split171[1]
            silu171: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0171)
            mul171: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu171, split_1171)
            lv865 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight7, model_layers_11_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims691: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv865, axes=None)
            matmul691: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul171, permute_dims691, out_dtype="void")
            add343: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul691, add342)
            rms_norm349: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add343, model_layers_12_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv866 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight7, model_layers_12_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims692: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv866, axes=None)
            matmul692: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm349, permute_dims692, out_dtype="void")
            reshape688: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul692, R.shape([batch_size, 1, 48, 128]))
            reshape689: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape688, R.shape([batch_size, 48, 128]))
            lv867 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape689), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape690: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv867, R.shape([batch_size, 1, 32, 128]))
            reshape691: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape690, R.shape([batch_size, 1, 4096]))
            lv868 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight7, model_layers_12_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims693: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv868, axes=None)
            matmul693: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape691, permute_dims693, out_dtype="void")
            add344: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul693, add343)
            rms_norm350: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add344, model_layers_12_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv869 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight7, model_layers_12_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims694: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv869, axes=None)
            matmul694: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm350, permute_dims694, out_dtype="void")
            split172: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul694, indices_or_sections=2, axis=-1)
            split_0172: R.Tensor((batch_size, 1, 14336), dtype="float16") = split172[0]
            split_1172: R.Tensor((batch_size, 1, 14336), dtype="float16") = split172[1]
            silu172: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0172)
            mul172: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu172, split_1172)
            lv870 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight7, model_layers_12_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims695: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv870, axes=None)
            matmul695: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul172, permute_dims695, out_dtype="void")
            add345: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul695, add344)
            rms_norm351: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add345, model_layers_13_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv871 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight7, model_layers_13_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims696: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv871, axes=None)
            matmul696: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm351, permute_dims696, out_dtype="void")
            reshape692: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul696, R.shape([batch_size, 1, 48, 128]))
            reshape693: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape692, R.shape([batch_size, 48, 128]))
            lv872 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape693), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape694: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv872, R.shape([batch_size, 1, 32, 128]))
            reshape695: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape694, R.shape([batch_size, 1, 4096]))
            lv873 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight7, model_layers_13_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims697: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv873, axes=None)
            matmul697: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape695, permute_dims697, out_dtype="void")
            add346: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul697, add345)
            rms_norm352: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add346, model_layers_13_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv874 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight7, model_layers_13_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims698: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv874, axes=None)
            matmul698: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm352, permute_dims698, out_dtype="void")
            split173: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul698, indices_or_sections=2, axis=-1)
            split_0173: R.Tensor((batch_size, 1, 14336), dtype="float16") = split173[0]
            split_1173: R.Tensor((batch_size, 1, 14336), dtype="float16") = split173[1]
            silu173: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0173)
            mul173: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu173, split_1173)
            lv875 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight7, model_layers_13_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims699: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv875, axes=None)
            matmul699: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul173, permute_dims699, out_dtype="void")
            add347: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul699, add346)
            rms_norm353: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add347, model_layers_14_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv876 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight7, model_layers_14_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims700: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv876, axes=None)
            matmul700: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm353, permute_dims700, out_dtype="void")
            reshape696: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul700, R.shape([batch_size, 1, 48, 128]))
            reshape697: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape696, R.shape([batch_size, 48, 128]))
            lv877 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape697), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape698: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv877, R.shape([batch_size, 1, 32, 128]))
            reshape699: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape698, R.shape([batch_size, 1, 4096]))
            lv878 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight7, model_layers_14_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims701: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv878, axes=None)
            matmul701: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape699, permute_dims701, out_dtype="void")
            add348: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul701, add347)
            rms_norm354: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add348, model_layers_14_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv879 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight7, model_layers_14_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims702: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv879, axes=None)
            matmul702: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm354, permute_dims702, out_dtype="void")
            split174: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul702, indices_or_sections=2, axis=-1)
            split_0174: R.Tensor((batch_size, 1, 14336), dtype="float16") = split174[0]
            split_1174: R.Tensor((batch_size, 1, 14336), dtype="float16") = split174[1]
            silu174: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0174)
            mul174: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu174, split_1174)
            lv880 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight7, model_layers_14_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims703: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv880, axes=None)
            matmul703: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul174, permute_dims703, out_dtype="void")
            add349: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul703, add348)
            rms_norm355: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add349, model_layers_15_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv881 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight7, model_layers_15_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims704: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv881, axes=None)
            matmul704: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm355, permute_dims704, out_dtype="void")
            reshape700: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul704, R.shape([batch_size, 1, 48, 128]))
            reshape701: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape700, R.shape([batch_size, 48, 128]))
            lv882 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape701), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape702: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv882, R.shape([batch_size, 1, 32, 128]))
            reshape703: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape702, R.shape([batch_size, 1, 4096]))
            lv883 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight7, model_layers_15_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims705: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv883, axes=None)
            matmul705: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape703, permute_dims705, out_dtype="void")
            add350: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul705, add349)
            rms_norm356: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add350, model_layers_15_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv884 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight7, model_layers_15_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims706: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv884, axes=None)
            matmul706: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm356, permute_dims706, out_dtype="void")
            split175: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul706, indices_or_sections=2, axis=-1)
            split_0175: R.Tensor((batch_size, 1, 14336), dtype="float16") = split175[0]
            split_1175: R.Tensor((batch_size, 1, 14336), dtype="float16") = split175[1]
            silu175: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0175)
            mul175: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu175, split_1175)
            lv885 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight7, model_layers_15_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims707: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv885, axes=None)
            matmul707: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul175, permute_dims707, out_dtype="void")
            add351: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul707, add350)
            rms_norm357: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add351, model_layers_16_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv886 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight7, model_layers_16_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims708: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv886, axes=None)
            matmul708: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm357, permute_dims708, out_dtype="void")
            reshape704: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul708, R.shape([batch_size, 1, 48, 128]))
            reshape705: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape704, R.shape([batch_size, 48, 128]))
            lv887 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape705), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape706: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv887, R.shape([batch_size, 1, 32, 128]))
            reshape707: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape706, R.shape([batch_size, 1, 4096]))
            lv888 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight7, model_layers_16_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims709: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv888, axes=None)
            matmul709: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape707, permute_dims709, out_dtype="void")
            add352: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul709, add351)
            rms_norm358: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add352, model_layers_16_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv889 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight7, model_layers_16_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims710: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv889, axes=None)
            matmul710: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm358, permute_dims710, out_dtype="void")
            split176: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul710, indices_or_sections=2, axis=-1)
            split_0176: R.Tensor((batch_size, 1, 14336), dtype="float16") = split176[0]
            split_1176: R.Tensor((batch_size, 1, 14336), dtype="float16") = split176[1]
            silu176: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0176)
            mul176: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu176, split_1176)
            lv890 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight7, model_layers_16_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims711: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv890, axes=None)
            matmul711: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul176, permute_dims711, out_dtype="void")
            add353: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul711, add352)
            rms_norm359: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add353, model_layers_17_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv891 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight7, model_layers_17_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims712: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv891, axes=None)
            matmul712: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm359, permute_dims712, out_dtype="void")
            reshape708: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul712, R.shape([batch_size, 1, 48, 128]))
            reshape709: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape708, R.shape([batch_size, 48, 128]))
            lv892 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape709), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape710: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv892, R.shape([batch_size, 1, 32, 128]))
            reshape711: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape710, R.shape([batch_size, 1, 4096]))
            lv893 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight7, model_layers_17_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims713: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv893, axes=None)
            matmul713: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape711, permute_dims713, out_dtype="void")
            add354: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul713, add353)
            rms_norm360: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add354, model_layers_17_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv894 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight7, model_layers_17_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims714: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv894, axes=None)
            matmul714: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm360, permute_dims714, out_dtype="void")
            split177: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul714, indices_or_sections=2, axis=-1)
            split_0177: R.Tensor((batch_size, 1, 14336), dtype="float16") = split177[0]
            split_1177: R.Tensor((batch_size, 1, 14336), dtype="float16") = split177[1]
            silu177: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0177)
            mul177: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu177, split_1177)
            lv895 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight7, model_layers_17_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims715: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv895, axes=None)
            matmul715: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul177, permute_dims715, out_dtype="void")
            add355: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul715, add354)
            rms_norm361: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add355, model_layers_18_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv896 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight7, model_layers_18_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims716: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv896, axes=None)
            matmul716: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm361, permute_dims716, out_dtype="void")
            reshape712: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul716, R.shape([batch_size, 1, 48, 128]))
            reshape713: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape712, R.shape([batch_size, 48, 128]))
            lv897 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape713), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape714: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv897, R.shape([batch_size, 1, 32, 128]))
            reshape715: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape714, R.shape([batch_size, 1, 4096]))
            lv898 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight7, model_layers_18_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims717: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv898, axes=None)
            matmul717: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape715, permute_dims717, out_dtype="void")
            add356: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul717, add355)
            rms_norm362: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add356, model_layers_18_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv899 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight7, model_layers_18_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims718: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv899, axes=None)
            matmul718: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm362, permute_dims718, out_dtype="void")
            split178: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul718, indices_or_sections=2, axis=-1)
            split_0178: R.Tensor((batch_size, 1, 14336), dtype="float16") = split178[0]
            split_1178: R.Tensor((batch_size, 1, 14336), dtype="float16") = split178[1]
            silu178: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0178)
            mul178: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu178, split_1178)
            lv900 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight7, model_layers_18_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims719: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv900, axes=None)
            matmul719: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul178, permute_dims719, out_dtype="void")
            add357: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul719, add356)
            rms_norm363: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add357, model_layers_19_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv901 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight7, model_layers_19_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims720: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv901, axes=None)
            matmul720: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm363, permute_dims720, out_dtype="void")
            reshape716: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul720, R.shape([batch_size, 1, 48, 128]))
            reshape717: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape716, R.shape([batch_size, 48, 128]))
            lv902 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape717), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape718: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv902, R.shape([batch_size, 1, 32, 128]))
            reshape719: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape718, R.shape([batch_size, 1, 4096]))
            lv903 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight7, model_layers_19_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims721: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv903, axes=None)
            matmul721: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape719, permute_dims721, out_dtype="void")
            add358: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul721, add357)
            rms_norm364: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add358, model_layers_19_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv904 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight7, model_layers_19_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims722: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv904, axes=None)
            matmul722: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm364, permute_dims722, out_dtype="void")
            split179: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul722, indices_or_sections=2, axis=-1)
            split_0179: R.Tensor((batch_size, 1, 14336), dtype="float16") = split179[0]
            split_1179: R.Tensor((batch_size, 1, 14336), dtype="float16") = split179[1]
            silu179: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0179)
            mul179: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu179, split_1179)
            lv905 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight7, model_layers_19_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims723: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv905, axes=None)
            matmul723: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul179, permute_dims723, out_dtype="void")
            add359: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul723, add358)
            rms_norm365: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add359, model_layers_20_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv906 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight7, model_layers_20_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims724: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv906, axes=None)
            matmul724: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm365, permute_dims724, out_dtype="void")
            reshape720: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul724, R.shape([batch_size, 1, 48, 128]))
            reshape721: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape720, R.shape([batch_size, 48, 128]))
            lv907 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape721), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape722: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv907, R.shape([batch_size, 1, 32, 128]))
            reshape723: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape722, R.shape([batch_size, 1, 4096]))
            lv908 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight7, model_layers_20_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims725: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv908, axes=None)
            matmul725: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape723, permute_dims725, out_dtype="void")
            add360: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul725, add359)
            rms_norm366: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add360, model_layers_20_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv909 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight7, model_layers_20_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims726: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv909, axes=None)
            matmul726: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm366, permute_dims726, out_dtype="void")
            split180: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul726, indices_or_sections=2, axis=-1)
            split_0180: R.Tensor((batch_size, 1, 14336), dtype="float16") = split180[0]
            split_1180: R.Tensor((batch_size, 1, 14336), dtype="float16") = split180[1]
            silu180: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0180)
            mul180: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu180, split_1180)
            lv910 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight7, model_layers_20_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims727: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv910, axes=None)
            matmul727: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul180, permute_dims727, out_dtype="void")
            add361: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul727, add360)
            rms_norm367: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add361, model_layers_21_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv911 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight7, model_layers_21_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims728: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv911, axes=None)
            matmul728: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm367, permute_dims728, out_dtype="void")
            reshape724: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul728, R.shape([batch_size, 1, 48, 128]))
            reshape725: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape724, R.shape([batch_size, 48, 128]))
            lv912 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape725), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape726: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv912, R.shape([batch_size, 1, 32, 128]))
            reshape727: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape726, R.shape([batch_size, 1, 4096]))
            lv913 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight7, model_layers_21_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims729: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv913, axes=None)
            matmul729: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape727, permute_dims729, out_dtype="void")
            add362: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul729, add361)
            rms_norm368: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add362, model_layers_21_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv914 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight7, model_layers_21_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims730: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv914, axes=None)
            matmul730: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm368, permute_dims730, out_dtype="void")
            split181: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul730, indices_or_sections=2, axis=-1)
            split_0181: R.Tensor((batch_size, 1, 14336), dtype="float16") = split181[0]
            split_1181: R.Tensor((batch_size, 1, 14336), dtype="float16") = split181[1]
            silu181: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0181)
            mul181: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu181, split_1181)
            lv915 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight7, model_layers_21_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims731: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv915, axes=None)
            matmul731: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul181, permute_dims731, out_dtype="void")
            add363: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul731, add362)
            rms_norm369: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add363, model_layers_22_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv916 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight7, model_layers_22_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims732: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv916, axes=None)
            matmul732: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm369, permute_dims732, out_dtype="void")
            reshape728: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul732, R.shape([batch_size, 1, 48, 128]))
            reshape729: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape728, R.shape([batch_size, 48, 128]))
            lv917 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape729), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape730: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv917, R.shape([batch_size, 1, 32, 128]))
            reshape731: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape730, R.shape([batch_size, 1, 4096]))
            lv918 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight7, model_layers_22_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims733: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv918, axes=None)
            matmul733: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape731, permute_dims733, out_dtype="void")
            add364: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul733, add363)
            rms_norm370: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add364, model_layers_22_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv919 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight7, model_layers_22_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims734: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv919, axes=None)
            matmul734: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm370, permute_dims734, out_dtype="void")
            split182: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul734, indices_or_sections=2, axis=-1)
            split_0182: R.Tensor((batch_size, 1, 14336), dtype="float16") = split182[0]
            split_1182: R.Tensor((batch_size, 1, 14336), dtype="float16") = split182[1]
            silu182: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0182)
            mul182: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu182, split_1182)
            lv920 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight7, model_layers_22_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims735: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv920, axes=None)
            matmul735: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul182, permute_dims735, out_dtype="void")
            add365: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul735, add364)
            rms_norm371: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add365, model_layers_23_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv921 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight7, model_layers_23_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims736: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv921, axes=None)
            matmul736: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm371, permute_dims736, out_dtype="void")
            reshape732: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul736, R.shape([batch_size, 1, 48, 128]))
            reshape733: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape732, R.shape([batch_size, 48, 128]))
            lv922 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape733), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape734: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv922, R.shape([batch_size, 1, 32, 128]))
            reshape735: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape734, R.shape([batch_size, 1, 4096]))
            lv923 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight7, model_layers_23_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims737: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv923, axes=None)
            matmul737: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape735, permute_dims737, out_dtype="void")
            add366: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul737, add365)
            rms_norm372: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add366, model_layers_23_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv924 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight7, model_layers_23_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims738: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv924, axes=None)
            matmul738: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm372, permute_dims738, out_dtype="void")
            split183: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul738, indices_or_sections=2, axis=-1)
            split_0183: R.Tensor((batch_size, 1, 14336), dtype="float16") = split183[0]
            split_1183: R.Tensor((batch_size, 1, 14336), dtype="float16") = split183[1]
            silu183: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0183)
            mul183: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu183, split_1183)
            lv925 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight7, model_layers_23_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims739: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv925, axes=None)
            matmul739: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul183, permute_dims739, out_dtype="void")
            add367: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul739, add366)
            rms_norm373: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add367, model_layers_24_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv926 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight7, model_layers_24_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims740: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv926, axes=None)
            matmul740: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm373, permute_dims740, out_dtype="void")
            reshape736: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul740, R.shape([batch_size, 1, 48, 128]))
            reshape737: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape736, R.shape([batch_size, 48, 128]))
            lv927 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape737), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape738: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv927, R.shape([batch_size, 1, 32, 128]))
            reshape739: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape738, R.shape([batch_size, 1, 4096]))
            lv928 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight7, model_layers_24_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims741: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv928, axes=None)
            matmul741: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape739, permute_dims741, out_dtype="void")
            add368: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul741, add367)
            rms_norm374: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add368, model_layers_24_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv929 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight7, model_layers_24_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims742: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv929, axes=None)
            matmul742: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm374, permute_dims742, out_dtype="void")
            split184: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul742, indices_or_sections=2, axis=-1)
            split_0184: R.Tensor((batch_size, 1, 14336), dtype="float16") = split184[0]
            split_1184: R.Tensor((batch_size, 1, 14336), dtype="float16") = split184[1]
            silu184: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0184)
            mul184: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu184, split_1184)
            lv930 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight7, model_layers_24_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims743: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv930, axes=None)
            matmul743: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul184, permute_dims743, out_dtype="void")
            add369: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul743, add368)
            rms_norm375: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add369, model_layers_25_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv931 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight7, model_layers_25_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims744: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv931, axes=None)
            matmul744: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm375, permute_dims744, out_dtype="void")
            reshape740: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul744, R.shape([batch_size, 1, 48, 128]))
            reshape741: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape740, R.shape([batch_size, 48, 128]))
            lv932 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape741), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape742: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv932, R.shape([batch_size, 1, 32, 128]))
            reshape743: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape742, R.shape([batch_size, 1, 4096]))
            lv933 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight7, model_layers_25_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims745: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv933, axes=None)
            matmul745: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape743, permute_dims745, out_dtype="void")
            add370: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul745, add369)
            rms_norm376: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add370, model_layers_25_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv934 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight7, model_layers_25_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims746: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv934, axes=None)
            matmul746: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm376, permute_dims746, out_dtype="void")
            split185: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul746, indices_or_sections=2, axis=-1)
            split_0185: R.Tensor((batch_size, 1, 14336), dtype="float16") = split185[0]
            split_1185: R.Tensor((batch_size, 1, 14336), dtype="float16") = split185[1]
            silu185: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0185)
            mul185: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu185, split_1185)
            lv935 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight7, model_layers_25_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims747: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv935, axes=None)
            matmul747: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul185, permute_dims747, out_dtype="void")
            add371: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul747, add370)
            rms_norm377: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add371, model_layers_26_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv936 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight7, model_layers_26_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims748: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv936, axes=None)
            matmul748: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm377, permute_dims748, out_dtype="void")
            reshape744: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul748, R.shape([batch_size, 1, 48, 128]))
            reshape745: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape744, R.shape([batch_size, 48, 128]))
            lv937 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape745), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape746: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv937, R.shape([batch_size, 1, 32, 128]))
            reshape747: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape746, R.shape([batch_size, 1, 4096]))
            lv938 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight7, model_layers_26_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims749: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv938, axes=None)
            matmul749: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape747, permute_dims749, out_dtype="void")
            add372: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul749, add371)
            rms_norm378: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add372, model_layers_26_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv939 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight7, model_layers_26_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims750: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv939, axes=None)
            matmul750: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm378, permute_dims750, out_dtype="void")
            split186: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul750, indices_or_sections=2, axis=-1)
            split_0186: R.Tensor((batch_size, 1, 14336), dtype="float16") = split186[0]
            split_1186: R.Tensor((batch_size, 1, 14336), dtype="float16") = split186[1]
            silu186: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0186)
            mul186: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu186, split_1186)
            lv940 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight7, model_layers_26_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims751: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv940, axes=None)
            matmul751: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul186, permute_dims751, out_dtype="void")
            add373: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul751, add372)
            rms_norm379: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add373, model_layers_27_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv941 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight7, model_layers_27_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims752: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv941, axes=None)
            matmul752: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm379, permute_dims752, out_dtype="void")
            reshape748: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul752, R.shape([batch_size, 1, 48, 128]))
            reshape749: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape748, R.shape([batch_size, 48, 128]))
            lv942 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape749), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape750: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv942, R.shape([batch_size, 1, 32, 128]))
            reshape751: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape750, R.shape([batch_size, 1, 4096]))
            lv943 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight7, model_layers_27_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims753: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv943, axes=None)
            matmul753: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape751, permute_dims753, out_dtype="void")
            add374: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul753, add373)
            rms_norm380: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add374, model_layers_27_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv944 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight7, model_layers_27_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims754: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv944, axes=None)
            matmul754: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm380, permute_dims754, out_dtype="void")
            split187: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul754, indices_or_sections=2, axis=-1)
            split_0187: R.Tensor((batch_size, 1, 14336), dtype="float16") = split187[0]
            split_1187: R.Tensor((batch_size, 1, 14336), dtype="float16") = split187[1]
            silu187: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0187)
            mul187: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu187, split_1187)
            lv945 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight7, model_layers_27_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims755: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv945, axes=None)
            matmul755: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul187, permute_dims755, out_dtype="void")
            add375: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul755, add374)
            rms_norm381: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add375, model_layers_28_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv946 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight7, model_layers_28_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims756: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv946, axes=None)
            matmul756: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm381, permute_dims756, out_dtype="void")
            reshape752: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul756, R.shape([batch_size, 1, 48, 128]))
            reshape753: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape752, R.shape([batch_size, 48, 128]))
            lv947 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape753), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape754: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv947, R.shape([batch_size, 1, 32, 128]))
            reshape755: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape754, R.shape([batch_size, 1, 4096]))
            lv948 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight7, model_layers_28_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims757: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv948, axes=None)
            matmul757: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape755, permute_dims757, out_dtype="void")
            add376: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul757, add375)
            rms_norm382: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add376, model_layers_28_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv949 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight7, model_layers_28_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims758: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv949, axes=None)
            matmul758: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm382, permute_dims758, out_dtype="void")
            split188: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul758, indices_or_sections=2, axis=-1)
            split_0188: R.Tensor((batch_size, 1, 14336), dtype="float16") = split188[0]
            split_1188: R.Tensor((batch_size, 1, 14336), dtype="float16") = split188[1]
            silu188: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0188)
            mul188: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu188, split_1188)
            lv950 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight7, model_layers_28_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims759: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv950, axes=None)
            matmul759: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul188, permute_dims759, out_dtype="void")
            add377: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul759, add376)
            rms_norm383: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add377, model_layers_29_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv951 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight7, model_layers_29_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims760: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv951, axes=None)
            matmul760: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm383, permute_dims760, out_dtype="void")
            reshape756: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul760, R.shape([batch_size, 1, 48, 128]))
            reshape757: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape756, R.shape([batch_size, 48, 128]))
            lv952 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape757), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape758: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv952, R.shape([batch_size, 1, 32, 128]))
            reshape759: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape758, R.shape([batch_size, 1, 4096]))
            lv953 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight7, model_layers_29_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims761: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv953, axes=None)
            matmul761: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape759, permute_dims761, out_dtype="void")
            add378: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul761, add377)
            rms_norm384: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add378, model_layers_29_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv954 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight7, model_layers_29_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims762: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv954, axes=None)
            matmul762: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm384, permute_dims762, out_dtype="void")
            split189: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul762, indices_or_sections=2, axis=-1)
            split_0189: R.Tensor((batch_size, 1, 14336), dtype="float16") = split189[0]
            split_1189: R.Tensor((batch_size, 1, 14336), dtype="float16") = split189[1]
            silu189: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0189)
            mul189: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu189, split_1189)
            lv955 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight7, model_layers_29_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims763: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv955, axes=None)
            matmul763: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul189, permute_dims763, out_dtype="void")
            add379: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul763, add378)
            rms_norm385: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add379, model_layers_30_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv956 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight7, model_layers_30_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims764: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv956, axes=None)
            matmul764: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm385, permute_dims764, out_dtype="void")
            reshape760: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul764, R.shape([batch_size, 1, 48, 128]))
            reshape761: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape760, R.shape([batch_size, 48, 128]))
            lv957 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape761), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape762: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv957, R.shape([batch_size, 1, 32, 128]))
            reshape763: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape762, R.shape([batch_size, 1, 4096]))
            lv958 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight7, model_layers_30_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims765: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv958, axes=None)
            matmul765: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape763, permute_dims765, out_dtype="void")
            add380: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul765, add379)
            rms_norm386: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add380, model_layers_30_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv959 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight7, model_layers_30_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims766: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv959, axes=None)
            matmul766: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm386, permute_dims766, out_dtype="void")
            split190: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul766, indices_or_sections=2, axis=-1)
            split_0190: R.Tensor((batch_size, 1, 14336), dtype="float16") = split190[0]
            split_1190: R.Tensor((batch_size, 1, 14336), dtype="float16") = split190[1]
            silu190: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0190)
            mul190: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu190, split_1190)
            lv960 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight7, model_layers_30_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims767: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv960, axes=None)
            matmul767: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul190, permute_dims767, out_dtype="void")
            add381: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul767, add380)
            rms_norm387: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add381, model_layers_31_input_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv961 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight7, model_layers_31_self_attn_qkv_proj_q_scale7), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims768: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv961, axes=None)
            matmul768: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm387, permute_dims768, out_dtype="void")
            reshape764: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul768, R.shape([batch_size, 1, 48, 128]))
            reshape765: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape764, R.shape([batch_size, 48, 128]))
            lv962 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape765), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape766: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv962, R.shape([batch_size, 1, 32, 128]))
            reshape767: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape766, R.shape([batch_size, 1, 4096]))
            lv963 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight7, model_layers_31_self_attn_o_proj_q_scale7), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims769: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv963, axes=None)
            matmul769: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape767, permute_dims769, out_dtype="void")
            add382: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul769, add381)
            rms_norm388: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add382, model_layers_31_post_attention_layernorm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv964 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight7, model_layers_31_mlp_gate_up_proj_q_scale7), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims770: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv964, axes=None)
            matmul770: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm388, permute_dims770, out_dtype="void")
            split191: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul770, indices_or_sections=2, axis=-1)
            split_0191: R.Tensor((batch_size, 1, 14336), dtype="float16") = split191[0]
            split_1191: R.Tensor((batch_size, 1, 14336), dtype="float16") = split191[1]
            silu191: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0191)
            mul191: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu191, split_1191)
            lv965 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight7, model_layers_31_mlp_down_proj_q_scale7), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims771: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv965, axes=None)
            matmul771: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul191, permute_dims771, out_dtype="void")
            add383: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul771, add382)
            rms_norm389: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add383, model_norm_weight7, axes=[-1], epsilon=1.0000000000000001e-05)
            lv966 = R.call_tir(cls.dequantize, (lm_head_q_weight7, lm_head_q_scale7), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            permute_dims772: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv966, axes=None)
            matmul772: R.Tensor((batch_size, 1, vocab_size), dtype="float16") = R.matmul(rms_norm389, permute_dims772, out_dtype="void")
            astype4: R.Tensor((batch_size, 1, vocab_size), dtype="float32") = R.astype(matmul772, dtype="float32")
            gv8: R.Tuple(R.Tensor((batch_size, 1, vocab_size), dtype="float32"), R.Object) = astype4, paged_kv_cache
            R.output(gv8)
        return gv8

    @R.function
    def batch_decode_to_last_hidden_states(input_embeds: R.Tensor(("batch_size", 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor(("batch_size", 1, 4096), dtype="float16"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight10: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale10: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight10: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale10: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight10: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale10: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight10: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale10: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight10: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale10: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight10: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight10: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale10: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm520: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1288 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight10, model_layers_0_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1030: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1288, axes=None)
            matmul1030: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm520, permute_dims1030, out_dtype="void")
            reshape1024: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1030, R.shape([batch_size, 1, 48, 128]))
            reshape1025: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1024, R.shape([batch_size, 48, 128]))
            lv1289 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1025), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1026: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1289, R.shape([batch_size, 1, 32, 128]))
            reshape1027: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1026, R.shape([batch_size, 1, 4096]))
            lv1290 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight10, model_layers_0_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1031: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1290, axes=None)
            matmul1031: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1027, permute_dims1031, out_dtype="void")
            add512: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1031, input_embeds)
            rms_norm521: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add512, model_layers_0_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1291 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight10, model_layers_0_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1032: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1291, axes=None)
            matmul1032: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm521, permute_dims1032, out_dtype="void")
            split256: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1032, indices_or_sections=2, axis=-1)
            split_0256: R.Tensor((batch_size, 1, 14336), dtype="float16") = split256[0]
            split_1256: R.Tensor((batch_size, 1, 14336), dtype="float16") = split256[1]
            silu256: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0256)
            mul256: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu256, split_1256)
            lv1292 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight10, model_layers_0_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1033: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1292, axes=None)
            matmul1033: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul256, permute_dims1033, out_dtype="void")
            add513: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1033, add512)
            rms_norm522: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add513, model_layers_1_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1293 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight10, model_layers_1_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1034: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1293, axes=None)
            matmul1034: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm522, permute_dims1034, out_dtype="void")
            reshape1028: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1034, R.shape([batch_size, 1, 48, 128]))
            reshape1029: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1028, R.shape([batch_size, 48, 128]))
            lv1294 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape1029), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1030: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1294, R.shape([batch_size, 1, 32, 128]))
            reshape1031: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1030, R.shape([batch_size, 1, 4096]))
            lv1295 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight10, model_layers_1_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1035: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1295, axes=None)
            matmul1035: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1031, permute_dims1035, out_dtype="void")
            add514: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1035, add513)
            rms_norm523: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add514, model_layers_1_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1296 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight10, model_layers_1_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1036: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1296, axes=None)
            matmul1036: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm523, permute_dims1036, out_dtype="void")
            split257: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1036, indices_or_sections=2, axis=-1)
            split_0257: R.Tensor((batch_size, 1, 14336), dtype="float16") = split257[0]
            split_1257: R.Tensor((batch_size, 1, 14336), dtype="float16") = split257[1]
            silu257: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0257)
            mul257: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu257, split_1257)
            lv1297 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight10, model_layers_1_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1037: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1297, axes=None)
            matmul1037: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul257, permute_dims1037, out_dtype="void")
            add515: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1037, add514)
            rms_norm524: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add515, model_layers_2_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1298 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight10, model_layers_2_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1038: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1298, axes=None)
            matmul1038: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm524, permute_dims1038, out_dtype="void")
            reshape1032: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1038, R.shape([batch_size, 1, 48, 128]))
            reshape1033: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1032, R.shape([batch_size, 48, 128]))
            lv1299 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape1033), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1034: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1299, R.shape([batch_size, 1, 32, 128]))
            reshape1035: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1034, R.shape([batch_size, 1, 4096]))
            lv1300 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight10, model_layers_2_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1039: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1300, axes=None)
            matmul1039: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1035, permute_dims1039, out_dtype="void")
            add516: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1039, add515)
            rms_norm525: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add516, model_layers_2_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1301 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight10, model_layers_2_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1040: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1301, axes=None)
            matmul1040: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm525, permute_dims1040, out_dtype="void")
            split258: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1040, indices_or_sections=2, axis=-1)
            split_0258: R.Tensor((batch_size, 1, 14336), dtype="float16") = split258[0]
            split_1258: R.Tensor((batch_size, 1, 14336), dtype="float16") = split258[1]
            silu258: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0258)
            mul258: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu258, split_1258)
            lv1302 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight10, model_layers_2_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1041: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1302, axes=None)
            matmul1041: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul258, permute_dims1041, out_dtype="void")
            add517: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1041, add516)
            rms_norm526: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add517, model_layers_3_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1303 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight10, model_layers_3_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1042: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1303, axes=None)
            matmul1042: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm526, permute_dims1042, out_dtype="void")
            reshape1036: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1042, R.shape([batch_size, 1, 48, 128]))
            reshape1037: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1036, R.shape([batch_size, 48, 128]))
            lv1304 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape1037), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1038: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1304, R.shape([batch_size, 1, 32, 128]))
            reshape1039: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1038, R.shape([batch_size, 1, 4096]))
            lv1305 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight10, model_layers_3_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1043: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1305, axes=None)
            matmul1043: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1039, permute_dims1043, out_dtype="void")
            add518: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1043, add517)
            rms_norm527: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add518, model_layers_3_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1306 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight10, model_layers_3_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1044: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1306, axes=None)
            matmul1044: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm527, permute_dims1044, out_dtype="void")
            split259: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1044, indices_or_sections=2, axis=-1)
            split_0259: R.Tensor((batch_size, 1, 14336), dtype="float16") = split259[0]
            split_1259: R.Tensor((batch_size, 1, 14336), dtype="float16") = split259[1]
            silu259: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0259)
            mul259: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu259, split_1259)
            lv1307 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight10, model_layers_3_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1045: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1307, axes=None)
            matmul1045: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul259, permute_dims1045, out_dtype="void")
            add519: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1045, add518)
            rms_norm528: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add519, model_layers_4_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1308 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight10, model_layers_4_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1046: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1308, axes=None)
            matmul1046: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm528, permute_dims1046, out_dtype="void")
            reshape1040: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1046, R.shape([batch_size, 1, 48, 128]))
            reshape1041: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1040, R.shape([batch_size, 48, 128]))
            lv1309 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape1041), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1042: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1309, R.shape([batch_size, 1, 32, 128]))
            reshape1043: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1042, R.shape([batch_size, 1, 4096]))
            lv1310 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight10, model_layers_4_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1047: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1310, axes=None)
            matmul1047: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1043, permute_dims1047, out_dtype="void")
            add520: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1047, add519)
            rms_norm529: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add520, model_layers_4_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1311 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight10, model_layers_4_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1048: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1311, axes=None)
            matmul1048: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm529, permute_dims1048, out_dtype="void")
            split260: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1048, indices_or_sections=2, axis=-1)
            split_0260: R.Tensor((batch_size, 1, 14336), dtype="float16") = split260[0]
            split_1260: R.Tensor((batch_size, 1, 14336), dtype="float16") = split260[1]
            silu260: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0260)
            mul260: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu260, split_1260)
            lv1312 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight10, model_layers_4_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1049: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1312, axes=None)
            matmul1049: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul260, permute_dims1049, out_dtype="void")
            add521: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1049, add520)
            rms_norm530: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add521, model_layers_5_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1313 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight10, model_layers_5_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1050: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1313, axes=None)
            matmul1050: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm530, permute_dims1050, out_dtype="void")
            reshape1044: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1050, R.shape([batch_size, 1, 48, 128]))
            reshape1045: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1044, R.shape([batch_size, 48, 128]))
            lv1314 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape1045), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1046: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1314, R.shape([batch_size, 1, 32, 128]))
            reshape1047: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1046, R.shape([batch_size, 1, 4096]))
            lv1315 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight10, model_layers_5_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1051: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1315, axes=None)
            matmul1051: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1047, permute_dims1051, out_dtype="void")
            add522: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1051, add521)
            rms_norm531: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add522, model_layers_5_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1316 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight10, model_layers_5_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1052: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1316, axes=None)
            matmul1052: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm531, permute_dims1052, out_dtype="void")
            split261: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1052, indices_or_sections=2, axis=-1)
            split_0261: R.Tensor((batch_size, 1, 14336), dtype="float16") = split261[0]
            split_1261: R.Tensor((batch_size, 1, 14336), dtype="float16") = split261[1]
            silu261: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0261)
            mul261: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu261, split_1261)
            lv1317 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight10, model_layers_5_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1053: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1317, axes=None)
            matmul1053: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul261, permute_dims1053, out_dtype="void")
            add523: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1053, add522)
            rms_norm532: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add523, model_layers_6_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1318 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight10, model_layers_6_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1054: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1318, axes=None)
            matmul1054: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm532, permute_dims1054, out_dtype="void")
            reshape1048: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1054, R.shape([batch_size, 1, 48, 128]))
            reshape1049: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1048, R.shape([batch_size, 48, 128]))
            lv1319 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape1049), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1050: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1319, R.shape([batch_size, 1, 32, 128]))
            reshape1051: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1050, R.shape([batch_size, 1, 4096]))
            lv1320 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight10, model_layers_6_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1055: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1320, axes=None)
            matmul1055: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1051, permute_dims1055, out_dtype="void")
            add524: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1055, add523)
            rms_norm533: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add524, model_layers_6_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1321 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight10, model_layers_6_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1056: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1321, axes=None)
            matmul1056: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm533, permute_dims1056, out_dtype="void")
            split262: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1056, indices_or_sections=2, axis=-1)
            split_0262: R.Tensor((batch_size, 1, 14336), dtype="float16") = split262[0]
            split_1262: R.Tensor((batch_size, 1, 14336), dtype="float16") = split262[1]
            silu262: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0262)
            mul262: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu262, split_1262)
            lv1322 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight10, model_layers_6_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1057: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1322, axes=None)
            matmul1057: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul262, permute_dims1057, out_dtype="void")
            add525: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1057, add524)
            rms_norm534: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add525, model_layers_7_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1323 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight10, model_layers_7_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1058: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1323, axes=None)
            matmul1058: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm534, permute_dims1058, out_dtype="void")
            reshape1052: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1058, R.shape([batch_size, 1, 48, 128]))
            reshape1053: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1052, R.shape([batch_size, 48, 128]))
            lv1324 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape1053), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1054: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1324, R.shape([batch_size, 1, 32, 128]))
            reshape1055: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1054, R.shape([batch_size, 1, 4096]))
            lv1325 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight10, model_layers_7_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1059: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1325, axes=None)
            matmul1059: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1055, permute_dims1059, out_dtype="void")
            add526: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1059, add525)
            rms_norm535: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add526, model_layers_7_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1326 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight10, model_layers_7_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1060: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1326, axes=None)
            matmul1060: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm535, permute_dims1060, out_dtype="void")
            split263: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1060, indices_or_sections=2, axis=-1)
            split_0263: R.Tensor((batch_size, 1, 14336), dtype="float16") = split263[0]
            split_1263: R.Tensor((batch_size, 1, 14336), dtype="float16") = split263[1]
            silu263: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0263)
            mul263: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu263, split_1263)
            lv1327 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight10, model_layers_7_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1061: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1327, axes=None)
            matmul1061: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul263, permute_dims1061, out_dtype="void")
            add527: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1061, add526)
            rms_norm536: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add527, model_layers_8_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1328 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight10, model_layers_8_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1062: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1328, axes=None)
            matmul1062: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm536, permute_dims1062, out_dtype="void")
            reshape1056: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1062, R.shape([batch_size, 1, 48, 128]))
            reshape1057: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1056, R.shape([batch_size, 48, 128]))
            lv1329 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape1057), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1058: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1329, R.shape([batch_size, 1, 32, 128]))
            reshape1059: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1058, R.shape([batch_size, 1, 4096]))
            lv1330 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight10, model_layers_8_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1063: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1330, axes=None)
            matmul1063: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1059, permute_dims1063, out_dtype="void")
            add528: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1063, add527)
            rms_norm537: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add528, model_layers_8_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1331 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight10, model_layers_8_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1064: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1331, axes=None)
            matmul1064: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm537, permute_dims1064, out_dtype="void")
            split264: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1064, indices_or_sections=2, axis=-1)
            split_0264: R.Tensor((batch_size, 1, 14336), dtype="float16") = split264[0]
            split_1264: R.Tensor((batch_size, 1, 14336), dtype="float16") = split264[1]
            silu264: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0264)
            mul264: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu264, split_1264)
            lv1332 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight10, model_layers_8_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1065: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1332, axes=None)
            matmul1065: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul264, permute_dims1065, out_dtype="void")
            add529: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1065, add528)
            rms_norm538: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add529, model_layers_9_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1333 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight10, model_layers_9_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1066: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1333, axes=None)
            matmul1066: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm538, permute_dims1066, out_dtype="void")
            reshape1060: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1066, R.shape([batch_size, 1, 48, 128]))
            reshape1061: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1060, R.shape([batch_size, 48, 128]))
            lv1334 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape1061), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1062: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1334, R.shape([batch_size, 1, 32, 128]))
            reshape1063: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1062, R.shape([batch_size, 1, 4096]))
            lv1335 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight10, model_layers_9_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1067: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1335, axes=None)
            matmul1067: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1063, permute_dims1067, out_dtype="void")
            add530: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1067, add529)
            rms_norm539: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add530, model_layers_9_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1336 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight10, model_layers_9_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1068: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1336, axes=None)
            matmul1068: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm539, permute_dims1068, out_dtype="void")
            split265: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1068, indices_or_sections=2, axis=-1)
            split_0265: R.Tensor((batch_size, 1, 14336), dtype="float16") = split265[0]
            split_1265: R.Tensor((batch_size, 1, 14336), dtype="float16") = split265[1]
            silu265: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0265)
            mul265: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu265, split_1265)
            lv1337 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight10, model_layers_9_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1069: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1337, axes=None)
            matmul1069: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul265, permute_dims1069, out_dtype="void")
            add531: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1069, add530)
            rms_norm540: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add531, model_layers_10_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1338 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight10, model_layers_10_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1070: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1338, axes=None)
            matmul1070: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm540, permute_dims1070, out_dtype="void")
            reshape1064: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1070, R.shape([batch_size, 1, 48, 128]))
            reshape1065: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1064, R.shape([batch_size, 48, 128]))
            lv1339 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape1065), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1066: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1339, R.shape([batch_size, 1, 32, 128]))
            reshape1067: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1066, R.shape([batch_size, 1, 4096]))
            lv1340 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight10, model_layers_10_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1071: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1340, axes=None)
            matmul1071: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1067, permute_dims1071, out_dtype="void")
            add532: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1071, add531)
            rms_norm541: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add532, model_layers_10_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1341 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight10, model_layers_10_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1072: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1341, axes=None)
            matmul1072: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm541, permute_dims1072, out_dtype="void")
            split266: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1072, indices_or_sections=2, axis=-1)
            split_0266: R.Tensor((batch_size, 1, 14336), dtype="float16") = split266[0]
            split_1266: R.Tensor((batch_size, 1, 14336), dtype="float16") = split266[1]
            silu266: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0266)
            mul266: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu266, split_1266)
            lv1342 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight10, model_layers_10_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1073: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1342, axes=None)
            matmul1073: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul266, permute_dims1073, out_dtype="void")
            add533: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1073, add532)
            rms_norm542: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add533, model_layers_11_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1343 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight10, model_layers_11_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1074: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1343, axes=None)
            matmul1074: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm542, permute_dims1074, out_dtype="void")
            reshape1068: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1074, R.shape([batch_size, 1, 48, 128]))
            reshape1069: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1068, R.shape([batch_size, 48, 128]))
            lv1344 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape1069), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1070: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1344, R.shape([batch_size, 1, 32, 128]))
            reshape1071: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1070, R.shape([batch_size, 1, 4096]))
            lv1345 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight10, model_layers_11_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1075: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1345, axes=None)
            matmul1075: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1071, permute_dims1075, out_dtype="void")
            add534: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1075, add533)
            rms_norm543: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add534, model_layers_11_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1346 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight10, model_layers_11_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1076: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1346, axes=None)
            matmul1076: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm543, permute_dims1076, out_dtype="void")
            split267: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1076, indices_or_sections=2, axis=-1)
            split_0267: R.Tensor((batch_size, 1, 14336), dtype="float16") = split267[0]
            split_1267: R.Tensor((batch_size, 1, 14336), dtype="float16") = split267[1]
            silu267: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0267)
            mul267: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu267, split_1267)
            lv1347 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight10, model_layers_11_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1077: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1347, axes=None)
            matmul1077: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul267, permute_dims1077, out_dtype="void")
            add535: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1077, add534)
            rms_norm544: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add535, model_layers_12_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1348 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight10, model_layers_12_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1078: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1348, axes=None)
            matmul1078: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm544, permute_dims1078, out_dtype="void")
            reshape1072: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1078, R.shape([batch_size, 1, 48, 128]))
            reshape1073: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1072, R.shape([batch_size, 48, 128]))
            lv1349 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape1073), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1074: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1349, R.shape([batch_size, 1, 32, 128]))
            reshape1075: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1074, R.shape([batch_size, 1, 4096]))
            lv1350 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight10, model_layers_12_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1079: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1350, axes=None)
            matmul1079: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1075, permute_dims1079, out_dtype="void")
            add536: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1079, add535)
            rms_norm545: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add536, model_layers_12_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1351 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight10, model_layers_12_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1080: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1351, axes=None)
            matmul1080: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm545, permute_dims1080, out_dtype="void")
            split268: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1080, indices_or_sections=2, axis=-1)
            split_0268: R.Tensor((batch_size, 1, 14336), dtype="float16") = split268[0]
            split_1268: R.Tensor((batch_size, 1, 14336), dtype="float16") = split268[1]
            silu268: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0268)
            mul268: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu268, split_1268)
            lv1352 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight10, model_layers_12_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1081: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1352, axes=None)
            matmul1081: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul268, permute_dims1081, out_dtype="void")
            add537: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1081, add536)
            rms_norm546: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add537, model_layers_13_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1353 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight10, model_layers_13_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1082: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1353, axes=None)
            matmul1082: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm546, permute_dims1082, out_dtype="void")
            reshape1076: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1082, R.shape([batch_size, 1, 48, 128]))
            reshape1077: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1076, R.shape([batch_size, 48, 128]))
            lv1354 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape1077), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1078: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1354, R.shape([batch_size, 1, 32, 128]))
            reshape1079: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1078, R.shape([batch_size, 1, 4096]))
            lv1355 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight10, model_layers_13_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1083: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1355, axes=None)
            matmul1083: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1079, permute_dims1083, out_dtype="void")
            add538: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1083, add537)
            rms_norm547: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add538, model_layers_13_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1356 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight10, model_layers_13_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1084: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1356, axes=None)
            matmul1084: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm547, permute_dims1084, out_dtype="void")
            split269: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1084, indices_or_sections=2, axis=-1)
            split_0269: R.Tensor((batch_size, 1, 14336), dtype="float16") = split269[0]
            split_1269: R.Tensor((batch_size, 1, 14336), dtype="float16") = split269[1]
            silu269: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0269)
            mul269: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu269, split_1269)
            lv1357 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight10, model_layers_13_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1085: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1357, axes=None)
            matmul1085: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul269, permute_dims1085, out_dtype="void")
            add539: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1085, add538)
            rms_norm548: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add539, model_layers_14_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1358 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight10, model_layers_14_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1086: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1358, axes=None)
            matmul1086: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm548, permute_dims1086, out_dtype="void")
            reshape1080: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1086, R.shape([batch_size, 1, 48, 128]))
            reshape1081: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1080, R.shape([batch_size, 48, 128]))
            lv1359 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape1081), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1082: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1359, R.shape([batch_size, 1, 32, 128]))
            reshape1083: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1082, R.shape([batch_size, 1, 4096]))
            lv1360 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight10, model_layers_14_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1087: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1360, axes=None)
            matmul1087: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1083, permute_dims1087, out_dtype="void")
            add540: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1087, add539)
            rms_norm549: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add540, model_layers_14_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1361 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight10, model_layers_14_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1088: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1361, axes=None)
            matmul1088: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm549, permute_dims1088, out_dtype="void")
            split270: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1088, indices_or_sections=2, axis=-1)
            split_0270: R.Tensor((batch_size, 1, 14336), dtype="float16") = split270[0]
            split_1270: R.Tensor((batch_size, 1, 14336), dtype="float16") = split270[1]
            silu270: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0270)
            mul270: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu270, split_1270)
            lv1362 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight10, model_layers_14_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1089: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1362, axes=None)
            matmul1089: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul270, permute_dims1089, out_dtype="void")
            add541: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1089, add540)
            rms_norm550: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add541, model_layers_15_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1363 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight10, model_layers_15_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1090: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1363, axes=None)
            matmul1090: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm550, permute_dims1090, out_dtype="void")
            reshape1084: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1090, R.shape([batch_size, 1, 48, 128]))
            reshape1085: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1084, R.shape([batch_size, 48, 128]))
            lv1364 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape1085), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1086: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1364, R.shape([batch_size, 1, 32, 128]))
            reshape1087: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1086, R.shape([batch_size, 1, 4096]))
            lv1365 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight10, model_layers_15_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1091: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1365, axes=None)
            matmul1091: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1087, permute_dims1091, out_dtype="void")
            add542: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1091, add541)
            rms_norm551: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add542, model_layers_15_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1366 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight10, model_layers_15_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1092: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1366, axes=None)
            matmul1092: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm551, permute_dims1092, out_dtype="void")
            split271: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1092, indices_or_sections=2, axis=-1)
            split_0271: R.Tensor((batch_size, 1, 14336), dtype="float16") = split271[0]
            split_1271: R.Tensor((batch_size, 1, 14336), dtype="float16") = split271[1]
            silu271: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0271)
            mul271: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu271, split_1271)
            lv1367 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight10, model_layers_15_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1093: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1367, axes=None)
            matmul1093: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul271, permute_dims1093, out_dtype="void")
            add543: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1093, add542)
            rms_norm552: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add543, model_layers_16_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1368 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight10, model_layers_16_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1094: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1368, axes=None)
            matmul1094: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm552, permute_dims1094, out_dtype="void")
            reshape1088: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1094, R.shape([batch_size, 1, 48, 128]))
            reshape1089: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1088, R.shape([batch_size, 48, 128]))
            lv1369 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape1089), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1090: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1369, R.shape([batch_size, 1, 32, 128]))
            reshape1091: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1090, R.shape([batch_size, 1, 4096]))
            lv1370 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight10, model_layers_16_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1095: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1370, axes=None)
            matmul1095: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1091, permute_dims1095, out_dtype="void")
            add544: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1095, add543)
            rms_norm553: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add544, model_layers_16_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1371 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight10, model_layers_16_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1096: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1371, axes=None)
            matmul1096: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm553, permute_dims1096, out_dtype="void")
            split272: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1096, indices_or_sections=2, axis=-1)
            split_0272: R.Tensor((batch_size, 1, 14336), dtype="float16") = split272[0]
            split_1272: R.Tensor((batch_size, 1, 14336), dtype="float16") = split272[1]
            silu272: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0272)
            mul272: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu272, split_1272)
            lv1372 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight10, model_layers_16_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1097: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1372, axes=None)
            matmul1097: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul272, permute_dims1097, out_dtype="void")
            add545: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1097, add544)
            rms_norm554: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add545, model_layers_17_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1373 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight10, model_layers_17_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1098: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1373, axes=None)
            matmul1098: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm554, permute_dims1098, out_dtype="void")
            reshape1092: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1098, R.shape([batch_size, 1, 48, 128]))
            reshape1093: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1092, R.shape([batch_size, 48, 128]))
            lv1374 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape1093), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1094: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1374, R.shape([batch_size, 1, 32, 128]))
            reshape1095: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1094, R.shape([batch_size, 1, 4096]))
            lv1375 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight10, model_layers_17_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1099: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1375, axes=None)
            matmul1099: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1095, permute_dims1099, out_dtype="void")
            add546: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1099, add545)
            rms_norm555: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add546, model_layers_17_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1376 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight10, model_layers_17_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1100: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1376, axes=None)
            matmul1100: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm555, permute_dims1100, out_dtype="void")
            split273: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1100, indices_or_sections=2, axis=-1)
            split_0273: R.Tensor((batch_size, 1, 14336), dtype="float16") = split273[0]
            split_1273: R.Tensor((batch_size, 1, 14336), dtype="float16") = split273[1]
            silu273: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0273)
            mul273: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu273, split_1273)
            lv1377 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight10, model_layers_17_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1101: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1377, axes=None)
            matmul1101: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul273, permute_dims1101, out_dtype="void")
            add547: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1101, add546)
            rms_norm556: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add547, model_layers_18_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1378 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight10, model_layers_18_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1102: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1378, axes=None)
            matmul1102: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm556, permute_dims1102, out_dtype="void")
            reshape1096: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1102, R.shape([batch_size, 1, 48, 128]))
            reshape1097: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1096, R.shape([batch_size, 48, 128]))
            lv1379 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape1097), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1098: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1379, R.shape([batch_size, 1, 32, 128]))
            reshape1099: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1098, R.shape([batch_size, 1, 4096]))
            lv1380 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight10, model_layers_18_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1103: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1380, axes=None)
            matmul1103: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1099, permute_dims1103, out_dtype="void")
            add548: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1103, add547)
            rms_norm557: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add548, model_layers_18_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1381 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight10, model_layers_18_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1104: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1381, axes=None)
            matmul1104: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm557, permute_dims1104, out_dtype="void")
            split274: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1104, indices_or_sections=2, axis=-1)
            split_0274: R.Tensor((batch_size, 1, 14336), dtype="float16") = split274[0]
            split_1274: R.Tensor((batch_size, 1, 14336), dtype="float16") = split274[1]
            silu274: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0274)
            mul274: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu274, split_1274)
            lv1382 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight10, model_layers_18_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1105: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1382, axes=None)
            matmul1105: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul274, permute_dims1105, out_dtype="void")
            add549: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1105, add548)
            rms_norm558: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add549, model_layers_19_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1383 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight10, model_layers_19_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1106: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1383, axes=None)
            matmul1106: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm558, permute_dims1106, out_dtype="void")
            reshape1100: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1106, R.shape([batch_size, 1, 48, 128]))
            reshape1101: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1100, R.shape([batch_size, 48, 128]))
            lv1384 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape1101), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1102: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1384, R.shape([batch_size, 1, 32, 128]))
            reshape1103: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1102, R.shape([batch_size, 1, 4096]))
            lv1385 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight10, model_layers_19_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1107: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1385, axes=None)
            matmul1107: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1103, permute_dims1107, out_dtype="void")
            add550: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1107, add549)
            rms_norm559: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add550, model_layers_19_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1386 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight10, model_layers_19_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1108: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1386, axes=None)
            matmul1108: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm559, permute_dims1108, out_dtype="void")
            split275: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1108, indices_or_sections=2, axis=-1)
            split_0275: R.Tensor((batch_size, 1, 14336), dtype="float16") = split275[0]
            split_1275: R.Tensor((batch_size, 1, 14336), dtype="float16") = split275[1]
            silu275: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0275)
            mul275: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu275, split_1275)
            lv1387 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight10, model_layers_19_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1109: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1387, axes=None)
            matmul1109: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul275, permute_dims1109, out_dtype="void")
            add551: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1109, add550)
            rms_norm560: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add551, model_layers_20_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1388 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight10, model_layers_20_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1110: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1388, axes=None)
            matmul1110: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm560, permute_dims1110, out_dtype="void")
            reshape1104: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1110, R.shape([batch_size, 1, 48, 128]))
            reshape1105: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1104, R.shape([batch_size, 48, 128]))
            lv1389 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape1105), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1106: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1389, R.shape([batch_size, 1, 32, 128]))
            reshape1107: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1106, R.shape([batch_size, 1, 4096]))
            lv1390 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight10, model_layers_20_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1111: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1390, axes=None)
            matmul1111: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1107, permute_dims1111, out_dtype="void")
            add552: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1111, add551)
            rms_norm561: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add552, model_layers_20_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1391 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight10, model_layers_20_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1112: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1391, axes=None)
            matmul1112: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm561, permute_dims1112, out_dtype="void")
            split276: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1112, indices_or_sections=2, axis=-1)
            split_0276: R.Tensor((batch_size, 1, 14336), dtype="float16") = split276[0]
            split_1276: R.Tensor((batch_size, 1, 14336), dtype="float16") = split276[1]
            silu276: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0276)
            mul276: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu276, split_1276)
            lv1392 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight10, model_layers_20_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1113: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1392, axes=None)
            matmul1113: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul276, permute_dims1113, out_dtype="void")
            add553: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1113, add552)
            rms_norm562: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add553, model_layers_21_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1393 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight10, model_layers_21_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1114: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1393, axes=None)
            matmul1114: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm562, permute_dims1114, out_dtype="void")
            reshape1108: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1114, R.shape([batch_size, 1, 48, 128]))
            reshape1109: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1108, R.shape([batch_size, 48, 128]))
            lv1394 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape1109), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1110: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1394, R.shape([batch_size, 1, 32, 128]))
            reshape1111: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1110, R.shape([batch_size, 1, 4096]))
            lv1395 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight10, model_layers_21_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1115: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1395, axes=None)
            matmul1115: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1111, permute_dims1115, out_dtype="void")
            add554: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1115, add553)
            rms_norm563: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add554, model_layers_21_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1396 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight10, model_layers_21_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1116: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1396, axes=None)
            matmul1116: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm563, permute_dims1116, out_dtype="void")
            split277: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1116, indices_or_sections=2, axis=-1)
            split_0277: R.Tensor((batch_size, 1, 14336), dtype="float16") = split277[0]
            split_1277: R.Tensor((batch_size, 1, 14336), dtype="float16") = split277[1]
            silu277: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0277)
            mul277: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu277, split_1277)
            lv1397 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight10, model_layers_21_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1117: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1397, axes=None)
            matmul1117: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul277, permute_dims1117, out_dtype="void")
            add555: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1117, add554)
            rms_norm564: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add555, model_layers_22_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1398 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight10, model_layers_22_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1118: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1398, axes=None)
            matmul1118: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm564, permute_dims1118, out_dtype="void")
            reshape1112: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1118, R.shape([batch_size, 1, 48, 128]))
            reshape1113: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1112, R.shape([batch_size, 48, 128]))
            lv1399 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape1113), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1114: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1399, R.shape([batch_size, 1, 32, 128]))
            reshape1115: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1114, R.shape([batch_size, 1, 4096]))
            lv1400 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight10, model_layers_22_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1119: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1400, axes=None)
            matmul1119: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1115, permute_dims1119, out_dtype="void")
            add556: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1119, add555)
            rms_norm565: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add556, model_layers_22_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1401 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight10, model_layers_22_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1120: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1401, axes=None)
            matmul1120: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm565, permute_dims1120, out_dtype="void")
            split278: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1120, indices_or_sections=2, axis=-1)
            split_0278: R.Tensor((batch_size, 1, 14336), dtype="float16") = split278[0]
            split_1278: R.Tensor((batch_size, 1, 14336), dtype="float16") = split278[1]
            silu278: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0278)
            mul278: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu278, split_1278)
            lv1402 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight10, model_layers_22_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1121: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1402, axes=None)
            matmul1121: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul278, permute_dims1121, out_dtype="void")
            add557: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1121, add556)
            rms_norm566: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add557, model_layers_23_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1403 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight10, model_layers_23_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1122: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1403, axes=None)
            matmul1122: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm566, permute_dims1122, out_dtype="void")
            reshape1116: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1122, R.shape([batch_size, 1, 48, 128]))
            reshape1117: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1116, R.shape([batch_size, 48, 128]))
            lv1404 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape1117), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1118: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1404, R.shape([batch_size, 1, 32, 128]))
            reshape1119: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1118, R.shape([batch_size, 1, 4096]))
            lv1405 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight10, model_layers_23_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1123: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1405, axes=None)
            matmul1123: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1119, permute_dims1123, out_dtype="void")
            add558: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1123, add557)
            rms_norm567: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add558, model_layers_23_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1406 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight10, model_layers_23_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1124: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1406, axes=None)
            matmul1124: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm567, permute_dims1124, out_dtype="void")
            split279: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1124, indices_or_sections=2, axis=-1)
            split_0279: R.Tensor((batch_size, 1, 14336), dtype="float16") = split279[0]
            split_1279: R.Tensor((batch_size, 1, 14336), dtype="float16") = split279[1]
            silu279: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0279)
            mul279: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu279, split_1279)
            lv1407 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight10, model_layers_23_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1125: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1407, axes=None)
            matmul1125: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul279, permute_dims1125, out_dtype="void")
            add559: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1125, add558)
            rms_norm568: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add559, model_layers_24_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1408 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight10, model_layers_24_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1126: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1408, axes=None)
            matmul1126: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm568, permute_dims1126, out_dtype="void")
            reshape1120: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1126, R.shape([batch_size, 1, 48, 128]))
            reshape1121: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1120, R.shape([batch_size, 48, 128]))
            lv1409 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape1121), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1122: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1409, R.shape([batch_size, 1, 32, 128]))
            reshape1123: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1122, R.shape([batch_size, 1, 4096]))
            lv1410 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight10, model_layers_24_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1127: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1410, axes=None)
            matmul1127: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1123, permute_dims1127, out_dtype="void")
            add560: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1127, add559)
            rms_norm569: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add560, model_layers_24_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1411 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight10, model_layers_24_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1128: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1411, axes=None)
            matmul1128: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm569, permute_dims1128, out_dtype="void")
            split280: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1128, indices_or_sections=2, axis=-1)
            split_0280: R.Tensor((batch_size, 1, 14336), dtype="float16") = split280[0]
            split_1280: R.Tensor((batch_size, 1, 14336), dtype="float16") = split280[1]
            silu280: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0280)
            mul280: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu280, split_1280)
            lv1412 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight10, model_layers_24_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1129: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1412, axes=None)
            matmul1129: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul280, permute_dims1129, out_dtype="void")
            add561: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1129, add560)
            rms_norm570: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add561, model_layers_25_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1413 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight10, model_layers_25_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1130: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1413, axes=None)
            matmul1130: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm570, permute_dims1130, out_dtype="void")
            reshape1124: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1130, R.shape([batch_size, 1, 48, 128]))
            reshape1125: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1124, R.shape([batch_size, 48, 128]))
            lv1414 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape1125), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1126: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1414, R.shape([batch_size, 1, 32, 128]))
            reshape1127: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1126, R.shape([batch_size, 1, 4096]))
            lv1415 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight10, model_layers_25_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1131: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1415, axes=None)
            matmul1131: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1127, permute_dims1131, out_dtype="void")
            add562: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1131, add561)
            rms_norm571: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add562, model_layers_25_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1416 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight10, model_layers_25_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1132: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1416, axes=None)
            matmul1132: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm571, permute_dims1132, out_dtype="void")
            split281: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1132, indices_or_sections=2, axis=-1)
            split_0281: R.Tensor((batch_size, 1, 14336), dtype="float16") = split281[0]
            split_1281: R.Tensor((batch_size, 1, 14336), dtype="float16") = split281[1]
            silu281: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0281)
            mul281: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu281, split_1281)
            lv1417 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight10, model_layers_25_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1133: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1417, axes=None)
            matmul1133: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul281, permute_dims1133, out_dtype="void")
            add563: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1133, add562)
            rms_norm572: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add563, model_layers_26_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1418 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight10, model_layers_26_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1134: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1418, axes=None)
            matmul1134: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm572, permute_dims1134, out_dtype="void")
            reshape1128: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1134, R.shape([batch_size, 1, 48, 128]))
            reshape1129: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1128, R.shape([batch_size, 48, 128]))
            lv1419 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1129), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1130: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1419, R.shape([batch_size, 1, 32, 128]))
            reshape1131: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1130, R.shape([batch_size, 1, 4096]))
            lv1420 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight10, model_layers_26_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1135: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1420, axes=None)
            matmul1135: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1131, permute_dims1135, out_dtype="void")
            add564: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1135, add563)
            rms_norm573: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add564, model_layers_26_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1421 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight10, model_layers_26_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1136: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1421, axes=None)
            matmul1136: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm573, permute_dims1136, out_dtype="void")
            split282: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1136, indices_or_sections=2, axis=-1)
            split_0282: R.Tensor((batch_size, 1, 14336), dtype="float16") = split282[0]
            split_1282: R.Tensor((batch_size, 1, 14336), dtype="float16") = split282[1]
            silu282: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0282)
            mul282: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu282, split_1282)
            lv1422 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight10, model_layers_26_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1137: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1422, axes=None)
            matmul1137: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul282, permute_dims1137, out_dtype="void")
            add565: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1137, add564)
            rms_norm574: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add565, model_layers_27_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1423 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight10, model_layers_27_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1138: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1423, axes=None)
            matmul1138: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm574, permute_dims1138, out_dtype="void")
            reshape1132: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1138, R.shape([batch_size, 1, 48, 128]))
            reshape1133: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1132, R.shape([batch_size, 48, 128]))
            lv1424 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1133), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1134: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1424, R.shape([batch_size, 1, 32, 128]))
            reshape1135: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1134, R.shape([batch_size, 1, 4096]))
            lv1425 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight10, model_layers_27_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1139: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1425, axes=None)
            matmul1139: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1135, permute_dims1139, out_dtype="void")
            add566: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1139, add565)
            rms_norm575: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add566, model_layers_27_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1426 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight10, model_layers_27_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1140: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1426, axes=None)
            matmul1140: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm575, permute_dims1140, out_dtype="void")
            split283: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1140, indices_or_sections=2, axis=-1)
            split_0283: R.Tensor((batch_size, 1, 14336), dtype="float16") = split283[0]
            split_1283: R.Tensor((batch_size, 1, 14336), dtype="float16") = split283[1]
            silu283: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0283)
            mul283: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu283, split_1283)
            lv1427 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight10, model_layers_27_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1141: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1427, axes=None)
            matmul1141: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul283, permute_dims1141, out_dtype="void")
            add567: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1141, add566)
            rms_norm576: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add567, model_layers_28_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1428 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight10, model_layers_28_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1142: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1428, axes=None)
            matmul1142: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm576, permute_dims1142, out_dtype="void")
            reshape1136: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1142, R.shape([batch_size, 1, 48, 128]))
            reshape1137: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1136, R.shape([batch_size, 48, 128]))
            lv1429 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1137), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1138: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1429, R.shape([batch_size, 1, 32, 128]))
            reshape1139: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1138, R.shape([batch_size, 1, 4096]))
            lv1430 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight10, model_layers_28_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1143: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1430, axes=None)
            matmul1143: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1139, permute_dims1143, out_dtype="void")
            add568: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1143, add567)
            rms_norm577: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add568, model_layers_28_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1431 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight10, model_layers_28_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1144: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1431, axes=None)
            matmul1144: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm577, permute_dims1144, out_dtype="void")
            split284: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1144, indices_or_sections=2, axis=-1)
            split_0284: R.Tensor((batch_size, 1, 14336), dtype="float16") = split284[0]
            split_1284: R.Tensor((batch_size, 1, 14336), dtype="float16") = split284[1]
            silu284: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0284)
            mul284: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu284, split_1284)
            lv1432 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight10, model_layers_28_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1145: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1432, axes=None)
            matmul1145: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul284, permute_dims1145, out_dtype="void")
            add569: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1145, add568)
            rms_norm578: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add569, model_layers_29_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1433 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight10, model_layers_29_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1146: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1433, axes=None)
            matmul1146: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm578, permute_dims1146, out_dtype="void")
            reshape1140: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1146, R.shape([batch_size, 1, 48, 128]))
            reshape1141: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1140, R.shape([batch_size, 48, 128]))
            lv1434 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1141), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1142: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1434, R.shape([batch_size, 1, 32, 128]))
            reshape1143: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1142, R.shape([batch_size, 1, 4096]))
            lv1435 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight10, model_layers_29_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1147: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1435, axes=None)
            matmul1147: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1143, permute_dims1147, out_dtype="void")
            add570: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1147, add569)
            rms_norm579: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add570, model_layers_29_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1436 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight10, model_layers_29_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1148: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1436, axes=None)
            matmul1148: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm579, permute_dims1148, out_dtype="void")
            split285: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1148, indices_or_sections=2, axis=-1)
            split_0285: R.Tensor((batch_size, 1, 14336), dtype="float16") = split285[0]
            split_1285: R.Tensor((batch_size, 1, 14336), dtype="float16") = split285[1]
            silu285: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0285)
            mul285: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu285, split_1285)
            lv1437 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight10, model_layers_29_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1149: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1437, axes=None)
            matmul1149: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul285, permute_dims1149, out_dtype="void")
            add571: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1149, add570)
            rms_norm580: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add571, model_layers_30_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1438 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight10, model_layers_30_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1150: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1438, axes=None)
            matmul1150: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm580, permute_dims1150, out_dtype="void")
            reshape1144: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1150, R.shape([batch_size, 1, 48, 128]))
            reshape1145: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1144, R.shape([batch_size, 48, 128]))
            lv1439 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1145), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1146: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1439, R.shape([batch_size, 1, 32, 128]))
            reshape1147: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1146, R.shape([batch_size, 1, 4096]))
            lv1440 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight10, model_layers_30_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1151: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1440, axes=None)
            matmul1151: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1147, permute_dims1151, out_dtype="void")
            add572: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1151, add571)
            rms_norm581: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add572, model_layers_30_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1441 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight10, model_layers_30_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1152: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1441, axes=None)
            matmul1152: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm581, permute_dims1152, out_dtype="void")
            split286: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1152, indices_or_sections=2, axis=-1)
            split_0286: R.Tensor((batch_size, 1, 14336), dtype="float16") = split286[0]
            split_1286: R.Tensor((batch_size, 1, 14336), dtype="float16") = split286[1]
            silu286: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0286)
            mul286: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu286, split_1286)
            lv1442 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight10, model_layers_30_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1153: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1442, axes=None)
            matmul1153: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul286, permute_dims1153, out_dtype="void")
            add573: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1153, add572)
            rms_norm582: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add573, model_layers_31_input_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1443 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight10, model_layers_31_self_attn_qkv_proj_q_scale10), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1154: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1443, axes=None)
            matmul1154: R.Tensor((batch_size, 1, 6144), dtype="float16") = R.matmul(rms_norm582, permute_dims1154, out_dtype="void")
            reshape1148: R.Tensor((batch_size, 1, 48, 128), dtype="float16") = R.reshape(matmul1154, R.shape([batch_size, 1, 48, 128]))
            reshape1149: R.Tensor((batch_size, 48, 128), dtype="float16") = R.reshape(reshape1148, R.shape([batch_size, 48, 128]))
            lv1444 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1149), out_sinfo=R.Tensor((batch_size, 32, 128), dtype="float16"))
            reshape1150: R.Tensor((batch_size, 1, 32, 128), dtype="float16") = R.reshape(lv1444, R.shape([batch_size, 1, 32, 128]))
            reshape1151: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.reshape(reshape1150, R.shape([batch_size, 1, 4096]))
            lv1445 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight10, model_layers_31_self_attn_o_proj_q_scale10), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1155: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1445, axes=None)
            matmul1155: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(reshape1151, permute_dims1155, out_dtype="void")
            add574: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1155, add573)
            rms_norm583: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add574, model_layers_31_post_attention_layernorm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1446 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight10, model_layers_31_mlp_gate_up_proj_q_scale10), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1156: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1446, axes=None)
            matmul1156: R.Tensor((batch_size, 1, 28672), dtype="float16") = R.matmul(rms_norm583, permute_dims1156, out_dtype="void")
            split287: R.Tuple(R.Tensor((batch_size, 1, 14336), dtype="float16"), R.Tensor((batch_size, 1, 14336), dtype="float16")) = R.split(matmul1156, indices_or_sections=2, axis=-1)
            split_0287: R.Tensor((batch_size, 1, 14336), dtype="float16") = split287[0]
            split_1287: R.Tensor((batch_size, 1, 14336), dtype="float16") = split287[1]
            silu287: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.nn.silu(split_0287)
            mul287: R.Tensor((batch_size, 1, 14336), dtype="float16") = R.multiply(silu287, split_1287)
            lv1447 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight10, model_layers_31_mlp_down_proj_q_scale10), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1157: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1447, axes=None)
            matmul1157: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.matmul(mul287, permute_dims1157, out_dtype="void")
            add575: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.add(matmul1157, add574)
            rms_norm584: R.Tensor((batch_size, 1, 4096), dtype="float16") = R.nn.rms_norm(add575, model_norm_weight10, axes=[-1], epsilon=1.0000000000000001e-05)
            gv11: R.Tuple(R.Tensor((batch_size, 1, 4096), dtype="float16"), R.Object) = rms_norm584, paged_kv_cache
            R.output(gv11)
        return gv11

    @R.function
    def batch_prefill(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), logit_positions: R.Tensor(("batch_size",), dtype="int32"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "batch_size", "vocab_size"), dtype="float32"), R.Object):
        batch_size = T.int64()
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 3, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight6: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale6: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight6: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale6: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight6: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale6: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight6: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale6: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight6: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale6: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight6: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight6: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale6: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm260: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv645 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight6, model_layers_0_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims515: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv645, axes=None)
            matmul515: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm260, permute_dims515, out_dtype="void")
            reshape512: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul515, R.shape([1, seq_len, 48, 128]))
            reshape513: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape512, R.shape([seq_len, 48, 128]))
            lv646 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape513), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape514: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv646, R.shape([1, seq_len, 32, 128]))
            reshape515: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape514, R.shape([1, seq_len, 4096]))
            lv647 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight6, model_layers_0_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims516: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv647, axes=None)
            matmul516: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape515, permute_dims516, out_dtype="void")
            add256: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul516, input_embeds)
            rms_norm261: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add256, model_layers_0_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv648 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight6, model_layers_0_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims517: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv648, axes=None)
            matmul517: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm261, permute_dims517, out_dtype="void")
            split128: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul517, indices_or_sections=2, axis=-1)
            split_0128: R.Tensor((1, seq_len, 14336), dtype="float16") = split128[0]
            split_1128: R.Tensor((1, seq_len, 14336), dtype="float16") = split128[1]
            silu128: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0128)
            mul128: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu128, split_1128)
            lv649 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight6, model_layers_0_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims518: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv649, axes=None)
            matmul518: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul128, permute_dims518, out_dtype="void")
            add257: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul518, add256)
            rms_norm262: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add257, model_layers_1_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv650 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight6, model_layers_1_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims519: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv650, axes=None)
            matmul519: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm262, permute_dims519, out_dtype="void")
            reshape516: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul519, R.shape([1, seq_len, 48, 128]))
            reshape517: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape516, R.shape([seq_len, 48, 128]))
            lv651 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape517), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape518: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv651, R.shape([1, seq_len, 32, 128]))
            reshape519: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape518, R.shape([1, seq_len, 4096]))
            lv652 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight6, model_layers_1_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims520: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv652, axes=None)
            matmul520: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape519, permute_dims520, out_dtype="void")
            add258: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul520, add257)
            rms_norm263: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add258, model_layers_1_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv653 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight6, model_layers_1_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims521: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv653, axes=None)
            matmul521: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm263, permute_dims521, out_dtype="void")
            split129: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul521, indices_or_sections=2, axis=-1)
            split_0129: R.Tensor((1, seq_len, 14336), dtype="float16") = split129[0]
            split_1129: R.Tensor((1, seq_len, 14336), dtype="float16") = split129[1]
            silu129: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0129)
            mul129: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu129, split_1129)
            lv654 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight6, model_layers_1_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims522: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv654, axes=None)
            matmul522: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul129, permute_dims522, out_dtype="void")
            add259: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul522, add258)
            rms_norm264: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add259, model_layers_2_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv655 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight6, model_layers_2_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims523: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv655, axes=None)
            matmul523: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm264, permute_dims523, out_dtype="void")
            reshape520: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul523, R.shape([1, seq_len, 48, 128]))
            reshape521: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape520, R.shape([seq_len, 48, 128]))
            lv656 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape521), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape522: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv656, R.shape([1, seq_len, 32, 128]))
            reshape523: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape522, R.shape([1, seq_len, 4096]))
            lv657 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight6, model_layers_2_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims524: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv657, axes=None)
            matmul524: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape523, permute_dims524, out_dtype="void")
            add260: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul524, add259)
            rms_norm265: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add260, model_layers_2_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv658 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight6, model_layers_2_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims525: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv658, axes=None)
            matmul525: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm265, permute_dims525, out_dtype="void")
            split130: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul525, indices_or_sections=2, axis=-1)
            split_0130: R.Tensor((1, seq_len, 14336), dtype="float16") = split130[0]
            split_1130: R.Tensor((1, seq_len, 14336), dtype="float16") = split130[1]
            silu130: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0130)
            mul130: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu130, split_1130)
            lv659 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight6, model_layers_2_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims526: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv659, axes=None)
            matmul526: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul130, permute_dims526, out_dtype="void")
            add261: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul526, add260)
            rms_norm266: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add261, model_layers_3_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv660 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight6, model_layers_3_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims527: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv660, axes=None)
            matmul527: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm266, permute_dims527, out_dtype="void")
            reshape524: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul527, R.shape([1, seq_len, 48, 128]))
            reshape525: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape524, R.shape([seq_len, 48, 128]))
            lv661 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape525), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape526: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv661, R.shape([1, seq_len, 32, 128]))
            reshape527: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape526, R.shape([1, seq_len, 4096]))
            lv662 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight6, model_layers_3_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims528: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv662, axes=None)
            matmul528: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape527, permute_dims528, out_dtype="void")
            add262: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul528, add261)
            rms_norm267: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add262, model_layers_3_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv663 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight6, model_layers_3_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims529: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv663, axes=None)
            matmul529: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm267, permute_dims529, out_dtype="void")
            split131: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul529, indices_or_sections=2, axis=-1)
            split_0131: R.Tensor((1, seq_len, 14336), dtype="float16") = split131[0]
            split_1131: R.Tensor((1, seq_len, 14336), dtype="float16") = split131[1]
            silu131: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0131)
            mul131: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu131, split_1131)
            lv664 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight6, model_layers_3_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims530: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv664, axes=None)
            matmul530: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul131, permute_dims530, out_dtype="void")
            add263: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul530, add262)
            rms_norm268: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add263, model_layers_4_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv665 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight6, model_layers_4_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims531: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv665, axes=None)
            matmul531: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm268, permute_dims531, out_dtype="void")
            reshape528: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul531, R.shape([1, seq_len, 48, 128]))
            reshape529: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape528, R.shape([seq_len, 48, 128]))
            lv666 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape529), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape530: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv666, R.shape([1, seq_len, 32, 128]))
            reshape531: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape530, R.shape([1, seq_len, 4096]))
            lv667 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight6, model_layers_4_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims532: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv667, axes=None)
            matmul532: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape531, permute_dims532, out_dtype="void")
            add264: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul532, add263)
            rms_norm269: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add264, model_layers_4_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv668 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight6, model_layers_4_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims533: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv668, axes=None)
            matmul533: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm269, permute_dims533, out_dtype="void")
            split132: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul533, indices_or_sections=2, axis=-1)
            split_0132: R.Tensor((1, seq_len, 14336), dtype="float16") = split132[0]
            split_1132: R.Tensor((1, seq_len, 14336), dtype="float16") = split132[1]
            silu132: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0132)
            mul132: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu132, split_1132)
            lv669 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight6, model_layers_4_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims534: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv669, axes=None)
            matmul534: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul132, permute_dims534, out_dtype="void")
            add265: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul534, add264)
            rms_norm270: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add265, model_layers_5_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv670 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight6, model_layers_5_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims535: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv670, axes=None)
            matmul535: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm270, permute_dims535, out_dtype="void")
            reshape532: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul535, R.shape([1, seq_len, 48, 128]))
            reshape533: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape532, R.shape([seq_len, 48, 128]))
            lv671 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape533), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape534: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv671, R.shape([1, seq_len, 32, 128]))
            reshape535: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape534, R.shape([1, seq_len, 4096]))
            lv672 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight6, model_layers_5_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims536: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv672, axes=None)
            matmul536: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape535, permute_dims536, out_dtype="void")
            add266: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul536, add265)
            rms_norm271: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add266, model_layers_5_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv673 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight6, model_layers_5_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims537: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv673, axes=None)
            matmul537: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm271, permute_dims537, out_dtype="void")
            split133: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul537, indices_or_sections=2, axis=-1)
            split_0133: R.Tensor((1, seq_len, 14336), dtype="float16") = split133[0]
            split_1133: R.Tensor((1, seq_len, 14336), dtype="float16") = split133[1]
            silu133: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0133)
            mul133: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu133, split_1133)
            lv674 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight6, model_layers_5_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims538: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv674, axes=None)
            matmul538: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul133, permute_dims538, out_dtype="void")
            add267: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul538, add266)
            rms_norm272: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add267, model_layers_6_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv675 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight6, model_layers_6_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims539: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv675, axes=None)
            matmul539: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm272, permute_dims539, out_dtype="void")
            reshape536: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul539, R.shape([1, seq_len, 48, 128]))
            reshape537: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape536, R.shape([seq_len, 48, 128]))
            lv676 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape537), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape538: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv676, R.shape([1, seq_len, 32, 128]))
            reshape539: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape538, R.shape([1, seq_len, 4096]))
            lv677 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight6, model_layers_6_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims540: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv677, axes=None)
            matmul540: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape539, permute_dims540, out_dtype="void")
            add268: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul540, add267)
            rms_norm273: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add268, model_layers_6_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv678 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight6, model_layers_6_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims541: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv678, axes=None)
            matmul541: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm273, permute_dims541, out_dtype="void")
            split134: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul541, indices_or_sections=2, axis=-1)
            split_0134: R.Tensor((1, seq_len, 14336), dtype="float16") = split134[0]
            split_1134: R.Tensor((1, seq_len, 14336), dtype="float16") = split134[1]
            silu134: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0134)
            mul134: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu134, split_1134)
            lv679 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight6, model_layers_6_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims542: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv679, axes=None)
            matmul542: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul134, permute_dims542, out_dtype="void")
            add269: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul542, add268)
            rms_norm274: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add269, model_layers_7_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv680 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight6, model_layers_7_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims543: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv680, axes=None)
            matmul543: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm274, permute_dims543, out_dtype="void")
            reshape540: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul543, R.shape([1, seq_len, 48, 128]))
            reshape541: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape540, R.shape([seq_len, 48, 128]))
            lv681 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape541), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape542: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv681, R.shape([1, seq_len, 32, 128]))
            reshape543: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape542, R.shape([1, seq_len, 4096]))
            lv682 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight6, model_layers_7_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims544: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv682, axes=None)
            matmul544: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape543, permute_dims544, out_dtype="void")
            add270: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul544, add269)
            rms_norm275: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add270, model_layers_7_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv683 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight6, model_layers_7_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims545: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv683, axes=None)
            matmul545: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm275, permute_dims545, out_dtype="void")
            split135: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul545, indices_or_sections=2, axis=-1)
            split_0135: R.Tensor((1, seq_len, 14336), dtype="float16") = split135[0]
            split_1135: R.Tensor((1, seq_len, 14336), dtype="float16") = split135[1]
            silu135: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0135)
            mul135: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu135, split_1135)
            lv684 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight6, model_layers_7_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims546: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv684, axes=None)
            matmul546: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul135, permute_dims546, out_dtype="void")
            add271: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul546, add270)
            rms_norm276: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add271, model_layers_8_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv685 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight6, model_layers_8_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims547: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv685, axes=None)
            matmul547: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm276, permute_dims547, out_dtype="void")
            reshape544: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul547, R.shape([1, seq_len, 48, 128]))
            reshape545: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape544, R.shape([seq_len, 48, 128]))
            lv686 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape545), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape546: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv686, R.shape([1, seq_len, 32, 128]))
            reshape547: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape546, R.shape([1, seq_len, 4096]))
            lv687 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight6, model_layers_8_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims548: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv687, axes=None)
            matmul548: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape547, permute_dims548, out_dtype="void")
            add272: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul548, add271)
            rms_norm277: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add272, model_layers_8_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv688 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight6, model_layers_8_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims549: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv688, axes=None)
            matmul549: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm277, permute_dims549, out_dtype="void")
            split136: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul549, indices_or_sections=2, axis=-1)
            split_0136: R.Tensor((1, seq_len, 14336), dtype="float16") = split136[0]
            split_1136: R.Tensor((1, seq_len, 14336), dtype="float16") = split136[1]
            silu136: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0136)
            mul136: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu136, split_1136)
            lv689 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight6, model_layers_8_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims550: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv689, axes=None)
            matmul550: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul136, permute_dims550, out_dtype="void")
            add273: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul550, add272)
            rms_norm278: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add273, model_layers_9_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv690 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight6, model_layers_9_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims551: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv690, axes=None)
            matmul551: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm278, permute_dims551, out_dtype="void")
            reshape548: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul551, R.shape([1, seq_len, 48, 128]))
            reshape549: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape548, R.shape([seq_len, 48, 128]))
            lv691 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape549), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape550: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv691, R.shape([1, seq_len, 32, 128]))
            reshape551: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape550, R.shape([1, seq_len, 4096]))
            lv692 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight6, model_layers_9_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims552: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv692, axes=None)
            matmul552: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape551, permute_dims552, out_dtype="void")
            add274: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul552, add273)
            rms_norm279: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add274, model_layers_9_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv693 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight6, model_layers_9_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims553: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv693, axes=None)
            matmul553: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm279, permute_dims553, out_dtype="void")
            split137: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul553, indices_or_sections=2, axis=-1)
            split_0137: R.Tensor((1, seq_len, 14336), dtype="float16") = split137[0]
            split_1137: R.Tensor((1, seq_len, 14336), dtype="float16") = split137[1]
            silu137: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0137)
            mul137: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu137, split_1137)
            lv694 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight6, model_layers_9_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims554: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv694, axes=None)
            matmul554: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul137, permute_dims554, out_dtype="void")
            add275: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul554, add274)
            rms_norm280: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add275, model_layers_10_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv695 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight6, model_layers_10_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims555: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv695, axes=None)
            matmul555: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm280, permute_dims555, out_dtype="void")
            reshape552: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul555, R.shape([1, seq_len, 48, 128]))
            reshape553: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape552, R.shape([seq_len, 48, 128]))
            lv696 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape553), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape554: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv696, R.shape([1, seq_len, 32, 128]))
            reshape555: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape554, R.shape([1, seq_len, 4096]))
            lv697 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight6, model_layers_10_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims556: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv697, axes=None)
            matmul556: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape555, permute_dims556, out_dtype="void")
            add276: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul556, add275)
            rms_norm281: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add276, model_layers_10_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv698 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight6, model_layers_10_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims557: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv698, axes=None)
            matmul557: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm281, permute_dims557, out_dtype="void")
            split138: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul557, indices_or_sections=2, axis=-1)
            split_0138: R.Tensor((1, seq_len, 14336), dtype="float16") = split138[0]
            split_1138: R.Tensor((1, seq_len, 14336), dtype="float16") = split138[1]
            silu138: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0138)
            mul138: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu138, split_1138)
            lv699 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight6, model_layers_10_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims558: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv699, axes=None)
            matmul558: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul138, permute_dims558, out_dtype="void")
            add277: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul558, add276)
            rms_norm282: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add277, model_layers_11_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv700 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight6, model_layers_11_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims559: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv700, axes=None)
            matmul559: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm282, permute_dims559, out_dtype="void")
            reshape556: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul559, R.shape([1, seq_len, 48, 128]))
            reshape557: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape556, R.shape([seq_len, 48, 128]))
            lv701 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape557), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape558: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv701, R.shape([1, seq_len, 32, 128]))
            reshape559: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape558, R.shape([1, seq_len, 4096]))
            lv702 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight6, model_layers_11_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims560: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv702, axes=None)
            matmul560: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape559, permute_dims560, out_dtype="void")
            add278: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul560, add277)
            rms_norm283: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add278, model_layers_11_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv703 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight6, model_layers_11_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims561: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv703, axes=None)
            matmul561: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm283, permute_dims561, out_dtype="void")
            split139: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul561, indices_or_sections=2, axis=-1)
            split_0139: R.Tensor((1, seq_len, 14336), dtype="float16") = split139[0]
            split_1139: R.Tensor((1, seq_len, 14336), dtype="float16") = split139[1]
            silu139: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0139)
            mul139: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu139, split_1139)
            lv704 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight6, model_layers_11_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims562: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv704, axes=None)
            matmul562: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul139, permute_dims562, out_dtype="void")
            add279: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul562, add278)
            rms_norm284: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add279, model_layers_12_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv705 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight6, model_layers_12_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims563: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv705, axes=None)
            matmul563: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm284, permute_dims563, out_dtype="void")
            reshape560: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul563, R.shape([1, seq_len, 48, 128]))
            reshape561: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape560, R.shape([seq_len, 48, 128]))
            lv706 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape561), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape562: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv706, R.shape([1, seq_len, 32, 128]))
            reshape563: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape562, R.shape([1, seq_len, 4096]))
            lv707 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight6, model_layers_12_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims564: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv707, axes=None)
            matmul564: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape563, permute_dims564, out_dtype="void")
            add280: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul564, add279)
            rms_norm285: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add280, model_layers_12_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv708 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight6, model_layers_12_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims565: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv708, axes=None)
            matmul565: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm285, permute_dims565, out_dtype="void")
            split140: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul565, indices_or_sections=2, axis=-1)
            split_0140: R.Tensor((1, seq_len, 14336), dtype="float16") = split140[0]
            split_1140: R.Tensor((1, seq_len, 14336), dtype="float16") = split140[1]
            silu140: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0140)
            mul140: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu140, split_1140)
            lv709 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight6, model_layers_12_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims566: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv709, axes=None)
            matmul566: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul140, permute_dims566, out_dtype="void")
            add281: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul566, add280)
            rms_norm286: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add281, model_layers_13_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv710 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight6, model_layers_13_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims567: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv710, axes=None)
            matmul567: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm286, permute_dims567, out_dtype="void")
            reshape564: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul567, R.shape([1, seq_len, 48, 128]))
            reshape565: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape564, R.shape([seq_len, 48, 128]))
            lv711 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape565), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape566: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv711, R.shape([1, seq_len, 32, 128]))
            reshape567: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape566, R.shape([1, seq_len, 4096]))
            lv712 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight6, model_layers_13_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims568: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv712, axes=None)
            matmul568: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape567, permute_dims568, out_dtype="void")
            add282: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul568, add281)
            rms_norm287: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add282, model_layers_13_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv713 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight6, model_layers_13_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims569: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv713, axes=None)
            matmul569: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm287, permute_dims569, out_dtype="void")
            split141: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul569, indices_or_sections=2, axis=-1)
            split_0141: R.Tensor((1, seq_len, 14336), dtype="float16") = split141[0]
            split_1141: R.Tensor((1, seq_len, 14336), dtype="float16") = split141[1]
            silu141: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0141)
            mul141: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu141, split_1141)
            lv714 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight6, model_layers_13_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims570: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv714, axes=None)
            matmul570: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul141, permute_dims570, out_dtype="void")
            add283: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul570, add282)
            rms_norm288: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add283, model_layers_14_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv715 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight6, model_layers_14_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims571: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv715, axes=None)
            matmul571: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm288, permute_dims571, out_dtype="void")
            reshape568: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul571, R.shape([1, seq_len, 48, 128]))
            reshape569: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape568, R.shape([seq_len, 48, 128]))
            lv716 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape569), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape570: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv716, R.shape([1, seq_len, 32, 128]))
            reshape571: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape570, R.shape([1, seq_len, 4096]))
            lv717 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight6, model_layers_14_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims572: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv717, axes=None)
            matmul572: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape571, permute_dims572, out_dtype="void")
            add284: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul572, add283)
            rms_norm289: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add284, model_layers_14_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv718 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight6, model_layers_14_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims573: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv718, axes=None)
            matmul573: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm289, permute_dims573, out_dtype="void")
            split142: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul573, indices_or_sections=2, axis=-1)
            split_0142: R.Tensor((1, seq_len, 14336), dtype="float16") = split142[0]
            split_1142: R.Tensor((1, seq_len, 14336), dtype="float16") = split142[1]
            silu142: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0142)
            mul142: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu142, split_1142)
            lv719 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight6, model_layers_14_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims574: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv719, axes=None)
            matmul574: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul142, permute_dims574, out_dtype="void")
            add285: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul574, add284)
            rms_norm290: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add285, model_layers_15_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv720 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight6, model_layers_15_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims575: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv720, axes=None)
            matmul575: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm290, permute_dims575, out_dtype="void")
            reshape572: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul575, R.shape([1, seq_len, 48, 128]))
            reshape573: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape572, R.shape([seq_len, 48, 128]))
            lv721 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape573), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape574: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv721, R.shape([1, seq_len, 32, 128]))
            reshape575: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape574, R.shape([1, seq_len, 4096]))
            lv722 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight6, model_layers_15_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims576: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv722, axes=None)
            matmul576: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape575, permute_dims576, out_dtype="void")
            add286: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul576, add285)
            rms_norm291: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add286, model_layers_15_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv723 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight6, model_layers_15_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims577: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv723, axes=None)
            matmul577: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm291, permute_dims577, out_dtype="void")
            split143: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul577, indices_or_sections=2, axis=-1)
            split_0143: R.Tensor((1, seq_len, 14336), dtype="float16") = split143[0]
            split_1143: R.Tensor((1, seq_len, 14336), dtype="float16") = split143[1]
            silu143: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0143)
            mul143: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu143, split_1143)
            lv724 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight6, model_layers_15_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims578: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv724, axes=None)
            matmul578: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul143, permute_dims578, out_dtype="void")
            add287: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul578, add286)
            rms_norm292: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add287, model_layers_16_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv725 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight6, model_layers_16_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims579: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv725, axes=None)
            matmul579: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm292, permute_dims579, out_dtype="void")
            reshape576: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul579, R.shape([1, seq_len, 48, 128]))
            reshape577: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape576, R.shape([seq_len, 48, 128]))
            lv726 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape577), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape578: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv726, R.shape([1, seq_len, 32, 128]))
            reshape579: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape578, R.shape([1, seq_len, 4096]))
            lv727 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight6, model_layers_16_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims580: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv727, axes=None)
            matmul580: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape579, permute_dims580, out_dtype="void")
            add288: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul580, add287)
            rms_norm293: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add288, model_layers_16_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv728 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight6, model_layers_16_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims581: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv728, axes=None)
            matmul581: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm293, permute_dims581, out_dtype="void")
            split144: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul581, indices_or_sections=2, axis=-1)
            split_0144: R.Tensor((1, seq_len, 14336), dtype="float16") = split144[0]
            split_1144: R.Tensor((1, seq_len, 14336), dtype="float16") = split144[1]
            silu144: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0144)
            mul144: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu144, split_1144)
            lv729 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight6, model_layers_16_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims582: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv729, axes=None)
            matmul582: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul144, permute_dims582, out_dtype="void")
            add289: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul582, add288)
            rms_norm294: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add289, model_layers_17_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv730 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight6, model_layers_17_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims583: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv730, axes=None)
            matmul583: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm294, permute_dims583, out_dtype="void")
            reshape580: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul583, R.shape([1, seq_len, 48, 128]))
            reshape581: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape580, R.shape([seq_len, 48, 128]))
            lv731 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape581), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape582: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv731, R.shape([1, seq_len, 32, 128]))
            reshape583: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape582, R.shape([1, seq_len, 4096]))
            lv732 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight6, model_layers_17_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims584: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv732, axes=None)
            matmul584: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape583, permute_dims584, out_dtype="void")
            add290: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul584, add289)
            rms_norm295: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add290, model_layers_17_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv733 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight6, model_layers_17_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims585: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv733, axes=None)
            matmul585: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm295, permute_dims585, out_dtype="void")
            split145: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul585, indices_or_sections=2, axis=-1)
            split_0145: R.Tensor((1, seq_len, 14336), dtype="float16") = split145[0]
            split_1145: R.Tensor((1, seq_len, 14336), dtype="float16") = split145[1]
            silu145: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0145)
            mul145: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu145, split_1145)
            lv734 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight6, model_layers_17_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims586: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv734, axes=None)
            matmul586: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul145, permute_dims586, out_dtype="void")
            add291: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul586, add290)
            rms_norm296: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add291, model_layers_18_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv735 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight6, model_layers_18_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims587: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv735, axes=None)
            matmul587: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm296, permute_dims587, out_dtype="void")
            reshape584: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul587, R.shape([1, seq_len, 48, 128]))
            reshape585: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape584, R.shape([seq_len, 48, 128]))
            lv736 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape585), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape586: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv736, R.shape([1, seq_len, 32, 128]))
            reshape587: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape586, R.shape([1, seq_len, 4096]))
            lv737 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight6, model_layers_18_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims588: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv737, axes=None)
            matmul588: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape587, permute_dims588, out_dtype="void")
            add292: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul588, add291)
            rms_norm297: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add292, model_layers_18_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv738 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight6, model_layers_18_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims589: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv738, axes=None)
            matmul589: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm297, permute_dims589, out_dtype="void")
            split146: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul589, indices_or_sections=2, axis=-1)
            split_0146: R.Tensor((1, seq_len, 14336), dtype="float16") = split146[0]
            split_1146: R.Tensor((1, seq_len, 14336), dtype="float16") = split146[1]
            silu146: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0146)
            mul146: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu146, split_1146)
            lv739 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight6, model_layers_18_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims590: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv739, axes=None)
            matmul590: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul146, permute_dims590, out_dtype="void")
            add293: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul590, add292)
            rms_norm298: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add293, model_layers_19_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv740 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight6, model_layers_19_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims591: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv740, axes=None)
            matmul591: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm298, permute_dims591, out_dtype="void")
            reshape588: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul591, R.shape([1, seq_len, 48, 128]))
            reshape589: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape588, R.shape([seq_len, 48, 128]))
            lv741 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape589), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape590: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv741, R.shape([1, seq_len, 32, 128]))
            reshape591: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape590, R.shape([1, seq_len, 4096]))
            lv742 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight6, model_layers_19_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims592: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv742, axes=None)
            matmul592: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape591, permute_dims592, out_dtype="void")
            add294: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul592, add293)
            rms_norm299: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add294, model_layers_19_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv743 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight6, model_layers_19_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims593: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv743, axes=None)
            matmul593: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm299, permute_dims593, out_dtype="void")
            split147: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul593, indices_or_sections=2, axis=-1)
            split_0147: R.Tensor((1, seq_len, 14336), dtype="float16") = split147[0]
            split_1147: R.Tensor((1, seq_len, 14336), dtype="float16") = split147[1]
            silu147: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0147)
            mul147: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu147, split_1147)
            lv744 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight6, model_layers_19_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims594: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv744, axes=None)
            matmul594: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul147, permute_dims594, out_dtype="void")
            add295: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul594, add294)
            rms_norm300: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add295, model_layers_20_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv745 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight6, model_layers_20_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims595: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv745, axes=None)
            matmul595: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm300, permute_dims595, out_dtype="void")
            reshape592: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul595, R.shape([1, seq_len, 48, 128]))
            reshape593: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape592, R.shape([seq_len, 48, 128]))
            lv746 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape593), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape594: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv746, R.shape([1, seq_len, 32, 128]))
            reshape595: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape594, R.shape([1, seq_len, 4096]))
            lv747 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight6, model_layers_20_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims596: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv747, axes=None)
            matmul596: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape595, permute_dims596, out_dtype="void")
            add296: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul596, add295)
            rms_norm301: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add296, model_layers_20_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv748 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight6, model_layers_20_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims597: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv748, axes=None)
            matmul597: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm301, permute_dims597, out_dtype="void")
            split148: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul597, indices_or_sections=2, axis=-1)
            split_0148: R.Tensor((1, seq_len, 14336), dtype="float16") = split148[0]
            split_1148: R.Tensor((1, seq_len, 14336), dtype="float16") = split148[1]
            silu148: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0148)
            mul148: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu148, split_1148)
            lv749 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight6, model_layers_20_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims598: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv749, axes=None)
            matmul598: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul148, permute_dims598, out_dtype="void")
            add297: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul598, add296)
            rms_norm302: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add297, model_layers_21_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv750 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight6, model_layers_21_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims599: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv750, axes=None)
            matmul599: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm302, permute_dims599, out_dtype="void")
            reshape596: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul599, R.shape([1, seq_len, 48, 128]))
            reshape597: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape596, R.shape([seq_len, 48, 128]))
            lv751 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape597), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape598: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv751, R.shape([1, seq_len, 32, 128]))
            reshape599: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape598, R.shape([1, seq_len, 4096]))
            lv752 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight6, model_layers_21_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims600: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv752, axes=None)
            matmul600: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape599, permute_dims600, out_dtype="void")
            add298: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul600, add297)
            rms_norm303: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add298, model_layers_21_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv753 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight6, model_layers_21_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims601: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv753, axes=None)
            matmul601: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm303, permute_dims601, out_dtype="void")
            split149: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul601, indices_or_sections=2, axis=-1)
            split_0149: R.Tensor((1, seq_len, 14336), dtype="float16") = split149[0]
            split_1149: R.Tensor((1, seq_len, 14336), dtype="float16") = split149[1]
            silu149: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0149)
            mul149: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu149, split_1149)
            lv754 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight6, model_layers_21_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims602: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv754, axes=None)
            matmul602: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul149, permute_dims602, out_dtype="void")
            add299: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul602, add298)
            rms_norm304: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add299, model_layers_22_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv755 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight6, model_layers_22_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims603: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv755, axes=None)
            matmul603: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm304, permute_dims603, out_dtype="void")
            reshape600: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul603, R.shape([1, seq_len, 48, 128]))
            reshape601: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape600, R.shape([seq_len, 48, 128]))
            lv756 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape601), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape602: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv756, R.shape([1, seq_len, 32, 128]))
            reshape603: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape602, R.shape([1, seq_len, 4096]))
            lv757 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight6, model_layers_22_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims604: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv757, axes=None)
            matmul604: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape603, permute_dims604, out_dtype="void")
            add300: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul604, add299)
            rms_norm305: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add300, model_layers_22_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv758 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight6, model_layers_22_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims605: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv758, axes=None)
            matmul605: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm305, permute_dims605, out_dtype="void")
            split150: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul605, indices_or_sections=2, axis=-1)
            split_0150: R.Tensor((1, seq_len, 14336), dtype="float16") = split150[0]
            split_1150: R.Tensor((1, seq_len, 14336), dtype="float16") = split150[1]
            silu150: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0150)
            mul150: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu150, split_1150)
            lv759 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight6, model_layers_22_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims606: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv759, axes=None)
            matmul606: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul150, permute_dims606, out_dtype="void")
            add301: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul606, add300)
            rms_norm306: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add301, model_layers_23_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv760 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight6, model_layers_23_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims607: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv760, axes=None)
            matmul607: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm306, permute_dims607, out_dtype="void")
            reshape604: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul607, R.shape([1, seq_len, 48, 128]))
            reshape605: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape604, R.shape([seq_len, 48, 128]))
            lv761 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape605), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape606: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv761, R.shape([1, seq_len, 32, 128]))
            reshape607: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape606, R.shape([1, seq_len, 4096]))
            lv762 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight6, model_layers_23_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims608: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv762, axes=None)
            matmul608: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape607, permute_dims608, out_dtype="void")
            add302: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul608, add301)
            rms_norm307: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add302, model_layers_23_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv763 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight6, model_layers_23_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims609: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv763, axes=None)
            matmul609: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm307, permute_dims609, out_dtype="void")
            split151: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul609, indices_or_sections=2, axis=-1)
            split_0151: R.Tensor((1, seq_len, 14336), dtype="float16") = split151[0]
            split_1151: R.Tensor((1, seq_len, 14336), dtype="float16") = split151[1]
            silu151: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0151)
            mul151: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu151, split_1151)
            lv764 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight6, model_layers_23_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims610: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv764, axes=None)
            matmul610: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul151, permute_dims610, out_dtype="void")
            add303: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul610, add302)
            rms_norm308: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add303, model_layers_24_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv765 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight6, model_layers_24_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims611: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv765, axes=None)
            matmul611: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm308, permute_dims611, out_dtype="void")
            reshape608: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul611, R.shape([1, seq_len, 48, 128]))
            reshape609: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape608, R.shape([seq_len, 48, 128]))
            lv766 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape609), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape610: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv766, R.shape([1, seq_len, 32, 128]))
            reshape611: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape610, R.shape([1, seq_len, 4096]))
            lv767 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight6, model_layers_24_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims612: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv767, axes=None)
            matmul612: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape611, permute_dims612, out_dtype="void")
            add304: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul612, add303)
            rms_norm309: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add304, model_layers_24_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv768 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight6, model_layers_24_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims613: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv768, axes=None)
            matmul613: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm309, permute_dims613, out_dtype="void")
            split152: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul613, indices_or_sections=2, axis=-1)
            split_0152: R.Tensor((1, seq_len, 14336), dtype="float16") = split152[0]
            split_1152: R.Tensor((1, seq_len, 14336), dtype="float16") = split152[1]
            silu152: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0152)
            mul152: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu152, split_1152)
            lv769 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight6, model_layers_24_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims614: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv769, axes=None)
            matmul614: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul152, permute_dims614, out_dtype="void")
            add305: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul614, add304)
            rms_norm310: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add305, model_layers_25_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv770 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight6, model_layers_25_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims615: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv770, axes=None)
            matmul615: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm310, permute_dims615, out_dtype="void")
            reshape612: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul615, R.shape([1, seq_len, 48, 128]))
            reshape613: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape612, R.shape([seq_len, 48, 128]))
            lv771 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape613), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape614: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv771, R.shape([1, seq_len, 32, 128]))
            reshape615: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape614, R.shape([1, seq_len, 4096]))
            lv772 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight6, model_layers_25_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims616: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv772, axes=None)
            matmul616: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape615, permute_dims616, out_dtype="void")
            add306: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul616, add305)
            rms_norm311: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add306, model_layers_25_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv773 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight6, model_layers_25_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims617: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv773, axes=None)
            matmul617: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm311, permute_dims617, out_dtype="void")
            split153: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul617, indices_or_sections=2, axis=-1)
            split_0153: R.Tensor((1, seq_len, 14336), dtype="float16") = split153[0]
            split_1153: R.Tensor((1, seq_len, 14336), dtype="float16") = split153[1]
            silu153: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0153)
            mul153: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu153, split_1153)
            lv774 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight6, model_layers_25_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims618: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv774, axes=None)
            matmul618: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul153, permute_dims618, out_dtype="void")
            add307: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul618, add306)
            rms_norm312: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add307, model_layers_26_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv775 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight6, model_layers_26_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims619: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv775, axes=None)
            matmul619: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm312, permute_dims619, out_dtype="void")
            reshape616: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul619, R.shape([1, seq_len, 48, 128]))
            reshape617: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape616, R.shape([seq_len, 48, 128]))
            lv776 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape617), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape618: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv776, R.shape([1, seq_len, 32, 128]))
            reshape619: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape618, R.shape([1, seq_len, 4096]))
            lv777 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight6, model_layers_26_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims620: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv777, axes=None)
            matmul620: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape619, permute_dims620, out_dtype="void")
            add308: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul620, add307)
            rms_norm313: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add308, model_layers_26_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv778 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight6, model_layers_26_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims621: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv778, axes=None)
            matmul621: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm313, permute_dims621, out_dtype="void")
            split154: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul621, indices_or_sections=2, axis=-1)
            split_0154: R.Tensor((1, seq_len, 14336), dtype="float16") = split154[0]
            split_1154: R.Tensor((1, seq_len, 14336), dtype="float16") = split154[1]
            silu154: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0154)
            mul154: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu154, split_1154)
            lv779 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight6, model_layers_26_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims622: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv779, axes=None)
            matmul622: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul154, permute_dims622, out_dtype="void")
            add309: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul622, add308)
            rms_norm314: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add309, model_layers_27_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv780 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight6, model_layers_27_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims623: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv780, axes=None)
            matmul623: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm314, permute_dims623, out_dtype="void")
            reshape620: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul623, R.shape([1, seq_len, 48, 128]))
            reshape621: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape620, R.shape([seq_len, 48, 128]))
            lv781 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape621), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape622: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv781, R.shape([1, seq_len, 32, 128]))
            reshape623: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape622, R.shape([1, seq_len, 4096]))
            lv782 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight6, model_layers_27_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims624: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv782, axes=None)
            matmul624: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape623, permute_dims624, out_dtype="void")
            add310: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul624, add309)
            rms_norm315: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add310, model_layers_27_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv783 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight6, model_layers_27_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims625: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv783, axes=None)
            matmul625: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm315, permute_dims625, out_dtype="void")
            split155: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul625, indices_or_sections=2, axis=-1)
            split_0155: R.Tensor((1, seq_len, 14336), dtype="float16") = split155[0]
            split_1155: R.Tensor((1, seq_len, 14336), dtype="float16") = split155[1]
            silu155: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0155)
            mul155: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu155, split_1155)
            lv784 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight6, model_layers_27_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims626: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv784, axes=None)
            matmul626: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul155, permute_dims626, out_dtype="void")
            add311: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul626, add310)
            rms_norm316: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add311, model_layers_28_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv785 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight6, model_layers_28_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims627: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv785, axes=None)
            matmul627: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm316, permute_dims627, out_dtype="void")
            reshape624: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul627, R.shape([1, seq_len, 48, 128]))
            reshape625: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape624, R.shape([seq_len, 48, 128]))
            lv786 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape625), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape626: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv786, R.shape([1, seq_len, 32, 128]))
            reshape627: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape626, R.shape([1, seq_len, 4096]))
            lv787 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight6, model_layers_28_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims628: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv787, axes=None)
            matmul628: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape627, permute_dims628, out_dtype="void")
            add312: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul628, add311)
            rms_norm317: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add312, model_layers_28_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv788 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight6, model_layers_28_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims629: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv788, axes=None)
            matmul629: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm317, permute_dims629, out_dtype="void")
            split156: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul629, indices_or_sections=2, axis=-1)
            split_0156: R.Tensor((1, seq_len, 14336), dtype="float16") = split156[0]
            split_1156: R.Tensor((1, seq_len, 14336), dtype="float16") = split156[1]
            silu156: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0156)
            mul156: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu156, split_1156)
            lv789 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight6, model_layers_28_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims630: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv789, axes=None)
            matmul630: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul156, permute_dims630, out_dtype="void")
            add313: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul630, add312)
            rms_norm318: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add313, model_layers_29_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv790 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight6, model_layers_29_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims631: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv790, axes=None)
            matmul631: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm318, permute_dims631, out_dtype="void")
            reshape628: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul631, R.shape([1, seq_len, 48, 128]))
            reshape629: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape628, R.shape([seq_len, 48, 128]))
            lv791 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape629), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape630: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv791, R.shape([1, seq_len, 32, 128]))
            reshape631: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape630, R.shape([1, seq_len, 4096]))
            lv792 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight6, model_layers_29_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims632: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv792, axes=None)
            matmul632: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape631, permute_dims632, out_dtype="void")
            add314: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul632, add313)
            rms_norm319: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add314, model_layers_29_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv793 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight6, model_layers_29_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims633: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv793, axes=None)
            matmul633: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm319, permute_dims633, out_dtype="void")
            split157: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul633, indices_or_sections=2, axis=-1)
            split_0157: R.Tensor((1, seq_len, 14336), dtype="float16") = split157[0]
            split_1157: R.Tensor((1, seq_len, 14336), dtype="float16") = split157[1]
            silu157: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0157)
            mul157: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu157, split_1157)
            lv794 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight6, model_layers_29_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims634: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv794, axes=None)
            matmul634: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul157, permute_dims634, out_dtype="void")
            add315: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul634, add314)
            rms_norm320: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add315, model_layers_30_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv795 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight6, model_layers_30_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims635: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv795, axes=None)
            matmul635: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm320, permute_dims635, out_dtype="void")
            reshape632: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul635, R.shape([1, seq_len, 48, 128]))
            reshape633: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape632, R.shape([seq_len, 48, 128]))
            lv796 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape633), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape634: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv796, R.shape([1, seq_len, 32, 128]))
            reshape635: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape634, R.shape([1, seq_len, 4096]))
            lv797 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight6, model_layers_30_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims636: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv797, axes=None)
            matmul636: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape635, permute_dims636, out_dtype="void")
            add316: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul636, add315)
            rms_norm321: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add316, model_layers_30_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv798 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight6, model_layers_30_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims637: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv798, axes=None)
            matmul637: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm321, permute_dims637, out_dtype="void")
            split158: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul637, indices_or_sections=2, axis=-1)
            split_0158: R.Tensor((1, seq_len, 14336), dtype="float16") = split158[0]
            split_1158: R.Tensor((1, seq_len, 14336), dtype="float16") = split158[1]
            silu158: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0158)
            mul158: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu158, split_1158)
            lv799 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight6, model_layers_30_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims638: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv799, axes=None)
            matmul638: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul158, permute_dims638, out_dtype="void")
            add317: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul638, add316)
            rms_norm322: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add317, model_layers_31_input_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv800 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight6, model_layers_31_self_attn_qkv_proj_q_scale6), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims639: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv800, axes=None)
            matmul639: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm322, permute_dims639, out_dtype="void")
            reshape636: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul639, R.shape([1, seq_len, 48, 128]))
            reshape637: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape636, R.shape([seq_len, 48, 128]))
            lv801 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape637), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape638: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv801, R.shape([1, seq_len, 32, 128]))
            reshape639: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape638, R.shape([1, seq_len, 4096]))
            lv802 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight6, model_layers_31_self_attn_o_proj_q_scale6), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims640: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv802, axes=None)
            matmul640: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape639, permute_dims640, out_dtype="void")
            add318: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul640, add317)
            rms_norm323: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add318, model_layers_31_post_attention_layernorm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            lv803 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight6, model_layers_31_mlp_gate_up_proj_q_scale6), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims641: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv803, axes=None)
            matmul641: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm323, permute_dims641, out_dtype="void")
            split159: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul641, indices_or_sections=2, axis=-1)
            split_0159: R.Tensor((1, seq_len, 14336), dtype="float16") = split159[0]
            split_1159: R.Tensor((1, seq_len, 14336), dtype="float16") = split159[1]
            silu159: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0159)
            mul159: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu159, split_1159)
            lv804 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight6, model_layers_31_mlp_down_proj_q_scale6), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims642: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv804, axes=None)
            matmul642: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul159, permute_dims642, out_dtype="void")
            add319: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul642, add318)
            rms_norm324: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add319, model_norm_weight6, axes=[-1], epsilon=1.0000000000000001e-05)
            take2: R.Tensor((1, batch_size, 4096), dtype="float16") = R.take(rms_norm324, logit_positions, axis=1)
            lv805 = R.call_tir(cls.dequantize, (lm_head_q_weight6, lm_head_q_scale6), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            permute_dims643: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv805, axes=None)
            matmul643: R.Tensor((1, batch_size, vocab_size), dtype="float16") = R.matmul(take2, permute_dims643, out_dtype="void")
            astype3: R.Tensor((1, batch_size, vocab_size), dtype="float32") = R.astype(matmul643, dtype="float32")
            gv7: R.Tuple(R.Tensor((1, batch_size, vocab_size), dtype="float32"), R.Object) = astype3, paged_kv_cache
            R.output(gv7)
        return gv7

    @R.function
    def batch_prefill_to_last_hidden_states(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight9: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale9: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight9: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale9: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight9: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale9: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight9: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale9: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight9: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale9: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight9: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight9: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale9: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm455: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1128 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight9, model_layers_0_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims902: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1128, axes=None)
            matmul902: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm455, permute_dims902, out_dtype="void")
            reshape896: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul902, R.shape([1, seq_len, 48, 128]))
            reshape897: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape896, R.shape([seq_len, 48, 128]))
            lv1129 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape897), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape898: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1129, R.shape([1, seq_len, 32, 128]))
            reshape899: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape898, R.shape([1, seq_len, 4096]))
            lv1130 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight9, model_layers_0_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims903: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1130, axes=None)
            matmul903: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape899, permute_dims903, out_dtype="void")
            add448: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul903, input_embeds)
            rms_norm456: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add448, model_layers_0_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1131 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight9, model_layers_0_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims904: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1131, axes=None)
            matmul904: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm456, permute_dims904, out_dtype="void")
            split224: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul904, indices_or_sections=2, axis=-1)
            split_0224: R.Tensor((1, seq_len, 14336), dtype="float16") = split224[0]
            split_1224: R.Tensor((1, seq_len, 14336), dtype="float16") = split224[1]
            silu224: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0224)
            mul224: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu224, split_1224)
            lv1132 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight9, model_layers_0_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims905: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1132, axes=None)
            matmul905: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul224, permute_dims905, out_dtype="void")
            add449: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul905, add448)
            rms_norm457: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add449, model_layers_1_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1133 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight9, model_layers_1_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims906: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1133, axes=None)
            matmul906: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm457, permute_dims906, out_dtype="void")
            reshape900: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul906, R.shape([1, seq_len, 48, 128]))
            reshape901: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape900, R.shape([seq_len, 48, 128]))
            lv1134 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape901), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape902: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1134, R.shape([1, seq_len, 32, 128]))
            reshape903: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape902, R.shape([1, seq_len, 4096]))
            lv1135 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight9, model_layers_1_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims907: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1135, axes=None)
            matmul907: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape903, permute_dims907, out_dtype="void")
            add450: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul907, add449)
            rms_norm458: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add450, model_layers_1_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1136 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight9, model_layers_1_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims908: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1136, axes=None)
            matmul908: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm458, permute_dims908, out_dtype="void")
            split225: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul908, indices_or_sections=2, axis=-1)
            split_0225: R.Tensor((1, seq_len, 14336), dtype="float16") = split225[0]
            split_1225: R.Tensor((1, seq_len, 14336), dtype="float16") = split225[1]
            silu225: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0225)
            mul225: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu225, split_1225)
            lv1137 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight9, model_layers_1_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims909: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1137, axes=None)
            matmul909: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul225, permute_dims909, out_dtype="void")
            add451: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul909, add450)
            rms_norm459: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add451, model_layers_2_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1138 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight9, model_layers_2_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims910: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1138, axes=None)
            matmul910: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm459, permute_dims910, out_dtype="void")
            reshape904: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul910, R.shape([1, seq_len, 48, 128]))
            reshape905: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape904, R.shape([seq_len, 48, 128]))
            lv1139 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape905), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape906: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1139, R.shape([1, seq_len, 32, 128]))
            reshape907: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape906, R.shape([1, seq_len, 4096]))
            lv1140 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight9, model_layers_2_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims911: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1140, axes=None)
            matmul911: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape907, permute_dims911, out_dtype="void")
            add452: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul911, add451)
            rms_norm460: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add452, model_layers_2_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1141 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight9, model_layers_2_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims912: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1141, axes=None)
            matmul912: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm460, permute_dims912, out_dtype="void")
            split226: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul912, indices_or_sections=2, axis=-1)
            split_0226: R.Tensor((1, seq_len, 14336), dtype="float16") = split226[0]
            split_1226: R.Tensor((1, seq_len, 14336), dtype="float16") = split226[1]
            silu226: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0226)
            mul226: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu226, split_1226)
            lv1142 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight9, model_layers_2_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims913: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1142, axes=None)
            matmul913: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul226, permute_dims913, out_dtype="void")
            add453: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul913, add452)
            rms_norm461: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add453, model_layers_3_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1143 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight9, model_layers_3_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims914: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1143, axes=None)
            matmul914: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm461, permute_dims914, out_dtype="void")
            reshape908: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul914, R.shape([1, seq_len, 48, 128]))
            reshape909: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape908, R.shape([seq_len, 48, 128]))
            lv1144 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape909), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape910: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1144, R.shape([1, seq_len, 32, 128]))
            reshape911: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape910, R.shape([1, seq_len, 4096]))
            lv1145 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight9, model_layers_3_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims915: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1145, axes=None)
            matmul915: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape911, permute_dims915, out_dtype="void")
            add454: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul915, add453)
            rms_norm462: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add454, model_layers_3_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1146 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight9, model_layers_3_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims916: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1146, axes=None)
            matmul916: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm462, permute_dims916, out_dtype="void")
            split227: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul916, indices_or_sections=2, axis=-1)
            split_0227: R.Tensor((1, seq_len, 14336), dtype="float16") = split227[0]
            split_1227: R.Tensor((1, seq_len, 14336), dtype="float16") = split227[1]
            silu227: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0227)
            mul227: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu227, split_1227)
            lv1147 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight9, model_layers_3_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims917: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1147, axes=None)
            matmul917: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul227, permute_dims917, out_dtype="void")
            add455: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul917, add454)
            rms_norm463: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add455, model_layers_4_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1148 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight9, model_layers_4_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims918: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1148, axes=None)
            matmul918: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm463, permute_dims918, out_dtype="void")
            reshape912: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul918, R.shape([1, seq_len, 48, 128]))
            reshape913: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape912, R.shape([seq_len, 48, 128]))
            lv1149 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape913), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape914: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1149, R.shape([1, seq_len, 32, 128]))
            reshape915: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape914, R.shape([1, seq_len, 4096]))
            lv1150 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight9, model_layers_4_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims919: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1150, axes=None)
            matmul919: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape915, permute_dims919, out_dtype="void")
            add456: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul919, add455)
            rms_norm464: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add456, model_layers_4_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1151 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight9, model_layers_4_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims920: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1151, axes=None)
            matmul920: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm464, permute_dims920, out_dtype="void")
            split228: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul920, indices_or_sections=2, axis=-1)
            split_0228: R.Tensor((1, seq_len, 14336), dtype="float16") = split228[0]
            split_1228: R.Tensor((1, seq_len, 14336), dtype="float16") = split228[1]
            silu228: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0228)
            mul228: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu228, split_1228)
            lv1152 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight9, model_layers_4_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims921: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1152, axes=None)
            matmul921: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul228, permute_dims921, out_dtype="void")
            add457: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul921, add456)
            rms_norm465: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add457, model_layers_5_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1153 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight9, model_layers_5_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims922: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1153, axes=None)
            matmul922: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm465, permute_dims922, out_dtype="void")
            reshape916: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul922, R.shape([1, seq_len, 48, 128]))
            reshape917: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape916, R.shape([seq_len, 48, 128]))
            lv1154 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape917), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape918: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1154, R.shape([1, seq_len, 32, 128]))
            reshape919: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape918, R.shape([1, seq_len, 4096]))
            lv1155 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight9, model_layers_5_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims923: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1155, axes=None)
            matmul923: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape919, permute_dims923, out_dtype="void")
            add458: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul923, add457)
            rms_norm466: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add458, model_layers_5_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1156 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight9, model_layers_5_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims924: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1156, axes=None)
            matmul924: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm466, permute_dims924, out_dtype="void")
            split229: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul924, indices_or_sections=2, axis=-1)
            split_0229: R.Tensor((1, seq_len, 14336), dtype="float16") = split229[0]
            split_1229: R.Tensor((1, seq_len, 14336), dtype="float16") = split229[1]
            silu229: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0229)
            mul229: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu229, split_1229)
            lv1157 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight9, model_layers_5_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims925: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1157, axes=None)
            matmul925: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul229, permute_dims925, out_dtype="void")
            add459: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul925, add458)
            rms_norm467: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add459, model_layers_6_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1158 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight9, model_layers_6_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims926: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1158, axes=None)
            matmul926: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm467, permute_dims926, out_dtype="void")
            reshape920: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul926, R.shape([1, seq_len, 48, 128]))
            reshape921: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape920, R.shape([seq_len, 48, 128]))
            lv1159 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape921), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape922: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1159, R.shape([1, seq_len, 32, 128]))
            reshape923: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape922, R.shape([1, seq_len, 4096]))
            lv1160 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight9, model_layers_6_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims927: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1160, axes=None)
            matmul927: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape923, permute_dims927, out_dtype="void")
            add460: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul927, add459)
            rms_norm468: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add460, model_layers_6_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1161 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight9, model_layers_6_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims928: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1161, axes=None)
            matmul928: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm468, permute_dims928, out_dtype="void")
            split230: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul928, indices_or_sections=2, axis=-1)
            split_0230: R.Tensor((1, seq_len, 14336), dtype="float16") = split230[0]
            split_1230: R.Tensor((1, seq_len, 14336), dtype="float16") = split230[1]
            silu230: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0230)
            mul230: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu230, split_1230)
            lv1162 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight9, model_layers_6_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims929: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1162, axes=None)
            matmul929: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul230, permute_dims929, out_dtype="void")
            add461: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul929, add460)
            rms_norm469: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add461, model_layers_7_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1163 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight9, model_layers_7_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims930: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1163, axes=None)
            matmul930: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm469, permute_dims930, out_dtype="void")
            reshape924: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul930, R.shape([1, seq_len, 48, 128]))
            reshape925: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape924, R.shape([seq_len, 48, 128]))
            lv1164 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape925), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape926: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1164, R.shape([1, seq_len, 32, 128]))
            reshape927: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape926, R.shape([1, seq_len, 4096]))
            lv1165 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight9, model_layers_7_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims931: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1165, axes=None)
            matmul931: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape927, permute_dims931, out_dtype="void")
            add462: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul931, add461)
            rms_norm470: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add462, model_layers_7_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1166 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight9, model_layers_7_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims932: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1166, axes=None)
            matmul932: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm470, permute_dims932, out_dtype="void")
            split231: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul932, indices_or_sections=2, axis=-1)
            split_0231: R.Tensor((1, seq_len, 14336), dtype="float16") = split231[0]
            split_1231: R.Tensor((1, seq_len, 14336), dtype="float16") = split231[1]
            silu231: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0231)
            mul231: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu231, split_1231)
            lv1167 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight9, model_layers_7_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims933: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1167, axes=None)
            matmul933: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul231, permute_dims933, out_dtype="void")
            add463: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul933, add462)
            rms_norm471: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add463, model_layers_8_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1168 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight9, model_layers_8_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims934: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1168, axes=None)
            matmul934: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm471, permute_dims934, out_dtype="void")
            reshape928: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul934, R.shape([1, seq_len, 48, 128]))
            reshape929: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape928, R.shape([seq_len, 48, 128]))
            lv1169 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape929), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape930: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1169, R.shape([1, seq_len, 32, 128]))
            reshape931: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape930, R.shape([1, seq_len, 4096]))
            lv1170 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight9, model_layers_8_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims935: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1170, axes=None)
            matmul935: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape931, permute_dims935, out_dtype="void")
            add464: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul935, add463)
            rms_norm472: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add464, model_layers_8_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1171 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight9, model_layers_8_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims936: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1171, axes=None)
            matmul936: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm472, permute_dims936, out_dtype="void")
            split232: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul936, indices_or_sections=2, axis=-1)
            split_0232: R.Tensor((1, seq_len, 14336), dtype="float16") = split232[0]
            split_1232: R.Tensor((1, seq_len, 14336), dtype="float16") = split232[1]
            silu232: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0232)
            mul232: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu232, split_1232)
            lv1172 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight9, model_layers_8_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims937: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1172, axes=None)
            matmul937: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul232, permute_dims937, out_dtype="void")
            add465: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul937, add464)
            rms_norm473: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add465, model_layers_9_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1173 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight9, model_layers_9_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims938: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1173, axes=None)
            matmul938: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm473, permute_dims938, out_dtype="void")
            reshape932: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul938, R.shape([1, seq_len, 48, 128]))
            reshape933: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape932, R.shape([seq_len, 48, 128]))
            lv1174 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape933), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape934: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1174, R.shape([1, seq_len, 32, 128]))
            reshape935: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape934, R.shape([1, seq_len, 4096]))
            lv1175 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight9, model_layers_9_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims939: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1175, axes=None)
            matmul939: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape935, permute_dims939, out_dtype="void")
            add466: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul939, add465)
            rms_norm474: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add466, model_layers_9_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1176 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight9, model_layers_9_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims940: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1176, axes=None)
            matmul940: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm474, permute_dims940, out_dtype="void")
            split233: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul940, indices_or_sections=2, axis=-1)
            split_0233: R.Tensor((1, seq_len, 14336), dtype="float16") = split233[0]
            split_1233: R.Tensor((1, seq_len, 14336), dtype="float16") = split233[1]
            silu233: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0233)
            mul233: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu233, split_1233)
            lv1177 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight9, model_layers_9_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims941: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1177, axes=None)
            matmul941: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul233, permute_dims941, out_dtype="void")
            add467: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul941, add466)
            rms_norm475: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add467, model_layers_10_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1178 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight9, model_layers_10_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims942: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1178, axes=None)
            matmul942: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm475, permute_dims942, out_dtype="void")
            reshape936: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul942, R.shape([1, seq_len, 48, 128]))
            reshape937: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape936, R.shape([seq_len, 48, 128]))
            lv1179 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape937), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape938: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1179, R.shape([1, seq_len, 32, 128]))
            reshape939: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape938, R.shape([1, seq_len, 4096]))
            lv1180 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight9, model_layers_10_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims943: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1180, axes=None)
            matmul943: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape939, permute_dims943, out_dtype="void")
            add468: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul943, add467)
            rms_norm476: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add468, model_layers_10_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1181 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight9, model_layers_10_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims944: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1181, axes=None)
            matmul944: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm476, permute_dims944, out_dtype="void")
            split234: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul944, indices_or_sections=2, axis=-1)
            split_0234: R.Tensor((1, seq_len, 14336), dtype="float16") = split234[0]
            split_1234: R.Tensor((1, seq_len, 14336), dtype="float16") = split234[1]
            silu234: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0234)
            mul234: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu234, split_1234)
            lv1182 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight9, model_layers_10_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims945: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1182, axes=None)
            matmul945: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul234, permute_dims945, out_dtype="void")
            add469: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul945, add468)
            rms_norm477: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add469, model_layers_11_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1183 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight9, model_layers_11_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims946: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1183, axes=None)
            matmul946: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm477, permute_dims946, out_dtype="void")
            reshape940: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul946, R.shape([1, seq_len, 48, 128]))
            reshape941: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape940, R.shape([seq_len, 48, 128]))
            lv1184 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape941), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape942: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1184, R.shape([1, seq_len, 32, 128]))
            reshape943: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape942, R.shape([1, seq_len, 4096]))
            lv1185 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight9, model_layers_11_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims947: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1185, axes=None)
            matmul947: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape943, permute_dims947, out_dtype="void")
            add470: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul947, add469)
            rms_norm478: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add470, model_layers_11_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1186 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight9, model_layers_11_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims948: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1186, axes=None)
            matmul948: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm478, permute_dims948, out_dtype="void")
            split235: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul948, indices_or_sections=2, axis=-1)
            split_0235: R.Tensor((1, seq_len, 14336), dtype="float16") = split235[0]
            split_1235: R.Tensor((1, seq_len, 14336), dtype="float16") = split235[1]
            silu235: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0235)
            mul235: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu235, split_1235)
            lv1187 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight9, model_layers_11_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims949: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1187, axes=None)
            matmul949: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul235, permute_dims949, out_dtype="void")
            add471: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul949, add470)
            rms_norm479: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add471, model_layers_12_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1188 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight9, model_layers_12_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims950: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1188, axes=None)
            matmul950: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm479, permute_dims950, out_dtype="void")
            reshape944: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul950, R.shape([1, seq_len, 48, 128]))
            reshape945: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape944, R.shape([seq_len, 48, 128]))
            lv1189 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape945), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape946: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1189, R.shape([1, seq_len, 32, 128]))
            reshape947: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape946, R.shape([1, seq_len, 4096]))
            lv1190 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight9, model_layers_12_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims951: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1190, axes=None)
            matmul951: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape947, permute_dims951, out_dtype="void")
            add472: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul951, add471)
            rms_norm480: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add472, model_layers_12_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1191 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight9, model_layers_12_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims952: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1191, axes=None)
            matmul952: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm480, permute_dims952, out_dtype="void")
            split236: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul952, indices_or_sections=2, axis=-1)
            split_0236: R.Tensor((1, seq_len, 14336), dtype="float16") = split236[0]
            split_1236: R.Tensor((1, seq_len, 14336), dtype="float16") = split236[1]
            silu236: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0236)
            mul236: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu236, split_1236)
            lv1192 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight9, model_layers_12_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims953: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1192, axes=None)
            matmul953: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul236, permute_dims953, out_dtype="void")
            add473: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul953, add472)
            rms_norm481: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add473, model_layers_13_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1193 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight9, model_layers_13_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims954: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1193, axes=None)
            matmul954: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm481, permute_dims954, out_dtype="void")
            reshape948: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul954, R.shape([1, seq_len, 48, 128]))
            reshape949: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape948, R.shape([seq_len, 48, 128]))
            lv1194 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape949), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape950: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1194, R.shape([1, seq_len, 32, 128]))
            reshape951: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape950, R.shape([1, seq_len, 4096]))
            lv1195 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight9, model_layers_13_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims955: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1195, axes=None)
            matmul955: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape951, permute_dims955, out_dtype="void")
            add474: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul955, add473)
            rms_norm482: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add474, model_layers_13_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1196 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight9, model_layers_13_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims956: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1196, axes=None)
            matmul956: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm482, permute_dims956, out_dtype="void")
            split237: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul956, indices_or_sections=2, axis=-1)
            split_0237: R.Tensor((1, seq_len, 14336), dtype="float16") = split237[0]
            split_1237: R.Tensor((1, seq_len, 14336), dtype="float16") = split237[1]
            silu237: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0237)
            mul237: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu237, split_1237)
            lv1197 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight9, model_layers_13_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims957: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1197, axes=None)
            matmul957: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul237, permute_dims957, out_dtype="void")
            add475: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul957, add474)
            rms_norm483: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add475, model_layers_14_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1198 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight9, model_layers_14_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims958: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1198, axes=None)
            matmul958: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm483, permute_dims958, out_dtype="void")
            reshape952: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul958, R.shape([1, seq_len, 48, 128]))
            reshape953: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape952, R.shape([seq_len, 48, 128]))
            lv1199 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape953), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape954: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1199, R.shape([1, seq_len, 32, 128]))
            reshape955: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape954, R.shape([1, seq_len, 4096]))
            lv1200 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight9, model_layers_14_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims959: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1200, axes=None)
            matmul959: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape955, permute_dims959, out_dtype="void")
            add476: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul959, add475)
            rms_norm484: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add476, model_layers_14_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1201 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight9, model_layers_14_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims960: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1201, axes=None)
            matmul960: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm484, permute_dims960, out_dtype="void")
            split238: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul960, indices_or_sections=2, axis=-1)
            split_0238: R.Tensor((1, seq_len, 14336), dtype="float16") = split238[0]
            split_1238: R.Tensor((1, seq_len, 14336), dtype="float16") = split238[1]
            silu238: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0238)
            mul238: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu238, split_1238)
            lv1202 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight9, model_layers_14_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims961: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1202, axes=None)
            matmul961: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul238, permute_dims961, out_dtype="void")
            add477: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul961, add476)
            rms_norm485: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add477, model_layers_15_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1203 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight9, model_layers_15_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims962: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1203, axes=None)
            matmul962: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm485, permute_dims962, out_dtype="void")
            reshape956: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul962, R.shape([1, seq_len, 48, 128]))
            reshape957: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape956, R.shape([seq_len, 48, 128]))
            lv1204 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape957), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape958: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1204, R.shape([1, seq_len, 32, 128]))
            reshape959: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape958, R.shape([1, seq_len, 4096]))
            lv1205 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight9, model_layers_15_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims963: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1205, axes=None)
            matmul963: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape959, permute_dims963, out_dtype="void")
            add478: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul963, add477)
            rms_norm486: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add478, model_layers_15_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1206 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight9, model_layers_15_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims964: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1206, axes=None)
            matmul964: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm486, permute_dims964, out_dtype="void")
            split239: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul964, indices_or_sections=2, axis=-1)
            split_0239: R.Tensor((1, seq_len, 14336), dtype="float16") = split239[0]
            split_1239: R.Tensor((1, seq_len, 14336), dtype="float16") = split239[1]
            silu239: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0239)
            mul239: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu239, split_1239)
            lv1207 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight9, model_layers_15_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims965: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1207, axes=None)
            matmul965: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul239, permute_dims965, out_dtype="void")
            add479: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul965, add478)
            rms_norm487: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add479, model_layers_16_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1208 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight9, model_layers_16_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims966: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1208, axes=None)
            matmul966: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm487, permute_dims966, out_dtype="void")
            reshape960: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul966, R.shape([1, seq_len, 48, 128]))
            reshape961: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape960, R.shape([seq_len, 48, 128]))
            lv1209 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape961), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape962: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1209, R.shape([1, seq_len, 32, 128]))
            reshape963: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape962, R.shape([1, seq_len, 4096]))
            lv1210 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight9, model_layers_16_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims967: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1210, axes=None)
            matmul967: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape963, permute_dims967, out_dtype="void")
            add480: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul967, add479)
            rms_norm488: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add480, model_layers_16_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1211 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight9, model_layers_16_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims968: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1211, axes=None)
            matmul968: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm488, permute_dims968, out_dtype="void")
            split240: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul968, indices_or_sections=2, axis=-1)
            split_0240: R.Tensor((1, seq_len, 14336), dtype="float16") = split240[0]
            split_1240: R.Tensor((1, seq_len, 14336), dtype="float16") = split240[1]
            silu240: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0240)
            mul240: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu240, split_1240)
            lv1212 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight9, model_layers_16_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims969: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1212, axes=None)
            matmul969: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul240, permute_dims969, out_dtype="void")
            add481: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul969, add480)
            rms_norm489: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add481, model_layers_17_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1213 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight9, model_layers_17_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims970: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1213, axes=None)
            matmul970: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm489, permute_dims970, out_dtype="void")
            reshape964: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul970, R.shape([1, seq_len, 48, 128]))
            reshape965: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape964, R.shape([seq_len, 48, 128]))
            lv1214 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape965), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape966: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1214, R.shape([1, seq_len, 32, 128]))
            reshape967: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape966, R.shape([1, seq_len, 4096]))
            lv1215 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight9, model_layers_17_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims971: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1215, axes=None)
            matmul971: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape967, permute_dims971, out_dtype="void")
            add482: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul971, add481)
            rms_norm490: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add482, model_layers_17_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1216 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight9, model_layers_17_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims972: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1216, axes=None)
            matmul972: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm490, permute_dims972, out_dtype="void")
            split241: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul972, indices_or_sections=2, axis=-1)
            split_0241: R.Tensor((1, seq_len, 14336), dtype="float16") = split241[0]
            split_1241: R.Tensor((1, seq_len, 14336), dtype="float16") = split241[1]
            silu241: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0241)
            mul241: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu241, split_1241)
            lv1217 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight9, model_layers_17_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims973: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1217, axes=None)
            matmul973: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul241, permute_dims973, out_dtype="void")
            add483: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul973, add482)
            rms_norm491: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add483, model_layers_18_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1218 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight9, model_layers_18_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims974: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1218, axes=None)
            matmul974: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm491, permute_dims974, out_dtype="void")
            reshape968: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul974, R.shape([1, seq_len, 48, 128]))
            reshape969: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape968, R.shape([seq_len, 48, 128]))
            lv1219 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape969), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape970: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1219, R.shape([1, seq_len, 32, 128]))
            reshape971: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape970, R.shape([1, seq_len, 4096]))
            lv1220 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight9, model_layers_18_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims975: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1220, axes=None)
            matmul975: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape971, permute_dims975, out_dtype="void")
            add484: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul975, add483)
            rms_norm492: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add484, model_layers_18_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1221 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight9, model_layers_18_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims976: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1221, axes=None)
            matmul976: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm492, permute_dims976, out_dtype="void")
            split242: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul976, indices_or_sections=2, axis=-1)
            split_0242: R.Tensor((1, seq_len, 14336), dtype="float16") = split242[0]
            split_1242: R.Tensor((1, seq_len, 14336), dtype="float16") = split242[1]
            silu242: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0242)
            mul242: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu242, split_1242)
            lv1222 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight9, model_layers_18_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims977: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1222, axes=None)
            matmul977: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul242, permute_dims977, out_dtype="void")
            add485: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul977, add484)
            rms_norm493: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add485, model_layers_19_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1223 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight9, model_layers_19_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims978: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1223, axes=None)
            matmul978: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm493, permute_dims978, out_dtype="void")
            reshape972: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul978, R.shape([1, seq_len, 48, 128]))
            reshape973: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape972, R.shape([seq_len, 48, 128]))
            lv1224 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape973), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape974: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1224, R.shape([1, seq_len, 32, 128]))
            reshape975: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape974, R.shape([1, seq_len, 4096]))
            lv1225 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight9, model_layers_19_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims979: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1225, axes=None)
            matmul979: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape975, permute_dims979, out_dtype="void")
            add486: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul979, add485)
            rms_norm494: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add486, model_layers_19_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1226 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight9, model_layers_19_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims980: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1226, axes=None)
            matmul980: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm494, permute_dims980, out_dtype="void")
            split243: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul980, indices_or_sections=2, axis=-1)
            split_0243: R.Tensor((1, seq_len, 14336), dtype="float16") = split243[0]
            split_1243: R.Tensor((1, seq_len, 14336), dtype="float16") = split243[1]
            silu243: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0243)
            mul243: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu243, split_1243)
            lv1227 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight9, model_layers_19_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims981: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1227, axes=None)
            matmul981: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul243, permute_dims981, out_dtype="void")
            add487: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul981, add486)
            rms_norm495: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add487, model_layers_20_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1228 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight9, model_layers_20_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims982: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1228, axes=None)
            matmul982: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm495, permute_dims982, out_dtype="void")
            reshape976: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul982, R.shape([1, seq_len, 48, 128]))
            reshape977: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape976, R.shape([seq_len, 48, 128]))
            lv1229 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape977), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape978: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1229, R.shape([1, seq_len, 32, 128]))
            reshape979: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape978, R.shape([1, seq_len, 4096]))
            lv1230 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight9, model_layers_20_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims983: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1230, axes=None)
            matmul983: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape979, permute_dims983, out_dtype="void")
            add488: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul983, add487)
            rms_norm496: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add488, model_layers_20_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1231 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight9, model_layers_20_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims984: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1231, axes=None)
            matmul984: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm496, permute_dims984, out_dtype="void")
            split244: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul984, indices_or_sections=2, axis=-1)
            split_0244: R.Tensor((1, seq_len, 14336), dtype="float16") = split244[0]
            split_1244: R.Tensor((1, seq_len, 14336), dtype="float16") = split244[1]
            silu244: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0244)
            mul244: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu244, split_1244)
            lv1232 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight9, model_layers_20_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims985: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1232, axes=None)
            matmul985: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul244, permute_dims985, out_dtype="void")
            add489: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul985, add488)
            rms_norm497: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add489, model_layers_21_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1233 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight9, model_layers_21_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims986: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1233, axes=None)
            matmul986: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm497, permute_dims986, out_dtype="void")
            reshape980: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul986, R.shape([1, seq_len, 48, 128]))
            reshape981: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape980, R.shape([seq_len, 48, 128]))
            lv1234 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape981), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape982: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1234, R.shape([1, seq_len, 32, 128]))
            reshape983: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape982, R.shape([1, seq_len, 4096]))
            lv1235 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight9, model_layers_21_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims987: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1235, axes=None)
            matmul987: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape983, permute_dims987, out_dtype="void")
            add490: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul987, add489)
            rms_norm498: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add490, model_layers_21_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1236 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight9, model_layers_21_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims988: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1236, axes=None)
            matmul988: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm498, permute_dims988, out_dtype="void")
            split245: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul988, indices_or_sections=2, axis=-1)
            split_0245: R.Tensor((1, seq_len, 14336), dtype="float16") = split245[0]
            split_1245: R.Tensor((1, seq_len, 14336), dtype="float16") = split245[1]
            silu245: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0245)
            mul245: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu245, split_1245)
            lv1237 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight9, model_layers_21_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims989: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1237, axes=None)
            matmul989: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul245, permute_dims989, out_dtype="void")
            add491: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul989, add490)
            rms_norm499: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add491, model_layers_22_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1238 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight9, model_layers_22_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims990: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1238, axes=None)
            matmul990: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm499, permute_dims990, out_dtype="void")
            reshape984: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul990, R.shape([1, seq_len, 48, 128]))
            reshape985: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape984, R.shape([seq_len, 48, 128]))
            lv1239 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape985), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape986: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1239, R.shape([1, seq_len, 32, 128]))
            reshape987: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape986, R.shape([1, seq_len, 4096]))
            lv1240 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight9, model_layers_22_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims991: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1240, axes=None)
            matmul991: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape987, permute_dims991, out_dtype="void")
            add492: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul991, add491)
            rms_norm500: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add492, model_layers_22_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1241 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight9, model_layers_22_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims992: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1241, axes=None)
            matmul992: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm500, permute_dims992, out_dtype="void")
            split246: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul992, indices_or_sections=2, axis=-1)
            split_0246: R.Tensor((1, seq_len, 14336), dtype="float16") = split246[0]
            split_1246: R.Tensor((1, seq_len, 14336), dtype="float16") = split246[1]
            silu246: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0246)
            mul246: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu246, split_1246)
            lv1242 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight9, model_layers_22_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims993: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1242, axes=None)
            matmul993: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul246, permute_dims993, out_dtype="void")
            add493: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul993, add492)
            rms_norm501: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add493, model_layers_23_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1243 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight9, model_layers_23_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims994: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1243, axes=None)
            matmul994: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm501, permute_dims994, out_dtype="void")
            reshape988: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul994, R.shape([1, seq_len, 48, 128]))
            reshape989: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape988, R.shape([seq_len, 48, 128]))
            lv1244 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape989), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape990: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1244, R.shape([1, seq_len, 32, 128]))
            reshape991: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape990, R.shape([1, seq_len, 4096]))
            lv1245 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight9, model_layers_23_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims995: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1245, axes=None)
            matmul995: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape991, permute_dims995, out_dtype="void")
            add494: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul995, add493)
            rms_norm502: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add494, model_layers_23_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1246 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight9, model_layers_23_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims996: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1246, axes=None)
            matmul996: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm502, permute_dims996, out_dtype="void")
            split247: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul996, indices_or_sections=2, axis=-1)
            split_0247: R.Tensor((1, seq_len, 14336), dtype="float16") = split247[0]
            split_1247: R.Tensor((1, seq_len, 14336), dtype="float16") = split247[1]
            silu247: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0247)
            mul247: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu247, split_1247)
            lv1247 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight9, model_layers_23_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims997: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1247, axes=None)
            matmul997: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul247, permute_dims997, out_dtype="void")
            add495: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul997, add494)
            rms_norm503: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add495, model_layers_24_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1248 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight9, model_layers_24_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims998: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1248, axes=None)
            matmul998: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm503, permute_dims998, out_dtype="void")
            reshape992: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul998, R.shape([1, seq_len, 48, 128]))
            reshape993: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape992, R.shape([seq_len, 48, 128]))
            lv1249 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape993), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape994: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1249, R.shape([1, seq_len, 32, 128]))
            reshape995: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape994, R.shape([1, seq_len, 4096]))
            lv1250 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight9, model_layers_24_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims999: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1250, axes=None)
            matmul999: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape995, permute_dims999, out_dtype="void")
            add496: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul999, add495)
            rms_norm504: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add496, model_layers_24_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1251 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight9, model_layers_24_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1000: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1251, axes=None)
            matmul1000: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm504, permute_dims1000, out_dtype="void")
            split248: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1000, indices_or_sections=2, axis=-1)
            split_0248: R.Tensor((1, seq_len, 14336), dtype="float16") = split248[0]
            split_1248: R.Tensor((1, seq_len, 14336), dtype="float16") = split248[1]
            silu248: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0248)
            mul248: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu248, split_1248)
            lv1252 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight9, model_layers_24_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1001: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1252, axes=None)
            matmul1001: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul248, permute_dims1001, out_dtype="void")
            add497: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1001, add496)
            rms_norm505: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add497, model_layers_25_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1253 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight9, model_layers_25_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1002: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1253, axes=None)
            matmul1002: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm505, permute_dims1002, out_dtype="void")
            reshape996: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1002, R.shape([1, seq_len, 48, 128]))
            reshape997: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape996, R.shape([seq_len, 48, 128]))
            lv1254 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape997), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape998: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1254, R.shape([1, seq_len, 32, 128]))
            reshape999: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape998, R.shape([1, seq_len, 4096]))
            lv1255 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight9, model_layers_25_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1003: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1255, axes=None)
            matmul1003: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape999, permute_dims1003, out_dtype="void")
            add498: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1003, add497)
            rms_norm506: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add498, model_layers_25_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1256 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight9, model_layers_25_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1004: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1256, axes=None)
            matmul1004: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm506, permute_dims1004, out_dtype="void")
            split249: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1004, indices_or_sections=2, axis=-1)
            split_0249: R.Tensor((1, seq_len, 14336), dtype="float16") = split249[0]
            split_1249: R.Tensor((1, seq_len, 14336), dtype="float16") = split249[1]
            silu249: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0249)
            mul249: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu249, split_1249)
            lv1257 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight9, model_layers_25_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1005: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1257, axes=None)
            matmul1005: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul249, permute_dims1005, out_dtype="void")
            add499: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1005, add498)
            rms_norm507: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add499, model_layers_26_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1258 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight9, model_layers_26_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1006: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1258, axes=None)
            matmul1006: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm507, permute_dims1006, out_dtype="void")
            reshape1000: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1006, R.shape([1, seq_len, 48, 128]))
            reshape1001: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1000, R.shape([seq_len, 48, 128]))
            lv1259 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1001), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1002: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1259, R.shape([1, seq_len, 32, 128]))
            reshape1003: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1002, R.shape([1, seq_len, 4096]))
            lv1260 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight9, model_layers_26_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1007: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1260, axes=None)
            matmul1007: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1003, permute_dims1007, out_dtype="void")
            add500: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1007, add499)
            rms_norm508: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add500, model_layers_26_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1261 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight9, model_layers_26_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1008: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1261, axes=None)
            matmul1008: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm508, permute_dims1008, out_dtype="void")
            split250: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1008, indices_or_sections=2, axis=-1)
            split_0250: R.Tensor((1, seq_len, 14336), dtype="float16") = split250[0]
            split_1250: R.Tensor((1, seq_len, 14336), dtype="float16") = split250[1]
            silu250: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0250)
            mul250: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu250, split_1250)
            lv1262 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight9, model_layers_26_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1009: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1262, axes=None)
            matmul1009: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul250, permute_dims1009, out_dtype="void")
            add501: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1009, add500)
            rms_norm509: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add501, model_layers_27_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1263 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight9, model_layers_27_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1010: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1263, axes=None)
            matmul1010: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm509, permute_dims1010, out_dtype="void")
            reshape1004: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1010, R.shape([1, seq_len, 48, 128]))
            reshape1005: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1004, R.shape([seq_len, 48, 128]))
            lv1264 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1005), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1006: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1264, R.shape([1, seq_len, 32, 128]))
            reshape1007: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1006, R.shape([1, seq_len, 4096]))
            lv1265 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight9, model_layers_27_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1011: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1265, axes=None)
            matmul1011: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1007, permute_dims1011, out_dtype="void")
            add502: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1011, add501)
            rms_norm510: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add502, model_layers_27_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1266 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight9, model_layers_27_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1012: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1266, axes=None)
            matmul1012: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm510, permute_dims1012, out_dtype="void")
            split251: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1012, indices_or_sections=2, axis=-1)
            split_0251: R.Tensor((1, seq_len, 14336), dtype="float16") = split251[0]
            split_1251: R.Tensor((1, seq_len, 14336), dtype="float16") = split251[1]
            silu251: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0251)
            mul251: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu251, split_1251)
            lv1267 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight9, model_layers_27_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1013: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1267, axes=None)
            matmul1013: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul251, permute_dims1013, out_dtype="void")
            add503: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1013, add502)
            rms_norm511: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add503, model_layers_28_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1268 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight9, model_layers_28_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1014: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1268, axes=None)
            matmul1014: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm511, permute_dims1014, out_dtype="void")
            reshape1008: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1014, R.shape([1, seq_len, 48, 128]))
            reshape1009: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1008, R.shape([seq_len, 48, 128]))
            lv1269 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1009), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1010: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1269, R.shape([1, seq_len, 32, 128]))
            reshape1011: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1010, R.shape([1, seq_len, 4096]))
            lv1270 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight9, model_layers_28_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1015: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1270, axes=None)
            matmul1015: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1011, permute_dims1015, out_dtype="void")
            add504: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1015, add503)
            rms_norm512: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add504, model_layers_28_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1271 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight9, model_layers_28_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1016: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1271, axes=None)
            matmul1016: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm512, permute_dims1016, out_dtype="void")
            split252: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1016, indices_or_sections=2, axis=-1)
            split_0252: R.Tensor((1, seq_len, 14336), dtype="float16") = split252[0]
            split_1252: R.Tensor((1, seq_len, 14336), dtype="float16") = split252[1]
            silu252: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0252)
            mul252: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu252, split_1252)
            lv1272 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight9, model_layers_28_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1017: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1272, axes=None)
            matmul1017: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul252, permute_dims1017, out_dtype="void")
            add505: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1017, add504)
            rms_norm513: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add505, model_layers_29_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1273 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight9, model_layers_29_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1018: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1273, axes=None)
            matmul1018: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm513, permute_dims1018, out_dtype="void")
            reshape1012: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1018, R.shape([1, seq_len, 48, 128]))
            reshape1013: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1012, R.shape([seq_len, 48, 128]))
            lv1274 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1013), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1014: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1274, R.shape([1, seq_len, 32, 128]))
            reshape1015: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1014, R.shape([1, seq_len, 4096]))
            lv1275 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight9, model_layers_29_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1019: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1275, axes=None)
            matmul1019: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1015, permute_dims1019, out_dtype="void")
            add506: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1019, add505)
            rms_norm514: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add506, model_layers_29_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1276 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight9, model_layers_29_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1020: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1276, axes=None)
            matmul1020: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm514, permute_dims1020, out_dtype="void")
            split253: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1020, indices_or_sections=2, axis=-1)
            split_0253: R.Tensor((1, seq_len, 14336), dtype="float16") = split253[0]
            split_1253: R.Tensor((1, seq_len, 14336), dtype="float16") = split253[1]
            silu253: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0253)
            mul253: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu253, split_1253)
            lv1277 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight9, model_layers_29_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1021: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1277, axes=None)
            matmul1021: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul253, permute_dims1021, out_dtype="void")
            add507: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1021, add506)
            rms_norm515: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add507, model_layers_30_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1278 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight9, model_layers_30_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1022: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1278, axes=None)
            matmul1022: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm515, permute_dims1022, out_dtype="void")
            reshape1016: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1022, R.shape([1, seq_len, 48, 128]))
            reshape1017: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1016, R.shape([seq_len, 48, 128]))
            lv1279 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1017), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1018: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1279, R.shape([1, seq_len, 32, 128]))
            reshape1019: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1018, R.shape([1, seq_len, 4096]))
            lv1280 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight9, model_layers_30_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1023: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1280, axes=None)
            matmul1023: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1019, permute_dims1023, out_dtype="void")
            add508: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1023, add507)
            rms_norm516: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add508, model_layers_30_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1281 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight9, model_layers_30_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1024: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1281, axes=None)
            matmul1024: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm516, permute_dims1024, out_dtype="void")
            split254: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1024, indices_or_sections=2, axis=-1)
            split_0254: R.Tensor((1, seq_len, 14336), dtype="float16") = split254[0]
            split_1254: R.Tensor((1, seq_len, 14336), dtype="float16") = split254[1]
            silu254: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0254)
            mul254: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu254, split_1254)
            lv1282 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight9, model_layers_30_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1025: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1282, axes=None)
            matmul1025: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul254, permute_dims1025, out_dtype="void")
            add509: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1025, add508)
            rms_norm517: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add509, model_layers_31_input_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1283 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight9, model_layers_31_self_attn_qkv_proj_q_scale9), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1026: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1283, axes=None)
            matmul1026: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm517, permute_dims1026, out_dtype="void")
            reshape1020: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1026, R.shape([1, seq_len, 48, 128]))
            reshape1021: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1020, R.shape([seq_len, 48, 128]))
            lv1284 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1021), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1022: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1284, R.shape([1, seq_len, 32, 128]))
            reshape1023: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1022, R.shape([1, seq_len, 4096]))
            lv1285 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight9, model_layers_31_self_attn_o_proj_q_scale9), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1027: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1285, axes=None)
            matmul1027: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1023, permute_dims1027, out_dtype="void")
            add510: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1027, add509)
            rms_norm518: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add510, model_layers_31_post_attention_layernorm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1286 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight9, model_layers_31_mlp_gate_up_proj_q_scale9), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1028: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1286, axes=None)
            matmul1028: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm518, permute_dims1028, out_dtype="void")
            split255: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1028, indices_or_sections=2, axis=-1)
            split_0255: R.Tensor((1, seq_len, 14336), dtype="float16") = split255[0]
            split_1255: R.Tensor((1, seq_len, 14336), dtype="float16") = split255[1]
            silu255: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0255)
            mul255: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu255, split_1255)
            lv1287 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight9, model_layers_31_mlp_down_proj_q_scale9), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1029: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1287, axes=None)
            matmul1029: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul255, permute_dims1029, out_dtype="void")
            add511: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1029, add510)
            rms_norm519: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add511, model_norm_weight9, axes=[-1], epsilon=1.0000000000000001e-05)
            gv10: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm519, paged_kv_cache
            R.output(gv10)
        return gv10

    @R.function
    def batch_select_last_hidden_states(hidden_states: R.Tensor(("seq_len", 4096), dtype="float16"), logit_positions: R.Tensor(("batch_size",), dtype="int32")) -> R.Tensor(("batch_size", 4096), dtype="float16"):
        batch_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 2, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        with R.dataflow():
            take1: R.Tensor((batch_size, 4096), dtype="float16") = R.take(hidden_states, logit_positions, axis=0)
            gv2: R.Tensor((batch_size, 4096), dtype="float16") = take1
            R.output(gv2)
        return gv2

    @R.function
    def batch_verify(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", "vocab_size"), dtype="float32"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size", "seq_len"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight8: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale8: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight8: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale8: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight8: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale8: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight8: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale8: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight8: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale8: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight8: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight8: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale8: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm390: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv967 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight8, model_layers_0_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims773: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv967, axes=None)
            matmul773: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm390, permute_dims773, out_dtype="void")
            reshape768: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul773, R.shape([1, seq_len, 48, 128]))
            reshape769: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape768, R.shape([seq_len, 48, 128]))
            lv968 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape769), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape770: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv968, R.shape([1, seq_len, 32, 128]))
            reshape771: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape770, R.shape([1, seq_len, 4096]))
            lv969 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight8, model_layers_0_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims774: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv969, axes=None)
            matmul774: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape771, permute_dims774, out_dtype="void")
            add384: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul774, input_embeds)
            rms_norm391: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add384, model_layers_0_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv970 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight8, model_layers_0_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims775: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv970, axes=None)
            matmul775: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm391, permute_dims775, out_dtype="void")
            split192: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul775, indices_or_sections=2, axis=-1)
            split_0192: R.Tensor((1, seq_len, 14336), dtype="float16") = split192[0]
            split_1192: R.Tensor((1, seq_len, 14336), dtype="float16") = split192[1]
            silu192: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0192)
            mul192: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu192, split_1192)
            lv971 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight8, model_layers_0_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims776: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv971, axes=None)
            matmul776: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul192, permute_dims776, out_dtype="void")
            add385: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul776, add384)
            rms_norm392: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add385, model_layers_1_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv972 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight8, model_layers_1_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims777: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv972, axes=None)
            matmul777: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm392, permute_dims777, out_dtype="void")
            reshape772: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul777, R.shape([1, seq_len, 48, 128]))
            reshape773: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape772, R.shape([seq_len, 48, 128]))
            lv973 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape773), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape774: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv973, R.shape([1, seq_len, 32, 128]))
            reshape775: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape774, R.shape([1, seq_len, 4096]))
            lv974 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight8, model_layers_1_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims778: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv974, axes=None)
            matmul778: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape775, permute_dims778, out_dtype="void")
            add386: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul778, add385)
            rms_norm393: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add386, model_layers_1_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv975 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight8, model_layers_1_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims779: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv975, axes=None)
            matmul779: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm393, permute_dims779, out_dtype="void")
            split193: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul779, indices_or_sections=2, axis=-1)
            split_0193: R.Tensor((1, seq_len, 14336), dtype="float16") = split193[0]
            split_1193: R.Tensor((1, seq_len, 14336), dtype="float16") = split193[1]
            silu193: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0193)
            mul193: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu193, split_1193)
            lv976 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight8, model_layers_1_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims780: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv976, axes=None)
            matmul780: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul193, permute_dims780, out_dtype="void")
            add387: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul780, add386)
            rms_norm394: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add387, model_layers_2_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv977 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight8, model_layers_2_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims781: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv977, axes=None)
            matmul781: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm394, permute_dims781, out_dtype="void")
            reshape776: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul781, R.shape([1, seq_len, 48, 128]))
            reshape777: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape776, R.shape([seq_len, 48, 128]))
            lv978 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape777), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape778: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv978, R.shape([1, seq_len, 32, 128]))
            reshape779: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape778, R.shape([1, seq_len, 4096]))
            lv979 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight8, model_layers_2_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims782: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv979, axes=None)
            matmul782: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape779, permute_dims782, out_dtype="void")
            add388: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul782, add387)
            rms_norm395: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add388, model_layers_2_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv980 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight8, model_layers_2_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims783: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv980, axes=None)
            matmul783: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm395, permute_dims783, out_dtype="void")
            split194: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul783, indices_or_sections=2, axis=-1)
            split_0194: R.Tensor((1, seq_len, 14336), dtype="float16") = split194[0]
            split_1194: R.Tensor((1, seq_len, 14336), dtype="float16") = split194[1]
            silu194: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0194)
            mul194: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu194, split_1194)
            lv981 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight8, model_layers_2_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims784: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv981, axes=None)
            matmul784: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul194, permute_dims784, out_dtype="void")
            add389: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul784, add388)
            rms_norm396: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add389, model_layers_3_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv982 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight8, model_layers_3_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims785: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv982, axes=None)
            matmul785: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm396, permute_dims785, out_dtype="void")
            reshape780: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul785, R.shape([1, seq_len, 48, 128]))
            reshape781: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape780, R.shape([seq_len, 48, 128]))
            lv983 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape781), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape782: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv983, R.shape([1, seq_len, 32, 128]))
            reshape783: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape782, R.shape([1, seq_len, 4096]))
            lv984 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight8, model_layers_3_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims786: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv984, axes=None)
            matmul786: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape783, permute_dims786, out_dtype="void")
            add390: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul786, add389)
            rms_norm397: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add390, model_layers_3_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv985 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight8, model_layers_3_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims787: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv985, axes=None)
            matmul787: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm397, permute_dims787, out_dtype="void")
            split195: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul787, indices_or_sections=2, axis=-1)
            split_0195: R.Tensor((1, seq_len, 14336), dtype="float16") = split195[0]
            split_1195: R.Tensor((1, seq_len, 14336), dtype="float16") = split195[1]
            silu195: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0195)
            mul195: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu195, split_1195)
            lv986 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight8, model_layers_3_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims788: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv986, axes=None)
            matmul788: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul195, permute_dims788, out_dtype="void")
            add391: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul788, add390)
            rms_norm398: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add391, model_layers_4_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv987 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight8, model_layers_4_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims789: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv987, axes=None)
            matmul789: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm398, permute_dims789, out_dtype="void")
            reshape784: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul789, R.shape([1, seq_len, 48, 128]))
            reshape785: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape784, R.shape([seq_len, 48, 128]))
            lv988 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape785), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape786: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv988, R.shape([1, seq_len, 32, 128]))
            reshape787: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape786, R.shape([1, seq_len, 4096]))
            lv989 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight8, model_layers_4_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims790: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv989, axes=None)
            matmul790: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape787, permute_dims790, out_dtype="void")
            add392: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul790, add391)
            rms_norm399: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add392, model_layers_4_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv990 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight8, model_layers_4_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims791: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv990, axes=None)
            matmul791: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm399, permute_dims791, out_dtype="void")
            split196: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul791, indices_or_sections=2, axis=-1)
            split_0196: R.Tensor((1, seq_len, 14336), dtype="float16") = split196[0]
            split_1196: R.Tensor((1, seq_len, 14336), dtype="float16") = split196[1]
            silu196: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0196)
            mul196: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu196, split_1196)
            lv991 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight8, model_layers_4_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims792: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv991, axes=None)
            matmul792: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul196, permute_dims792, out_dtype="void")
            add393: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul792, add392)
            rms_norm400: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add393, model_layers_5_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv992 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight8, model_layers_5_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims793: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv992, axes=None)
            matmul793: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm400, permute_dims793, out_dtype="void")
            reshape788: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul793, R.shape([1, seq_len, 48, 128]))
            reshape789: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape788, R.shape([seq_len, 48, 128]))
            lv993 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape789), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape790: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv993, R.shape([1, seq_len, 32, 128]))
            reshape791: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape790, R.shape([1, seq_len, 4096]))
            lv994 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight8, model_layers_5_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims794: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv994, axes=None)
            matmul794: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape791, permute_dims794, out_dtype="void")
            add394: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul794, add393)
            rms_norm401: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add394, model_layers_5_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv995 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight8, model_layers_5_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims795: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv995, axes=None)
            matmul795: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm401, permute_dims795, out_dtype="void")
            split197: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul795, indices_or_sections=2, axis=-1)
            split_0197: R.Tensor((1, seq_len, 14336), dtype="float16") = split197[0]
            split_1197: R.Tensor((1, seq_len, 14336), dtype="float16") = split197[1]
            silu197: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0197)
            mul197: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu197, split_1197)
            lv996 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight8, model_layers_5_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims796: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv996, axes=None)
            matmul796: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul197, permute_dims796, out_dtype="void")
            add395: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul796, add394)
            rms_norm402: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add395, model_layers_6_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv997 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight8, model_layers_6_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims797: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv997, axes=None)
            matmul797: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm402, permute_dims797, out_dtype="void")
            reshape792: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul797, R.shape([1, seq_len, 48, 128]))
            reshape793: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape792, R.shape([seq_len, 48, 128]))
            lv998 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape793), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape794: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv998, R.shape([1, seq_len, 32, 128]))
            reshape795: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape794, R.shape([1, seq_len, 4096]))
            lv999 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight8, model_layers_6_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims798: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv999, axes=None)
            matmul798: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape795, permute_dims798, out_dtype="void")
            add396: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul798, add395)
            rms_norm403: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add396, model_layers_6_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1000 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight8, model_layers_6_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims799: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1000, axes=None)
            matmul799: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm403, permute_dims799, out_dtype="void")
            split198: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul799, indices_or_sections=2, axis=-1)
            split_0198: R.Tensor((1, seq_len, 14336), dtype="float16") = split198[0]
            split_1198: R.Tensor((1, seq_len, 14336), dtype="float16") = split198[1]
            silu198: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0198)
            mul198: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu198, split_1198)
            lv1001 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight8, model_layers_6_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims800: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1001, axes=None)
            matmul800: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul198, permute_dims800, out_dtype="void")
            add397: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul800, add396)
            rms_norm404: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add397, model_layers_7_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1002 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight8, model_layers_7_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims801: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1002, axes=None)
            matmul801: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm404, permute_dims801, out_dtype="void")
            reshape796: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul801, R.shape([1, seq_len, 48, 128]))
            reshape797: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape796, R.shape([seq_len, 48, 128]))
            lv1003 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape797), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape798: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1003, R.shape([1, seq_len, 32, 128]))
            reshape799: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape798, R.shape([1, seq_len, 4096]))
            lv1004 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight8, model_layers_7_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims802: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1004, axes=None)
            matmul802: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape799, permute_dims802, out_dtype="void")
            add398: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul802, add397)
            rms_norm405: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add398, model_layers_7_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1005 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight8, model_layers_7_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims803: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1005, axes=None)
            matmul803: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm405, permute_dims803, out_dtype="void")
            split199: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul803, indices_or_sections=2, axis=-1)
            split_0199: R.Tensor((1, seq_len, 14336), dtype="float16") = split199[0]
            split_1199: R.Tensor((1, seq_len, 14336), dtype="float16") = split199[1]
            silu199: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0199)
            mul199: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu199, split_1199)
            lv1006 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight8, model_layers_7_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims804: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1006, axes=None)
            matmul804: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul199, permute_dims804, out_dtype="void")
            add399: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul804, add398)
            rms_norm406: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add399, model_layers_8_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1007 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight8, model_layers_8_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims805: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1007, axes=None)
            matmul805: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm406, permute_dims805, out_dtype="void")
            reshape800: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul805, R.shape([1, seq_len, 48, 128]))
            reshape801: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape800, R.shape([seq_len, 48, 128]))
            lv1008 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape801), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape802: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1008, R.shape([1, seq_len, 32, 128]))
            reshape803: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape802, R.shape([1, seq_len, 4096]))
            lv1009 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight8, model_layers_8_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims806: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1009, axes=None)
            matmul806: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape803, permute_dims806, out_dtype="void")
            add400: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul806, add399)
            rms_norm407: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add400, model_layers_8_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1010 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight8, model_layers_8_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims807: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1010, axes=None)
            matmul807: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm407, permute_dims807, out_dtype="void")
            split200: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul807, indices_or_sections=2, axis=-1)
            split_0200: R.Tensor((1, seq_len, 14336), dtype="float16") = split200[0]
            split_1200: R.Tensor((1, seq_len, 14336), dtype="float16") = split200[1]
            silu200: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0200)
            mul200: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu200, split_1200)
            lv1011 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight8, model_layers_8_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims808: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1011, axes=None)
            matmul808: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul200, permute_dims808, out_dtype="void")
            add401: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul808, add400)
            rms_norm408: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add401, model_layers_9_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1012 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight8, model_layers_9_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims809: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1012, axes=None)
            matmul809: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm408, permute_dims809, out_dtype="void")
            reshape804: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul809, R.shape([1, seq_len, 48, 128]))
            reshape805: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape804, R.shape([seq_len, 48, 128]))
            lv1013 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape805), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape806: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1013, R.shape([1, seq_len, 32, 128]))
            reshape807: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape806, R.shape([1, seq_len, 4096]))
            lv1014 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight8, model_layers_9_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims810: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1014, axes=None)
            matmul810: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape807, permute_dims810, out_dtype="void")
            add402: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul810, add401)
            rms_norm409: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add402, model_layers_9_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1015 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight8, model_layers_9_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims811: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1015, axes=None)
            matmul811: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm409, permute_dims811, out_dtype="void")
            split201: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul811, indices_or_sections=2, axis=-1)
            split_0201: R.Tensor((1, seq_len, 14336), dtype="float16") = split201[0]
            split_1201: R.Tensor((1, seq_len, 14336), dtype="float16") = split201[1]
            silu201: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0201)
            mul201: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu201, split_1201)
            lv1016 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight8, model_layers_9_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims812: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1016, axes=None)
            matmul812: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul201, permute_dims812, out_dtype="void")
            add403: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul812, add402)
            rms_norm410: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add403, model_layers_10_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1017 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight8, model_layers_10_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims813: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1017, axes=None)
            matmul813: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm410, permute_dims813, out_dtype="void")
            reshape808: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul813, R.shape([1, seq_len, 48, 128]))
            reshape809: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape808, R.shape([seq_len, 48, 128]))
            lv1018 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape809), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape810: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1018, R.shape([1, seq_len, 32, 128]))
            reshape811: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape810, R.shape([1, seq_len, 4096]))
            lv1019 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight8, model_layers_10_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims814: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1019, axes=None)
            matmul814: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape811, permute_dims814, out_dtype="void")
            add404: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul814, add403)
            rms_norm411: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add404, model_layers_10_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1020 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight8, model_layers_10_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims815: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1020, axes=None)
            matmul815: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm411, permute_dims815, out_dtype="void")
            split202: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul815, indices_or_sections=2, axis=-1)
            split_0202: R.Tensor((1, seq_len, 14336), dtype="float16") = split202[0]
            split_1202: R.Tensor((1, seq_len, 14336), dtype="float16") = split202[1]
            silu202: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0202)
            mul202: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu202, split_1202)
            lv1021 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight8, model_layers_10_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims816: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1021, axes=None)
            matmul816: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul202, permute_dims816, out_dtype="void")
            add405: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul816, add404)
            rms_norm412: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add405, model_layers_11_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1022 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight8, model_layers_11_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims817: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1022, axes=None)
            matmul817: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm412, permute_dims817, out_dtype="void")
            reshape812: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul817, R.shape([1, seq_len, 48, 128]))
            reshape813: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape812, R.shape([seq_len, 48, 128]))
            lv1023 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape813), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape814: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1023, R.shape([1, seq_len, 32, 128]))
            reshape815: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape814, R.shape([1, seq_len, 4096]))
            lv1024 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight8, model_layers_11_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims818: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1024, axes=None)
            matmul818: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape815, permute_dims818, out_dtype="void")
            add406: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul818, add405)
            rms_norm413: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add406, model_layers_11_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1025 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight8, model_layers_11_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims819: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1025, axes=None)
            matmul819: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm413, permute_dims819, out_dtype="void")
            split203: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul819, indices_or_sections=2, axis=-1)
            split_0203: R.Tensor((1, seq_len, 14336), dtype="float16") = split203[0]
            split_1203: R.Tensor((1, seq_len, 14336), dtype="float16") = split203[1]
            silu203: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0203)
            mul203: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu203, split_1203)
            lv1026 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight8, model_layers_11_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims820: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1026, axes=None)
            matmul820: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul203, permute_dims820, out_dtype="void")
            add407: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul820, add406)
            rms_norm414: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add407, model_layers_12_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1027 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight8, model_layers_12_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims821: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1027, axes=None)
            matmul821: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm414, permute_dims821, out_dtype="void")
            reshape816: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul821, R.shape([1, seq_len, 48, 128]))
            reshape817: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape816, R.shape([seq_len, 48, 128]))
            lv1028 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape817), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape818: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1028, R.shape([1, seq_len, 32, 128]))
            reshape819: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape818, R.shape([1, seq_len, 4096]))
            lv1029 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight8, model_layers_12_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims822: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1029, axes=None)
            matmul822: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape819, permute_dims822, out_dtype="void")
            add408: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul822, add407)
            rms_norm415: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add408, model_layers_12_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1030 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight8, model_layers_12_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims823: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1030, axes=None)
            matmul823: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm415, permute_dims823, out_dtype="void")
            split204: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul823, indices_or_sections=2, axis=-1)
            split_0204: R.Tensor((1, seq_len, 14336), dtype="float16") = split204[0]
            split_1204: R.Tensor((1, seq_len, 14336), dtype="float16") = split204[1]
            silu204: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0204)
            mul204: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu204, split_1204)
            lv1031 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight8, model_layers_12_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims824: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1031, axes=None)
            matmul824: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul204, permute_dims824, out_dtype="void")
            add409: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul824, add408)
            rms_norm416: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add409, model_layers_13_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1032 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight8, model_layers_13_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims825: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1032, axes=None)
            matmul825: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm416, permute_dims825, out_dtype="void")
            reshape820: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul825, R.shape([1, seq_len, 48, 128]))
            reshape821: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape820, R.shape([seq_len, 48, 128]))
            lv1033 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape821), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape822: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1033, R.shape([1, seq_len, 32, 128]))
            reshape823: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape822, R.shape([1, seq_len, 4096]))
            lv1034 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight8, model_layers_13_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims826: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1034, axes=None)
            matmul826: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape823, permute_dims826, out_dtype="void")
            add410: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul826, add409)
            rms_norm417: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add410, model_layers_13_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1035 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight8, model_layers_13_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims827: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1035, axes=None)
            matmul827: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm417, permute_dims827, out_dtype="void")
            split205: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul827, indices_or_sections=2, axis=-1)
            split_0205: R.Tensor((1, seq_len, 14336), dtype="float16") = split205[0]
            split_1205: R.Tensor((1, seq_len, 14336), dtype="float16") = split205[1]
            silu205: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0205)
            mul205: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu205, split_1205)
            lv1036 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight8, model_layers_13_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims828: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1036, axes=None)
            matmul828: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul205, permute_dims828, out_dtype="void")
            add411: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul828, add410)
            rms_norm418: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add411, model_layers_14_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1037 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight8, model_layers_14_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims829: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1037, axes=None)
            matmul829: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm418, permute_dims829, out_dtype="void")
            reshape824: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul829, R.shape([1, seq_len, 48, 128]))
            reshape825: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape824, R.shape([seq_len, 48, 128]))
            lv1038 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape825), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape826: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1038, R.shape([1, seq_len, 32, 128]))
            reshape827: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape826, R.shape([1, seq_len, 4096]))
            lv1039 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight8, model_layers_14_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims830: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1039, axes=None)
            matmul830: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape827, permute_dims830, out_dtype="void")
            add412: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul830, add411)
            rms_norm419: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add412, model_layers_14_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1040 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight8, model_layers_14_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims831: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1040, axes=None)
            matmul831: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm419, permute_dims831, out_dtype="void")
            split206: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul831, indices_or_sections=2, axis=-1)
            split_0206: R.Tensor((1, seq_len, 14336), dtype="float16") = split206[0]
            split_1206: R.Tensor((1, seq_len, 14336), dtype="float16") = split206[1]
            silu206: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0206)
            mul206: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu206, split_1206)
            lv1041 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight8, model_layers_14_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims832: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1041, axes=None)
            matmul832: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul206, permute_dims832, out_dtype="void")
            add413: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul832, add412)
            rms_norm420: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add413, model_layers_15_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1042 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight8, model_layers_15_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims833: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1042, axes=None)
            matmul833: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm420, permute_dims833, out_dtype="void")
            reshape828: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul833, R.shape([1, seq_len, 48, 128]))
            reshape829: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape828, R.shape([seq_len, 48, 128]))
            lv1043 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape829), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape830: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1043, R.shape([1, seq_len, 32, 128]))
            reshape831: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape830, R.shape([1, seq_len, 4096]))
            lv1044 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight8, model_layers_15_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims834: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1044, axes=None)
            matmul834: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape831, permute_dims834, out_dtype="void")
            add414: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul834, add413)
            rms_norm421: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add414, model_layers_15_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1045 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight8, model_layers_15_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims835: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1045, axes=None)
            matmul835: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm421, permute_dims835, out_dtype="void")
            split207: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul835, indices_or_sections=2, axis=-1)
            split_0207: R.Tensor((1, seq_len, 14336), dtype="float16") = split207[0]
            split_1207: R.Tensor((1, seq_len, 14336), dtype="float16") = split207[1]
            silu207: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0207)
            mul207: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu207, split_1207)
            lv1046 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight8, model_layers_15_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims836: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1046, axes=None)
            matmul836: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul207, permute_dims836, out_dtype="void")
            add415: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul836, add414)
            rms_norm422: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add415, model_layers_16_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1047 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight8, model_layers_16_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims837: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1047, axes=None)
            matmul837: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm422, permute_dims837, out_dtype="void")
            reshape832: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul837, R.shape([1, seq_len, 48, 128]))
            reshape833: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape832, R.shape([seq_len, 48, 128]))
            lv1048 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape833), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape834: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1048, R.shape([1, seq_len, 32, 128]))
            reshape835: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape834, R.shape([1, seq_len, 4096]))
            lv1049 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight8, model_layers_16_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims838: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1049, axes=None)
            matmul838: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape835, permute_dims838, out_dtype="void")
            add416: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul838, add415)
            rms_norm423: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add416, model_layers_16_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1050 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight8, model_layers_16_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims839: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1050, axes=None)
            matmul839: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm423, permute_dims839, out_dtype="void")
            split208: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul839, indices_or_sections=2, axis=-1)
            split_0208: R.Tensor((1, seq_len, 14336), dtype="float16") = split208[0]
            split_1208: R.Tensor((1, seq_len, 14336), dtype="float16") = split208[1]
            silu208: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0208)
            mul208: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu208, split_1208)
            lv1051 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight8, model_layers_16_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims840: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1051, axes=None)
            matmul840: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul208, permute_dims840, out_dtype="void")
            add417: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul840, add416)
            rms_norm424: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add417, model_layers_17_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1052 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight8, model_layers_17_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims841: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1052, axes=None)
            matmul841: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm424, permute_dims841, out_dtype="void")
            reshape836: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul841, R.shape([1, seq_len, 48, 128]))
            reshape837: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape836, R.shape([seq_len, 48, 128]))
            lv1053 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape837), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape838: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1053, R.shape([1, seq_len, 32, 128]))
            reshape839: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape838, R.shape([1, seq_len, 4096]))
            lv1054 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight8, model_layers_17_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims842: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1054, axes=None)
            matmul842: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape839, permute_dims842, out_dtype="void")
            add418: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul842, add417)
            rms_norm425: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add418, model_layers_17_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1055 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight8, model_layers_17_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims843: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1055, axes=None)
            matmul843: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm425, permute_dims843, out_dtype="void")
            split209: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul843, indices_or_sections=2, axis=-1)
            split_0209: R.Tensor((1, seq_len, 14336), dtype="float16") = split209[0]
            split_1209: R.Tensor((1, seq_len, 14336), dtype="float16") = split209[1]
            silu209: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0209)
            mul209: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu209, split_1209)
            lv1056 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight8, model_layers_17_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims844: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1056, axes=None)
            matmul844: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul209, permute_dims844, out_dtype="void")
            add419: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul844, add418)
            rms_norm426: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add419, model_layers_18_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1057 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight8, model_layers_18_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims845: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1057, axes=None)
            matmul845: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm426, permute_dims845, out_dtype="void")
            reshape840: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul845, R.shape([1, seq_len, 48, 128]))
            reshape841: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape840, R.shape([seq_len, 48, 128]))
            lv1058 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape841), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape842: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1058, R.shape([1, seq_len, 32, 128]))
            reshape843: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape842, R.shape([1, seq_len, 4096]))
            lv1059 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight8, model_layers_18_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims846: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1059, axes=None)
            matmul846: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape843, permute_dims846, out_dtype="void")
            add420: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul846, add419)
            rms_norm427: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add420, model_layers_18_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1060 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight8, model_layers_18_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims847: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1060, axes=None)
            matmul847: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm427, permute_dims847, out_dtype="void")
            split210: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul847, indices_or_sections=2, axis=-1)
            split_0210: R.Tensor((1, seq_len, 14336), dtype="float16") = split210[0]
            split_1210: R.Tensor((1, seq_len, 14336), dtype="float16") = split210[1]
            silu210: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0210)
            mul210: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu210, split_1210)
            lv1061 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight8, model_layers_18_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims848: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1061, axes=None)
            matmul848: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul210, permute_dims848, out_dtype="void")
            add421: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul848, add420)
            rms_norm428: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add421, model_layers_19_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1062 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight8, model_layers_19_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims849: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1062, axes=None)
            matmul849: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm428, permute_dims849, out_dtype="void")
            reshape844: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul849, R.shape([1, seq_len, 48, 128]))
            reshape845: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape844, R.shape([seq_len, 48, 128]))
            lv1063 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape845), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape846: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1063, R.shape([1, seq_len, 32, 128]))
            reshape847: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape846, R.shape([1, seq_len, 4096]))
            lv1064 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight8, model_layers_19_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims850: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1064, axes=None)
            matmul850: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape847, permute_dims850, out_dtype="void")
            add422: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul850, add421)
            rms_norm429: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add422, model_layers_19_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1065 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight8, model_layers_19_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims851: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1065, axes=None)
            matmul851: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm429, permute_dims851, out_dtype="void")
            split211: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul851, indices_or_sections=2, axis=-1)
            split_0211: R.Tensor((1, seq_len, 14336), dtype="float16") = split211[0]
            split_1211: R.Tensor((1, seq_len, 14336), dtype="float16") = split211[1]
            silu211: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0211)
            mul211: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu211, split_1211)
            lv1066 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight8, model_layers_19_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims852: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1066, axes=None)
            matmul852: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul211, permute_dims852, out_dtype="void")
            add423: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul852, add422)
            rms_norm430: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add423, model_layers_20_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1067 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight8, model_layers_20_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims853: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1067, axes=None)
            matmul853: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm430, permute_dims853, out_dtype="void")
            reshape848: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul853, R.shape([1, seq_len, 48, 128]))
            reshape849: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape848, R.shape([seq_len, 48, 128]))
            lv1068 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape849), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape850: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1068, R.shape([1, seq_len, 32, 128]))
            reshape851: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape850, R.shape([1, seq_len, 4096]))
            lv1069 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight8, model_layers_20_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims854: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1069, axes=None)
            matmul854: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape851, permute_dims854, out_dtype="void")
            add424: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul854, add423)
            rms_norm431: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add424, model_layers_20_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1070 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight8, model_layers_20_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims855: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1070, axes=None)
            matmul855: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm431, permute_dims855, out_dtype="void")
            split212: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul855, indices_or_sections=2, axis=-1)
            split_0212: R.Tensor((1, seq_len, 14336), dtype="float16") = split212[0]
            split_1212: R.Tensor((1, seq_len, 14336), dtype="float16") = split212[1]
            silu212: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0212)
            mul212: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu212, split_1212)
            lv1071 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight8, model_layers_20_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims856: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1071, axes=None)
            matmul856: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul212, permute_dims856, out_dtype="void")
            add425: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul856, add424)
            rms_norm432: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add425, model_layers_21_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1072 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight8, model_layers_21_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims857: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1072, axes=None)
            matmul857: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm432, permute_dims857, out_dtype="void")
            reshape852: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul857, R.shape([1, seq_len, 48, 128]))
            reshape853: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape852, R.shape([seq_len, 48, 128]))
            lv1073 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape853), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape854: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1073, R.shape([1, seq_len, 32, 128]))
            reshape855: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape854, R.shape([1, seq_len, 4096]))
            lv1074 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight8, model_layers_21_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims858: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1074, axes=None)
            matmul858: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape855, permute_dims858, out_dtype="void")
            add426: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul858, add425)
            rms_norm433: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add426, model_layers_21_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1075 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight8, model_layers_21_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims859: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1075, axes=None)
            matmul859: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm433, permute_dims859, out_dtype="void")
            split213: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul859, indices_or_sections=2, axis=-1)
            split_0213: R.Tensor((1, seq_len, 14336), dtype="float16") = split213[0]
            split_1213: R.Tensor((1, seq_len, 14336), dtype="float16") = split213[1]
            silu213: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0213)
            mul213: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu213, split_1213)
            lv1076 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight8, model_layers_21_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims860: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1076, axes=None)
            matmul860: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul213, permute_dims860, out_dtype="void")
            add427: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul860, add426)
            rms_norm434: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add427, model_layers_22_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1077 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight8, model_layers_22_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims861: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1077, axes=None)
            matmul861: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm434, permute_dims861, out_dtype="void")
            reshape856: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul861, R.shape([1, seq_len, 48, 128]))
            reshape857: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape856, R.shape([seq_len, 48, 128]))
            lv1078 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape857), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape858: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1078, R.shape([1, seq_len, 32, 128]))
            reshape859: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape858, R.shape([1, seq_len, 4096]))
            lv1079 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight8, model_layers_22_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims862: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1079, axes=None)
            matmul862: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape859, permute_dims862, out_dtype="void")
            add428: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul862, add427)
            rms_norm435: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add428, model_layers_22_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1080 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight8, model_layers_22_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims863: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1080, axes=None)
            matmul863: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm435, permute_dims863, out_dtype="void")
            split214: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul863, indices_or_sections=2, axis=-1)
            split_0214: R.Tensor((1, seq_len, 14336), dtype="float16") = split214[0]
            split_1214: R.Tensor((1, seq_len, 14336), dtype="float16") = split214[1]
            silu214: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0214)
            mul214: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu214, split_1214)
            lv1081 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight8, model_layers_22_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims864: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1081, axes=None)
            matmul864: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul214, permute_dims864, out_dtype="void")
            add429: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul864, add428)
            rms_norm436: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add429, model_layers_23_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1082 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight8, model_layers_23_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims865: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1082, axes=None)
            matmul865: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm436, permute_dims865, out_dtype="void")
            reshape860: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul865, R.shape([1, seq_len, 48, 128]))
            reshape861: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape860, R.shape([seq_len, 48, 128]))
            lv1083 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape861), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape862: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1083, R.shape([1, seq_len, 32, 128]))
            reshape863: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape862, R.shape([1, seq_len, 4096]))
            lv1084 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight8, model_layers_23_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims866: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1084, axes=None)
            matmul866: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape863, permute_dims866, out_dtype="void")
            add430: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul866, add429)
            rms_norm437: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add430, model_layers_23_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1085 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight8, model_layers_23_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims867: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1085, axes=None)
            matmul867: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm437, permute_dims867, out_dtype="void")
            split215: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul867, indices_or_sections=2, axis=-1)
            split_0215: R.Tensor((1, seq_len, 14336), dtype="float16") = split215[0]
            split_1215: R.Tensor((1, seq_len, 14336), dtype="float16") = split215[1]
            silu215: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0215)
            mul215: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu215, split_1215)
            lv1086 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight8, model_layers_23_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims868: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1086, axes=None)
            matmul868: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul215, permute_dims868, out_dtype="void")
            add431: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul868, add430)
            rms_norm438: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add431, model_layers_24_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1087 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight8, model_layers_24_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims869: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1087, axes=None)
            matmul869: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm438, permute_dims869, out_dtype="void")
            reshape864: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul869, R.shape([1, seq_len, 48, 128]))
            reshape865: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape864, R.shape([seq_len, 48, 128]))
            lv1088 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape865), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape866: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1088, R.shape([1, seq_len, 32, 128]))
            reshape867: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape866, R.shape([1, seq_len, 4096]))
            lv1089 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight8, model_layers_24_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims870: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1089, axes=None)
            matmul870: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape867, permute_dims870, out_dtype="void")
            add432: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul870, add431)
            rms_norm439: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add432, model_layers_24_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1090 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight8, model_layers_24_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims871: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1090, axes=None)
            matmul871: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm439, permute_dims871, out_dtype="void")
            split216: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul871, indices_or_sections=2, axis=-1)
            split_0216: R.Tensor((1, seq_len, 14336), dtype="float16") = split216[0]
            split_1216: R.Tensor((1, seq_len, 14336), dtype="float16") = split216[1]
            silu216: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0216)
            mul216: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu216, split_1216)
            lv1091 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight8, model_layers_24_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims872: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1091, axes=None)
            matmul872: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul216, permute_dims872, out_dtype="void")
            add433: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul872, add432)
            rms_norm440: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add433, model_layers_25_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1092 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight8, model_layers_25_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims873: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1092, axes=None)
            matmul873: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm440, permute_dims873, out_dtype="void")
            reshape868: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul873, R.shape([1, seq_len, 48, 128]))
            reshape869: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape868, R.shape([seq_len, 48, 128]))
            lv1093 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape869), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape870: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1093, R.shape([1, seq_len, 32, 128]))
            reshape871: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape870, R.shape([1, seq_len, 4096]))
            lv1094 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight8, model_layers_25_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims874: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1094, axes=None)
            matmul874: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape871, permute_dims874, out_dtype="void")
            add434: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul874, add433)
            rms_norm441: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add434, model_layers_25_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1095 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight8, model_layers_25_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims875: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1095, axes=None)
            matmul875: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm441, permute_dims875, out_dtype="void")
            split217: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul875, indices_or_sections=2, axis=-1)
            split_0217: R.Tensor((1, seq_len, 14336), dtype="float16") = split217[0]
            split_1217: R.Tensor((1, seq_len, 14336), dtype="float16") = split217[1]
            silu217: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0217)
            mul217: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu217, split_1217)
            lv1096 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight8, model_layers_25_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims876: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1096, axes=None)
            matmul876: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul217, permute_dims876, out_dtype="void")
            add435: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul876, add434)
            rms_norm442: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add435, model_layers_26_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1097 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight8, model_layers_26_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims877: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1097, axes=None)
            matmul877: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm442, permute_dims877, out_dtype="void")
            reshape872: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul877, R.shape([1, seq_len, 48, 128]))
            reshape873: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape872, R.shape([seq_len, 48, 128]))
            lv1098 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape873), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape874: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1098, R.shape([1, seq_len, 32, 128]))
            reshape875: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape874, R.shape([1, seq_len, 4096]))
            lv1099 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight8, model_layers_26_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims878: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1099, axes=None)
            matmul878: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape875, permute_dims878, out_dtype="void")
            add436: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul878, add435)
            rms_norm443: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add436, model_layers_26_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1100 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight8, model_layers_26_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims879: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1100, axes=None)
            matmul879: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm443, permute_dims879, out_dtype="void")
            split218: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul879, indices_or_sections=2, axis=-1)
            split_0218: R.Tensor((1, seq_len, 14336), dtype="float16") = split218[0]
            split_1218: R.Tensor((1, seq_len, 14336), dtype="float16") = split218[1]
            silu218: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0218)
            mul218: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu218, split_1218)
            lv1101 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight8, model_layers_26_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims880: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1101, axes=None)
            matmul880: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul218, permute_dims880, out_dtype="void")
            add437: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul880, add436)
            rms_norm444: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add437, model_layers_27_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1102 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight8, model_layers_27_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims881: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1102, axes=None)
            matmul881: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm444, permute_dims881, out_dtype="void")
            reshape876: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul881, R.shape([1, seq_len, 48, 128]))
            reshape877: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape876, R.shape([seq_len, 48, 128]))
            lv1103 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape877), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape878: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1103, R.shape([1, seq_len, 32, 128]))
            reshape879: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape878, R.shape([1, seq_len, 4096]))
            lv1104 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight8, model_layers_27_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims882: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1104, axes=None)
            matmul882: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape879, permute_dims882, out_dtype="void")
            add438: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul882, add437)
            rms_norm445: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add438, model_layers_27_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1105 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight8, model_layers_27_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims883: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1105, axes=None)
            matmul883: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm445, permute_dims883, out_dtype="void")
            split219: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul883, indices_or_sections=2, axis=-1)
            split_0219: R.Tensor((1, seq_len, 14336), dtype="float16") = split219[0]
            split_1219: R.Tensor((1, seq_len, 14336), dtype="float16") = split219[1]
            silu219: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0219)
            mul219: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu219, split_1219)
            lv1106 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight8, model_layers_27_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims884: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1106, axes=None)
            matmul884: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul219, permute_dims884, out_dtype="void")
            add439: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul884, add438)
            rms_norm446: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add439, model_layers_28_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1107 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight8, model_layers_28_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims885: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1107, axes=None)
            matmul885: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm446, permute_dims885, out_dtype="void")
            reshape880: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul885, R.shape([1, seq_len, 48, 128]))
            reshape881: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape880, R.shape([seq_len, 48, 128]))
            lv1108 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape881), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape882: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1108, R.shape([1, seq_len, 32, 128]))
            reshape883: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape882, R.shape([1, seq_len, 4096]))
            lv1109 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight8, model_layers_28_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims886: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1109, axes=None)
            matmul886: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape883, permute_dims886, out_dtype="void")
            add440: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul886, add439)
            rms_norm447: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add440, model_layers_28_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1110 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight8, model_layers_28_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims887: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1110, axes=None)
            matmul887: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm447, permute_dims887, out_dtype="void")
            split220: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul887, indices_or_sections=2, axis=-1)
            split_0220: R.Tensor((1, seq_len, 14336), dtype="float16") = split220[0]
            split_1220: R.Tensor((1, seq_len, 14336), dtype="float16") = split220[1]
            silu220: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0220)
            mul220: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu220, split_1220)
            lv1111 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight8, model_layers_28_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims888: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1111, axes=None)
            matmul888: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul220, permute_dims888, out_dtype="void")
            add441: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul888, add440)
            rms_norm448: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add441, model_layers_29_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1112 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight8, model_layers_29_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims889: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1112, axes=None)
            matmul889: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm448, permute_dims889, out_dtype="void")
            reshape884: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul889, R.shape([1, seq_len, 48, 128]))
            reshape885: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape884, R.shape([seq_len, 48, 128]))
            lv1113 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape885), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape886: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1113, R.shape([1, seq_len, 32, 128]))
            reshape887: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape886, R.shape([1, seq_len, 4096]))
            lv1114 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight8, model_layers_29_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims890: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1114, axes=None)
            matmul890: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape887, permute_dims890, out_dtype="void")
            add442: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul890, add441)
            rms_norm449: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add442, model_layers_29_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1115 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight8, model_layers_29_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims891: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1115, axes=None)
            matmul891: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm449, permute_dims891, out_dtype="void")
            split221: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul891, indices_or_sections=2, axis=-1)
            split_0221: R.Tensor((1, seq_len, 14336), dtype="float16") = split221[0]
            split_1221: R.Tensor((1, seq_len, 14336), dtype="float16") = split221[1]
            silu221: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0221)
            mul221: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu221, split_1221)
            lv1116 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight8, model_layers_29_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims892: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1116, axes=None)
            matmul892: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul221, permute_dims892, out_dtype="void")
            add443: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul892, add442)
            rms_norm450: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add443, model_layers_30_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1117 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight8, model_layers_30_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims893: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1117, axes=None)
            matmul893: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm450, permute_dims893, out_dtype="void")
            reshape888: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul893, R.shape([1, seq_len, 48, 128]))
            reshape889: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape888, R.shape([seq_len, 48, 128]))
            lv1118 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape889), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape890: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1118, R.shape([1, seq_len, 32, 128]))
            reshape891: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape890, R.shape([1, seq_len, 4096]))
            lv1119 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight8, model_layers_30_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims894: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1119, axes=None)
            matmul894: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape891, permute_dims894, out_dtype="void")
            add444: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul894, add443)
            rms_norm451: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add444, model_layers_30_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1120 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight8, model_layers_30_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims895: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1120, axes=None)
            matmul895: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm451, permute_dims895, out_dtype="void")
            split222: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul895, indices_or_sections=2, axis=-1)
            split_0222: R.Tensor((1, seq_len, 14336), dtype="float16") = split222[0]
            split_1222: R.Tensor((1, seq_len, 14336), dtype="float16") = split222[1]
            silu222: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0222)
            mul222: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu222, split_1222)
            lv1121 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight8, model_layers_30_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims896: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1121, axes=None)
            matmul896: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul222, permute_dims896, out_dtype="void")
            add445: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul896, add444)
            rms_norm452: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add445, model_layers_31_input_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1122 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight8, model_layers_31_self_attn_qkv_proj_q_scale8), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims897: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1122, axes=None)
            matmul897: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm452, permute_dims897, out_dtype="void")
            reshape892: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul897, R.shape([1, seq_len, 48, 128]))
            reshape893: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape892, R.shape([seq_len, 48, 128]))
            lv1123 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape893), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape894: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1123, R.shape([1, seq_len, 32, 128]))
            reshape895: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape894, R.shape([1, seq_len, 4096]))
            lv1124 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight8, model_layers_31_self_attn_o_proj_q_scale8), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims898: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1124, axes=None)
            matmul898: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape895, permute_dims898, out_dtype="void")
            add446: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul898, add445)
            rms_norm453: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add446, model_layers_31_post_attention_layernorm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1125 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight8, model_layers_31_mlp_gate_up_proj_q_scale8), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims899: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1125, axes=None)
            matmul899: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm453, permute_dims899, out_dtype="void")
            split223: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul899, indices_or_sections=2, axis=-1)
            split_0223: R.Tensor((1, seq_len, 14336), dtype="float16") = split223[0]
            split_1223: R.Tensor((1, seq_len, 14336), dtype="float16") = split223[1]
            silu223: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0223)
            mul223: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu223, split_1223)
            lv1126 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight8, model_layers_31_mlp_down_proj_q_scale8), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims900: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1126, axes=None)
            matmul900: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul223, permute_dims900, out_dtype="void")
            add447: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul900, add446)
            rms_norm454: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add447, model_norm_weight8, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1127 = R.call_tir(cls.dequantize, (lm_head_q_weight8, lm_head_q_scale8), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            permute_dims901: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv1127, axes=None)
            matmul901: R.Tensor((1, seq_len, vocab_size), dtype="float16") = R.matmul(rms_norm454, permute_dims901, out_dtype="void")
            astype5: R.Tensor((1, seq_len, vocab_size), dtype="float32") = R.astype(matmul901, dtype="float32")
            gv9: R.Tuple(R.Tensor((1, seq_len, vocab_size), dtype="float32"), R.Object) = astype5, paged_kv_cache
            R.output(gv9)
        return gv9

    @R.function
    def batch_verify_to_last_hidden_states(input_embeds: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "relax.rewrite_cuda_graph.capture_symbolic_vars": ["batch_size", "seq_len"], "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight11: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale11: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight11: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale11: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight11: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale11: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight11: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale11: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight11: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale11: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight11: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight11: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale11: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm585: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embeds, model_layers_0_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1448 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight11, model_layers_0_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1158: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1448, axes=None)
            matmul1158: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm585, permute_dims1158, out_dtype="void")
            reshape1152: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1158, R.shape([1, seq_len, 48, 128]))
            reshape1153: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1152, R.shape([seq_len, 48, 128]))
            lv1449 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1153), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1154: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1449, R.shape([1, seq_len, 32, 128]))
            reshape1155: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1154, R.shape([1, seq_len, 4096]))
            lv1450 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight11, model_layers_0_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1159: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1450, axes=None)
            matmul1159: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1155, permute_dims1159, out_dtype="void")
            add576: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1159, input_embeds)
            rms_norm586: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add576, model_layers_0_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1451 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight11, model_layers_0_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1160: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1451, axes=None)
            matmul1160: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm586, permute_dims1160, out_dtype="void")
            split288: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1160, indices_or_sections=2, axis=-1)
            split_0288: R.Tensor((1, seq_len, 14336), dtype="float16") = split288[0]
            split_1288: R.Tensor((1, seq_len, 14336), dtype="float16") = split288[1]
            silu288: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0288)
            mul288: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu288, split_1288)
            lv1452 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight11, model_layers_0_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1161: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1452, axes=None)
            matmul1161: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul288, permute_dims1161, out_dtype="void")
            add577: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1161, add576)
            rms_norm587: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add577, model_layers_1_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1453 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight11, model_layers_1_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1162: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1453, axes=None)
            matmul1162: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm587, permute_dims1162, out_dtype="void")
            reshape1156: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1162, R.shape([1, seq_len, 48, 128]))
            reshape1157: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1156, R.shape([seq_len, 48, 128]))
            lv1454 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape1157), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1158: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1454, R.shape([1, seq_len, 32, 128]))
            reshape1159: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1158, R.shape([1, seq_len, 4096]))
            lv1455 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight11, model_layers_1_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1163: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1455, axes=None)
            matmul1163: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1159, permute_dims1163, out_dtype="void")
            add578: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1163, add577)
            rms_norm588: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add578, model_layers_1_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1456 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight11, model_layers_1_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1164: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1456, axes=None)
            matmul1164: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm588, permute_dims1164, out_dtype="void")
            split289: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1164, indices_or_sections=2, axis=-1)
            split_0289: R.Tensor((1, seq_len, 14336), dtype="float16") = split289[0]
            split_1289: R.Tensor((1, seq_len, 14336), dtype="float16") = split289[1]
            silu289: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0289)
            mul289: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu289, split_1289)
            lv1457 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight11, model_layers_1_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1165: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1457, axes=None)
            matmul1165: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul289, permute_dims1165, out_dtype="void")
            add579: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1165, add578)
            rms_norm589: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add579, model_layers_2_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1458 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight11, model_layers_2_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1166: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1458, axes=None)
            matmul1166: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm589, permute_dims1166, out_dtype="void")
            reshape1160: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1166, R.shape([1, seq_len, 48, 128]))
            reshape1161: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1160, R.shape([seq_len, 48, 128]))
            lv1459 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape1161), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1162: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1459, R.shape([1, seq_len, 32, 128]))
            reshape1163: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1162, R.shape([1, seq_len, 4096]))
            lv1460 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight11, model_layers_2_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1167: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1460, axes=None)
            matmul1167: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1163, permute_dims1167, out_dtype="void")
            add580: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1167, add579)
            rms_norm590: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add580, model_layers_2_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1461 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight11, model_layers_2_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1168: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1461, axes=None)
            matmul1168: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm590, permute_dims1168, out_dtype="void")
            split290: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1168, indices_or_sections=2, axis=-1)
            split_0290: R.Tensor((1, seq_len, 14336), dtype="float16") = split290[0]
            split_1290: R.Tensor((1, seq_len, 14336), dtype="float16") = split290[1]
            silu290: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0290)
            mul290: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu290, split_1290)
            lv1462 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight11, model_layers_2_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1169: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1462, axes=None)
            matmul1169: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul290, permute_dims1169, out_dtype="void")
            add581: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1169, add580)
            rms_norm591: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add581, model_layers_3_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1463 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight11, model_layers_3_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1170: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1463, axes=None)
            matmul1170: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm591, permute_dims1170, out_dtype="void")
            reshape1164: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1170, R.shape([1, seq_len, 48, 128]))
            reshape1165: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1164, R.shape([seq_len, 48, 128]))
            lv1464 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape1165), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1166: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1464, R.shape([1, seq_len, 32, 128]))
            reshape1167: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1166, R.shape([1, seq_len, 4096]))
            lv1465 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight11, model_layers_3_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1171: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1465, axes=None)
            matmul1171: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1167, permute_dims1171, out_dtype="void")
            add582: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1171, add581)
            rms_norm592: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add582, model_layers_3_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1466 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight11, model_layers_3_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1172: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1466, axes=None)
            matmul1172: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm592, permute_dims1172, out_dtype="void")
            split291: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1172, indices_or_sections=2, axis=-1)
            split_0291: R.Tensor((1, seq_len, 14336), dtype="float16") = split291[0]
            split_1291: R.Tensor((1, seq_len, 14336), dtype="float16") = split291[1]
            silu291: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0291)
            mul291: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu291, split_1291)
            lv1467 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight11, model_layers_3_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1173: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1467, axes=None)
            matmul1173: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul291, permute_dims1173, out_dtype="void")
            add583: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1173, add582)
            rms_norm593: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add583, model_layers_4_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1468 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight11, model_layers_4_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1174: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1468, axes=None)
            matmul1174: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm593, permute_dims1174, out_dtype="void")
            reshape1168: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1174, R.shape([1, seq_len, 48, 128]))
            reshape1169: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1168, R.shape([seq_len, 48, 128]))
            lv1469 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape1169), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1170: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1469, R.shape([1, seq_len, 32, 128]))
            reshape1171: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1170, R.shape([1, seq_len, 4096]))
            lv1470 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight11, model_layers_4_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1175: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1470, axes=None)
            matmul1175: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1171, permute_dims1175, out_dtype="void")
            add584: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1175, add583)
            rms_norm594: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add584, model_layers_4_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1471 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight11, model_layers_4_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1176: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1471, axes=None)
            matmul1176: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm594, permute_dims1176, out_dtype="void")
            split292: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1176, indices_or_sections=2, axis=-1)
            split_0292: R.Tensor((1, seq_len, 14336), dtype="float16") = split292[0]
            split_1292: R.Tensor((1, seq_len, 14336), dtype="float16") = split292[1]
            silu292: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0292)
            mul292: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu292, split_1292)
            lv1472 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight11, model_layers_4_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1177: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1472, axes=None)
            matmul1177: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul292, permute_dims1177, out_dtype="void")
            add585: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1177, add584)
            rms_norm595: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add585, model_layers_5_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1473 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight11, model_layers_5_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1178: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1473, axes=None)
            matmul1178: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm595, permute_dims1178, out_dtype="void")
            reshape1172: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1178, R.shape([1, seq_len, 48, 128]))
            reshape1173: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1172, R.shape([seq_len, 48, 128]))
            lv1474 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape1173), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1174: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1474, R.shape([1, seq_len, 32, 128]))
            reshape1175: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1174, R.shape([1, seq_len, 4096]))
            lv1475 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight11, model_layers_5_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1179: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1475, axes=None)
            matmul1179: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1175, permute_dims1179, out_dtype="void")
            add586: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1179, add585)
            rms_norm596: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add586, model_layers_5_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1476 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight11, model_layers_5_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1180: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1476, axes=None)
            matmul1180: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm596, permute_dims1180, out_dtype="void")
            split293: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1180, indices_or_sections=2, axis=-1)
            split_0293: R.Tensor((1, seq_len, 14336), dtype="float16") = split293[0]
            split_1293: R.Tensor((1, seq_len, 14336), dtype="float16") = split293[1]
            silu293: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0293)
            mul293: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu293, split_1293)
            lv1477 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight11, model_layers_5_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1181: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1477, axes=None)
            matmul1181: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul293, permute_dims1181, out_dtype="void")
            add587: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1181, add586)
            rms_norm597: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add587, model_layers_6_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1478 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight11, model_layers_6_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1182: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1478, axes=None)
            matmul1182: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm597, permute_dims1182, out_dtype="void")
            reshape1176: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1182, R.shape([1, seq_len, 48, 128]))
            reshape1177: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1176, R.shape([seq_len, 48, 128]))
            lv1479 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape1177), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1178: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1479, R.shape([1, seq_len, 32, 128]))
            reshape1179: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1178, R.shape([1, seq_len, 4096]))
            lv1480 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight11, model_layers_6_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1183: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1480, axes=None)
            matmul1183: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1179, permute_dims1183, out_dtype="void")
            add588: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1183, add587)
            rms_norm598: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add588, model_layers_6_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1481 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight11, model_layers_6_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1184: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1481, axes=None)
            matmul1184: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm598, permute_dims1184, out_dtype="void")
            split294: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1184, indices_or_sections=2, axis=-1)
            split_0294: R.Tensor((1, seq_len, 14336), dtype="float16") = split294[0]
            split_1294: R.Tensor((1, seq_len, 14336), dtype="float16") = split294[1]
            silu294: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0294)
            mul294: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu294, split_1294)
            lv1482 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight11, model_layers_6_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1185: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1482, axes=None)
            matmul1185: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul294, permute_dims1185, out_dtype="void")
            add589: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1185, add588)
            rms_norm599: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add589, model_layers_7_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1483 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight11, model_layers_7_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1186: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1483, axes=None)
            matmul1186: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm599, permute_dims1186, out_dtype="void")
            reshape1180: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1186, R.shape([1, seq_len, 48, 128]))
            reshape1181: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1180, R.shape([seq_len, 48, 128]))
            lv1484 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape1181), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1182: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1484, R.shape([1, seq_len, 32, 128]))
            reshape1183: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1182, R.shape([1, seq_len, 4096]))
            lv1485 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight11, model_layers_7_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1187: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1485, axes=None)
            matmul1187: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1183, permute_dims1187, out_dtype="void")
            add590: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1187, add589)
            rms_norm600: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add590, model_layers_7_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1486 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight11, model_layers_7_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1188: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1486, axes=None)
            matmul1188: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm600, permute_dims1188, out_dtype="void")
            split295: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1188, indices_or_sections=2, axis=-1)
            split_0295: R.Tensor((1, seq_len, 14336), dtype="float16") = split295[0]
            split_1295: R.Tensor((1, seq_len, 14336), dtype="float16") = split295[1]
            silu295: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0295)
            mul295: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu295, split_1295)
            lv1487 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight11, model_layers_7_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1189: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1487, axes=None)
            matmul1189: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul295, permute_dims1189, out_dtype="void")
            add591: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1189, add590)
            rms_norm601: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add591, model_layers_8_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1488 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight11, model_layers_8_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1190: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1488, axes=None)
            matmul1190: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm601, permute_dims1190, out_dtype="void")
            reshape1184: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1190, R.shape([1, seq_len, 48, 128]))
            reshape1185: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1184, R.shape([seq_len, 48, 128]))
            lv1489 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape1185), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1186: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1489, R.shape([1, seq_len, 32, 128]))
            reshape1187: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1186, R.shape([1, seq_len, 4096]))
            lv1490 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight11, model_layers_8_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1191: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1490, axes=None)
            matmul1191: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1187, permute_dims1191, out_dtype="void")
            add592: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1191, add591)
            rms_norm602: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add592, model_layers_8_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1491 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight11, model_layers_8_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1192: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1491, axes=None)
            matmul1192: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm602, permute_dims1192, out_dtype="void")
            split296: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1192, indices_or_sections=2, axis=-1)
            split_0296: R.Tensor((1, seq_len, 14336), dtype="float16") = split296[0]
            split_1296: R.Tensor((1, seq_len, 14336), dtype="float16") = split296[1]
            silu296: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0296)
            mul296: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu296, split_1296)
            lv1492 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight11, model_layers_8_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1193: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1492, axes=None)
            matmul1193: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul296, permute_dims1193, out_dtype="void")
            add593: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1193, add592)
            rms_norm603: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add593, model_layers_9_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1493 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight11, model_layers_9_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1194: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1493, axes=None)
            matmul1194: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm603, permute_dims1194, out_dtype="void")
            reshape1188: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1194, R.shape([1, seq_len, 48, 128]))
            reshape1189: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1188, R.shape([seq_len, 48, 128]))
            lv1494 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape1189), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1190: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1494, R.shape([1, seq_len, 32, 128]))
            reshape1191: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1190, R.shape([1, seq_len, 4096]))
            lv1495 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight11, model_layers_9_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1195: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1495, axes=None)
            matmul1195: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1191, permute_dims1195, out_dtype="void")
            add594: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1195, add593)
            rms_norm604: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add594, model_layers_9_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1496 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight11, model_layers_9_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1196: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1496, axes=None)
            matmul1196: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm604, permute_dims1196, out_dtype="void")
            split297: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1196, indices_or_sections=2, axis=-1)
            split_0297: R.Tensor((1, seq_len, 14336), dtype="float16") = split297[0]
            split_1297: R.Tensor((1, seq_len, 14336), dtype="float16") = split297[1]
            silu297: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0297)
            mul297: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu297, split_1297)
            lv1497 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight11, model_layers_9_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1197: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1497, axes=None)
            matmul1197: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul297, permute_dims1197, out_dtype="void")
            add595: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1197, add594)
            rms_norm605: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add595, model_layers_10_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1498 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight11, model_layers_10_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1198: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1498, axes=None)
            matmul1198: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm605, permute_dims1198, out_dtype="void")
            reshape1192: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1198, R.shape([1, seq_len, 48, 128]))
            reshape1193: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1192, R.shape([seq_len, 48, 128]))
            lv1499 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape1193), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1194: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1499, R.shape([1, seq_len, 32, 128]))
            reshape1195: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1194, R.shape([1, seq_len, 4096]))
            lv1500 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight11, model_layers_10_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1199: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1500, axes=None)
            matmul1199: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1195, permute_dims1199, out_dtype="void")
            add596: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1199, add595)
            rms_norm606: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add596, model_layers_10_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1501 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight11, model_layers_10_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1200: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1501, axes=None)
            matmul1200: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm606, permute_dims1200, out_dtype="void")
            split298: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1200, indices_or_sections=2, axis=-1)
            split_0298: R.Tensor((1, seq_len, 14336), dtype="float16") = split298[0]
            split_1298: R.Tensor((1, seq_len, 14336), dtype="float16") = split298[1]
            silu298: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0298)
            mul298: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu298, split_1298)
            lv1502 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight11, model_layers_10_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1201: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1502, axes=None)
            matmul1201: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul298, permute_dims1201, out_dtype="void")
            add597: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1201, add596)
            rms_norm607: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add597, model_layers_11_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1503 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight11, model_layers_11_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1202: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1503, axes=None)
            matmul1202: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm607, permute_dims1202, out_dtype="void")
            reshape1196: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1202, R.shape([1, seq_len, 48, 128]))
            reshape1197: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1196, R.shape([seq_len, 48, 128]))
            lv1504 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape1197), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1198: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1504, R.shape([1, seq_len, 32, 128]))
            reshape1199: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1198, R.shape([1, seq_len, 4096]))
            lv1505 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight11, model_layers_11_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1203: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1505, axes=None)
            matmul1203: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1199, permute_dims1203, out_dtype="void")
            add598: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1203, add597)
            rms_norm608: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add598, model_layers_11_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1506 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight11, model_layers_11_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1204: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1506, axes=None)
            matmul1204: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm608, permute_dims1204, out_dtype="void")
            split299: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1204, indices_or_sections=2, axis=-1)
            split_0299: R.Tensor((1, seq_len, 14336), dtype="float16") = split299[0]
            split_1299: R.Tensor((1, seq_len, 14336), dtype="float16") = split299[1]
            silu299: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0299)
            mul299: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu299, split_1299)
            lv1507 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight11, model_layers_11_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1205: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1507, axes=None)
            matmul1205: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul299, permute_dims1205, out_dtype="void")
            add599: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1205, add598)
            rms_norm609: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add599, model_layers_12_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1508 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight11, model_layers_12_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1206: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1508, axes=None)
            matmul1206: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm609, permute_dims1206, out_dtype="void")
            reshape1200: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1206, R.shape([1, seq_len, 48, 128]))
            reshape1201: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1200, R.shape([seq_len, 48, 128]))
            lv1509 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape1201), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1202: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1509, R.shape([1, seq_len, 32, 128]))
            reshape1203: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1202, R.shape([1, seq_len, 4096]))
            lv1510 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight11, model_layers_12_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1207: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1510, axes=None)
            matmul1207: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1203, permute_dims1207, out_dtype="void")
            add600: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1207, add599)
            rms_norm610: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add600, model_layers_12_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1511 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight11, model_layers_12_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1208: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1511, axes=None)
            matmul1208: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm610, permute_dims1208, out_dtype="void")
            split300: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1208, indices_or_sections=2, axis=-1)
            split_0300: R.Tensor((1, seq_len, 14336), dtype="float16") = split300[0]
            split_1300: R.Tensor((1, seq_len, 14336), dtype="float16") = split300[1]
            silu300: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0300)
            mul300: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu300, split_1300)
            lv1512 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight11, model_layers_12_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1209: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1512, axes=None)
            matmul1209: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul300, permute_dims1209, out_dtype="void")
            add601: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1209, add600)
            rms_norm611: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add601, model_layers_13_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1513 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight11, model_layers_13_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1210: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1513, axes=None)
            matmul1210: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm611, permute_dims1210, out_dtype="void")
            reshape1204: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1210, R.shape([1, seq_len, 48, 128]))
            reshape1205: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1204, R.shape([seq_len, 48, 128]))
            lv1514 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape1205), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1206: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1514, R.shape([1, seq_len, 32, 128]))
            reshape1207: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1206, R.shape([1, seq_len, 4096]))
            lv1515 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight11, model_layers_13_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1211: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1515, axes=None)
            matmul1211: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1207, permute_dims1211, out_dtype="void")
            add602: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1211, add601)
            rms_norm612: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add602, model_layers_13_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1516 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight11, model_layers_13_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1212: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1516, axes=None)
            matmul1212: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm612, permute_dims1212, out_dtype="void")
            split301: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1212, indices_or_sections=2, axis=-1)
            split_0301: R.Tensor((1, seq_len, 14336), dtype="float16") = split301[0]
            split_1301: R.Tensor((1, seq_len, 14336), dtype="float16") = split301[1]
            silu301: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0301)
            mul301: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu301, split_1301)
            lv1517 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight11, model_layers_13_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1213: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1517, axes=None)
            matmul1213: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul301, permute_dims1213, out_dtype="void")
            add603: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1213, add602)
            rms_norm613: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add603, model_layers_14_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1518 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight11, model_layers_14_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1214: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1518, axes=None)
            matmul1214: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm613, permute_dims1214, out_dtype="void")
            reshape1208: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1214, R.shape([1, seq_len, 48, 128]))
            reshape1209: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1208, R.shape([seq_len, 48, 128]))
            lv1519 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape1209), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1210: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1519, R.shape([1, seq_len, 32, 128]))
            reshape1211: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1210, R.shape([1, seq_len, 4096]))
            lv1520 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight11, model_layers_14_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1215: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1520, axes=None)
            matmul1215: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1211, permute_dims1215, out_dtype="void")
            add604: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1215, add603)
            rms_norm614: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add604, model_layers_14_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1521 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight11, model_layers_14_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1216: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1521, axes=None)
            matmul1216: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm614, permute_dims1216, out_dtype="void")
            split302: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1216, indices_or_sections=2, axis=-1)
            split_0302: R.Tensor((1, seq_len, 14336), dtype="float16") = split302[0]
            split_1302: R.Tensor((1, seq_len, 14336), dtype="float16") = split302[1]
            silu302: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0302)
            mul302: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu302, split_1302)
            lv1522 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight11, model_layers_14_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1217: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1522, axes=None)
            matmul1217: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul302, permute_dims1217, out_dtype="void")
            add605: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1217, add604)
            rms_norm615: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add605, model_layers_15_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1523 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight11, model_layers_15_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1218: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1523, axes=None)
            matmul1218: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm615, permute_dims1218, out_dtype="void")
            reshape1212: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1218, R.shape([1, seq_len, 48, 128]))
            reshape1213: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1212, R.shape([seq_len, 48, 128]))
            lv1524 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape1213), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1214: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1524, R.shape([1, seq_len, 32, 128]))
            reshape1215: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1214, R.shape([1, seq_len, 4096]))
            lv1525 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight11, model_layers_15_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1219: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1525, axes=None)
            matmul1219: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1215, permute_dims1219, out_dtype="void")
            add606: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1219, add605)
            rms_norm616: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add606, model_layers_15_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1526 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight11, model_layers_15_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1220: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1526, axes=None)
            matmul1220: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm616, permute_dims1220, out_dtype="void")
            split303: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1220, indices_or_sections=2, axis=-1)
            split_0303: R.Tensor((1, seq_len, 14336), dtype="float16") = split303[0]
            split_1303: R.Tensor((1, seq_len, 14336), dtype="float16") = split303[1]
            silu303: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0303)
            mul303: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu303, split_1303)
            lv1527 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight11, model_layers_15_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1221: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1527, axes=None)
            matmul1221: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul303, permute_dims1221, out_dtype="void")
            add607: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1221, add606)
            rms_norm617: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add607, model_layers_16_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1528 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight11, model_layers_16_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1222: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1528, axes=None)
            matmul1222: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm617, permute_dims1222, out_dtype="void")
            reshape1216: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1222, R.shape([1, seq_len, 48, 128]))
            reshape1217: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1216, R.shape([seq_len, 48, 128]))
            lv1529 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape1217), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1218: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1529, R.shape([1, seq_len, 32, 128]))
            reshape1219: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1218, R.shape([1, seq_len, 4096]))
            lv1530 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight11, model_layers_16_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1223: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1530, axes=None)
            matmul1223: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1219, permute_dims1223, out_dtype="void")
            add608: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1223, add607)
            rms_norm618: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add608, model_layers_16_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1531 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight11, model_layers_16_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1224: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1531, axes=None)
            matmul1224: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm618, permute_dims1224, out_dtype="void")
            split304: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1224, indices_or_sections=2, axis=-1)
            split_0304: R.Tensor((1, seq_len, 14336), dtype="float16") = split304[0]
            split_1304: R.Tensor((1, seq_len, 14336), dtype="float16") = split304[1]
            silu304: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0304)
            mul304: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu304, split_1304)
            lv1532 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight11, model_layers_16_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1225: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1532, axes=None)
            matmul1225: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul304, permute_dims1225, out_dtype="void")
            add609: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1225, add608)
            rms_norm619: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add609, model_layers_17_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1533 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight11, model_layers_17_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1226: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1533, axes=None)
            matmul1226: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm619, permute_dims1226, out_dtype="void")
            reshape1220: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1226, R.shape([1, seq_len, 48, 128]))
            reshape1221: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1220, R.shape([seq_len, 48, 128]))
            lv1534 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape1221), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1222: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1534, R.shape([1, seq_len, 32, 128]))
            reshape1223: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1222, R.shape([1, seq_len, 4096]))
            lv1535 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight11, model_layers_17_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1227: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1535, axes=None)
            matmul1227: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1223, permute_dims1227, out_dtype="void")
            add610: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1227, add609)
            rms_norm620: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add610, model_layers_17_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1536 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight11, model_layers_17_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1228: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1536, axes=None)
            matmul1228: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm620, permute_dims1228, out_dtype="void")
            split305: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1228, indices_or_sections=2, axis=-1)
            split_0305: R.Tensor((1, seq_len, 14336), dtype="float16") = split305[0]
            split_1305: R.Tensor((1, seq_len, 14336), dtype="float16") = split305[1]
            silu305: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0305)
            mul305: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu305, split_1305)
            lv1537 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight11, model_layers_17_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1229: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1537, axes=None)
            matmul1229: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul305, permute_dims1229, out_dtype="void")
            add611: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1229, add610)
            rms_norm621: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add611, model_layers_18_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1538 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight11, model_layers_18_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1230: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1538, axes=None)
            matmul1230: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm621, permute_dims1230, out_dtype="void")
            reshape1224: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1230, R.shape([1, seq_len, 48, 128]))
            reshape1225: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1224, R.shape([seq_len, 48, 128]))
            lv1539 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape1225), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1226: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1539, R.shape([1, seq_len, 32, 128]))
            reshape1227: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1226, R.shape([1, seq_len, 4096]))
            lv1540 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight11, model_layers_18_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1231: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1540, axes=None)
            matmul1231: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1227, permute_dims1231, out_dtype="void")
            add612: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1231, add611)
            rms_norm622: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add612, model_layers_18_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1541 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight11, model_layers_18_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1232: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1541, axes=None)
            matmul1232: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm622, permute_dims1232, out_dtype="void")
            split306: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1232, indices_or_sections=2, axis=-1)
            split_0306: R.Tensor((1, seq_len, 14336), dtype="float16") = split306[0]
            split_1306: R.Tensor((1, seq_len, 14336), dtype="float16") = split306[1]
            silu306: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0306)
            mul306: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu306, split_1306)
            lv1542 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight11, model_layers_18_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1233: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1542, axes=None)
            matmul1233: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul306, permute_dims1233, out_dtype="void")
            add613: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1233, add612)
            rms_norm623: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add613, model_layers_19_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1543 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight11, model_layers_19_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1234: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1543, axes=None)
            matmul1234: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm623, permute_dims1234, out_dtype="void")
            reshape1228: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1234, R.shape([1, seq_len, 48, 128]))
            reshape1229: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1228, R.shape([seq_len, 48, 128]))
            lv1544 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape1229), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1230: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1544, R.shape([1, seq_len, 32, 128]))
            reshape1231: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1230, R.shape([1, seq_len, 4096]))
            lv1545 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight11, model_layers_19_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1235: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1545, axes=None)
            matmul1235: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1231, permute_dims1235, out_dtype="void")
            add614: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1235, add613)
            rms_norm624: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add614, model_layers_19_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1546 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight11, model_layers_19_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1236: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1546, axes=None)
            matmul1236: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm624, permute_dims1236, out_dtype="void")
            split307: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1236, indices_or_sections=2, axis=-1)
            split_0307: R.Tensor((1, seq_len, 14336), dtype="float16") = split307[0]
            split_1307: R.Tensor((1, seq_len, 14336), dtype="float16") = split307[1]
            silu307: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0307)
            mul307: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu307, split_1307)
            lv1547 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight11, model_layers_19_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1237: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1547, axes=None)
            matmul1237: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul307, permute_dims1237, out_dtype="void")
            add615: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1237, add614)
            rms_norm625: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add615, model_layers_20_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1548 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight11, model_layers_20_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1238: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1548, axes=None)
            matmul1238: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm625, permute_dims1238, out_dtype="void")
            reshape1232: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1238, R.shape([1, seq_len, 48, 128]))
            reshape1233: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1232, R.shape([seq_len, 48, 128]))
            lv1549 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape1233), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1234: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1549, R.shape([1, seq_len, 32, 128]))
            reshape1235: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1234, R.shape([1, seq_len, 4096]))
            lv1550 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight11, model_layers_20_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1239: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1550, axes=None)
            matmul1239: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1235, permute_dims1239, out_dtype="void")
            add616: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1239, add615)
            rms_norm626: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add616, model_layers_20_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1551 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight11, model_layers_20_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1240: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1551, axes=None)
            matmul1240: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm626, permute_dims1240, out_dtype="void")
            split308: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1240, indices_or_sections=2, axis=-1)
            split_0308: R.Tensor((1, seq_len, 14336), dtype="float16") = split308[0]
            split_1308: R.Tensor((1, seq_len, 14336), dtype="float16") = split308[1]
            silu308: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0308)
            mul308: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu308, split_1308)
            lv1552 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight11, model_layers_20_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1241: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1552, axes=None)
            matmul1241: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul308, permute_dims1241, out_dtype="void")
            add617: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1241, add616)
            rms_norm627: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add617, model_layers_21_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1553 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight11, model_layers_21_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1242: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1553, axes=None)
            matmul1242: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm627, permute_dims1242, out_dtype="void")
            reshape1236: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1242, R.shape([1, seq_len, 48, 128]))
            reshape1237: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1236, R.shape([seq_len, 48, 128]))
            lv1554 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape1237), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1238: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1554, R.shape([1, seq_len, 32, 128]))
            reshape1239: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1238, R.shape([1, seq_len, 4096]))
            lv1555 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight11, model_layers_21_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1243: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1555, axes=None)
            matmul1243: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1239, permute_dims1243, out_dtype="void")
            add618: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1243, add617)
            rms_norm628: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add618, model_layers_21_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1556 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight11, model_layers_21_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1244: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1556, axes=None)
            matmul1244: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm628, permute_dims1244, out_dtype="void")
            split309: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1244, indices_or_sections=2, axis=-1)
            split_0309: R.Tensor((1, seq_len, 14336), dtype="float16") = split309[0]
            split_1309: R.Tensor((1, seq_len, 14336), dtype="float16") = split309[1]
            silu309: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0309)
            mul309: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu309, split_1309)
            lv1557 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight11, model_layers_21_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1245: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1557, axes=None)
            matmul1245: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul309, permute_dims1245, out_dtype="void")
            add619: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1245, add618)
            rms_norm629: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add619, model_layers_22_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1558 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight11, model_layers_22_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1246: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1558, axes=None)
            matmul1246: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm629, permute_dims1246, out_dtype="void")
            reshape1240: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1246, R.shape([1, seq_len, 48, 128]))
            reshape1241: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1240, R.shape([seq_len, 48, 128]))
            lv1559 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape1241), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1242: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1559, R.shape([1, seq_len, 32, 128]))
            reshape1243: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1242, R.shape([1, seq_len, 4096]))
            lv1560 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight11, model_layers_22_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1247: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1560, axes=None)
            matmul1247: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1243, permute_dims1247, out_dtype="void")
            add620: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1247, add619)
            rms_norm630: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add620, model_layers_22_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1561 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight11, model_layers_22_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1248: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1561, axes=None)
            matmul1248: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm630, permute_dims1248, out_dtype="void")
            split310: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1248, indices_or_sections=2, axis=-1)
            split_0310: R.Tensor((1, seq_len, 14336), dtype="float16") = split310[0]
            split_1310: R.Tensor((1, seq_len, 14336), dtype="float16") = split310[1]
            silu310: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0310)
            mul310: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu310, split_1310)
            lv1562 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight11, model_layers_22_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1249: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1562, axes=None)
            matmul1249: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul310, permute_dims1249, out_dtype="void")
            add621: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1249, add620)
            rms_norm631: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add621, model_layers_23_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1563 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight11, model_layers_23_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1250: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1563, axes=None)
            matmul1250: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm631, permute_dims1250, out_dtype="void")
            reshape1244: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1250, R.shape([1, seq_len, 48, 128]))
            reshape1245: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1244, R.shape([seq_len, 48, 128]))
            lv1564 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape1245), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1246: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1564, R.shape([1, seq_len, 32, 128]))
            reshape1247: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1246, R.shape([1, seq_len, 4096]))
            lv1565 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight11, model_layers_23_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1251: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1565, axes=None)
            matmul1251: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1247, permute_dims1251, out_dtype="void")
            add622: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1251, add621)
            rms_norm632: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add622, model_layers_23_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1566 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight11, model_layers_23_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1252: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1566, axes=None)
            matmul1252: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm632, permute_dims1252, out_dtype="void")
            split311: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1252, indices_or_sections=2, axis=-1)
            split_0311: R.Tensor((1, seq_len, 14336), dtype="float16") = split311[0]
            split_1311: R.Tensor((1, seq_len, 14336), dtype="float16") = split311[1]
            silu311: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0311)
            mul311: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu311, split_1311)
            lv1567 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight11, model_layers_23_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1253: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1567, axes=None)
            matmul1253: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul311, permute_dims1253, out_dtype="void")
            add623: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1253, add622)
            rms_norm633: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add623, model_layers_24_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1568 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight11, model_layers_24_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1254: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1568, axes=None)
            matmul1254: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm633, permute_dims1254, out_dtype="void")
            reshape1248: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1254, R.shape([1, seq_len, 48, 128]))
            reshape1249: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1248, R.shape([seq_len, 48, 128]))
            lv1569 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape1249), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1250: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1569, R.shape([1, seq_len, 32, 128]))
            reshape1251: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1250, R.shape([1, seq_len, 4096]))
            lv1570 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight11, model_layers_24_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1255: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1570, axes=None)
            matmul1255: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1251, permute_dims1255, out_dtype="void")
            add624: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1255, add623)
            rms_norm634: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add624, model_layers_24_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1571 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight11, model_layers_24_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1256: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1571, axes=None)
            matmul1256: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm634, permute_dims1256, out_dtype="void")
            split312: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1256, indices_or_sections=2, axis=-1)
            split_0312: R.Tensor((1, seq_len, 14336), dtype="float16") = split312[0]
            split_1312: R.Tensor((1, seq_len, 14336), dtype="float16") = split312[1]
            silu312: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0312)
            mul312: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu312, split_1312)
            lv1572 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight11, model_layers_24_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1257: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1572, axes=None)
            matmul1257: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul312, permute_dims1257, out_dtype="void")
            add625: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1257, add624)
            rms_norm635: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add625, model_layers_25_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1573 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight11, model_layers_25_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1258: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1573, axes=None)
            matmul1258: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm635, permute_dims1258, out_dtype="void")
            reshape1252: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1258, R.shape([1, seq_len, 48, 128]))
            reshape1253: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1252, R.shape([seq_len, 48, 128]))
            lv1574 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape1253), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1254: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1574, R.shape([1, seq_len, 32, 128]))
            reshape1255: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1254, R.shape([1, seq_len, 4096]))
            lv1575 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight11, model_layers_25_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1259: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1575, axes=None)
            matmul1259: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1255, permute_dims1259, out_dtype="void")
            add626: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1259, add625)
            rms_norm636: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add626, model_layers_25_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1576 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight11, model_layers_25_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1260: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1576, axes=None)
            matmul1260: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm636, permute_dims1260, out_dtype="void")
            split313: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1260, indices_or_sections=2, axis=-1)
            split_0313: R.Tensor((1, seq_len, 14336), dtype="float16") = split313[0]
            split_1313: R.Tensor((1, seq_len, 14336), dtype="float16") = split313[1]
            silu313: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0313)
            mul313: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu313, split_1313)
            lv1577 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight11, model_layers_25_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1261: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1577, axes=None)
            matmul1261: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul313, permute_dims1261, out_dtype="void")
            add627: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1261, add626)
            rms_norm637: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add627, model_layers_26_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1578 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight11, model_layers_26_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1262: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1578, axes=None)
            matmul1262: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm637, permute_dims1262, out_dtype="void")
            reshape1256: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1262, R.shape([1, seq_len, 48, 128]))
            reshape1257: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1256, R.shape([seq_len, 48, 128]))
            lv1579 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape1257), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1258: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1579, R.shape([1, seq_len, 32, 128]))
            reshape1259: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1258, R.shape([1, seq_len, 4096]))
            lv1580 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight11, model_layers_26_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1263: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1580, axes=None)
            matmul1263: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1259, permute_dims1263, out_dtype="void")
            add628: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1263, add627)
            rms_norm638: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add628, model_layers_26_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1581 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight11, model_layers_26_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1264: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1581, axes=None)
            matmul1264: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm638, permute_dims1264, out_dtype="void")
            split314: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1264, indices_or_sections=2, axis=-1)
            split_0314: R.Tensor((1, seq_len, 14336), dtype="float16") = split314[0]
            split_1314: R.Tensor((1, seq_len, 14336), dtype="float16") = split314[1]
            silu314: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0314)
            mul314: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu314, split_1314)
            lv1582 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight11, model_layers_26_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1265: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1582, axes=None)
            matmul1265: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul314, permute_dims1265, out_dtype="void")
            add629: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1265, add628)
            rms_norm639: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add629, model_layers_27_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1583 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight11, model_layers_27_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1266: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1583, axes=None)
            matmul1266: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm639, permute_dims1266, out_dtype="void")
            reshape1260: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1266, R.shape([1, seq_len, 48, 128]))
            reshape1261: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1260, R.shape([seq_len, 48, 128]))
            lv1584 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape1261), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1262: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1584, R.shape([1, seq_len, 32, 128]))
            reshape1263: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1262, R.shape([1, seq_len, 4096]))
            lv1585 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight11, model_layers_27_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1267: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1585, axes=None)
            matmul1267: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1263, permute_dims1267, out_dtype="void")
            add630: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1267, add629)
            rms_norm640: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add630, model_layers_27_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1586 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight11, model_layers_27_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1268: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1586, axes=None)
            matmul1268: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm640, permute_dims1268, out_dtype="void")
            split315: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1268, indices_or_sections=2, axis=-1)
            split_0315: R.Tensor((1, seq_len, 14336), dtype="float16") = split315[0]
            split_1315: R.Tensor((1, seq_len, 14336), dtype="float16") = split315[1]
            silu315: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0315)
            mul315: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu315, split_1315)
            lv1587 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight11, model_layers_27_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1269: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1587, axes=None)
            matmul1269: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul315, permute_dims1269, out_dtype="void")
            add631: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1269, add630)
            rms_norm641: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add631, model_layers_28_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1588 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight11, model_layers_28_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1270: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1588, axes=None)
            matmul1270: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm641, permute_dims1270, out_dtype="void")
            reshape1264: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1270, R.shape([1, seq_len, 48, 128]))
            reshape1265: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1264, R.shape([seq_len, 48, 128]))
            lv1589 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape1265), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1266: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1589, R.shape([1, seq_len, 32, 128]))
            reshape1267: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1266, R.shape([1, seq_len, 4096]))
            lv1590 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight11, model_layers_28_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1271: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1590, axes=None)
            matmul1271: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1267, permute_dims1271, out_dtype="void")
            add632: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1271, add631)
            rms_norm642: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add632, model_layers_28_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1591 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight11, model_layers_28_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1272: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1591, axes=None)
            matmul1272: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm642, permute_dims1272, out_dtype="void")
            split316: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1272, indices_or_sections=2, axis=-1)
            split_0316: R.Tensor((1, seq_len, 14336), dtype="float16") = split316[0]
            split_1316: R.Tensor((1, seq_len, 14336), dtype="float16") = split316[1]
            silu316: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0316)
            mul316: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu316, split_1316)
            lv1592 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight11, model_layers_28_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1273: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1592, axes=None)
            matmul1273: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul316, permute_dims1273, out_dtype="void")
            add633: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1273, add632)
            rms_norm643: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add633, model_layers_29_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1593 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight11, model_layers_29_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1274: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1593, axes=None)
            matmul1274: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm643, permute_dims1274, out_dtype="void")
            reshape1268: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1274, R.shape([1, seq_len, 48, 128]))
            reshape1269: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1268, R.shape([seq_len, 48, 128]))
            lv1594 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape1269), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1270: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1594, R.shape([1, seq_len, 32, 128]))
            reshape1271: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1270, R.shape([1, seq_len, 4096]))
            lv1595 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight11, model_layers_29_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1275: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1595, axes=None)
            matmul1275: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1271, permute_dims1275, out_dtype="void")
            add634: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1275, add633)
            rms_norm644: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add634, model_layers_29_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1596 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight11, model_layers_29_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1276: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1596, axes=None)
            matmul1276: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm644, permute_dims1276, out_dtype="void")
            split317: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1276, indices_or_sections=2, axis=-1)
            split_0317: R.Tensor((1, seq_len, 14336), dtype="float16") = split317[0]
            split_1317: R.Tensor((1, seq_len, 14336), dtype="float16") = split317[1]
            silu317: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0317)
            mul317: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu317, split_1317)
            lv1597 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight11, model_layers_29_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1277: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1597, axes=None)
            matmul1277: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul317, permute_dims1277, out_dtype="void")
            add635: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1277, add634)
            rms_norm645: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add635, model_layers_30_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1598 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight11, model_layers_30_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1278: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1598, axes=None)
            matmul1278: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm645, permute_dims1278, out_dtype="void")
            reshape1272: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1278, R.shape([1, seq_len, 48, 128]))
            reshape1273: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1272, R.shape([seq_len, 48, 128]))
            lv1599 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape1273), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1274: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1599, R.shape([1, seq_len, 32, 128]))
            reshape1275: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1274, R.shape([1, seq_len, 4096]))
            lv1600 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight11, model_layers_30_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1279: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1600, axes=None)
            matmul1279: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1275, permute_dims1279, out_dtype="void")
            add636: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1279, add635)
            rms_norm646: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add636, model_layers_30_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1601 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight11, model_layers_30_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1280: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1601, axes=None)
            matmul1280: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm646, permute_dims1280, out_dtype="void")
            split318: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1280, indices_or_sections=2, axis=-1)
            split_0318: R.Tensor((1, seq_len, 14336), dtype="float16") = split318[0]
            split_1318: R.Tensor((1, seq_len, 14336), dtype="float16") = split318[1]
            silu318: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0318)
            mul318: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu318, split_1318)
            lv1602 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight11, model_layers_30_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1281: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1602, axes=None)
            matmul1281: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul318, permute_dims1281, out_dtype="void")
            add637: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1281, add636)
            rms_norm647: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add637, model_layers_31_input_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1603 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight11, model_layers_31_self_attn_qkv_proj_q_scale11), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1282: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv1603, axes=None)
            matmul1282: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm647, permute_dims1282, out_dtype="void")
            reshape1276: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1282, R.shape([1, seq_len, 48, 128]))
            reshape1277: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape1276, R.shape([seq_len, 48, 128]))
            lv1604 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape1277), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape1278: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv1604, R.shape([1, seq_len, 32, 128]))
            reshape1279: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape1278, R.shape([1, seq_len, 4096]))
            lv1605 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight11, model_layers_31_self_attn_o_proj_q_scale11), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims1283: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv1605, axes=None)
            matmul1283: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape1279, permute_dims1283, out_dtype="void")
            add638: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1283, add637)
            rms_norm648: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add638, model_layers_31_post_attention_layernorm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            lv1606 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight11, model_layers_31_mlp_gate_up_proj_q_scale11), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims1284: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv1606, axes=None)
            matmul1284: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm648, permute_dims1284, out_dtype="void")
            split319: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul1284, indices_or_sections=2, axis=-1)
            split_0319: R.Tensor((1, seq_len, 14336), dtype="float16") = split319[0]
            split_1319: R.Tensor((1, seq_len, 14336), dtype="float16") = split319[1]
            silu319: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0319)
            mul319: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu319, split_1319)
            lv1607 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight11, model_layers_31_mlp_down_proj_q_scale11), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims1285: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv1607, axes=None)
            matmul1285: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul319, permute_dims1285, out_dtype="void")
            add639: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul1285, add638)
            rms_norm649: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add639, model_norm_weight11, axes=[-1], epsilon=1.0000000000000001e-05)
            gv12: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm649, paged_kv_cache
            R.output(gv12)
        return gv12

    @R.function
    def create_tir_paged_kv_cache(max_batch_size_: R.Shape(["max_batch_size"]), max_total_seq_len_: R.Shape(["max_total_seq_len"]), prefill_chunk_size_: R.Shape(["prefill_chunk_size"]), page_size_: R.Shape(["page_size"]), support_sliding_window_: R.Shape(["support_sliding_window"])) -> R.Object:
        max_batch_size = T.int64()
        max_total_seq_len = T.int64()
        prefill_chunk_size = T.int64()
        page_size = T.int64()
        support_sliding_window = T.int64()
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        gv: R.Tensor((), dtype="float16") = R.zeros(R.shape([]), dtype="float16")
        paged_kv_cache: R.Object = R.call_pure_packed("vm.builtin.paged_attention_kv_cache_create_reduced", R.shape([max_batch_size, max_total_seq_len, prefill_chunk_size, page_size, support_sliding_window]), R.shape([0, 32]), R.prim_value(32), R.prim_value(8), R.prim_value(128), R.prim_value(1), R.prim_value(1), R.prim_value(T.float32(500000.0)), gv, cls.tir_kv_cache_transpose_append, cls.batch_prefill_paged_kv, cls.batch_decode_paged_kv, cls.batch_prefill_paged_kv_sliding_window, cls.batch_decode_paged_kv_sliding_window, cls.batch_prefill_ragged_kv, cls.merge_state_inplace, cls.fused_rope, cls.copy_single_page, cls.tir_kv_cache_debug_get_kv, cls.compact_kv_copy, cls.batch_tree_attn, cls.tree_attn_paged_kv, R.prim_value(0), sinfo_args=(R.Object,))
        return paged_kv_cache

    @R.function
    def decode(input_embed: R.Tensor((1, 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight3: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale3: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight3: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale3: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight3: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale3: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight3: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale3: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight3: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale3: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight3: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight3: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale3: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm65: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv164 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight3, model_layers_0_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims130: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv164, axes=None)
            matmul130: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm65, permute_dims130, out_dtype="void")
            reshape128: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul130, R.shape([1, 1, 48, 128]))
            reshape129: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape128, R.shape([1, 48, 128]))
            lv165 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape129), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape130: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv165, R.shape([1, 1, 32, 128]))
            reshape131: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape130, R.shape([1, 1, 4096]))
            lv166 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight3, model_layers_0_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims131: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv166, axes=None)
            matmul131: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape131, permute_dims131, out_dtype="void")
            add64: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul131, input_embed)
            rms_norm66: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add64, model_layers_0_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv167 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight3, model_layers_0_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims132: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv167, axes=None)
            matmul132: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm66, permute_dims132, out_dtype="void")
            split32: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul132, indices_or_sections=2, axis=-1)
            split_032: R.Tensor((1, 1, 14336), dtype="float16") = split32[0]
            split_132: R.Tensor((1, 1, 14336), dtype="float16") = split32[1]
            silu32: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_032)
            mul32: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu32, split_132)
            lv168 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight3, model_layers_0_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims133: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv168, axes=None)
            matmul133: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul32, permute_dims133, out_dtype="void")
            add65: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul133, add64)
            rms_norm67: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add65, model_layers_1_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv169 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight3, model_layers_1_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims134: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv169, axes=None)
            matmul134: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm67, permute_dims134, out_dtype="void")
            reshape132: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul134, R.shape([1, 1, 48, 128]))
            reshape133: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape132, R.shape([1, 48, 128]))
            lv170 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape133), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape134: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv170, R.shape([1, 1, 32, 128]))
            reshape135: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape134, R.shape([1, 1, 4096]))
            lv171 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight3, model_layers_1_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims135: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv171, axes=None)
            matmul135: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape135, permute_dims135, out_dtype="void")
            add66: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul135, add65)
            rms_norm68: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add66, model_layers_1_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv172 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight3, model_layers_1_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims136: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv172, axes=None)
            matmul136: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm68, permute_dims136, out_dtype="void")
            split33: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul136, indices_or_sections=2, axis=-1)
            split_033: R.Tensor((1, 1, 14336), dtype="float16") = split33[0]
            split_133: R.Tensor((1, 1, 14336), dtype="float16") = split33[1]
            silu33: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_033)
            mul33: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu33, split_133)
            lv173 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight3, model_layers_1_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims137: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv173, axes=None)
            matmul137: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul33, permute_dims137, out_dtype="void")
            add67: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul137, add66)
            rms_norm69: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add67, model_layers_2_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv174 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight3, model_layers_2_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims138: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv174, axes=None)
            matmul138: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm69, permute_dims138, out_dtype="void")
            reshape136: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul138, R.shape([1, 1, 48, 128]))
            reshape137: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape136, R.shape([1, 48, 128]))
            lv175 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape137), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape138: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv175, R.shape([1, 1, 32, 128]))
            reshape139: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape138, R.shape([1, 1, 4096]))
            lv176 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight3, model_layers_2_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims139: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv176, axes=None)
            matmul139: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape139, permute_dims139, out_dtype="void")
            add68: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul139, add67)
            rms_norm70: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add68, model_layers_2_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv177 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight3, model_layers_2_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims140: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv177, axes=None)
            matmul140: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm70, permute_dims140, out_dtype="void")
            split34: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul140, indices_or_sections=2, axis=-1)
            split_034: R.Tensor((1, 1, 14336), dtype="float16") = split34[0]
            split_134: R.Tensor((1, 1, 14336), dtype="float16") = split34[1]
            silu34: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_034)
            mul34: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu34, split_134)
            lv178 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight3, model_layers_2_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims141: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv178, axes=None)
            matmul141: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul34, permute_dims141, out_dtype="void")
            add69: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul141, add68)
            rms_norm71: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add69, model_layers_3_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv179 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight3, model_layers_3_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims142: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv179, axes=None)
            matmul142: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm71, permute_dims142, out_dtype="void")
            reshape140: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul142, R.shape([1, 1, 48, 128]))
            reshape141: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape140, R.shape([1, 48, 128]))
            lv180 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape141), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape142: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv180, R.shape([1, 1, 32, 128]))
            reshape143: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape142, R.shape([1, 1, 4096]))
            lv181 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight3, model_layers_3_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims143: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv181, axes=None)
            matmul143: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape143, permute_dims143, out_dtype="void")
            add70: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul143, add69)
            rms_norm72: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add70, model_layers_3_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv182 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight3, model_layers_3_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims144: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv182, axes=None)
            matmul144: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm72, permute_dims144, out_dtype="void")
            split35: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul144, indices_or_sections=2, axis=-1)
            split_035: R.Tensor((1, 1, 14336), dtype="float16") = split35[0]
            split_135: R.Tensor((1, 1, 14336), dtype="float16") = split35[1]
            silu35: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_035)
            mul35: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu35, split_135)
            lv183 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight3, model_layers_3_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims145: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv183, axes=None)
            matmul145: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul35, permute_dims145, out_dtype="void")
            add71: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul145, add70)
            rms_norm73: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add71, model_layers_4_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv184 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight3, model_layers_4_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims146: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv184, axes=None)
            matmul146: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm73, permute_dims146, out_dtype="void")
            reshape144: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul146, R.shape([1, 1, 48, 128]))
            reshape145: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape144, R.shape([1, 48, 128]))
            lv185 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape145), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape146: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv185, R.shape([1, 1, 32, 128]))
            reshape147: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape146, R.shape([1, 1, 4096]))
            lv186 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight3, model_layers_4_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims147: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv186, axes=None)
            matmul147: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape147, permute_dims147, out_dtype="void")
            add72: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul147, add71)
            rms_norm74: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add72, model_layers_4_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv187 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight3, model_layers_4_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims148: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv187, axes=None)
            matmul148: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm74, permute_dims148, out_dtype="void")
            split36: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul148, indices_or_sections=2, axis=-1)
            split_036: R.Tensor((1, 1, 14336), dtype="float16") = split36[0]
            split_136: R.Tensor((1, 1, 14336), dtype="float16") = split36[1]
            silu36: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_036)
            mul36: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu36, split_136)
            lv188 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight3, model_layers_4_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims149: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv188, axes=None)
            matmul149: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul36, permute_dims149, out_dtype="void")
            add73: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul149, add72)
            rms_norm75: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add73, model_layers_5_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv189 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight3, model_layers_5_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims150: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv189, axes=None)
            matmul150: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm75, permute_dims150, out_dtype="void")
            reshape148: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul150, R.shape([1, 1, 48, 128]))
            reshape149: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape148, R.shape([1, 48, 128]))
            lv190 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape149), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape150: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv190, R.shape([1, 1, 32, 128]))
            reshape151: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape150, R.shape([1, 1, 4096]))
            lv191 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight3, model_layers_5_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims151: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv191, axes=None)
            matmul151: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape151, permute_dims151, out_dtype="void")
            add74: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul151, add73)
            rms_norm76: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add74, model_layers_5_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv192 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight3, model_layers_5_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims152: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv192, axes=None)
            matmul152: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm76, permute_dims152, out_dtype="void")
            split37: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul152, indices_or_sections=2, axis=-1)
            split_037: R.Tensor((1, 1, 14336), dtype="float16") = split37[0]
            split_137: R.Tensor((1, 1, 14336), dtype="float16") = split37[1]
            silu37: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_037)
            mul37: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu37, split_137)
            lv193 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight3, model_layers_5_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims153: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv193, axes=None)
            matmul153: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul37, permute_dims153, out_dtype="void")
            add75: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul153, add74)
            rms_norm77: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add75, model_layers_6_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv194 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight3, model_layers_6_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims154: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv194, axes=None)
            matmul154: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm77, permute_dims154, out_dtype="void")
            reshape152: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul154, R.shape([1, 1, 48, 128]))
            reshape153: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape152, R.shape([1, 48, 128]))
            lv195 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape153), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape154: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv195, R.shape([1, 1, 32, 128]))
            reshape155: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape154, R.shape([1, 1, 4096]))
            lv196 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight3, model_layers_6_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims155: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv196, axes=None)
            matmul155: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape155, permute_dims155, out_dtype="void")
            add76: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul155, add75)
            rms_norm78: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add76, model_layers_6_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv197 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight3, model_layers_6_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims156: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv197, axes=None)
            matmul156: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm78, permute_dims156, out_dtype="void")
            split38: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul156, indices_or_sections=2, axis=-1)
            split_038: R.Tensor((1, 1, 14336), dtype="float16") = split38[0]
            split_138: R.Tensor((1, 1, 14336), dtype="float16") = split38[1]
            silu38: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_038)
            mul38: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu38, split_138)
            lv198 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight3, model_layers_6_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims157: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv198, axes=None)
            matmul157: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul38, permute_dims157, out_dtype="void")
            add77: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul157, add76)
            rms_norm79: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add77, model_layers_7_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv199 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight3, model_layers_7_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims158: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv199, axes=None)
            matmul158: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm79, permute_dims158, out_dtype="void")
            reshape156: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul158, R.shape([1, 1, 48, 128]))
            reshape157: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape156, R.shape([1, 48, 128]))
            lv200 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape157), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape158: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv200, R.shape([1, 1, 32, 128]))
            reshape159: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape158, R.shape([1, 1, 4096]))
            lv201 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight3, model_layers_7_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims159: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv201, axes=None)
            matmul159: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape159, permute_dims159, out_dtype="void")
            add78: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul159, add77)
            rms_norm80: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add78, model_layers_7_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv202 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight3, model_layers_7_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims160: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv202, axes=None)
            matmul160: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm80, permute_dims160, out_dtype="void")
            split39: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul160, indices_or_sections=2, axis=-1)
            split_039: R.Tensor((1, 1, 14336), dtype="float16") = split39[0]
            split_139: R.Tensor((1, 1, 14336), dtype="float16") = split39[1]
            silu39: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_039)
            mul39: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu39, split_139)
            lv203 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight3, model_layers_7_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims161: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv203, axes=None)
            matmul161: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul39, permute_dims161, out_dtype="void")
            add79: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul161, add78)
            rms_norm81: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add79, model_layers_8_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv204 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight3, model_layers_8_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims162: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv204, axes=None)
            matmul162: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm81, permute_dims162, out_dtype="void")
            reshape160: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul162, R.shape([1, 1, 48, 128]))
            reshape161: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape160, R.shape([1, 48, 128]))
            lv205 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape161), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape162: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv205, R.shape([1, 1, 32, 128]))
            reshape163: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape162, R.shape([1, 1, 4096]))
            lv206 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight3, model_layers_8_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims163: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv206, axes=None)
            matmul163: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape163, permute_dims163, out_dtype="void")
            add80: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul163, add79)
            rms_norm82: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add80, model_layers_8_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv207 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight3, model_layers_8_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims164: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv207, axes=None)
            matmul164: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm82, permute_dims164, out_dtype="void")
            split40: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul164, indices_or_sections=2, axis=-1)
            split_040: R.Tensor((1, 1, 14336), dtype="float16") = split40[0]
            split_140: R.Tensor((1, 1, 14336), dtype="float16") = split40[1]
            silu40: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_040)
            mul40: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu40, split_140)
            lv208 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight3, model_layers_8_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims165: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv208, axes=None)
            matmul165: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul40, permute_dims165, out_dtype="void")
            add81: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul165, add80)
            rms_norm83: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add81, model_layers_9_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv209 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight3, model_layers_9_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims166: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv209, axes=None)
            matmul166: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm83, permute_dims166, out_dtype="void")
            reshape164: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul166, R.shape([1, 1, 48, 128]))
            reshape165: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape164, R.shape([1, 48, 128]))
            lv210 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape165), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape166: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv210, R.shape([1, 1, 32, 128]))
            reshape167: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape166, R.shape([1, 1, 4096]))
            lv211 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight3, model_layers_9_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims167: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv211, axes=None)
            matmul167: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape167, permute_dims167, out_dtype="void")
            add82: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul167, add81)
            rms_norm84: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add82, model_layers_9_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv212 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight3, model_layers_9_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims168: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv212, axes=None)
            matmul168: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm84, permute_dims168, out_dtype="void")
            split41: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul168, indices_or_sections=2, axis=-1)
            split_041: R.Tensor((1, 1, 14336), dtype="float16") = split41[0]
            split_141: R.Tensor((1, 1, 14336), dtype="float16") = split41[1]
            silu41: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_041)
            mul41: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu41, split_141)
            lv213 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight3, model_layers_9_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims169: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv213, axes=None)
            matmul169: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul41, permute_dims169, out_dtype="void")
            add83: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul169, add82)
            rms_norm85: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add83, model_layers_10_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv214 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight3, model_layers_10_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims170: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv214, axes=None)
            matmul170: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm85, permute_dims170, out_dtype="void")
            reshape168: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul170, R.shape([1, 1, 48, 128]))
            reshape169: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape168, R.shape([1, 48, 128]))
            lv215 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape169), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape170: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv215, R.shape([1, 1, 32, 128]))
            reshape171: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape170, R.shape([1, 1, 4096]))
            lv216 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight3, model_layers_10_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims171: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv216, axes=None)
            matmul171: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape171, permute_dims171, out_dtype="void")
            add84: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul171, add83)
            rms_norm86: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add84, model_layers_10_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv217 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight3, model_layers_10_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims172: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv217, axes=None)
            matmul172: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm86, permute_dims172, out_dtype="void")
            split42: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul172, indices_or_sections=2, axis=-1)
            split_042: R.Tensor((1, 1, 14336), dtype="float16") = split42[0]
            split_142: R.Tensor((1, 1, 14336), dtype="float16") = split42[1]
            silu42: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_042)
            mul42: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu42, split_142)
            lv218 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight3, model_layers_10_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims173: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv218, axes=None)
            matmul173: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul42, permute_dims173, out_dtype="void")
            add85: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul173, add84)
            rms_norm87: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add85, model_layers_11_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv219 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight3, model_layers_11_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims174: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv219, axes=None)
            matmul174: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm87, permute_dims174, out_dtype="void")
            reshape172: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul174, R.shape([1, 1, 48, 128]))
            reshape173: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape172, R.shape([1, 48, 128]))
            lv220 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape173), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape174: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv220, R.shape([1, 1, 32, 128]))
            reshape175: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape174, R.shape([1, 1, 4096]))
            lv221 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight3, model_layers_11_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims175: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv221, axes=None)
            matmul175: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape175, permute_dims175, out_dtype="void")
            add86: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul175, add85)
            rms_norm88: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add86, model_layers_11_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv222 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight3, model_layers_11_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims176: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv222, axes=None)
            matmul176: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm88, permute_dims176, out_dtype="void")
            split43: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul176, indices_or_sections=2, axis=-1)
            split_043: R.Tensor((1, 1, 14336), dtype="float16") = split43[0]
            split_143: R.Tensor((1, 1, 14336), dtype="float16") = split43[1]
            silu43: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_043)
            mul43: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu43, split_143)
            lv223 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight3, model_layers_11_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims177: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv223, axes=None)
            matmul177: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul43, permute_dims177, out_dtype="void")
            add87: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul177, add86)
            rms_norm89: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add87, model_layers_12_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv224 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight3, model_layers_12_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims178: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv224, axes=None)
            matmul178: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm89, permute_dims178, out_dtype="void")
            reshape176: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul178, R.shape([1, 1, 48, 128]))
            reshape177: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape176, R.shape([1, 48, 128]))
            lv225 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape177), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape178: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv225, R.shape([1, 1, 32, 128]))
            reshape179: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape178, R.shape([1, 1, 4096]))
            lv226 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight3, model_layers_12_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims179: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv226, axes=None)
            matmul179: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape179, permute_dims179, out_dtype="void")
            add88: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul179, add87)
            rms_norm90: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add88, model_layers_12_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv227 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight3, model_layers_12_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims180: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv227, axes=None)
            matmul180: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm90, permute_dims180, out_dtype="void")
            split44: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul180, indices_or_sections=2, axis=-1)
            split_044: R.Tensor((1, 1, 14336), dtype="float16") = split44[0]
            split_144: R.Tensor((1, 1, 14336), dtype="float16") = split44[1]
            silu44: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_044)
            mul44: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu44, split_144)
            lv228 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight3, model_layers_12_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims181: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv228, axes=None)
            matmul181: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul44, permute_dims181, out_dtype="void")
            add89: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul181, add88)
            rms_norm91: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add89, model_layers_13_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv229 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight3, model_layers_13_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims182: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv229, axes=None)
            matmul182: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm91, permute_dims182, out_dtype="void")
            reshape180: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul182, R.shape([1, 1, 48, 128]))
            reshape181: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape180, R.shape([1, 48, 128]))
            lv230 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape181), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape182: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv230, R.shape([1, 1, 32, 128]))
            reshape183: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape182, R.shape([1, 1, 4096]))
            lv231 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight3, model_layers_13_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims183: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv231, axes=None)
            matmul183: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape183, permute_dims183, out_dtype="void")
            add90: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul183, add89)
            rms_norm92: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add90, model_layers_13_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv232 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight3, model_layers_13_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims184: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv232, axes=None)
            matmul184: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm92, permute_dims184, out_dtype="void")
            split45: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul184, indices_or_sections=2, axis=-1)
            split_045: R.Tensor((1, 1, 14336), dtype="float16") = split45[0]
            split_145: R.Tensor((1, 1, 14336), dtype="float16") = split45[1]
            silu45: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_045)
            mul45: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu45, split_145)
            lv233 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight3, model_layers_13_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims185: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv233, axes=None)
            matmul185: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul45, permute_dims185, out_dtype="void")
            add91: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul185, add90)
            rms_norm93: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add91, model_layers_14_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv234 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight3, model_layers_14_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims186: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv234, axes=None)
            matmul186: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm93, permute_dims186, out_dtype="void")
            reshape184: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul186, R.shape([1, 1, 48, 128]))
            reshape185: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape184, R.shape([1, 48, 128]))
            lv235 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape185), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape186: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv235, R.shape([1, 1, 32, 128]))
            reshape187: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape186, R.shape([1, 1, 4096]))
            lv236 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight3, model_layers_14_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims187: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv236, axes=None)
            matmul187: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape187, permute_dims187, out_dtype="void")
            add92: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul187, add91)
            rms_norm94: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add92, model_layers_14_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv237 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight3, model_layers_14_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims188: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv237, axes=None)
            matmul188: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm94, permute_dims188, out_dtype="void")
            split46: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul188, indices_or_sections=2, axis=-1)
            split_046: R.Tensor((1, 1, 14336), dtype="float16") = split46[0]
            split_146: R.Tensor((1, 1, 14336), dtype="float16") = split46[1]
            silu46: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_046)
            mul46: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu46, split_146)
            lv238 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight3, model_layers_14_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims189: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv238, axes=None)
            matmul189: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul46, permute_dims189, out_dtype="void")
            add93: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul189, add92)
            rms_norm95: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add93, model_layers_15_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv239 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight3, model_layers_15_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims190: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv239, axes=None)
            matmul190: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm95, permute_dims190, out_dtype="void")
            reshape188: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul190, R.shape([1, 1, 48, 128]))
            reshape189: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape188, R.shape([1, 48, 128]))
            lv240 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape189), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape190: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv240, R.shape([1, 1, 32, 128]))
            reshape191: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape190, R.shape([1, 1, 4096]))
            lv241 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight3, model_layers_15_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims191: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv241, axes=None)
            matmul191: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape191, permute_dims191, out_dtype="void")
            add94: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul191, add93)
            rms_norm96: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add94, model_layers_15_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv242 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight3, model_layers_15_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims192: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv242, axes=None)
            matmul192: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm96, permute_dims192, out_dtype="void")
            split47: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul192, indices_or_sections=2, axis=-1)
            split_047: R.Tensor((1, 1, 14336), dtype="float16") = split47[0]
            split_147: R.Tensor((1, 1, 14336), dtype="float16") = split47[1]
            silu47: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_047)
            mul47: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu47, split_147)
            lv243 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight3, model_layers_15_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims193: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv243, axes=None)
            matmul193: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul47, permute_dims193, out_dtype="void")
            add95: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul193, add94)
            rms_norm97: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add95, model_layers_16_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv244 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight3, model_layers_16_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims194: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv244, axes=None)
            matmul194: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm97, permute_dims194, out_dtype="void")
            reshape192: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul194, R.shape([1, 1, 48, 128]))
            reshape193: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape192, R.shape([1, 48, 128]))
            lv245 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape193), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape194: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv245, R.shape([1, 1, 32, 128]))
            reshape195: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape194, R.shape([1, 1, 4096]))
            lv246 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight3, model_layers_16_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims195: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv246, axes=None)
            matmul195: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape195, permute_dims195, out_dtype="void")
            add96: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul195, add95)
            rms_norm98: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add96, model_layers_16_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv247 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight3, model_layers_16_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims196: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv247, axes=None)
            matmul196: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm98, permute_dims196, out_dtype="void")
            split48: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul196, indices_or_sections=2, axis=-1)
            split_048: R.Tensor((1, 1, 14336), dtype="float16") = split48[0]
            split_148: R.Tensor((1, 1, 14336), dtype="float16") = split48[1]
            silu48: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_048)
            mul48: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu48, split_148)
            lv248 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight3, model_layers_16_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims197: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv248, axes=None)
            matmul197: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul48, permute_dims197, out_dtype="void")
            add97: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul197, add96)
            rms_norm99: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add97, model_layers_17_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv249 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight3, model_layers_17_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims198: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv249, axes=None)
            matmul198: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm99, permute_dims198, out_dtype="void")
            reshape196: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul198, R.shape([1, 1, 48, 128]))
            reshape197: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape196, R.shape([1, 48, 128]))
            lv250 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape197), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape198: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv250, R.shape([1, 1, 32, 128]))
            reshape199: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape198, R.shape([1, 1, 4096]))
            lv251 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight3, model_layers_17_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims199: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv251, axes=None)
            matmul199: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape199, permute_dims199, out_dtype="void")
            add98: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul199, add97)
            rms_norm100: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add98, model_layers_17_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv252 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight3, model_layers_17_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims200: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv252, axes=None)
            matmul200: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm100, permute_dims200, out_dtype="void")
            split49: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul200, indices_or_sections=2, axis=-1)
            split_049: R.Tensor((1, 1, 14336), dtype="float16") = split49[0]
            split_149: R.Tensor((1, 1, 14336), dtype="float16") = split49[1]
            silu49: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_049)
            mul49: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu49, split_149)
            lv253 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight3, model_layers_17_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims201: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv253, axes=None)
            matmul201: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul49, permute_dims201, out_dtype="void")
            add99: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul201, add98)
            rms_norm101: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add99, model_layers_18_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv254 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight3, model_layers_18_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims202: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv254, axes=None)
            matmul202: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm101, permute_dims202, out_dtype="void")
            reshape200: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul202, R.shape([1, 1, 48, 128]))
            reshape201: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape200, R.shape([1, 48, 128]))
            lv255 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape201), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape202: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv255, R.shape([1, 1, 32, 128]))
            reshape203: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape202, R.shape([1, 1, 4096]))
            lv256 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight3, model_layers_18_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims203: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv256, axes=None)
            matmul203: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape203, permute_dims203, out_dtype="void")
            add100: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul203, add99)
            rms_norm102: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add100, model_layers_18_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv257 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight3, model_layers_18_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims204: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv257, axes=None)
            matmul204: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm102, permute_dims204, out_dtype="void")
            split50: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul204, indices_or_sections=2, axis=-1)
            split_050: R.Tensor((1, 1, 14336), dtype="float16") = split50[0]
            split_150: R.Tensor((1, 1, 14336), dtype="float16") = split50[1]
            silu50: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_050)
            mul50: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu50, split_150)
            lv258 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight3, model_layers_18_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims205: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv258, axes=None)
            matmul205: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul50, permute_dims205, out_dtype="void")
            add101: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul205, add100)
            rms_norm103: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add101, model_layers_19_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv259 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight3, model_layers_19_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims206: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv259, axes=None)
            matmul206: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm103, permute_dims206, out_dtype="void")
            reshape204: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul206, R.shape([1, 1, 48, 128]))
            reshape205: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape204, R.shape([1, 48, 128]))
            lv260 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape205), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape206: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv260, R.shape([1, 1, 32, 128]))
            reshape207: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape206, R.shape([1, 1, 4096]))
            lv261 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight3, model_layers_19_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims207: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv261, axes=None)
            matmul207: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape207, permute_dims207, out_dtype="void")
            add102: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul207, add101)
            rms_norm104: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add102, model_layers_19_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv262 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight3, model_layers_19_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims208: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv262, axes=None)
            matmul208: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm104, permute_dims208, out_dtype="void")
            split51: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul208, indices_or_sections=2, axis=-1)
            split_051: R.Tensor((1, 1, 14336), dtype="float16") = split51[0]
            split_151: R.Tensor((1, 1, 14336), dtype="float16") = split51[1]
            silu51: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_051)
            mul51: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu51, split_151)
            lv263 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight3, model_layers_19_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims209: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv263, axes=None)
            matmul209: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul51, permute_dims209, out_dtype="void")
            add103: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul209, add102)
            rms_norm105: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add103, model_layers_20_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv264 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight3, model_layers_20_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims210: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv264, axes=None)
            matmul210: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm105, permute_dims210, out_dtype="void")
            reshape208: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul210, R.shape([1, 1, 48, 128]))
            reshape209: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape208, R.shape([1, 48, 128]))
            lv265 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape209), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape210: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv265, R.shape([1, 1, 32, 128]))
            reshape211: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape210, R.shape([1, 1, 4096]))
            lv266 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight3, model_layers_20_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims211: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv266, axes=None)
            matmul211: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape211, permute_dims211, out_dtype="void")
            add104: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul211, add103)
            rms_norm106: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add104, model_layers_20_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv267 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight3, model_layers_20_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims212: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv267, axes=None)
            matmul212: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm106, permute_dims212, out_dtype="void")
            split52: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul212, indices_or_sections=2, axis=-1)
            split_052: R.Tensor((1, 1, 14336), dtype="float16") = split52[0]
            split_152: R.Tensor((1, 1, 14336), dtype="float16") = split52[1]
            silu52: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_052)
            mul52: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu52, split_152)
            lv268 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight3, model_layers_20_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims213: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv268, axes=None)
            matmul213: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul52, permute_dims213, out_dtype="void")
            add105: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul213, add104)
            rms_norm107: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add105, model_layers_21_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv269 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight3, model_layers_21_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims214: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv269, axes=None)
            matmul214: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm107, permute_dims214, out_dtype="void")
            reshape212: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul214, R.shape([1, 1, 48, 128]))
            reshape213: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape212, R.shape([1, 48, 128]))
            lv270 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape213), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape214: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv270, R.shape([1, 1, 32, 128]))
            reshape215: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape214, R.shape([1, 1, 4096]))
            lv271 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight3, model_layers_21_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims215: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv271, axes=None)
            matmul215: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape215, permute_dims215, out_dtype="void")
            add106: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul215, add105)
            rms_norm108: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add106, model_layers_21_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv272 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight3, model_layers_21_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims216: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv272, axes=None)
            matmul216: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm108, permute_dims216, out_dtype="void")
            split53: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul216, indices_or_sections=2, axis=-1)
            split_053: R.Tensor((1, 1, 14336), dtype="float16") = split53[0]
            split_153: R.Tensor((1, 1, 14336), dtype="float16") = split53[1]
            silu53: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_053)
            mul53: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu53, split_153)
            lv273 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight3, model_layers_21_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims217: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv273, axes=None)
            matmul217: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul53, permute_dims217, out_dtype="void")
            add107: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul217, add106)
            rms_norm109: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add107, model_layers_22_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv274 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight3, model_layers_22_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims218: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv274, axes=None)
            matmul218: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm109, permute_dims218, out_dtype="void")
            reshape216: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul218, R.shape([1, 1, 48, 128]))
            reshape217: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape216, R.shape([1, 48, 128]))
            lv275 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape217), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape218: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv275, R.shape([1, 1, 32, 128]))
            reshape219: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape218, R.shape([1, 1, 4096]))
            lv276 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight3, model_layers_22_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims219: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv276, axes=None)
            matmul219: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape219, permute_dims219, out_dtype="void")
            add108: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul219, add107)
            rms_norm110: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add108, model_layers_22_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv277 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight3, model_layers_22_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims220: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv277, axes=None)
            matmul220: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm110, permute_dims220, out_dtype="void")
            split54: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul220, indices_or_sections=2, axis=-1)
            split_054: R.Tensor((1, 1, 14336), dtype="float16") = split54[0]
            split_154: R.Tensor((1, 1, 14336), dtype="float16") = split54[1]
            silu54: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_054)
            mul54: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu54, split_154)
            lv278 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight3, model_layers_22_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims221: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv278, axes=None)
            matmul221: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul54, permute_dims221, out_dtype="void")
            add109: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul221, add108)
            rms_norm111: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add109, model_layers_23_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv279 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight3, model_layers_23_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims222: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv279, axes=None)
            matmul222: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm111, permute_dims222, out_dtype="void")
            reshape220: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul222, R.shape([1, 1, 48, 128]))
            reshape221: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape220, R.shape([1, 48, 128]))
            lv280 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape221), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape222: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv280, R.shape([1, 1, 32, 128]))
            reshape223: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape222, R.shape([1, 1, 4096]))
            lv281 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight3, model_layers_23_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims223: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv281, axes=None)
            matmul223: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape223, permute_dims223, out_dtype="void")
            add110: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul223, add109)
            rms_norm112: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add110, model_layers_23_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv282 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight3, model_layers_23_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims224: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv282, axes=None)
            matmul224: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm112, permute_dims224, out_dtype="void")
            split55: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul224, indices_or_sections=2, axis=-1)
            split_055: R.Tensor((1, 1, 14336), dtype="float16") = split55[0]
            split_155: R.Tensor((1, 1, 14336), dtype="float16") = split55[1]
            silu55: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_055)
            mul55: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu55, split_155)
            lv283 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight3, model_layers_23_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims225: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv283, axes=None)
            matmul225: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul55, permute_dims225, out_dtype="void")
            add111: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul225, add110)
            rms_norm113: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add111, model_layers_24_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv284 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight3, model_layers_24_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims226: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv284, axes=None)
            matmul226: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm113, permute_dims226, out_dtype="void")
            reshape224: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul226, R.shape([1, 1, 48, 128]))
            reshape225: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape224, R.shape([1, 48, 128]))
            lv285 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape225), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape226: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv285, R.shape([1, 1, 32, 128]))
            reshape227: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape226, R.shape([1, 1, 4096]))
            lv286 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight3, model_layers_24_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims227: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv286, axes=None)
            matmul227: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape227, permute_dims227, out_dtype="void")
            add112: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul227, add111)
            rms_norm114: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add112, model_layers_24_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv287 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight3, model_layers_24_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims228: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv287, axes=None)
            matmul228: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm114, permute_dims228, out_dtype="void")
            split56: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul228, indices_or_sections=2, axis=-1)
            split_056: R.Tensor((1, 1, 14336), dtype="float16") = split56[0]
            split_156: R.Tensor((1, 1, 14336), dtype="float16") = split56[1]
            silu56: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_056)
            mul56: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu56, split_156)
            lv288 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight3, model_layers_24_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims229: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv288, axes=None)
            matmul229: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul56, permute_dims229, out_dtype="void")
            add113: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul229, add112)
            rms_norm115: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add113, model_layers_25_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv289 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight3, model_layers_25_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims230: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv289, axes=None)
            matmul230: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm115, permute_dims230, out_dtype="void")
            reshape228: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul230, R.shape([1, 1, 48, 128]))
            reshape229: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape228, R.shape([1, 48, 128]))
            lv290 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape229), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape230: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv290, R.shape([1, 1, 32, 128]))
            reshape231: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape230, R.shape([1, 1, 4096]))
            lv291 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight3, model_layers_25_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims231: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv291, axes=None)
            matmul231: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape231, permute_dims231, out_dtype="void")
            add114: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul231, add113)
            rms_norm116: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add114, model_layers_25_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv292 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight3, model_layers_25_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims232: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv292, axes=None)
            matmul232: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm116, permute_dims232, out_dtype="void")
            split57: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul232, indices_or_sections=2, axis=-1)
            split_057: R.Tensor((1, 1, 14336), dtype="float16") = split57[0]
            split_157: R.Tensor((1, 1, 14336), dtype="float16") = split57[1]
            silu57: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_057)
            mul57: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu57, split_157)
            lv293 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight3, model_layers_25_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims233: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv293, axes=None)
            matmul233: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul57, permute_dims233, out_dtype="void")
            add115: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul233, add114)
            rms_norm117: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add115, model_layers_26_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv294 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight3, model_layers_26_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims234: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv294, axes=None)
            matmul234: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm117, permute_dims234, out_dtype="void")
            reshape232: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul234, R.shape([1, 1, 48, 128]))
            reshape233: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape232, R.shape([1, 48, 128]))
            lv295 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape233), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape234: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv295, R.shape([1, 1, 32, 128]))
            reshape235: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape234, R.shape([1, 1, 4096]))
            lv296 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight3, model_layers_26_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims235: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv296, axes=None)
            matmul235: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape235, permute_dims235, out_dtype="void")
            add116: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul235, add115)
            rms_norm118: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add116, model_layers_26_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv297 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight3, model_layers_26_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims236: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv297, axes=None)
            matmul236: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm118, permute_dims236, out_dtype="void")
            split58: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul236, indices_or_sections=2, axis=-1)
            split_058: R.Tensor((1, 1, 14336), dtype="float16") = split58[0]
            split_158: R.Tensor((1, 1, 14336), dtype="float16") = split58[1]
            silu58: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_058)
            mul58: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu58, split_158)
            lv298 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight3, model_layers_26_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims237: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv298, axes=None)
            matmul237: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul58, permute_dims237, out_dtype="void")
            add117: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul237, add116)
            rms_norm119: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add117, model_layers_27_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv299 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight3, model_layers_27_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims238: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv299, axes=None)
            matmul238: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm119, permute_dims238, out_dtype="void")
            reshape236: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul238, R.shape([1, 1, 48, 128]))
            reshape237: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape236, R.shape([1, 48, 128]))
            lv300 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape237), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape238: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv300, R.shape([1, 1, 32, 128]))
            reshape239: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape238, R.shape([1, 1, 4096]))
            lv301 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight3, model_layers_27_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims239: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv301, axes=None)
            matmul239: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape239, permute_dims239, out_dtype="void")
            add118: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul239, add117)
            rms_norm120: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add118, model_layers_27_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv302 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight3, model_layers_27_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims240: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv302, axes=None)
            matmul240: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm120, permute_dims240, out_dtype="void")
            split59: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul240, indices_or_sections=2, axis=-1)
            split_059: R.Tensor((1, 1, 14336), dtype="float16") = split59[0]
            split_159: R.Tensor((1, 1, 14336), dtype="float16") = split59[1]
            silu59: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_059)
            mul59: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu59, split_159)
            lv303 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight3, model_layers_27_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims241: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv303, axes=None)
            matmul241: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul59, permute_dims241, out_dtype="void")
            add119: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul241, add118)
            rms_norm121: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add119, model_layers_28_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv304 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight3, model_layers_28_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims242: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv304, axes=None)
            matmul242: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm121, permute_dims242, out_dtype="void")
            reshape240: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul242, R.shape([1, 1, 48, 128]))
            reshape241: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape240, R.shape([1, 48, 128]))
            lv305 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape241), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape242: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv305, R.shape([1, 1, 32, 128]))
            reshape243: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape242, R.shape([1, 1, 4096]))
            lv306 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight3, model_layers_28_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims243: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv306, axes=None)
            matmul243: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape243, permute_dims243, out_dtype="void")
            add120: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul243, add119)
            rms_norm122: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add120, model_layers_28_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv307 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight3, model_layers_28_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims244: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv307, axes=None)
            matmul244: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm122, permute_dims244, out_dtype="void")
            split60: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul244, indices_or_sections=2, axis=-1)
            split_060: R.Tensor((1, 1, 14336), dtype="float16") = split60[0]
            split_160: R.Tensor((1, 1, 14336), dtype="float16") = split60[1]
            silu60: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_060)
            mul60: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu60, split_160)
            lv308 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight3, model_layers_28_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims245: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv308, axes=None)
            matmul245: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul60, permute_dims245, out_dtype="void")
            add121: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul245, add120)
            rms_norm123: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add121, model_layers_29_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv309 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight3, model_layers_29_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims246: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv309, axes=None)
            matmul246: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm123, permute_dims246, out_dtype="void")
            reshape244: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul246, R.shape([1, 1, 48, 128]))
            reshape245: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape244, R.shape([1, 48, 128]))
            lv310 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape245), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape246: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv310, R.shape([1, 1, 32, 128]))
            reshape247: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape246, R.shape([1, 1, 4096]))
            lv311 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight3, model_layers_29_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims247: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv311, axes=None)
            matmul247: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape247, permute_dims247, out_dtype="void")
            add122: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul247, add121)
            rms_norm124: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add122, model_layers_29_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv312 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight3, model_layers_29_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims248: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv312, axes=None)
            matmul248: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm124, permute_dims248, out_dtype="void")
            split61: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul248, indices_or_sections=2, axis=-1)
            split_061: R.Tensor((1, 1, 14336), dtype="float16") = split61[0]
            split_161: R.Tensor((1, 1, 14336), dtype="float16") = split61[1]
            silu61: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_061)
            mul61: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu61, split_161)
            lv313 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight3, model_layers_29_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims249: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv313, axes=None)
            matmul249: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul61, permute_dims249, out_dtype="void")
            add123: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul249, add122)
            rms_norm125: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add123, model_layers_30_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv314 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight3, model_layers_30_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims250: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv314, axes=None)
            matmul250: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm125, permute_dims250, out_dtype="void")
            reshape248: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul250, R.shape([1, 1, 48, 128]))
            reshape249: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape248, R.shape([1, 48, 128]))
            lv315 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape249), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape250: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv315, R.shape([1, 1, 32, 128]))
            reshape251: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape250, R.shape([1, 1, 4096]))
            lv316 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight3, model_layers_30_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims251: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv316, axes=None)
            matmul251: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape251, permute_dims251, out_dtype="void")
            add124: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul251, add123)
            rms_norm126: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add124, model_layers_30_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv317 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight3, model_layers_30_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims252: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv317, axes=None)
            matmul252: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm126, permute_dims252, out_dtype="void")
            split62: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul252, indices_or_sections=2, axis=-1)
            split_062: R.Tensor((1, 1, 14336), dtype="float16") = split62[0]
            split_162: R.Tensor((1, 1, 14336), dtype="float16") = split62[1]
            silu62: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_062)
            mul62: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu62, split_162)
            lv318 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight3, model_layers_30_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims253: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv318, axes=None)
            matmul253: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul62, permute_dims253, out_dtype="void")
            add125: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul253, add124)
            rms_norm127: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add125, model_layers_31_input_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv319 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight3, model_layers_31_self_attn_qkv_proj_q_scale3), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims254: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv319, axes=None)
            matmul254: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm127, permute_dims254, out_dtype="void")
            reshape252: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul254, R.shape([1, 1, 48, 128]))
            reshape253: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape252, R.shape([1, 48, 128]))
            lv320 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape253), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape254: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv320, R.shape([1, 1, 32, 128]))
            reshape255: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape254, R.shape([1, 1, 4096]))
            lv321 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight3, model_layers_31_self_attn_o_proj_q_scale3), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims255: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv321, axes=None)
            matmul255: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape255, permute_dims255, out_dtype="void")
            add126: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul255, add125)
            rms_norm128: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add126, model_layers_31_post_attention_layernorm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv322 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight3, model_layers_31_mlp_gate_up_proj_q_scale3), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims256: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv322, axes=None)
            matmul256: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm128, permute_dims256, out_dtype="void")
            split63: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul256, indices_or_sections=2, axis=-1)
            split_063: R.Tensor((1, 1, 14336), dtype="float16") = split63[0]
            split_163: R.Tensor((1, 1, 14336), dtype="float16") = split63[1]
            silu63: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_063)
            mul63: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu63, split_163)
            lv323 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight3, model_layers_31_mlp_down_proj_q_scale3), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims257: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv323, axes=None)
            matmul257: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul63, permute_dims257, out_dtype="void")
            add127: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul257, add126)
            rms_norm129: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add127, model_norm_weight3, axes=[-1], epsilon=1.0000000000000001e-05)
            lv324 = R.call_tir(cls.dequantize, (lm_head_q_weight3, lm_head_q_scale3), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            permute_dims258: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv324, axes=None)
            matmul258: R.Tensor((1, 1, vocab_size), dtype="float16") = R.matmul(rms_norm129, permute_dims258, out_dtype="void")
            astype2: R.Tensor((1, 1, vocab_size), dtype="float32") = R.astype(matmul258, dtype="float32")
            gv4: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = astype2, paged_kv_cache
            R.output(gv4)
        return gv4

    @R.function
    def decode_to_last_hidden_states(input_embed: R.Tensor((1, 1, 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, 4096), dtype="float16"), R.Object):
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight5: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale5: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight5: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale5: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight5: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale5: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight5: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale5: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight5: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale5: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight5: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight5: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale5: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm195: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv485 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight5, model_layers_0_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims387: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv485, axes=None)
            matmul387: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm195, permute_dims387, out_dtype="void")
            reshape384: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul387, R.shape([1, 1, 48, 128]))
            reshape385: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape384, R.shape([1, 48, 128]))
            lv486 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape385), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape386: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv486, R.shape([1, 1, 32, 128]))
            reshape387: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape386, R.shape([1, 1, 4096]))
            lv487 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight5, model_layers_0_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims388: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv487, axes=None)
            matmul388: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape387, permute_dims388, out_dtype="void")
            add192: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul388, input_embed)
            rms_norm196: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add192, model_layers_0_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv488 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight5, model_layers_0_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims389: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv488, axes=None)
            matmul389: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm196, permute_dims389, out_dtype="void")
            split96: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul389, indices_or_sections=2, axis=-1)
            split_096: R.Tensor((1, 1, 14336), dtype="float16") = split96[0]
            split_196: R.Tensor((1, 1, 14336), dtype="float16") = split96[1]
            silu96: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_096)
            mul96: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu96, split_196)
            lv489 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight5, model_layers_0_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims390: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv489, axes=None)
            matmul390: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul96, permute_dims390, out_dtype="void")
            add193: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul390, add192)
            rms_norm197: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add193, model_layers_1_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv490 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight5, model_layers_1_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims391: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv490, axes=None)
            matmul391: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm197, permute_dims391, out_dtype="void")
            reshape388: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul391, R.shape([1, 1, 48, 128]))
            reshape389: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape388, R.shape([1, 48, 128]))
            lv491 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape389), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape390: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv491, R.shape([1, 1, 32, 128]))
            reshape391: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape390, R.shape([1, 1, 4096]))
            lv492 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight5, model_layers_1_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims392: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv492, axes=None)
            matmul392: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape391, permute_dims392, out_dtype="void")
            add194: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul392, add193)
            rms_norm198: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add194, model_layers_1_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv493 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight5, model_layers_1_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims393: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv493, axes=None)
            matmul393: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm198, permute_dims393, out_dtype="void")
            split97: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul393, indices_or_sections=2, axis=-1)
            split_097: R.Tensor((1, 1, 14336), dtype="float16") = split97[0]
            split_197: R.Tensor((1, 1, 14336), dtype="float16") = split97[1]
            silu97: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_097)
            mul97: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu97, split_197)
            lv494 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight5, model_layers_1_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims394: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv494, axes=None)
            matmul394: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul97, permute_dims394, out_dtype="void")
            add195: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul394, add194)
            rms_norm199: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add195, model_layers_2_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv495 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight5, model_layers_2_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims395: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv495, axes=None)
            matmul395: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm199, permute_dims395, out_dtype="void")
            reshape392: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul395, R.shape([1, 1, 48, 128]))
            reshape393: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape392, R.shape([1, 48, 128]))
            lv496 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape393), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape394: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv496, R.shape([1, 1, 32, 128]))
            reshape395: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape394, R.shape([1, 1, 4096]))
            lv497 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight5, model_layers_2_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims396: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv497, axes=None)
            matmul396: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape395, permute_dims396, out_dtype="void")
            add196: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul396, add195)
            rms_norm200: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add196, model_layers_2_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv498 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight5, model_layers_2_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims397: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv498, axes=None)
            matmul397: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm200, permute_dims397, out_dtype="void")
            split98: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul397, indices_or_sections=2, axis=-1)
            split_098: R.Tensor((1, 1, 14336), dtype="float16") = split98[0]
            split_198: R.Tensor((1, 1, 14336), dtype="float16") = split98[1]
            silu98: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_098)
            mul98: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu98, split_198)
            lv499 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight5, model_layers_2_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims398: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv499, axes=None)
            matmul398: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul98, permute_dims398, out_dtype="void")
            add197: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul398, add196)
            rms_norm201: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add197, model_layers_3_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv500 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight5, model_layers_3_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims399: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv500, axes=None)
            matmul399: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm201, permute_dims399, out_dtype="void")
            reshape396: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul399, R.shape([1, 1, 48, 128]))
            reshape397: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape396, R.shape([1, 48, 128]))
            lv501 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape397), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape398: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv501, R.shape([1, 1, 32, 128]))
            reshape399: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape398, R.shape([1, 1, 4096]))
            lv502 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight5, model_layers_3_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims400: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv502, axes=None)
            matmul400: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape399, permute_dims400, out_dtype="void")
            add198: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul400, add197)
            rms_norm202: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add198, model_layers_3_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv503 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight5, model_layers_3_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims401: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv503, axes=None)
            matmul401: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm202, permute_dims401, out_dtype="void")
            split99: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul401, indices_or_sections=2, axis=-1)
            split_099: R.Tensor((1, 1, 14336), dtype="float16") = split99[0]
            split_199: R.Tensor((1, 1, 14336), dtype="float16") = split99[1]
            silu99: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_099)
            mul99: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu99, split_199)
            lv504 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight5, model_layers_3_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims402: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv504, axes=None)
            matmul402: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul99, permute_dims402, out_dtype="void")
            add199: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul402, add198)
            rms_norm203: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add199, model_layers_4_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv505 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight5, model_layers_4_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims403: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv505, axes=None)
            matmul403: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm203, permute_dims403, out_dtype="void")
            reshape400: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul403, R.shape([1, 1, 48, 128]))
            reshape401: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape400, R.shape([1, 48, 128]))
            lv506 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape401), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape402: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv506, R.shape([1, 1, 32, 128]))
            reshape403: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape402, R.shape([1, 1, 4096]))
            lv507 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight5, model_layers_4_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims404: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv507, axes=None)
            matmul404: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape403, permute_dims404, out_dtype="void")
            add200: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul404, add199)
            rms_norm204: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add200, model_layers_4_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv508 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight5, model_layers_4_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims405: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv508, axes=None)
            matmul405: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm204, permute_dims405, out_dtype="void")
            split100: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul405, indices_or_sections=2, axis=-1)
            split_0100: R.Tensor((1, 1, 14336), dtype="float16") = split100[0]
            split_1100: R.Tensor((1, 1, 14336), dtype="float16") = split100[1]
            silu100: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0100)
            mul100: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu100, split_1100)
            lv509 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight5, model_layers_4_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims406: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv509, axes=None)
            matmul406: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul100, permute_dims406, out_dtype="void")
            add201: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul406, add200)
            rms_norm205: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add201, model_layers_5_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv510 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight5, model_layers_5_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims407: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv510, axes=None)
            matmul407: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm205, permute_dims407, out_dtype="void")
            reshape404: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul407, R.shape([1, 1, 48, 128]))
            reshape405: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape404, R.shape([1, 48, 128]))
            lv511 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape405), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape406: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv511, R.shape([1, 1, 32, 128]))
            reshape407: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape406, R.shape([1, 1, 4096]))
            lv512 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight5, model_layers_5_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims408: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv512, axes=None)
            matmul408: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape407, permute_dims408, out_dtype="void")
            add202: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul408, add201)
            rms_norm206: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add202, model_layers_5_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv513 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight5, model_layers_5_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims409: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv513, axes=None)
            matmul409: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm206, permute_dims409, out_dtype="void")
            split101: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul409, indices_or_sections=2, axis=-1)
            split_0101: R.Tensor((1, 1, 14336), dtype="float16") = split101[0]
            split_1101: R.Tensor((1, 1, 14336), dtype="float16") = split101[1]
            silu101: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0101)
            mul101: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu101, split_1101)
            lv514 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight5, model_layers_5_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims410: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv514, axes=None)
            matmul410: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul101, permute_dims410, out_dtype="void")
            add203: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul410, add202)
            rms_norm207: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add203, model_layers_6_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv515 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight5, model_layers_6_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims411: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv515, axes=None)
            matmul411: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm207, permute_dims411, out_dtype="void")
            reshape408: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul411, R.shape([1, 1, 48, 128]))
            reshape409: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape408, R.shape([1, 48, 128]))
            lv516 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape409), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape410: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv516, R.shape([1, 1, 32, 128]))
            reshape411: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape410, R.shape([1, 1, 4096]))
            lv517 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight5, model_layers_6_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims412: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv517, axes=None)
            matmul412: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape411, permute_dims412, out_dtype="void")
            add204: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul412, add203)
            rms_norm208: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add204, model_layers_6_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv518 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight5, model_layers_6_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims413: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv518, axes=None)
            matmul413: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm208, permute_dims413, out_dtype="void")
            split102: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul413, indices_or_sections=2, axis=-1)
            split_0102: R.Tensor((1, 1, 14336), dtype="float16") = split102[0]
            split_1102: R.Tensor((1, 1, 14336), dtype="float16") = split102[1]
            silu102: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0102)
            mul102: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu102, split_1102)
            lv519 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight5, model_layers_6_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims414: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv519, axes=None)
            matmul414: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul102, permute_dims414, out_dtype="void")
            add205: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul414, add204)
            rms_norm209: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add205, model_layers_7_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv520 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight5, model_layers_7_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims415: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv520, axes=None)
            matmul415: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm209, permute_dims415, out_dtype="void")
            reshape412: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul415, R.shape([1, 1, 48, 128]))
            reshape413: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape412, R.shape([1, 48, 128]))
            lv521 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape413), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape414: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv521, R.shape([1, 1, 32, 128]))
            reshape415: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape414, R.shape([1, 1, 4096]))
            lv522 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight5, model_layers_7_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims416: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv522, axes=None)
            matmul416: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape415, permute_dims416, out_dtype="void")
            add206: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul416, add205)
            rms_norm210: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add206, model_layers_7_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv523 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight5, model_layers_7_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims417: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv523, axes=None)
            matmul417: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm210, permute_dims417, out_dtype="void")
            split103: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul417, indices_or_sections=2, axis=-1)
            split_0103: R.Tensor((1, 1, 14336), dtype="float16") = split103[0]
            split_1103: R.Tensor((1, 1, 14336), dtype="float16") = split103[1]
            silu103: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0103)
            mul103: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu103, split_1103)
            lv524 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight5, model_layers_7_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims418: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv524, axes=None)
            matmul418: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul103, permute_dims418, out_dtype="void")
            add207: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul418, add206)
            rms_norm211: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add207, model_layers_8_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv525 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight5, model_layers_8_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims419: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv525, axes=None)
            matmul419: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm211, permute_dims419, out_dtype="void")
            reshape416: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul419, R.shape([1, 1, 48, 128]))
            reshape417: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape416, R.shape([1, 48, 128]))
            lv526 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape417), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape418: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv526, R.shape([1, 1, 32, 128]))
            reshape419: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape418, R.shape([1, 1, 4096]))
            lv527 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight5, model_layers_8_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims420: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv527, axes=None)
            matmul420: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape419, permute_dims420, out_dtype="void")
            add208: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul420, add207)
            rms_norm212: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add208, model_layers_8_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv528 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight5, model_layers_8_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims421: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv528, axes=None)
            matmul421: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm212, permute_dims421, out_dtype="void")
            split104: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul421, indices_or_sections=2, axis=-1)
            split_0104: R.Tensor((1, 1, 14336), dtype="float16") = split104[0]
            split_1104: R.Tensor((1, 1, 14336), dtype="float16") = split104[1]
            silu104: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0104)
            mul104: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu104, split_1104)
            lv529 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight5, model_layers_8_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims422: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv529, axes=None)
            matmul422: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul104, permute_dims422, out_dtype="void")
            add209: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul422, add208)
            rms_norm213: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add209, model_layers_9_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv530 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight5, model_layers_9_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims423: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv530, axes=None)
            matmul423: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm213, permute_dims423, out_dtype="void")
            reshape420: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul423, R.shape([1, 1, 48, 128]))
            reshape421: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape420, R.shape([1, 48, 128]))
            lv531 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape421), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape422: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv531, R.shape([1, 1, 32, 128]))
            reshape423: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape422, R.shape([1, 1, 4096]))
            lv532 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight5, model_layers_9_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims424: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv532, axes=None)
            matmul424: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape423, permute_dims424, out_dtype="void")
            add210: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul424, add209)
            rms_norm214: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add210, model_layers_9_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv533 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight5, model_layers_9_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims425: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv533, axes=None)
            matmul425: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm214, permute_dims425, out_dtype="void")
            split105: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul425, indices_or_sections=2, axis=-1)
            split_0105: R.Tensor((1, 1, 14336), dtype="float16") = split105[0]
            split_1105: R.Tensor((1, 1, 14336), dtype="float16") = split105[1]
            silu105: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0105)
            mul105: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu105, split_1105)
            lv534 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight5, model_layers_9_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims426: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv534, axes=None)
            matmul426: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul105, permute_dims426, out_dtype="void")
            add211: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul426, add210)
            rms_norm215: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add211, model_layers_10_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv535 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight5, model_layers_10_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims427: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv535, axes=None)
            matmul427: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm215, permute_dims427, out_dtype="void")
            reshape424: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul427, R.shape([1, 1, 48, 128]))
            reshape425: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape424, R.shape([1, 48, 128]))
            lv536 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape425), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape426: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv536, R.shape([1, 1, 32, 128]))
            reshape427: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape426, R.shape([1, 1, 4096]))
            lv537 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight5, model_layers_10_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims428: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv537, axes=None)
            matmul428: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape427, permute_dims428, out_dtype="void")
            add212: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul428, add211)
            rms_norm216: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add212, model_layers_10_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv538 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight5, model_layers_10_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims429: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv538, axes=None)
            matmul429: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm216, permute_dims429, out_dtype="void")
            split106: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul429, indices_or_sections=2, axis=-1)
            split_0106: R.Tensor((1, 1, 14336), dtype="float16") = split106[0]
            split_1106: R.Tensor((1, 1, 14336), dtype="float16") = split106[1]
            silu106: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0106)
            mul106: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu106, split_1106)
            lv539 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight5, model_layers_10_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims430: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv539, axes=None)
            matmul430: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul106, permute_dims430, out_dtype="void")
            add213: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul430, add212)
            rms_norm217: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add213, model_layers_11_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv540 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight5, model_layers_11_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims431: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv540, axes=None)
            matmul431: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm217, permute_dims431, out_dtype="void")
            reshape428: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul431, R.shape([1, 1, 48, 128]))
            reshape429: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape428, R.shape([1, 48, 128]))
            lv541 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape429), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape430: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv541, R.shape([1, 1, 32, 128]))
            reshape431: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape430, R.shape([1, 1, 4096]))
            lv542 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight5, model_layers_11_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims432: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv542, axes=None)
            matmul432: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape431, permute_dims432, out_dtype="void")
            add214: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul432, add213)
            rms_norm218: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add214, model_layers_11_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv543 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight5, model_layers_11_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims433: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv543, axes=None)
            matmul433: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm218, permute_dims433, out_dtype="void")
            split107: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul433, indices_or_sections=2, axis=-1)
            split_0107: R.Tensor((1, 1, 14336), dtype="float16") = split107[0]
            split_1107: R.Tensor((1, 1, 14336), dtype="float16") = split107[1]
            silu107: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0107)
            mul107: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu107, split_1107)
            lv544 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight5, model_layers_11_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims434: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv544, axes=None)
            matmul434: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul107, permute_dims434, out_dtype="void")
            add215: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul434, add214)
            rms_norm219: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add215, model_layers_12_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv545 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight5, model_layers_12_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims435: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv545, axes=None)
            matmul435: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm219, permute_dims435, out_dtype="void")
            reshape432: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul435, R.shape([1, 1, 48, 128]))
            reshape433: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape432, R.shape([1, 48, 128]))
            lv546 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape433), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape434: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv546, R.shape([1, 1, 32, 128]))
            reshape435: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape434, R.shape([1, 1, 4096]))
            lv547 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight5, model_layers_12_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims436: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv547, axes=None)
            matmul436: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape435, permute_dims436, out_dtype="void")
            add216: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul436, add215)
            rms_norm220: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add216, model_layers_12_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv548 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight5, model_layers_12_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims437: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv548, axes=None)
            matmul437: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm220, permute_dims437, out_dtype="void")
            split108: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul437, indices_or_sections=2, axis=-1)
            split_0108: R.Tensor((1, 1, 14336), dtype="float16") = split108[0]
            split_1108: R.Tensor((1, 1, 14336), dtype="float16") = split108[1]
            silu108: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0108)
            mul108: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu108, split_1108)
            lv549 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight5, model_layers_12_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims438: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv549, axes=None)
            matmul438: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul108, permute_dims438, out_dtype="void")
            add217: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul438, add216)
            rms_norm221: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add217, model_layers_13_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv550 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight5, model_layers_13_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims439: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv550, axes=None)
            matmul439: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm221, permute_dims439, out_dtype="void")
            reshape436: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul439, R.shape([1, 1, 48, 128]))
            reshape437: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape436, R.shape([1, 48, 128]))
            lv551 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape437), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape438: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv551, R.shape([1, 1, 32, 128]))
            reshape439: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape438, R.shape([1, 1, 4096]))
            lv552 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight5, model_layers_13_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims440: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv552, axes=None)
            matmul440: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape439, permute_dims440, out_dtype="void")
            add218: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul440, add217)
            rms_norm222: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add218, model_layers_13_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv553 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight5, model_layers_13_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims441: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv553, axes=None)
            matmul441: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm222, permute_dims441, out_dtype="void")
            split109: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul441, indices_or_sections=2, axis=-1)
            split_0109: R.Tensor((1, 1, 14336), dtype="float16") = split109[0]
            split_1109: R.Tensor((1, 1, 14336), dtype="float16") = split109[1]
            silu109: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0109)
            mul109: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu109, split_1109)
            lv554 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight5, model_layers_13_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims442: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv554, axes=None)
            matmul442: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul109, permute_dims442, out_dtype="void")
            add219: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul442, add218)
            rms_norm223: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add219, model_layers_14_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv555 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight5, model_layers_14_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims443: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv555, axes=None)
            matmul443: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm223, permute_dims443, out_dtype="void")
            reshape440: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul443, R.shape([1, 1, 48, 128]))
            reshape441: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape440, R.shape([1, 48, 128]))
            lv556 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape441), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape442: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv556, R.shape([1, 1, 32, 128]))
            reshape443: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape442, R.shape([1, 1, 4096]))
            lv557 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight5, model_layers_14_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims444: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv557, axes=None)
            matmul444: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape443, permute_dims444, out_dtype="void")
            add220: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul444, add219)
            rms_norm224: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add220, model_layers_14_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv558 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight5, model_layers_14_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims445: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv558, axes=None)
            matmul445: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm224, permute_dims445, out_dtype="void")
            split110: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul445, indices_or_sections=2, axis=-1)
            split_0110: R.Tensor((1, 1, 14336), dtype="float16") = split110[0]
            split_1110: R.Tensor((1, 1, 14336), dtype="float16") = split110[1]
            silu110: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0110)
            mul110: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu110, split_1110)
            lv559 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight5, model_layers_14_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims446: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv559, axes=None)
            matmul446: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul110, permute_dims446, out_dtype="void")
            add221: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul446, add220)
            rms_norm225: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add221, model_layers_15_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv560 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight5, model_layers_15_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims447: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv560, axes=None)
            matmul447: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm225, permute_dims447, out_dtype="void")
            reshape444: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul447, R.shape([1, 1, 48, 128]))
            reshape445: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape444, R.shape([1, 48, 128]))
            lv561 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape445), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape446: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv561, R.shape([1, 1, 32, 128]))
            reshape447: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape446, R.shape([1, 1, 4096]))
            lv562 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight5, model_layers_15_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims448: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv562, axes=None)
            matmul448: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape447, permute_dims448, out_dtype="void")
            add222: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul448, add221)
            rms_norm226: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add222, model_layers_15_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv563 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight5, model_layers_15_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims449: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv563, axes=None)
            matmul449: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm226, permute_dims449, out_dtype="void")
            split111: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul449, indices_or_sections=2, axis=-1)
            split_0111: R.Tensor((1, 1, 14336), dtype="float16") = split111[0]
            split_1111: R.Tensor((1, 1, 14336), dtype="float16") = split111[1]
            silu111: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0111)
            mul111: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu111, split_1111)
            lv564 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight5, model_layers_15_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims450: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv564, axes=None)
            matmul450: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul111, permute_dims450, out_dtype="void")
            add223: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul450, add222)
            rms_norm227: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add223, model_layers_16_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv565 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight5, model_layers_16_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims451: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv565, axes=None)
            matmul451: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm227, permute_dims451, out_dtype="void")
            reshape448: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul451, R.shape([1, 1, 48, 128]))
            reshape449: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape448, R.shape([1, 48, 128]))
            lv566 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape449), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape450: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv566, R.shape([1, 1, 32, 128]))
            reshape451: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape450, R.shape([1, 1, 4096]))
            lv567 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight5, model_layers_16_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims452: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv567, axes=None)
            matmul452: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape451, permute_dims452, out_dtype="void")
            add224: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul452, add223)
            rms_norm228: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add224, model_layers_16_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv568 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight5, model_layers_16_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims453: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv568, axes=None)
            matmul453: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm228, permute_dims453, out_dtype="void")
            split112: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul453, indices_or_sections=2, axis=-1)
            split_0112: R.Tensor((1, 1, 14336), dtype="float16") = split112[0]
            split_1112: R.Tensor((1, 1, 14336), dtype="float16") = split112[1]
            silu112: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0112)
            mul112: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu112, split_1112)
            lv569 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight5, model_layers_16_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims454: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv569, axes=None)
            matmul454: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul112, permute_dims454, out_dtype="void")
            add225: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul454, add224)
            rms_norm229: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add225, model_layers_17_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv570 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight5, model_layers_17_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims455: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv570, axes=None)
            matmul455: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm229, permute_dims455, out_dtype="void")
            reshape452: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul455, R.shape([1, 1, 48, 128]))
            reshape453: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape452, R.shape([1, 48, 128]))
            lv571 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape453), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape454: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv571, R.shape([1, 1, 32, 128]))
            reshape455: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape454, R.shape([1, 1, 4096]))
            lv572 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight5, model_layers_17_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims456: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv572, axes=None)
            matmul456: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape455, permute_dims456, out_dtype="void")
            add226: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul456, add225)
            rms_norm230: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add226, model_layers_17_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv573 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight5, model_layers_17_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims457: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv573, axes=None)
            matmul457: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm230, permute_dims457, out_dtype="void")
            split113: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul457, indices_or_sections=2, axis=-1)
            split_0113: R.Tensor((1, 1, 14336), dtype="float16") = split113[0]
            split_1113: R.Tensor((1, 1, 14336), dtype="float16") = split113[1]
            silu113: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0113)
            mul113: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu113, split_1113)
            lv574 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight5, model_layers_17_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims458: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv574, axes=None)
            matmul458: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul113, permute_dims458, out_dtype="void")
            add227: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul458, add226)
            rms_norm231: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add227, model_layers_18_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv575 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight5, model_layers_18_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims459: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv575, axes=None)
            matmul459: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm231, permute_dims459, out_dtype="void")
            reshape456: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul459, R.shape([1, 1, 48, 128]))
            reshape457: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape456, R.shape([1, 48, 128]))
            lv576 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape457), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape458: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv576, R.shape([1, 1, 32, 128]))
            reshape459: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape458, R.shape([1, 1, 4096]))
            lv577 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight5, model_layers_18_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims460: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv577, axes=None)
            matmul460: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape459, permute_dims460, out_dtype="void")
            add228: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul460, add227)
            rms_norm232: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add228, model_layers_18_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv578 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight5, model_layers_18_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims461: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv578, axes=None)
            matmul461: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm232, permute_dims461, out_dtype="void")
            split114: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul461, indices_or_sections=2, axis=-1)
            split_0114: R.Tensor((1, 1, 14336), dtype="float16") = split114[0]
            split_1114: R.Tensor((1, 1, 14336), dtype="float16") = split114[1]
            silu114: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0114)
            mul114: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu114, split_1114)
            lv579 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight5, model_layers_18_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims462: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv579, axes=None)
            matmul462: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul114, permute_dims462, out_dtype="void")
            add229: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul462, add228)
            rms_norm233: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add229, model_layers_19_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv580 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight5, model_layers_19_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims463: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv580, axes=None)
            matmul463: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm233, permute_dims463, out_dtype="void")
            reshape460: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul463, R.shape([1, 1, 48, 128]))
            reshape461: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape460, R.shape([1, 48, 128]))
            lv581 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape461), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape462: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv581, R.shape([1, 1, 32, 128]))
            reshape463: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape462, R.shape([1, 1, 4096]))
            lv582 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight5, model_layers_19_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims464: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv582, axes=None)
            matmul464: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape463, permute_dims464, out_dtype="void")
            add230: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul464, add229)
            rms_norm234: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add230, model_layers_19_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv583 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight5, model_layers_19_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims465: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv583, axes=None)
            matmul465: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm234, permute_dims465, out_dtype="void")
            split115: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul465, indices_or_sections=2, axis=-1)
            split_0115: R.Tensor((1, 1, 14336), dtype="float16") = split115[0]
            split_1115: R.Tensor((1, 1, 14336), dtype="float16") = split115[1]
            silu115: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0115)
            mul115: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu115, split_1115)
            lv584 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight5, model_layers_19_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims466: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv584, axes=None)
            matmul466: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul115, permute_dims466, out_dtype="void")
            add231: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul466, add230)
            rms_norm235: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add231, model_layers_20_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv585 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight5, model_layers_20_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims467: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv585, axes=None)
            matmul467: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm235, permute_dims467, out_dtype="void")
            reshape464: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul467, R.shape([1, 1, 48, 128]))
            reshape465: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape464, R.shape([1, 48, 128]))
            lv586 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape465), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape466: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv586, R.shape([1, 1, 32, 128]))
            reshape467: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape466, R.shape([1, 1, 4096]))
            lv587 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight5, model_layers_20_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims468: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv587, axes=None)
            matmul468: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape467, permute_dims468, out_dtype="void")
            add232: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul468, add231)
            rms_norm236: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add232, model_layers_20_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv588 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight5, model_layers_20_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims469: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv588, axes=None)
            matmul469: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm236, permute_dims469, out_dtype="void")
            split116: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul469, indices_or_sections=2, axis=-1)
            split_0116: R.Tensor((1, 1, 14336), dtype="float16") = split116[0]
            split_1116: R.Tensor((1, 1, 14336), dtype="float16") = split116[1]
            silu116: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0116)
            mul116: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu116, split_1116)
            lv589 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight5, model_layers_20_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims470: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv589, axes=None)
            matmul470: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul116, permute_dims470, out_dtype="void")
            add233: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul470, add232)
            rms_norm237: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add233, model_layers_21_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv590 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight5, model_layers_21_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims471: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv590, axes=None)
            matmul471: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm237, permute_dims471, out_dtype="void")
            reshape468: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul471, R.shape([1, 1, 48, 128]))
            reshape469: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape468, R.shape([1, 48, 128]))
            lv591 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape469), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape470: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv591, R.shape([1, 1, 32, 128]))
            reshape471: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape470, R.shape([1, 1, 4096]))
            lv592 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight5, model_layers_21_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims472: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv592, axes=None)
            matmul472: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape471, permute_dims472, out_dtype="void")
            add234: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul472, add233)
            rms_norm238: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add234, model_layers_21_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv593 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight5, model_layers_21_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims473: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv593, axes=None)
            matmul473: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm238, permute_dims473, out_dtype="void")
            split117: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul473, indices_or_sections=2, axis=-1)
            split_0117: R.Tensor((1, 1, 14336), dtype="float16") = split117[0]
            split_1117: R.Tensor((1, 1, 14336), dtype="float16") = split117[1]
            silu117: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0117)
            mul117: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu117, split_1117)
            lv594 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight5, model_layers_21_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims474: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv594, axes=None)
            matmul474: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul117, permute_dims474, out_dtype="void")
            add235: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul474, add234)
            rms_norm239: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add235, model_layers_22_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv595 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight5, model_layers_22_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims475: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv595, axes=None)
            matmul475: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm239, permute_dims475, out_dtype="void")
            reshape472: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul475, R.shape([1, 1, 48, 128]))
            reshape473: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape472, R.shape([1, 48, 128]))
            lv596 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape473), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape474: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv596, R.shape([1, 1, 32, 128]))
            reshape475: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape474, R.shape([1, 1, 4096]))
            lv597 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight5, model_layers_22_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims476: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv597, axes=None)
            matmul476: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape475, permute_dims476, out_dtype="void")
            add236: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul476, add235)
            rms_norm240: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add236, model_layers_22_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv598 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight5, model_layers_22_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims477: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv598, axes=None)
            matmul477: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm240, permute_dims477, out_dtype="void")
            split118: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul477, indices_or_sections=2, axis=-1)
            split_0118: R.Tensor((1, 1, 14336), dtype="float16") = split118[0]
            split_1118: R.Tensor((1, 1, 14336), dtype="float16") = split118[1]
            silu118: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0118)
            mul118: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu118, split_1118)
            lv599 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight5, model_layers_22_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims478: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv599, axes=None)
            matmul478: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul118, permute_dims478, out_dtype="void")
            add237: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul478, add236)
            rms_norm241: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add237, model_layers_23_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv600 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight5, model_layers_23_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims479: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv600, axes=None)
            matmul479: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm241, permute_dims479, out_dtype="void")
            reshape476: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul479, R.shape([1, 1, 48, 128]))
            reshape477: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape476, R.shape([1, 48, 128]))
            lv601 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape477), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape478: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv601, R.shape([1, 1, 32, 128]))
            reshape479: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape478, R.shape([1, 1, 4096]))
            lv602 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight5, model_layers_23_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims480: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv602, axes=None)
            matmul480: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape479, permute_dims480, out_dtype="void")
            add238: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul480, add237)
            rms_norm242: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add238, model_layers_23_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv603 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight5, model_layers_23_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims481: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv603, axes=None)
            matmul481: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm242, permute_dims481, out_dtype="void")
            split119: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul481, indices_or_sections=2, axis=-1)
            split_0119: R.Tensor((1, 1, 14336), dtype="float16") = split119[0]
            split_1119: R.Tensor((1, 1, 14336), dtype="float16") = split119[1]
            silu119: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0119)
            mul119: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu119, split_1119)
            lv604 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight5, model_layers_23_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims482: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv604, axes=None)
            matmul482: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul119, permute_dims482, out_dtype="void")
            add239: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul482, add238)
            rms_norm243: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add239, model_layers_24_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv605 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight5, model_layers_24_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims483: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv605, axes=None)
            matmul483: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm243, permute_dims483, out_dtype="void")
            reshape480: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul483, R.shape([1, 1, 48, 128]))
            reshape481: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape480, R.shape([1, 48, 128]))
            lv606 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape481), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape482: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv606, R.shape([1, 1, 32, 128]))
            reshape483: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape482, R.shape([1, 1, 4096]))
            lv607 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight5, model_layers_24_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims484: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv607, axes=None)
            matmul484: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape483, permute_dims484, out_dtype="void")
            add240: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul484, add239)
            rms_norm244: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add240, model_layers_24_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv608 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight5, model_layers_24_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims485: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv608, axes=None)
            matmul485: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm244, permute_dims485, out_dtype="void")
            split120: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul485, indices_or_sections=2, axis=-1)
            split_0120: R.Tensor((1, 1, 14336), dtype="float16") = split120[0]
            split_1120: R.Tensor((1, 1, 14336), dtype="float16") = split120[1]
            silu120: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0120)
            mul120: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu120, split_1120)
            lv609 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight5, model_layers_24_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims486: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv609, axes=None)
            matmul486: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul120, permute_dims486, out_dtype="void")
            add241: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul486, add240)
            rms_norm245: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add241, model_layers_25_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv610 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight5, model_layers_25_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims487: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv610, axes=None)
            matmul487: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm245, permute_dims487, out_dtype="void")
            reshape484: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul487, R.shape([1, 1, 48, 128]))
            reshape485: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape484, R.shape([1, 48, 128]))
            lv611 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape485), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape486: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv611, R.shape([1, 1, 32, 128]))
            reshape487: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape486, R.shape([1, 1, 4096]))
            lv612 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight5, model_layers_25_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims488: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv612, axes=None)
            matmul488: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape487, permute_dims488, out_dtype="void")
            add242: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul488, add241)
            rms_norm246: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add242, model_layers_25_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv613 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight5, model_layers_25_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims489: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv613, axes=None)
            matmul489: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm246, permute_dims489, out_dtype="void")
            split121: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul489, indices_or_sections=2, axis=-1)
            split_0121: R.Tensor((1, 1, 14336), dtype="float16") = split121[0]
            split_1121: R.Tensor((1, 1, 14336), dtype="float16") = split121[1]
            silu121: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0121)
            mul121: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu121, split_1121)
            lv614 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight5, model_layers_25_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims490: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv614, axes=None)
            matmul490: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul121, permute_dims490, out_dtype="void")
            add243: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul490, add242)
            rms_norm247: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add243, model_layers_26_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv615 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight5, model_layers_26_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims491: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv615, axes=None)
            matmul491: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm247, permute_dims491, out_dtype="void")
            reshape488: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul491, R.shape([1, 1, 48, 128]))
            reshape489: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape488, R.shape([1, 48, 128]))
            lv616 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape489), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape490: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv616, R.shape([1, 1, 32, 128]))
            reshape491: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape490, R.shape([1, 1, 4096]))
            lv617 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight5, model_layers_26_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims492: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv617, axes=None)
            matmul492: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape491, permute_dims492, out_dtype="void")
            add244: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul492, add243)
            rms_norm248: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add244, model_layers_26_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv618 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight5, model_layers_26_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims493: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv618, axes=None)
            matmul493: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm248, permute_dims493, out_dtype="void")
            split122: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul493, indices_or_sections=2, axis=-1)
            split_0122: R.Tensor((1, 1, 14336), dtype="float16") = split122[0]
            split_1122: R.Tensor((1, 1, 14336), dtype="float16") = split122[1]
            silu122: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0122)
            mul122: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu122, split_1122)
            lv619 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight5, model_layers_26_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims494: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv619, axes=None)
            matmul494: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul122, permute_dims494, out_dtype="void")
            add245: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul494, add244)
            rms_norm249: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add245, model_layers_27_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv620 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight5, model_layers_27_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims495: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv620, axes=None)
            matmul495: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm249, permute_dims495, out_dtype="void")
            reshape492: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul495, R.shape([1, 1, 48, 128]))
            reshape493: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape492, R.shape([1, 48, 128]))
            lv621 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape493), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape494: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv621, R.shape([1, 1, 32, 128]))
            reshape495: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape494, R.shape([1, 1, 4096]))
            lv622 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight5, model_layers_27_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims496: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv622, axes=None)
            matmul496: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape495, permute_dims496, out_dtype="void")
            add246: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul496, add245)
            rms_norm250: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add246, model_layers_27_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv623 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight5, model_layers_27_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims497: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv623, axes=None)
            matmul497: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm250, permute_dims497, out_dtype="void")
            split123: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul497, indices_or_sections=2, axis=-1)
            split_0123: R.Tensor((1, 1, 14336), dtype="float16") = split123[0]
            split_1123: R.Tensor((1, 1, 14336), dtype="float16") = split123[1]
            silu123: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0123)
            mul123: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu123, split_1123)
            lv624 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight5, model_layers_27_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims498: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv624, axes=None)
            matmul498: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul123, permute_dims498, out_dtype="void")
            add247: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul498, add246)
            rms_norm251: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add247, model_layers_28_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv625 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight5, model_layers_28_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims499: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv625, axes=None)
            matmul499: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm251, permute_dims499, out_dtype="void")
            reshape496: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul499, R.shape([1, 1, 48, 128]))
            reshape497: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape496, R.shape([1, 48, 128]))
            lv626 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape497), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape498: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv626, R.shape([1, 1, 32, 128]))
            reshape499: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape498, R.shape([1, 1, 4096]))
            lv627 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight5, model_layers_28_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims500: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv627, axes=None)
            matmul500: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape499, permute_dims500, out_dtype="void")
            add248: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul500, add247)
            rms_norm252: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add248, model_layers_28_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv628 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight5, model_layers_28_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims501: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv628, axes=None)
            matmul501: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm252, permute_dims501, out_dtype="void")
            split124: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul501, indices_or_sections=2, axis=-1)
            split_0124: R.Tensor((1, 1, 14336), dtype="float16") = split124[0]
            split_1124: R.Tensor((1, 1, 14336), dtype="float16") = split124[1]
            silu124: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0124)
            mul124: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu124, split_1124)
            lv629 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight5, model_layers_28_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims502: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv629, axes=None)
            matmul502: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul124, permute_dims502, out_dtype="void")
            add249: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul502, add248)
            rms_norm253: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add249, model_layers_29_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv630 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight5, model_layers_29_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims503: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv630, axes=None)
            matmul503: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm253, permute_dims503, out_dtype="void")
            reshape500: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul503, R.shape([1, 1, 48, 128]))
            reshape501: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape500, R.shape([1, 48, 128]))
            lv631 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape501), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape502: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv631, R.shape([1, 1, 32, 128]))
            reshape503: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape502, R.shape([1, 1, 4096]))
            lv632 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight5, model_layers_29_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims504: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv632, axes=None)
            matmul504: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape503, permute_dims504, out_dtype="void")
            add250: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul504, add249)
            rms_norm254: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add250, model_layers_29_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv633 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight5, model_layers_29_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims505: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv633, axes=None)
            matmul505: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm254, permute_dims505, out_dtype="void")
            split125: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul505, indices_or_sections=2, axis=-1)
            split_0125: R.Tensor((1, 1, 14336), dtype="float16") = split125[0]
            split_1125: R.Tensor((1, 1, 14336), dtype="float16") = split125[1]
            silu125: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0125)
            mul125: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu125, split_1125)
            lv634 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight5, model_layers_29_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims506: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv634, axes=None)
            matmul506: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul125, permute_dims506, out_dtype="void")
            add251: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul506, add250)
            rms_norm255: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add251, model_layers_30_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv635 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight5, model_layers_30_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims507: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv635, axes=None)
            matmul507: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm255, permute_dims507, out_dtype="void")
            reshape504: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul507, R.shape([1, 1, 48, 128]))
            reshape505: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape504, R.shape([1, 48, 128]))
            lv636 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape505), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape506: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv636, R.shape([1, 1, 32, 128]))
            reshape507: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape506, R.shape([1, 1, 4096]))
            lv637 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight5, model_layers_30_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims508: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv637, axes=None)
            matmul508: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape507, permute_dims508, out_dtype="void")
            add252: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul508, add251)
            rms_norm256: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add252, model_layers_30_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv638 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight5, model_layers_30_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims509: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv638, axes=None)
            matmul509: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm256, permute_dims509, out_dtype="void")
            split126: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul509, indices_or_sections=2, axis=-1)
            split_0126: R.Tensor((1, 1, 14336), dtype="float16") = split126[0]
            split_1126: R.Tensor((1, 1, 14336), dtype="float16") = split126[1]
            silu126: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0126)
            mul126: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu126, split_1126)
            lv639 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight5, model_layers_30_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims510: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv639, axes=None)
            matmul510: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul126, permute_dims510, out_dtype="void")
            add253: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul510, add252)
            rms_norm257: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add253, model_layers_31_input_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv640 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight5, model_layers_31_self_attn_qkv_proj_q_scale5), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims511: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv640, axes=None)
            matmul511: R.Tensor((1, 1, 6144), dtype="float16") = R.matmul(rms_norm257, permute_dims511, out_dtype="void")
            reshape508: R.Tensor((1, 1, 48, 128), dtype="float16") = R.reshape(matmul511, R.shape([1, 1, 48, 128]))
            reshape509: R.Tensor((1, 48, 128), dtype="float16") = R.reshape(reshape508, R.shape([1, 48, 128]))
            lv641 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape509), out_sinfo=R.Tensor((1, 32, 128), dtype="float16"))
            reshape510: R.Tensor((1, 1, 32, 128), dtype="float16") = R.reshape(lv641, R.shape([1, 1, 32, 128]))
            reshape511: R.Tensor((1, 1, 4096), dtype="float16") = R.reshape(reshape510, R.shape([1, 1, 4096]))
            lv642 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight5, model_layers_31_self_attn_o_proj_q_scale5), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims512: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv642, axes=None)
            matmul512: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(reshape511, permute_dims512, out_dtype="void")
            add254: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul512, add253)
            rms_norm258: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add254, model_layers_31_post_attention_layernorm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            lv643 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight5, model_layers_31_mlp_gate_up_proj_q_scale5), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims513: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv643, axes=None)
            matmul513: R.Tensor((1, 1, 28672), dtype="float16") = R.matmul(rms_norm258, permute_dims513, out_dtype="void")
            split127: R.Tuple(R.Tensor((1, 1, 14336), dtype="float16"), R.Tensor((1, 1, 14336), dtype="float16")) = R.split(matmul513, indices_or_sections=2, axis=-1)
            split_0127: R.Tensor((1, 1, 14336), dtype="float16") = split127[0]
            split_1127: R.Tensor((1, 1, 14336), dtype="float16") = split127[1]
            silu127: R.Tensor((1, 1, 14336), dtype="float16") = R.nn.silu(split_0127)
            mul127: R.Tensor((1, 1, 14336), dtype="float16") = R.multiply(silu127, split_1127)
            lv644 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight5, model_layers_31_mlp_down_proj_q_scale5), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims514: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv644, axes=None)
            matmul514: R.Tensor((1, 1, 4096), dtype="float16") = R.matmul(mul127, permute_dims514, out_dtype="void")
            add255: R.Tensor((1, 1, 4096), dtype="float16") = R.add(matmul514, add254)
            rms_norm259: R.Tensor((1, 1, 4096), dtype="float16") = R.nn.rms_norm(add255, model_norm_weight5, axes=[-1], epsilon=1.0000000000000001e-05)
            gv6: R.Tuple(R.Tensor((1, 1, 4096), dtype="float16"), R.Object) = rms_norm259, paged_kv_cache
            R.output(gv6)
        return gv6

    @R.function
    def embed(input_ids: R.Tensor(("seq_len",), dtype="int32"), packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tensor(("seq_len", 4096), dtype="float16"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            lv = R.call_tir(cls.dequantize, (model_embed_tokens_q_weight, model_embed_tokens_q_scale), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            take: R.Tensor((seq_len, 4096), dtype="float16") = R.take(lv, input_ids, axis=0)
            gv: R.Tensor((seq_len, 4096), dtype="float16") = take
            R.output(gv)
        return gv

    @R.function
    def gather_hidden_states(src: R.Tensor(("m", "n"), dtype="float16"), indices: R.Tensor(("batch_size",), dtype="int32"), dst: R.Tensor(("batch_size", "n"), dtype="float16")) -> R.Tensor(("batch_size", "n"), dtype="float16"):
        batch_size = T.int64(is_size_var=True)
        n = T.int64(is_size_var=True)
        m = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        cls = Module
        with R.dataflow():
            gv: R.Tensor((batch_size, n), dtype="float16") = R.call_tir_inplace(cls._gather_hidden_states, (src, indices, dst), out_sinfo=R.Tensor((batch_size, n), dtype="float16"), inplace_indices=[2])
            R.output(gv)
        return gv

    @R.function
    def get_logits(hidden_states: R.Tensor(("seq_len", 4096), dtype="float16"), packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tensor(("seq_len", "vocab_size"), dtype="float32"):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight1: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale1: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight1: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale1: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight1: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale1: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight1: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale1: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight1: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale1: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight1: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight1: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale1: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            lv1 = R.call_tir(cls.dequantize, (lm_head_q_weight1, lm_head_q_scale1), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            permute_dims: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv1, axes=None)
            matmul: R.Tensor((seq_len, vocab_size), dtype="float16") = R.matmul(hidden_states, permute_dims, out_dtype="void")
            astype: R.Tensor((seq_len, vocab_size), dtype="float32") = R.astype(matmul, dtype="float32")
            gv1: R.Tensor((seq_len, vocab_size), dtype="float32") = astype
            R.output(gv1)
        return gv1

    @R.function
    def multinomial_from_uniform(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        with R.dataflow():
            probs_1: R.Tensor((batch_size, vocab_size), dtype="float32") = probs
            uniform_samples_1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices_1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            nn_multinomial_from_uniform: R.Tensor((num_samples, 1), dtype="int32") = R.multinomial_from_uniform(probs_1, uniform_samples_1, sample_indices_1, dtype="int32")
            lv: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", nn_multinomial_from_uniform, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            gv: R.Tensor((num_samples,), dtype="int32") = lv
            R.output(gv)
        return gv

    @R.function
    def prefill(input_embed: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, 1, "vocab_size"), dtype="float32"), R.Object):
        vocab_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight2: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale2: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight2: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale2: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight2: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale2: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight2: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale2: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight2: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale2: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight2: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight2: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale2: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv2 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight2, model_layers_0_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims1: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv2, axes=None)
            matmul1: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm, permute_dims1, out_dtype="void")
            reshape: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul1, R.shape([1, seq_len, 48, 128]))
            reshape1: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape, R.shape([seq_len, 48, 128]))
            lv3 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape1), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape2: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv3, R.shape([1, seq_len, 32, 128]))
            reshape3: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape2, R.shape([1, seq_len, 4096]))
            lv4 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight2, model_layers_0_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims2: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv4, axes=None)
            matmul2: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape3, permute_dims2, out_dtype="void")
            add: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul2, input_embed)
            rms_norm1: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add, model_layers_0_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv5 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight2, model_layers_0_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims3: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv5, axes=None)
            matmul3: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm1, permute_dims3, out_dtype="void")
            split: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul3, indices_or_sections=2, axis=-1)
            split_0: R.Tensor((1, seq_len, 14336), dtype="float16") = split[0]
            split_1: R.Tensor((1, seq_len, 14336), dtype="float16") = split[1]
            silu: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_0)
            mul: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu, split_1)
            lv6 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight2, model_layers_0_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims4: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv6, axes=None)
            matmul4: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul, permute_dims4, out_dtype="void")
            add1: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul4, add)
            rms_norm2: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add1, model_layers_1_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv7 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight2, model_layers_1_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims5: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv7, axes=None)
            matmul5: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm2, permute_dims5, out_dtype="void")
            reshape4: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul5, R.shape([1, seq_len, 48, 128]))
            reshape5: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape4, R.shape([seq_len, 48, 128]))
            lv8 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape5), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape6: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv8, R.shape([1, seq_len, 32, 128]))
            reshape7: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape6, R.shape([1, seq_len, 4096]))
            lv9 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight2, model_layers_1_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims6: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv9, axes=None)
            matmul6: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape7, permute_dims6, out_dtype="void")
            add2: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul6, add1)
            rms_norm3: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add2, model_layers_1_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv10 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight2, model_layers_1_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims7: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv10, axes=None)
            matmul7: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm3, permute_dims7, out_dtype="void")
            split1: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul7, indices_or_sections=2, axis=-1)
            split_01: R.Tensor((1, seq_len, 14336), dtype="float16") = split1[0]
            split_11: R.Tensor((1, seq_len, 14336), dtype="float16") = split1[1]
            silu1: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_01)
            mul1: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu1, split_11)
            lv11 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight2, model_layers_1_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims8: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv11, axes=None)
            matmul8: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul1, permute_dims8, out_dtype="void")
            add3: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul8, add2)
            rms_norm4: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add3, model_layers_2_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv12 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight2, model_layers_2_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims9: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv12, axes=None)
            matmul9: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm4, permute_dims9, out_dtype="void")
            reshape8: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul9, R.shape([1, seq_len, 48, 128]))
            reshape9: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape8, R.shape([seq_len, 48, 128]))
            lv13 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape9), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape10: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv13, R.shape([1, seq_len, 32, 128]))
            reshape11: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape10, R.shape([1, seq_len, 4096]))
            lv14 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight2, model_layers_2_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims10: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv14, axes=None)
            matmul10: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape11, permute_dims10, out_dtype="void")
            add4: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul10, add3)
            rms_norm5: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add4, model_layers_2_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv15 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight2, model_layers_2_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims11: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv15, axes=None)
            matmul11: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm5, permute_dims11, out_dtype="void")
            split2: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul11, indices_or_sections=2, axis=-1)
            split_02: R.Tensor((1, seq_len, 14336), dtype="float16") = split2[0]
            split_12: R.Tensor((1, seq_len, 14336), dtype="float16") = split2[1]
            silu2: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_02)
            mul2: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu2, split_12)
            lv16 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight2, model_layers_2_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims12: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv16, axes=None)
            matmul12: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul2, permute_dims12, out_dtype="void")
            add5: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul12, add4)
            rms_norm6: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add5, model_layers_3_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv17 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight2, model_layers_3_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims13: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv17, axes=None)
            matmul13: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm6, permute_dims13, out_dtype="void")
            reshape12: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul13, R.shape([1, seq_len, 48, 128]))
            reshape13: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape12, R.shape([seq_len, 48, 128]))
            lv18 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape13), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape14: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv18, R.shape([1, seq_len, 32, 128]))
            reshape15: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape14, R.shape([1, seq_len, 4096]))
            lv19 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight2, model_layers_3_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims14: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv19, axes=None)
            matmul14: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape15, permute_dims14, out_dtype="void")
            add6: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul14, add5)
            rms_norm7: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add6, model_layers_3_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv20 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight2, model_layers_3_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims15: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv20, axes=None)
            matmul15: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm7, permute_dims15, out_dtype="void")
            split3: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul15, indices_or_sections=2, axis=-1)
            split_03: R.Tensor((1, seq_len, 14336), dtype="float16") = split3[0]
            split_13: R.Tensor((1, seq_len, 14336), dtype="float16") = split3[1]
            silu3: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_03)
            mul3: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu3, split_13)
            lv21 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight2, model_layers_3_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims16: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv21, axes=None)
            matmul16: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul3, permute_dims16, out_dtype="void")
            add7: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul16, add6)
            rms_norm8: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add7, model_layers_4_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv22 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight2, model_layers_4_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims17: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv22, axes=None)
            matmul17: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm8, permute_dims17, out_dtype="void")
            reshape16: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul17, R.shape([1, seq_len, 48, 128]))
            reshape17: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape16, R.shape([seq_len, 48, 128]))
            lv23 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape17), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape18: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv23, R.shape([1, seq_len, 32, 128]))
            reshape19: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape18, R.shape([1, seq_len, 4096]))
            lv24 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight2, model_layers_4_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims18: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv24, axes=None)
            matmul18: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape19, permute_dims18, out_dtype="void")
            add8: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul18, add7)
            rms_norm9: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add8, model_layers_4_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv25 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight2, model_layers_4_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims19: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv25, axes=None)
            matmul19: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm9, permute_dims19, out_dtype="void")
            split4: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul19, indices_or_sections=2, axis=-1)
            split_04: R.Tensor((1, seq_len, 14336), dtype="float16") = split4[0]
            split_14: R.Tensor((1, seq_len, 14336), dtype="float16") = split4[1]
            silu4: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_04)
            mul4: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu4, split_14)
            lv26 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight2, model_layers_4_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims20: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv26, axes=None)
            matmul20: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul4, permute_dims20, out_dtype="void")
            add9: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul20, add8)
            rms_norm10: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add9, model_layers_5_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv27 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight2, model_layers_5_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims21: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv27, axes=None)
            matmul21: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm10, permute_dims21, out_dtype="void")
            reshape20: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul21, R.shape([1, seq_len, 48, 128]))
            reshape21: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape20, R.shape([seq_len, 48, 128]))
            lv28 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape21), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape22: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv28, R.shape([1, seq_len, 32, 128]))
            reshape23: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape22, R.shape([1, seq_len, 4096]))
            lv29 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight2, model_layers_5_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims22: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv29, axes=None)
            matmul22: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape23, permute_dims22, out_dtype="void")
            add10: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul22, add9)
            rms_norm11: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add10, model_layers_5_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv30 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight2, model_layers_5_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims23: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv30, axes=None)
            matmul23: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm11, permute_dims23, out_dtype="void")
            split5: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul23, indices_or_sections=2, axis=-1)
            split_05: R.Tensor((1, seq_len, 14336), dtype="float16") = split5[0]
            split_15: R.Tensor((1, seq_len, 14336), dtype="float16") = split5[1]
            silu5: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_05)
            mul5: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu5, split_15)
            lv31 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight2, model_layers_5_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims24: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv31, axes=None)
            matmul24: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul5, permute_dims24, out_dtype="void")
            add11: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul24, add10)
            rms_norm12: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add11, model_layers_6_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv32 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight2, model_layers_6_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims25: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv32, axes=None)
            matmul25: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm12, permute_dims25, out_dtype="void")
            reshape24: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul25, R.shape([1, seq_len, 48, 128]))
            reshape25: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape24, R.shape([seq_len, 48, 128]))
            lv33 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape25), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape26: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv33, R.shape([1, seq_len, 32, 128]))
            reshape27: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape26, R.shape([1, seq_len, 4096]))
            lv34 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight2, model_layers_6_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims26: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv34, axes=None)
            matmul26: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape27, permute_dims26, out_dtype="void")
            add12: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul26, add11)
            rms_norm13: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add12, model_layers_6_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv35 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight2, model_layers_6_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims27: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv35, axes=None)
            matmul27: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm13, permute_dims27, out_dtype="void")
            split6: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul27, indices_or_sections=2, axis=-1)
            split_06: R.Tensor((1, seq_len, 14336), dtype="float16") = split6[0]
            split_16: R.Tensor((1, seq_len, 14336), dtype="float16") = split6[1]
            silu6: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_06)
            mul6: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu6, split_16)
            lv36 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight2, model_layers_6_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims28: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv36, axes=None)
            matmul28: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul6, permute_dims28, out_dtype="void")
            add13: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul28, add12)
            rms_norm14: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add13, model_layers_7_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv37 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight2, model_layers_7_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims29: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv37, axes=None)
            matmul29: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm14, permute_dims29, out_dtype="void")
            reshape28: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul29, R.shape([1, seq_len, 48, 128]))
            reshape29: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape28, R.shape([seq_len, 48, 128]))
            lv38 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape29), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape30: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv38, R.shape([1, seq_len, 32, 128]))
            reshape31: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape30, R.shape([1, seq_len, 4096]))
            lv39 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight2, model_layers_7_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims30: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv39, axes=None)
            matmul30: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape31, permute_dims30, out_dtype="void")
            add14: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul30, add13)
            rms_norm15: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add14, model_layers_7_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv40 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight2, model_layers_7_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims31: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv40, axes=None)
            matmul31: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm15, permute_dims31, out_dtype="void")
            split7: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul31, indices_or_sections=2, axis=-1)
            split_07: R.Tensor((1, seq_len, 14336), dtype="float16") = split7[0]
            split_17: R.Tensor((1, seq_len, 14336), dtype="float16") = split7[1]
            silu7: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_07)
            mul7: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu7, split_17)
            lv41 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight2, model_layers_7_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims32: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv41, axes=None)
            matmul32: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul7, permute_dims32, out_dtype="void")
            add15: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul32, add14)
            rms_norm16: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add15, model_layers_8_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv42 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight2, model_layers_8_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims33: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv42, axes=None)
            matmul33: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm16, permute_dims33, out_dtype="void")
            reshape32: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul33, R.shape([1, seq_len, 48, 128]))
            reshape33: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape32, R.shape([seq_len, 48, 128]))
            lv43 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape33), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape34: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv43, R.shape([1, seq_len, 32, 128]))
            reshape35: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape34, R.shape([1, seq_len, 4096]))
            lv44 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight2, model_layers_8_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims34: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv44, axes=None)
            matmul34: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape35, permute_dims34, out_dtype="void")
            add16: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul34, add15)
            rms_norm17: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add16, model_layers_8_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv45 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight2, model_layers_8_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims35: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv45, axes=None)
            matmul35: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm17, permute_dims35, out_dtype="void")
            split8: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul35, indices_or_sections=2, axis=-1)
            split_08: R.Tensor((1, seq_len, 14336), dtype="float16") = split8[0]
            split_18: R.Tensor((1, seq_len, 14336), dtype="float16") = split8[1]
            silu8: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_08)
            mul8: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu8, split_18)
            lv46 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight2, model_layers_8_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims36: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv46, axes=None)
            matmul36: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul8, permute_dims36, out_dtype="void")
            add17: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul36, add16)
            rms_norm18: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add17, model_layers_9_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv47 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight2, model_layers_9_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims37: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv47, axes=None)
            matmul37: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm18, permute_dims37, out_dtype="void")
            reshape36: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul37, R.shape([1, seq_len, 48, 128]))
            reshape37: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape36, R.shape([seq_len, 48, 128]))
            lv48 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape37), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape38: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv48, R.shape([1, seq_len, 32, 128]))
            reshape39: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape38, R.shape([1, seq_len, 4096]))
            lv49 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight2, model_layers_9_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims38: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv49, axes=None)
            matmul38: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape39, permute_dims38, out_dtype="void")
            add18: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul38, add17)
            rms_norm19: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add18, model_layers_9_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv50 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight2, model_layers_9_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims39: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv50, axes=None)
            matmul39: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm19, permute_dims39, out_dtype="void")
            split9: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul39, indices_or_sections=2, axis=-1)
            split_09: R.Tensor((1, seq_len, 14336), dtype="float16") = split9[0]
            split_19: R.Tensor((1, seq_len, 14336), dtype="float16") = split9[1]
            silu9: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_09)
            mul9: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu9, split_19)
            lv51 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight2, model_layers_9_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims40: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv51, axes=None)
            matmul40: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul9, permute_dims40, out_dtype="void")
            add19: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul40, add18)
            rms_norm20: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add19, model_layers_10_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv52 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight2, model_layers_10_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims41: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv52, axes=None)
            matmul41: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm20, permute_dims41, out_dtype="void")
            reshape40: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul41, R.shape([1, seq_len, 48, 128]))
            reshape41: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape40, R.shape([seq_len, 48, 128]))
            lv53 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape41), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape42: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv53, R.shape([1, seq_len, 32, 128]))
            reshape43: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape42, R.shape([1, seq_len, 4096]))
            lv54 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight2, model_layers_10_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims42: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv54, axes=None)
            matmul42: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape43, permute_dims42, out_dtype="void")
            add20: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul42, add19)
            rms_norm21: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add20, model_layers_10_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv55 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight2, model_layers_10_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims43: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv55, axes=None)
            matmul43: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm21, permute_dims43, out_dtype="void")
            split10: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul43, indices_or_sections=2, axis=-1)
            split_010: R.Tensor((1, seq_len, 14336), dtype="float16") = split10[0]
            split_110: R.Tensor((1, seq_len, 14336), dtype="float16") = split10[1]
            silu10: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_010)
            mul10: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu10, split_110)
            lv56 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight2, model_layers_10_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims44: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv56, axes=None)
            matmul44: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul10, permute_dims44, out_dtype="void")
            add21: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul44, add20)
            rms_norm22: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add21, model_layers_11_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv57 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight2, model_layers_11_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims45: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv57, axes=None)
            matmul45: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm22, permute_dims45, out_dtype="void")
            reshape44: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul45, R.shape([1, seq_len, 48, 128]))
            reshape45: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape44, R.shape([seq_len, 48, 128]))
            lv58 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape45), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape46: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv58, R.shape([1, seq_len, 32, 128]))
            reshape47: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape46, R.shape([1, seq_len, 4096]))
            lv59 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight2, model_layers_11_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims46: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv59, axes=None)
            matmul46: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape47, permute_dims46, out_dtype="void")
            add22: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul46, add21)
            rms_norm23: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add22, model_layers_11_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv60 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight2, model_layers_11_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims47: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv60, axes=None)
            matmul47: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm23, permute_dims47, out_dtype="void")
            split11: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul47, indices_or_sections=2, axis=-1)
            split_011: R.Tensor((1, seq_len, 14336), dtype="float16") = split11[0]
            split_111: R.Tensor((1, seq_len, 14336), dtype="float16") = split11[1]
            silu11: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_011)
            mul11: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu11, split_111)
            lv61 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight2, model_layers_11_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims48: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv61, axes=None)
            matmul48: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul11, permute_dims48, out_dtype="void")
            add23: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul48, add22)
            rms_norm24: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add23, model_layers_12_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv62 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight2, model_layers_12_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims49: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv62, axes=None)
            matmul49: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm24, permute_dims49, out_dtype="void")
            reshape48: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul49, R.shape([1, seq_len, 48, 128]))
            reshape49: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape48, R.shape([seq_len, 48, 128]))
            lv63 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape49), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape50: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv63, R.shape([1, seq_len, 32, 128]))
            reshape51: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape50, R.shape([1, seq_len, 4096]))
            lv64 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight2, model_layers_12_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims50: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv64, axes=None)
            matmul50: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape51, permute_dims50, out_dtype="void")
            add24: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul50, add23)
            rms_norm25: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add24, model_layers_12_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv65 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight2, model_layers_12_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims51: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv65, axes=None)
            matmul51: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm25, permute_dims51, out_dtype="void")
            split12: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul51, indices_or_sections=2, axis=-1)
            split_012: R.Tensor((1, seq_len, 14336), dtype="float16") = split12[0]
            split_112: R.Tensor((1, seq_len, 14336), dtype="float16") = split12[1]
            silu12: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_012)
            mul12: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu12, split_112)
            lv66 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight2, model_layers_12_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims52: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv66, axes=None)
            matmul52: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul12, permute_dims52, out_dtype="void")
            add25: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul52, add24)
            rms_norm26: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add25, model_layers_13_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv67 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight2, model_layers_13_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims53: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv67, axes=None)
            matmul53: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm26, permute_dims53, out_dtype="void")
            reshape52: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul53, R.shape([1, seq_len, 48, 128]))
            reshape53: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape52, R.shape([seq_len, 48, 128]))
            lv68 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape53), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape54: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv68, R.shape([1, seq_len, 32, 128]))
            reshape55: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape54, R.shape([1, seq_len, 4096]))
            lv69 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight2, model_layers_13_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims54: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv69, axes=None)
            matmul54: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape55, permute_dims54, out_dtype="void")
            add26: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul54, add25)
            rms_norm27: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add26, model_layers_13_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv70 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight2, model_layers_13_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims55: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv70, axes=None)
            matmul55: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm27, permute_dims55, out_dtype="void")
            split13: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul55, indices_or_sections=2, axis=-1)
            split_013: R.Tensor((1, seq_len, 14336), dtype="float16") = split13[0]
            split_113: R.Tensor((1, seq_len, 14336), dtype="float16") = split13[1]
            silu13: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_013)
            mul13: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu13, split_113)
            lv71 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight2, model_layers_13_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims56: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv71, axes=None)
            matmul56: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul13, permute_dims56, out_dtype="void")
            add27: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul56, add26)
            rms_norm28: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add27, model_layers_14_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv72 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight2, model_layers_14_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims57: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv72, axes=None)
            matmul57: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm28, permute_dims57, out_dtype="void")
            reshape56: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul57, R.shape([1, seq_len, 48, 128]))
            reshape57: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape56, R.shape([seq_len, 48, 128]))
            lv73 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape57), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape58: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv73, R.shape([1, seq_len, 32, 128]))
            reshape59: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape58, R.shape([1, seq_len, 4096]))
            lv74 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight2, model_layers_14_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims58: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv74, axes=None)
            matmul58: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape59, permute_dims58, out_dtype="void")
            add28: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul58, add27)
            rms_norm29: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add28, model_layers_14_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv75 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight2, model_layers_14_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims59: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv75, axes=None)
            matmul59: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm29, permute_dims59, out_dtype="void")
            split14: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul59, indices_or_sections=2, axis=-1)
            split_014: R.Tensor((1, seq_len, 14336), dtype="float16") = split14[0]
            split_114: R.Tensor((1, seq_len, 14336), dtype="float16") = split14[1]
            silu14: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_014)
            mul14: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu14, split_114)
            lv76 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight2, model_layers_14_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims60: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv76, axes=None)
            matmul60: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul14, permute_dims60, out_dtype="void")
            add29: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul60, add28)
            rms_norm30: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add29, model_layers_15_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv77 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight2, model_layers_15_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims61: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv77, axes=None)
            matmul61: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm30, permute_dims61, out_dtype="void")
            reshape60: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul61, R.shape([1, seq_len, 48, 128]))
            reshape61: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape60, R.shape([seq_len, 48, 128]))
            lv78 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape61), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape62: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv78, R.shape([1, seq_len, 32, 128]))
            reshape63: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape62, R.shape([1, seq_len, 4096]))
            lv79 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight2, model_layers_15_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims62: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv79, axes=None)
            matmul62: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape63, permute_dims62, out_dtype="void")
            add30: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul62, add29)
            rms_norm31: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add30, model_layers_15_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv80 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight2, model_layers_15_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims63: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv80, axes=None)
            matmul63: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm31, permute_dims63, out_dtype="void")
            split15: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul63, indices_or_sections=2, axis=-1)
            split_015: R.Tensor((1, seq_len, 14336), dtype="float16") = split15[0]
            split_115: R.Tensor((1, seq_len, 14336), dtype="float16") = split15[1]
            silu15: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_015)
            mul15: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu15, split_115)
            lv81 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight2, model_layers_15_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims64: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv81, axes=None)
            matmul64: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul15, permute_dims64, out_dtype="void")
            add31: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul64, add30)
            rms_norm32: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add31, model_layers_16_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv82 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight2, model_layers_16_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims65: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv82, axes=None)
            matmul65: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm32, permute_dims65, out_dtype="void")
            reshape64: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul65, R.shape([1, seq_len, 48, 128]))
            reshape65: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape64, R.shape([seq_len, 48, 128]))
            lv83 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape65), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape66: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv83, R.shape([1, seq_len, 32, 128]))
            reshape67: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape66, R.shape([1, seq_len, 4096]))
            lv84 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight2, model_layers_16_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims66: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv84, axes=None)
            matmul66: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape67, permute_dims66, out_dtype="void")
            add32: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul66, add31)
            rms_norm33: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add32, model_layers_16_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv85 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight2, model_layers_16_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims67: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv85, axes=None)
            matmul67: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm33, permute_dims67, out_dtype="void")
            split16: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul67, indices_or_sections=2, axis=-1)
            split_016: R.Tensor((1, seq_len, 14336), dtype="float16") = split16[0]
            split_116: R.Tensor((1, seq_len, 14336), dtype="float16") = split16[1]
            silu16: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_016)
            mul16: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu16, split_116)
            lv86 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight2, model_layers_16_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims68: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv86, axes=None)
            matmul68: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul16, permute_dims68, out_dtype="void")
            add33: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul68, add32)
            rms_norm34: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add33, model_layers_17_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv87 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight2, model_layers_17_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims69: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv87, axes=None)
            matmul69: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm34, permute_dims69, out_dtype="void")
            reshape68: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul69, R.shape([1, seq_len, 48, 128]))
            reshape69: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape68, R.shape([seq_len, 48, 128]))
            lv88 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape69), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape70: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv88, R.shape([1, seq_len, 32, 128]))
            reshape71: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape70, R.shape([1, seq_len, 4096]))
            lv89 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight2, model_layers_17_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims70: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv89, axes=None)
            matmul70: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape71, permute_dims70, out_dtype="void")
            add34: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul70, add33)
            rms_norm35: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add34, model_layers_17_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv90 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight2, model_layers_17_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims71: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv90, axes=None)
            matmul71: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm35, permute_dims71, out_dtype="void")
            split17: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul71, indices_or_sections=2, axis=-1)
            split_017: R.Tensor((1, seq_len, 14336), dtype="float16") = split17[0]
            split_117: R.Tensor((1, seq_len, 14336), dtype="float16") = split17[1]
            silu17: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_017)
            mul17: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu17, split_117)
            lv91 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight2, model_layers_17_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims72: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv91, axes=None)
            matmul72: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul17, permute_dims72, out_dtype="void")
            add35: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul72, add34)
            rms_norm36: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add35, model_layers_18_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv92 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight2, model_layers_18_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims73: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv92, axes=None)
            matmul73: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm36, permute_dims73, out_dtype="void")
            reshape72: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul73, R.shape([1, seq_len, 48, 128]))
            reshape73: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape72, R.shape([seq_len, 48, 128]))
            lv93 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape73), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape74: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv93, R.shape([1, seq_len, 32, 128]))
            reshape75: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape74, R.shape([1, seq_len, 4096]))
            lv94 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight2, model_layers_18_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims74: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv94, axes=None)
            matmul74: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape75, permute_dims74, out_dtype="void")
            add36: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul74, add35)
            rms_norm37: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add36, model_layers_18_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv95 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight2, model_layers_18_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims75: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv95, axes=None)
            matmul75: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm37, permute_dims75, out_dtype="void")
            split18: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul75, indices_or_sections=2, axis=-1)
            split_018: R.Tensor((1, seq_len, 14336), dtype="float16") = split18[0]
            split_118: R.Tensor((1, seq_len, 14336), dtype="float16") = split18[1]
            silu18: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_018)
            mul18: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu18, split_118)
            lv96 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight2, model_layers_18_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims76: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv96, axes=None)
            matmul76: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul18, permute_dims76, out_dtype="void")
            add37: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul76, add36)
            rms_norm38: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add37, model_layers_19_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv97 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight2, model_layers_19_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims77: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv97, axes=None)
            matmul77: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm38, permute_dims77, out_dtype="void")
            reshape76: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul77, R.shape([1, seq_len, 48, 128]))
            reshape77: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape76, R.shape([seq_len, 48, 128]))
            lv98 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape77), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape78: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv98, R.shape([1, seq_len, 32, 128]))
            reshape79: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape78, R.shape([1, seq_len, 4096]))
            lv99 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight2, model_layers_19_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims78: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv99, axes=None)
            matmul78: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape79, permute_dims78, out_dtype="void")
            add38: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul78, add37)
            rms_norm39: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add38, model_layers_19_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv100 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight2, model_layers_19_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims79: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv100, axes=None)
            matmul79: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm39, permute_dims79, out_dtype="void")
            split19: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul79, indices_or_sections=2, axis=-1)
            split_019: R.Tensor((1, seq_len, 14336), dtype="float16") = split19[0]
            split_119: R.Tensor((1, seq_len, 14336), dtype="float16") = split19[1]
            silu19: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_019)
            mul19: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu19, split_119)
            lv101 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight2, model_layers_19_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims80: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv101, axes=None)
            matmul80: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul19, permute_dims80, out_dtype="void")
            add39: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul80, add38)
            rms_norm40: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add39, model_layers_20_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv102 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight2, model_layers_20_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims81: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv102, axes=None)
            matmul81: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm40, permute_dims81, out_dtype="void")
            reshape80: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul81, R.shape([1, seq_len, 48, 128]))
            reshape81: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape80, R.shape([seq_len, 48, 128]))
            lv103 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape81), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape82: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv103, R.shape([1, seq_len, 32, 128]))
            reshape83: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape82, R.shape([1, seq_len, 4096]))
            lv104 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight2, model_layers_20_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims82: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv104, axes=None)
            matmul82: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape83, permute_dims82, out_dtype="void")
            add40: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul82, add39)
            rms_norm41: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add40, model_layers_20_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv105 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight2, model_layers_20_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims83: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv105, axes=None)
            matmul83: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm41, permute_dims83, out_dtype="void")
            split20: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul83, indices_or_sections=2, axis=-1)
            split_020: R.Tensor((1, seq_len, 14336), dtype="float16") = split20[0]
            split_120: R.Tensor((1, seq_len, 14336), dtype="float16") = split20[1]
            silu20: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_020)
            mul20: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu20, split_120)
            lv106 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight2, model_layers_20_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims84: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv106, axes=None)
            matmul84: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul20, permute_dims84, out_dtype="void")
            add41: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul84, add40)
            rms_norm42: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add41, model_layers_21_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv107 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight2, model_layers_21_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims85: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv107, axes=None)
            matmul85: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm42, permute_dims85, out_dtype="void")
            reshape84: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul85, R.shape([1, seq_len, 48, 128]))
            reshape85: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape84, R.shape([seq_len, 48, 128]))
            lv108 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape85), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape86: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv108, R.shape([1, seq_len, 32, 128]))
            reshape87: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape86, R.shape([1, seq_len, 4096]))
            lv109 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight2, model_layers_21_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims86: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv109, axes=None)
            matmul86: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape87, permute_dims86, out_dtype="void")
            add42: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul86, add41)
            rms_norm43: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add42, model_layers_21_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv110 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight2, model_layers_21_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims87: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv110, axes=None)
            matmul87: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm43, permute_dims87, out_dtype="void")
            split21: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul87, indices_or_sections=2, axis=-1)
            split_021: R.Tensor((1, seq_len, 14336), dtype="float16") = split21[0]
            split_121: R.Tensor((1, seq_len, 14336), dtype="float16") = split21[1]
            silu21: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_021)
            mul21: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu21, split_121)
            lv111 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight2, model_layers_21_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims88: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv111, axes=None)
            matmul88: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul21, permute_dims88, out_dtype="void")
            add43: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul88, add42)
            rms_norm44: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add43, model_layers_22_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv112 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight2, model_layers_22_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims89: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv112, axes=None)
            matmul89: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm44, permute_dims89, out_dtype="void")
            reshape88: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul89, R.shape([1, seq_len, 48, 128]))
            reshape89: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape88, R.shape([seq_len, 48, 128]))
            lv113 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape89), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape90: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv113, R.shape([1, seq_len, 32, 128]))
            reshape91: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape90, R.shape([1, seq_len, 4096]))
            lv114 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight2, model_layers_22_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims90: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv114, axes=None)
            matmul90: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape91, permute_dims90, out_dtype="void")
            add44: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul90, add43)
            rms_norm45: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add44, model_layers_22_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv115 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight2, model_layers_22_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims91: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv115, axes=None)
            matmul91: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm45, permute_dims91, out_dtype="void")
            split22: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul91, indices_or_sections=2, axis=-1)
            split_022: R.Tensor((1, seq_len, 14336), dtype="float16") = split22[0]
            split_122: R.Tensor((1, seq_len, 14336), dtype="float16") = split22[1]
            silu22: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_022)
            mul22: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu22, split_122)
            lv116 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight2, model_layers_22_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims92: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv116, axes=None)
            matmul92: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul22, permute_dims92, out_dtype="void")
            add45: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul92, add44)
            rms_norm46: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add45, model_layers_23_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv117 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight2, model_layers_23_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims93: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv117, axes=None)
            matmul93: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm46, permute_dims93, out_dtype="void")
            reshape92: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul93, R.shape([1, seq_len, 48, 128]))
            reshape93: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape92, R.shape([seq_len, 48, 128]))
            lv118 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape93), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape94: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv118, R.shape([1, seq_len, 32, 128]))
            reshape95: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape94, R.shape([1, seq_len, 4096]))
            lv119 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight2, model_layers_23_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims94: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv119, axes=None)
            matmul94: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape95, permute_dims94, out_dtype="void")
            add46: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul94, add45)
            rms_norm47: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add46, model_layers_23_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv120 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight2, model_layers_23_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims95: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv120, axes=None)
            matmul95: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm47, permute_dims95, out_dtype="void")
            split23: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul95, indices_or_sections=2, axis=-1)
            split_023: R.Tensor((1, seq_len, 14336), dtype="float16") = split23[0]
            split_123: R.Tensor((1, seq_len, 14336), dtype="float16") = split23[1]
            silu23: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_023)
            mul23: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu23, split_123)
            lv121 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight2, model_layers_23_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims96: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv121, axes=None)
            matmul96: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul23, permute_dims96, out_dtype="void")
            add47: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul96, add46)
            rms_norm48: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add47, model_layers_24_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv122 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight2, model_layers_24_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims97: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv122, axes=None)
            matmul97: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm48, permute_dims97, out_dtype="void")
            reshape96: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul97, R.shape([1, seq_len, 48, 128]))
            reshape97: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape96, R.shape([seq_len, 48, 128]))
            lv123 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape97), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape98: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv123, R.shape([1, seq_len, 32, 128]))
            reshape99: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape98, R.shape([1, seq_len, 4096]))
            lv124 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight2, model_layers_24_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims98: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv124, axes=None)
            matmul98: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape99, permute_dims98, out_dtype="void")
            add48: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul98, add47)
            rms_norm49: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add48, model_layers_24_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv125 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight2, model_layers_24_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims99: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv125, axes=None)
            matmul99: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm49, permute_dims99, out_dtype="void")
            split24: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul99, indices_or_sections=2, axis=-1)
            split_024: R.Tensor((1, seq_len, 14336), dtype="float16") = split24[0]
            split_124: R.Tensor((1, seq_len, 14336), dtype="float16") = split24[1]
            silu24: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_024)
            mul24: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu24, split_124)
            lv126 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight2, model_layers_24_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims100: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv126, axes=None)
            matmul100: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul24, permute_dims100, out_dtype="void")
            add49: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul100, add48)
            rms_norm50: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add49, model_layers_25_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv127 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight2, model_layers_25_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims101: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv127, axes=None)
            matmul101: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm50, permute_dims101, out_dtype="void")
            reshape100: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul101, R.shape([1, seq_len, 48, 128]))
            reshape101: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape100, R.shape([seq_len, 48, 128]))
            lv128 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape101), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape102: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv128, R.shape([1, seq_len, 32, 128]))
            reshape103: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape102, R.shape([1, seq_len, 4096]))
            lv129 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight2, model_layers_25_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims102: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv129, axes=None)
            matmul102: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape103, permute_dims102, out_dtype="void")
            add50: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul102, add49)
            rms_norm51: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add50, model_layers_25_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv130 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight2, model_layers_25_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims103: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv130, axes=None)
            matmul103: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm51, permute_dims103, out_dtype="void")
            split25: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul103, indices_or_sections=2, axis=-1)
            split_025: R.Tensor((1, seq_len, 14336), dtype="float16") = split25[0]
            split_125: R.Tensor((1, seq_len, 14336), dtype="float16") = split25[1]
            silu25: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_025)
            mul25: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu25, split_125)
            lv131 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight2, model_layers_25_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims104: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv131, axes=None)
            matmul104: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul25, permute_dims104, out_dtype="void")
            add51: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul104, add50)
            rms_norm52: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add51, model_layers_26_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv132 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight2, model_layers_26_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims105: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv132, axes=None)
            matmul105: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm52, permute_dims105, out_dtype="void")
            reshape104: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul105, R.shape([1, seq_len, 48, 128]))
            reshape105: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape104, R.shape([seq_len, 48, 128]))
            lv133 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape105), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape106: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv133, R.shape([1, seq_len, 32, 128]))
            reshape107: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape106, R.shape([1, seq_len, 4096]))
            lv134 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight2, model_layers_26_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims106: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv134, axes=None)
            matmul106: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape107, permute_dims106, out_dtype="void")
            add52: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul106, add51)
            rms_norm53: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add52, model_layers_26_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv135 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight2, model_layers_26_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims107: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv135, axes=None)
            matmul107: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm53, permute_dims107, out_dtype="void")
            split26: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul107, indices_or_sections=2, axis=-1)
            split_026: R.Tensor((1, seq_len, 14336), dtype="float16") = split26[0]
            split_126: R.Tensor((1, seq_len, 14336), dtype="float16") = split26[1]
            silu26: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_026)
            mul26: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu26, split_126)
            lv136 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight2, model_layers_26_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims108: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv136, axes=None)
            matmul108: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul26, permute_dims108, out_dtype="void")
            add53: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul108, add52)
            rms_norm54: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add53, model_layers_27_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv137 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight2, model_layers_27_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims109: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv137, axes=None)
            matmul109: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm54, permute_dims109, out_dtype="void")
            reshape108: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul109, R.shape([1, seq_len, 48, 128]))
            reshape109: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape108, R.shape([seq_len, 48, 128]))
            lv138 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape109), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape110: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv138, R.shape([1, seq_len, 32, 128]))
            reshape111: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape110, R.shape([1, seq_len, 4096]))
            lv139 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight2, model_layers_27_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims110: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv139, axes=None)
            matmul110: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape111, permute_dims110, out_dtype="void")
            add54: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul110, add53)
            rms_norm55: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add54, model_layers_27_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv140 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight2, model_layers_27_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims111: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv140, axes=None)
            matmul111: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm55, permute_dims111, out_dtype="void")
            split27: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul111, indices_or_sections=2, axis=-1)
            split_027: R.Tensor((1, seq_len, 14336), dtype="float16") = split27[0]
            split_127: R.Tensor((1, seq_len, 14336), dtype="float16") = split27[1]
            silu27: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_027)
            mul27: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu27, split_127)
            lv141 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight2, model_layers_27_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims112: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv141, axes=None)
            matmul112: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul27, permute_dims112, out_dtype="void")
            add55: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul112, add54)
            rms_norm56: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add55, model_layers_28_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv142 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight2, model_layers_28_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims113: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv142, axes=None)
            matmul113: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm56, permute_dims113, out_dtype="void")
            reshape112: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul113, R.shape([1, seq_len, 48, 128]))
            reshape113: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape112, R.shape([seq_len, 48, 128]))
            lv143 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape113), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape114: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv143, R.shape([1, seq_len, 32, 128]))
            reshape115: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape114, R.shape([1, seq_len, 4096]))
            lv144 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight2, model_layers_28_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims114: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv144, axes=None)
            matmul114: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape115, permute_dims114, out_dtype="void")
            add56: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul114, add55)
            rms_norm57: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add56, model_layers_28_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv145 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight2, model_layers_28_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims115: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv145, axes=None)
            matmul115: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm57, permute_dims115, out_dtype="void")
            split28: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul115, indices_or_sections=2, axis=-1)
            split_028: R.Tensor((1, seq_len, 14336), dtype="float16") = split28[0]
            split_128: R.Tensor((1, seq_len, 14336), dtype="float16") = split28[1]
            silu28: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_028)
            mul28: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu28, split_128)
            lv146 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight2, model_layers_28_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims116: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv146, axes=None)
            matmul116: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul28, permute_dims116, out_dtype="void")
            add57: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul116, add56)
            rms_norm58: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add57, model_layers_29_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv147 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight2, model_layers_29_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims117: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv147, axes=None)
            matmul117: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm58, permute_dims117, out_dtype="void")
            reshape116: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul117, R.shape([1, seq_len, 48, 128]))
            reshape117: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape116, R.shape([seq_len, 48, 128]))
            lv148 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape117), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape118: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv148, R.shape([1, seq_len, 32, 128]))
            reshape119: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape118, R.shape([1, seq_len, 4096]))
            lv149 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight2, model_layers_29_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims118: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv149, axes=None)
            matmul118: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape119, permute_dims118, out_dtype="void")
            add58: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul118, add57)
            rms_norm59: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add58, model_layers_29_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv150 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight2, model_layers_29_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims119: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv150, axes=None)
            matmul119: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm59, permute_dims119, out_dtype="void")
            split29: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul119, indices_or_sections=2, axis=-1)
            split_029: R.Tensor((1, seq_len, 14336), dtype="float16") = split29[0]
            split_129: R.Tensor((1, seq_len, 14336), dtype="float16") = split29[1]
            silu29: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_029)
            mul29: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu29, split_129)
            lv151 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight2, model_layers_29_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims120: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv151, axes=None)
            matmul120: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul29, permute_dims120, out_dtype="void")
            add59: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul120, add58)
            rms_norm60: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add59, model_layers_30_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv152 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight2, model_layers_30_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims121: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv152, axes=None)
            matmul121: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm60, permute_dims121, out_dtype="void")
            reshape120: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul121, R.shape([1, seq_len, 48, 128]))
            reshape121: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape120, R.shape([seq_len, 48, 128]))
            lv153 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape121), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape122: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv153, R.shape([1, seq_len, 32, 128]))
            reshape123: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape122, R.shape([1, seq_len, 4096]))
            lv154 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight2, model_layers_30_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims122: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv154, axes=None)
            matmul122: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape123, permute_dims122, out_dtype="void")
            add60: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul122, add59)
            rms_norm61: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add60, model_layers_30_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv155 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight2, model_layers_30_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims123: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv155, axes=None)
            matmul123: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm61, permute_dims123, out_dtype="void")
            split30: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul123, indices_or_sections=2, axis=-1)
            split_030: R.Tensor((1, seq_len, 14336), dtype="float16") = split30[0]
            split_130: R.Tensor((1, seq_len, 14336), dtype="float16") = split30[1]
            silu30: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_030)
            mul30: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu30, split_130)
            lv156 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight2, model_layers_30_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims124: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv156, axes=None)
            matmul124: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul30, permute_dims124, out_dtype="void")
            add61: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul124, add60)
            rms_norm62: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add61, model_layers_31_input_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv157 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight2, model_layers_31_self_attn_qkv_proj_q_scale2), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims125: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv157, axes=None)
            matmul125: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm62, permute_dims125, out_dtype="void")
            reshape124: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul125, R.shape([1, seq_len, 48, 128]))
            reshape125: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape124, R.shape([seq_len, 48, 128]))
            lv158 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape125), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape126: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv158, R.shape([1, seq_len, 32, 128]))
            reshape127: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape126, R.shape([1, seq_len, 4096]))
            lv159 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight2, model_layers_31_self_attn_o_proj_q_scale2), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims126: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv159, axes=None)
            matmul126: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape127, permute_dims126, out_dtype="void")
            add62: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul126, add61)
            rms_norm63: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add62, model_layers_31_post_attention_layernorm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv160 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight2, model_layers_31_mlp_gate_up_proj_q_scale2), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims127: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv160, axes=None)
            matmul127: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm63, permute_dims127, out_dtype="void")
            split31: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul127, indices_or_sections=2, axis=-1)
            split_031: R.Tensor((1, seq_len, 14336), dtype="float16") = split31[0]
            split_131: R.Tensor((1, seq_len, 14336), dtype="float16") = split31[1]
            silu31: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_031)
            mul31: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu31, split_131)
            lv161 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight2, model_layers_31_mlp_down_proj_q_scale2), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims128: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv161, axes=None)
            matmul128: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul31, permute_dims128, out_dtype="void")
            add63: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul128, add62)
            rms_norm64: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add63, model_norm_weight2, axes=[-1], epsilon=1.0000000000000001e-05)
            lv162 = R.call_tir(cls.index, (rms_norm64,), out_sinfo=R.Tensor((1, 1, 4096), dtype="float16"))
            lv163 = R.call_tir(cls.dequantize, (lm_head_q_weight2, lm_head_q_scale2), out_sinfo=R.Tensor((vocab_size, 4096), dtype="float16"))
            permute_dims129: R.Tensor((4096, vocab_size), dtype="float16") = R.permute_dims(lv163, axes=None)
            matmul129: R.Tensor((1, 1, vocab_size), dtype="float16") = R.matmul(lv162, permute_dims129, out_dtype="void")
            astype1: R.Tensor((1, 1, vocab_size), dtype="float32") = R.astype(matmul129, dtype="float32")
            gv3: R.Tuple(R.Tensor((1, 1, vocab_size), dtype="float32"), R.Object) = astype1, paged_kv_cache
            R.output(gv3)
        return gv3

    @R.function
    def prefill_to_last_hidden_states(input_embed: R.Tensor((1, "seq_len", 4096), dtype="float16"), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((6144, 412), dtype="uint32"), R.Tensor((6144, 103), dtype="float16"), R.Tensor((4096, 412), dtype="uint32"), R.Tensor((4096, 103), dtype="float16"), R.Tensor((28672, 412), dtype="uint32"), R.Tensor((28672, 103), dtype="float16"), R.Tensor((4096, 1436), dtype="uint32"), R.Tensor((4096, 359), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor((4096,), dtype="float16"), R.Tensor(("vocab_size", 412), dtype="uint32"), R.Tensor(("vocab_size", 103), dtype="float16"))) -> R.Tuple(R.Tensor((1, "seq_len", 4096), dtype="float16"), R.Object):
        seq_len = T.int64()
        vocab_size = T.int64()
        R.func_attr({"num_input": 2, "pipeline_parallel_stages": 1, "relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            model_embed_tokens_q_weight4: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[0]
            model_embed_tokens_q_scale4: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[1]
            model_layers_0_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[2]
            model_layers_0_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[3]
            model_layers_0_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[4]
            model_layers_0_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[5]
            model_layers_0_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[6]
            model_layers_0_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[7]
            model_layers_0_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[8]
            model_layers_0_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[9]
            model_layers_0_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[10]
            model_layers_0_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[11]
            model_layers_1_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[12]
            model_layers_1_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[13]
            model_layers_1_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[14]
            model_layers_1_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[15]
            model_layers_1_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[16]
            model_layers_1_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[17]
            model_layers_1_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[18]
            model_layers_1_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[19]
            model_layers_1_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[20]
            model_layers_1_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[21]
            model_layers_2_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[22]
            model_layers_2_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[23]
            model_layers_2_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[24]
            model_layers_2_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[25]
            model_layers_2_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[26]
            model_layers_2_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[27]
            model_layers_2_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[28]
            model_layers_2_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[29]
            model_layers_2_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[30]
            model_layers_2_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[31]
            model_layers_3_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[32]
            model_layers_3_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[33]
            model_layers_3_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[34]
            model_layers_3_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[35]
            model_layers_3_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[36]
            model_layers_3_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[37]
            model_layers_3_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[38]
            model_layers_3_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[39]
            model_layers_3_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[40]
            model_layers_3_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[41]
            model_layers_4_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[42]
            model_layers_4_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[43]
            model_layers_4_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[44]
            model_layers_4_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[45]
            model_layers_4_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[46]
            model_layers_4_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[47]
            model_layers_4_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[48]
            model_layers_4_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[49]
            model_layers_4_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[50]
            model_layers_4_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[51]
            model_layers_5_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[52]
            model_layers_5_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[53]
            model_layers_5_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[54]
            model_layers_5_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[55]
            model_layers_5_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[56]
            model_layers_5_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[57]
            model_layers_5_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[58]
            model_layers_5_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[59]
            model_layers_5_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[60]
            model_layers_5_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[61]
            model_layers_6_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[62]
            model_layers_6_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[63]
            model_layers_6_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[64]
            model_layers_6_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[65]
            model_layers_6_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[66]
            model_layers_6_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[67]
            model_layers_6_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[68]
            model_layers_6_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[69]
            model_layers_6_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[70]
            model_layers_6_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[71]
            model_layers_7_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[72]
            model_layers_7_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[73]
            model_layers_7_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[74]
            model_layers_7_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[75]
            model_layers_7_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[76]
            model_layers_7_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[77]
            model_layers_7_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[78]
            model_layers_7_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[79]
            model_layers_7_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[80]
            model_layers_7_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[81]
            model_layers_8_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[82]
            model_layers_8_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[83]
            model_layers_8_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[84]
            model_layers_8_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[85]
            model_layers_8_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[86]
            model_layers_8_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[87]
            model_layers_8_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[88]
            model_layers_8_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[89]
            model_layers_8_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[90]
            model_layers_8_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[91]
            model_layers_9_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[92]
            model_layers_9_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[93]
            model_layers_9_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[94]
            model_layers_9_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[95]
            model_layers_9_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[96]
            model_layers_9_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[97]
            model_layers_9_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[98]
            model_layers_9_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[99]
            model_layers_9_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[100]
            model_layers_9_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[101]
            model_layers_10_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[102]
            model_layers_10_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[103]
            model_layers_10_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[104]
            model_layers_10_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[105]
            model_layers_10_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[106]
            model_layers_10_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[107]
            model_layers_10_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[108]
            model_layers_10_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[109]
            model_layers_10_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[110]
            model_layers_10_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[111]
            model_layers_11_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[112]
            model_layers_11_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[113]
            model_layers_11_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[114]
            model_layers_11_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[115]
            model_layers_11_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[116]
            model_layers_11_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[117]
            model_layers_11_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[118]
            model_layers_11_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[119]
            model_layers_11_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[120]
            model_layers_11_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[121]
            model_layers_12_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[122]
            model_layers_12_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[123]
            model_layers_12_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[124]
            model_layers_12_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[125]
            model_layers_12_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[126]
            model_layers_12_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[127]
            model_layers_12_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[128]
            model_layers_12_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[129]
            model_layers_12_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[130]
            model_layers_12_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[131]
            model_layers_13_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[132]
            model_layers_13_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[133]
            model_layers_13_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[134]
            model_layers_13_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[135]
            model_layers_13_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[136]
            model_layers_13_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[137]
            model_layers_13_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[138]
            model_layers_13_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[139]
            model_layers_13_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[140]
            model_layers_13_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[141]
            model_layers_14_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[142]
            model_layers_14_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[143]
            model_layers_14_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[144]
            model_layers_14_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[145]
            model_layers_14_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[146]
            model_layers_14_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[147]
            model_layers_14_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[148]
            model_layers_14_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[149]
            model_layers_14_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[150]
            model_layers_14_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[151]
            model_layers_15_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[152]
            model_layers_15_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[153]
            model_layers_15_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[154]
            model_layers_15_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[155]
            model_layers_15_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[156]
            model_layers_15_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[157]
            model_layers_15_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[158]
            model_layers_15_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[159]
            model_layers_15_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[160]
            model_layers_15_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[161]
            model_layers_16_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[162]
            model_layers_16_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[163]
            model_layers_16_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[164]
            model_layers_16_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[165]
            model_layers_16_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[166]
            model_layers_16_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[167]
            model_layers_16_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[168]
            model_layers_16_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[169]
            model_layers_16_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[170]
            model_layers_16_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[171]
            model_layers_17_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[172]
            model_layers_17_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[173]
            model_layers_17_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[174]
            model_layers_17_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[175]
            model_layers_17_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[176]
            model_layers_17_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[177]
            model_layers_17_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[178]
            model_layers_17_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[179]
            model_layers_17_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[180]
            model_layers_17_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[181]
            model_layers_18_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[182]
            model_layers_18_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[183]
            model_layers_18_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[184]
            model_layers_18_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[185]
            model_layers_18_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[186]
            model_layers_18_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[187]
            model_layers_18_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[188]
            model_layers_18_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[189]
            model_layers_18_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[190]
            model_layers_18_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[191]
            model_layers_19_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[192]
            model_layers_19_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[193]
            model_layers_19_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[194]
            model_layers_19_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[195]
            model_layers_19_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[196]
            model_layers_19_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[197]
            model_layers_19_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[198]
            model_layers_19_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[199]
            model_layers_19_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[200]
            model_layers_19_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[201]
            model_layers_20_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[202]
            model_layers_20_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[203]
            model_layers_20_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[204]
            model_layers_20_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[205]
            model_layers_20_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[206]
            model_layers_20_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[207]
            model_layers_20_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[208]
            model_layers_20_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[209]
            model_layers_20_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[210]
            model_layers_20_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[211]
            model_layers_21_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[212]
            model_layers_21_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[213]
            model_layers_21_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[214]
            model_layers_21_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[215]
            model_layers_21_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[216]
            model_layers_21_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[217]
            model_layers_21_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[218]
            model_layers_21_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[219]
            model_layers_21_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[220]
            model_layers_21_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[221]
            model_layers_22_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[222]
            model_layers_22_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[223]
            model_layers_22_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[224]
            model_layers_22_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[225]
            model_layers_22_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[226]
            model_layers_22_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[227]
            model_layers_22_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[228]
            model_layers_22_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[229]
            model_layers_22_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[230]
            model_layers_22_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[231]
            model_layers_23_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[232]
            model_layers_23_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[233]
            model_layers_23_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[234]
            model_layers_23_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[235]
            model_layers_23_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[236]
            model_layers_23_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[237]
            model_layers_23_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[238]
            model_layers_23_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[239]
            model_layers_23_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[240]
            model_layers_23_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[241]
            model_layers_24_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[242]
            model_layers_24_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[243]
            model_layers_24_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[244]
            model_layers_24_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[245]
            model_layers_24_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[246]
            model_layers_24_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[247]
            model_layers_24_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[248]
            model_layers_24_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[249]
            model_layers_24_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[250]
            model_layers_24_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[251]
            model_layers_25_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[252]
            model_layers_25_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[253]
            model_layers_25_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[254]
            model_layers_25_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[255]
            model_layers_25_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[256]
            model_layers_25_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[257]
            model_layers_25_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[258]
            model_layers_25_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[259]
            model_layers_25_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[260]
            model_layers_25_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[261]
            model_layers_26_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[262]
            model_layers_26_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[263]
            model_layers_26_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[264]
            model_layers_26_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[265]
            model_layers_26_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[266]
            model_layers_26_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[267]
            model_layers_26_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[268]
            model_layers_26_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[269]
            model_layers_26_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[270]
            model_layers_26_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[271]
            model_layers_27_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[272]
            model_layers_27_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[273]
            model_layers_27_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[274]
            model_layers_27_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[275]
            model_layers_27_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[276]
            model_layers_27_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[277]
            model_layers_27_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[278]
            model_layers_27_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[279]
            model_layers_27_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[280]
            model_layers_27_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[281]
            model_layers_28_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[282]
            model_layers_28_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[283]
            model_layers_28_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[284]
            model_layers_28_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[285]
            model_layers_28_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[286]
            model_layers_28_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[287]
            model_layers_28_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[288]
            model_layers_28_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[289]
            model_layers_28_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[290]
            model_layers_28_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[291]
            model_layers_29_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[292]
            model_layers_29_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[293]
            model_layers_29_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[294]
            model_layers_29_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[295]
            model_layers_29_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[296]
            model_layers_29_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[297]
            model_layers_29_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[298]
            model_layers_29_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[299]
            model_layers_29_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[300]
            model_layers_29_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[301]
            model_layers_30_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[302]
            model_layers_30_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[303]
            model_layers_30_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[304]
            model_layers_30_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[305]
            model_layers_30_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[306]
            model_layers_30_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[307]
            model_layers_30_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[308]
            model_layers_30_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[309]
            model_layers_30_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[310]
            model_layers_30_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[311]
            model_layers_31_self_attn_qkv_proj_q_weight4: R.Tensor((6144, 412), dtype="uint32") = packed_params[312]
            model_layers_31_self_attn_qkv_proj_q_scale4: R.Tensor((6144, 103), dtype="float16") = packed_params[313]
            model_layers_31_self_attn_o_proj_q_weight4: R.Tensor((4096, 412), dtype="uint32") = packed_params[314]
            model_layers_31_self_attn_o_proj_q_scale4: R.Tensor((4096, 103), dtype="float16") = packed_params[315]
            model_layers_31_mlp_gate_up_proj_q_weight4: R.Tensor((28672, 412), dtype="uint32") = packed_params[316]
            model_layers_31_mlp_gate_up_proj_q_scale4: R.Tensor((28672, 103), dtype="float16") = packed_params[317]
            model_layers_31_mlp_down_proj_q_weight4: R.Tensor((4096, 1436), dtype="uint32") = packed_params[318]
            model_layers_31_mlp_down_proj_q_scale4: R.Tensor((4096, 359), dtype="float16") = packed_params[319]
            model_layers_31_input_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[320]
            model_layers_31_post_attention_layernorm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[321]
            model_norm_weight4: R.Tensor((4096,), dtype="float16") = packed_params[322]
            lm_head_q_weight4: R.Tensor((vocab_size, 412), dtype="uint32") = packed_params[323]
            lm_head_q_scale4: R.Tensor((vocab_size, 103), dtype="float16") = packed_params[324]
            rms_norm130: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(input_embed, model_layers_0_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv325 = R.call_tir(cls.dequantize1, (model_layers_0_self_attn_qkv_proj_q_weight4, model_layers_0_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims259: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv325, axes=None)
            matmul259: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm130, permute_dims259, out_dtype="void")
            reshape256: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul259, R.shape([1, seq_len, 48, 128]))
            reshape257: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape256, R.shape([seq_len, 48, 128]))
            lv326 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(0), R.prim_value(T.float32(1.0)), reshape257), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape258: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv326, R.shape([1, seq_len, 32, 128]))
            reshape259: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape258, R.shape([1, seq_len, 4096]))
            lv327 = R.call_tir(cls.dequantize2, (model_layers_0_self_attn_o_proj_q_weight4, model_layers_0_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims260: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv327, axes=None)
            matmul260: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape259, permute_dims260, out_dtype="void")
            add128: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul260, input_embed)
            rms_norm131: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add128, model_layers_0_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv328 = R.call_tir(cls.dequantize3, (model_layers_0_mlp_gate_up_proj_q_weight4, model_layers_0_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims261: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv328, axes=None)
            matmul261: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm131, permute_dims261, out_dtype="void")
            split64: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul261, indices_or_sections=2, axis=-1)
            split_064: R.Tensor((1, seq_len, 14336), dtype="float16") = split64[0]
            split_164: R.Tensor((1, seq_len, 14336), dtype="float16") = split64[1]
            silu64: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_064)
            mul64: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu64, split_164)
            lv329 = R.call_tir(cls.dequantize4, (model_layers_0_mlp_down_proj_q_weight4, model_layers_0_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims262: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv329, axes=None)
            matmul262: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul64, permute_dims262, out_dtype="void")
            add129: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul262, add128)
            rms_norm132: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add129, model_layers_1_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv330 = R.call_tir(cls.dequantize1, (model_layers_1_self_attn_qkv_proj_q_weight4, model_layers_1_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims263: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv330, axes=None)
            matmul263: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm132, permute_dims263, out_dtype="void")
            reshape260: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul263, R.shape([1, seq_len, 48, 128]))
            reshape261: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape260, R.shape([seq_len, 48, 128]))
            lv331 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(1), R.prim_value(T.float32(1.0)), reshape261), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape262: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv331, R.shape([1, seq_len, 32, 128]))
            reshape263: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape262, R.shape([1, seq_len, 4096]))
            lv332 = R.call_tir(cls.dequantize2, (model_layers_1_self_attn_o_proj_q_weight4, model_layers_1_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims264: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv332, axes=None)
            matmul264: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape263, permute_dims264, out_dtype="void")
            add130: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul264, add129)
            rms_norm133: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add130, model_layers_1_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv333 = R.call_tir(cls.dequantize3, (model_layers_1_mlp_gate_up_proj_q_weight4, model_layers_1_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims265: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv333, axes=None)
            matmul265: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm133, permute_dims265, out_dtype="void")
            split65: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul265, indices_or_sections=2, axis=-1)
            split_065: R.Tensor((1, seq_len, 14336), dtype="float16") = split65[0]
            split_165: R.Tensor((1, seq_len, 14336), dtype="float16") = split65[1]
            silu65: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_065)
            mul65: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu65, split_165)
            lv334 = R.call_tir(cls.dequantize4, (model_layers_1_mlp_down_proj_q_weight4, model_layers_1_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims266: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv334, axes=None)
            matmul266: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul65, permute_dims266, out_dtype="void")
            add131: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul266, add130)
            rms_norm134: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add131, model_layers_2_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv335 = R.call_tir(cls.dequantize1, (model_layers_2_self_attn_qkv_proj_q_weight4, model_layers_2_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims267: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv335, axes=None)
            matmul267: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm134, permute_dims267, out_dtype="void")
            reshape264: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul267, R.shape([1, seq_len, 48, 128]))
            reshape265: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape264, R.shape([seq_len, 48, 128]))
            lv336 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(2), R.prim_value(T.float32(1.0)), reshape265), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape266: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv336, R.shape([1, seq_len, 32, 128]))
            reshape267: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape266, R.shape([1, seq_len, 4096]))
            lv337 = R.call_tir(cls.dequantize2, (model_layers_2_self_attn_o_proj_q_weight4, model_layers_2_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims268: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv337, axes=None)
            matmul268: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape267, permute_dims268, out_dtype="void")
            add132: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul268, add131)
            rms_norm135: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add132, model_layers_2_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv338 = R.call_tir(cls.dequantize3, (model_layers_2_mlp_gate_up_proj_q_weight4, model_layers_2_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims269: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv338, axes=None)
            matmul269: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm135, permute_dims269, out_dtype="void")
            split66: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul269, indices_or_sections=2, axis=-1)
            split_066: R.Tensor((1, seq_len, 14336), dtype="float16") = split66[0]
            split_166: R.Tensor((1, seq_len, 14336), dtype="float16") = split66[1]
            silu66: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_066)
            mul66: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu66, split_166)
            lv339 = R.call_tir(cls.dequantize4, (model_layers_2_mlp_down_proj_q_weight4, model_layers_2_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims270: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv339, axes=None)
            matmul270: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul66, permute_dims270, out_dtype="void")
            add133: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul270, add132)
            rms_norm136: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add133, model_layers_3_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv340 = R.call_tir(cls.dequantize1, (model_layers_3_self_attn_qkv_proj_q_weight4, model_layers_3_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims271: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv340, axes=None)
            matmul271: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm136, permute_dims271, out_dtype="void")
            reshape268: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul271, R.shape([1, seq_len, 48, 128]))
            reshape269: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape268, R.shape([seq_len, 48, 128]))
            lv341 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(3), R.prim_value(T.float32(1.0)), reshape269), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape270: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv341, R.shape([1, seq_len, 32, 128]))
            reshape271: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape270, R.shape([1, seq_len, 4096]))
            lv342 = R.call_tir(cls.dequantize2, (model_layers_3_self_attn_o_proj_q_weight4, model_layers_3_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims272: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv342, axes=None)
            matmul272: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape271, permute_dims272, out_dtype="void")
            add134: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul272, add133)
            rms_norm137: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add134, model_layers_3_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv343 = R.call_tir(cls.dequantize3, (model_layers_3_mlp_gate_up_proj_q_weight4, model_layers_3_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims273: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv343, axes=None)
            matmul273: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm137, permute_dims273, out_dtype="void")
            split67: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul273, indices_or_sections=2, axis=-1)
            split_067: R.Tensor((1, seq_len, 14336), dtype="float16") = split67[0]
            split_167: R.Tensor((1, seq_len, 14336), dtype="float16") = split67[1]
            silu67: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_067)
            mul67: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu67, split_167)
            lv344 = R.call_tir(cls.dequantize4, (model_layers_3_mlp_down_proj_q_weight4, model_layers_3_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims274: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv344, axes=None)
            matmul274: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul67, permute_dims274, out_dtype="void")
            add135: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul274, add134)
            rms_norm138: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add135, model_layers_4_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv345 = R.call_tir(cls.dequantize1, (model_layers_4_self_attn_qkv_proj_q_weight4, model_layers_4_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims275: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv345, axes=None)
            matmul275: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm138, permute_dims275, out_dtype="void")
            reshape272: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul275, R.shape([1, seq_len, 48, 128]))
            reshape273: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape272, R.shape([seq_len, 48, 128]))
            lv346 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(4), R.prim_value(T.float32(1.0)), reshape273), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape274: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv346, R.shape([1, seq_len, 32, 128]))
            reshape275: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape274, R.shape([1, seq_len, 4096]))
            lv347 = R.call_tir(cls.dequantize2, (model_layers_4_self_attn_o_proj_q_weight4, model_layers_4_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims276: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv347, axes=None)
            matmul276: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape275, permute_dims276, out_dtype="void")
            add136: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul276, add135)
            rms_norm139: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add136, model_layers_4_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv348 = R.call_tir(cls.dequantize3, (model_layers_4_mlp_gate_up_proj_q_weight4, model_layers_4_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims277: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv348, axes=None)
            matmul277: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm139, permute_dims277, out_dtype="void")
            split68: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul277, indices_or_sections=2, axis=-1)
            split_068: R.Tensor((1, seq_len, 14336), dtype="float16") = split68[0]
            split_168: R.Tensor((1, seq_len, 14336), dtype="float16") = split68[1]
            silu68: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_068)
            mul68: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu68, split_168)
            lv349 = R.call_tir(cls.dequantize4, (model_layers_4_mlp_down_proj_q_weight4, model_layers_4_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims278: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv349, axes=None)
            matmul278: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul68, permute_dims278, out_dtype="void")
            add137: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul278, add136)
            rms_norm140: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add137, model_layers_5_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv350 = R.call_tir(cls.dequantize1, (model_layers_5_self_attn_qkv_proj_q_weight4, model_layers_5_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims279: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv350, axes=None)
            matmul279: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm140, permute_dims279, out_dtype="void")
            reshape276: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul279, R.shape([1, seq_len, 48, 128]))
            reshape277: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape276, R.shape([seq_len, 48, 128]))
            lv351 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(5), R.prim_value(T.float32(1.0)), reshape277), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape278: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv351, R.shape([1, seq_len, 32, 128]))
            reshape279: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape278, R.shape([1, seq_len, 4096]))
            lv352 = R.call_tir(cls.dequantize2, (model_layers_5_self_attn_o_proj_q_weight4, model_layers_5_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims280: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv352, axes=None)
            matmul280: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape279, permute_dims280, out_dtype="void")
            add138: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul280, add137)
            rms_norm141: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add138, model_layers_5_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv353 = R.call_tir(cls.dequantize3, (model_layers_5_mlp_gate_up_proj_q_weight4, model_layers_5_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims281: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv353, axes=None)
            matmul281: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm141, permute_dims281, out_dtype="void")
            split69: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul281, indices_or_sections=2, axis=-1)
            split_069: R.Tensor((1, seq_len, 14336), dtype="float16") = split69[0]
            split_169: R.Tensor((1, seq_len, 14336), dtype="float16") = split69[1]
            silu69: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_069)
            mul69: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu69, split_169)
            lv354 = R.call_tir(cls.dequantize4, (model_layers_5_mlp_down_proj_q_weight4, model_layers_5_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims282: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv354, axes=None)
            matmul282: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul69, permute_dims282, out_dtype="void")
            add139: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul282, add138)
            rms_norm142: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add139, model_layers_6_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv355 = R.call_tir(cls.dequantize1, (model_layers_6_self_attn_qkv_proj_q_weight4, model_layers_6_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims283: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv355, axes=None)
            matmul283: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm142, permute_dims283, out_dtype="void")
            reshape280: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul283, R.shape([1, seq_len, 48, 128]))
            reshape281: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape280, R.shape([seq_len, 48, 128]))
            lv356 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(6), R.prim_value(T.float32(1.0)), reshape281), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape282: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv356, R.shape([1, seq_len, 32, 128]))
            reshape283: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape282, R.shape([1, seq_len, 4096]))
            lv357 = R.call_tir(cls.dequantize2, (model_layers_6_self_attn_o_proj_q_weight4, model_layers_6_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims284: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv357, axes=None)
            matmul284: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape283, permute_dims284, out_dtype="void")
            add140: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul284, add139)
            rms_norm143: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add140, model_layers_6_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv358 = R.call_tir(cls.dequantize3, (model_layers_6_mlp_gate_up_proj_q_weight4, model_layers_6_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims285: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv358, axes=None)
            matmul285: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm143, permute_dims285, out_dtype="void")
            split70: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul285, indices_or_sections=2, axis=-1)
            split_070: R.Tensor((1, seq_len, 14336), dtype="float16") = split70[0]
            split_170: R.Tensor((1, seq_len, 14336), dtype="float16") = split70[1]
            silu70: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_070)
            mul70: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu70, split_170)
            lv359 = R.call_tir(cls.dequantize4, (model_layers_6_mlp_down_proj_q_weight4, model_layers_6_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims286: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv359, axes=None)
            matmul286: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul70, permute_dims286, out_dtype="void")
            add141: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul286, add140)
            rms_norm144: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add141, model_layers_7_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv360 = R.call_tir(cls.dequantize1, (model_layers_7_self_attn_qkv_proj_q_weight4, model_layers_7_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims287: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv360, axes=None)
            matmul287: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm144, permute_dims287, out_dtype="void")
            reshape284: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul287, R.shape([1, seq_len, 48, 128]))
            reshape285: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape284, R.shape([seq_len, 48, 128]))
            lv361 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(7), R.prim_value(T.float32(1.0)), reshape285), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape286: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv361, R.shape([1, seq_len, 32, 128]))
            reshape287: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape286, R.shape([1, seq_len, 4096]))
            lv362 = R.call_tir(cls.dequantize2, (model_layers_7_self_attn_o_proj_q_weight4, model_layers_7_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims288: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv362, axes=None)
            matmul288: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape287, permute_dims288, out_dtype="void")
            add142: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul288, add141)
            rms_norm145: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add142, model_layers_7_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv363 = R.call_tir(cls.dequantize3, (model_layers_7_mlp_gate_up_proj_q_weight4, model_layers_7_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims289: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv363, axes=None)
            matmul289: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm145, permute_dims289, out_dtype="void")
            split71: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul289, indices_or_sections=2, axis=-1)
            split_071: R.Tensor((1, seq_len, 14336), dtype="float16") = split71[0]
            split_171: R.Tensor((1, seq_len, 14336), dtype="float16") = split71[1]
            silu71: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_071)
            mul71: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu71, split_171)
            lv364 = R.call_tir(cls.dequantize4, (model_layers_7_mlp_down_proj_q_weight4, model_layers_7_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims290: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv364, axes=None)
            matmul290: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul71, permute_dims290, out_dtype="void")
            add143: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul290, add142)
            rms_norm146: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add143, model_layers_8_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv365 = R.call_tir(cls.dequantize1, (model_layers_8_self_attn_qkv_proj_q_weight4, model_layers_8_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims291: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv365, axes=None)
            matmul291: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm146, permute_dims291, out_dtype="void")
            reshape288: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul291, R.shape([1, seq_len, 48, 128]))
            reshape289: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape288, R.shape([seq_len, 48, 128]))
            lv366 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(8), R.prim_value(T.float32(1.0)), reshape289), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape290: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv366, R.shape([1, seq_len, 32, 128]))
            reshape291: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape290, R.shape([1, seq_len, 4096]))
            lv367 = R.call_tir(cls.dequantize2, (model_layers_8_self_attn_o_proj_q_weight4, model_layers_8_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims292: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv367, axes=None)
            matmul292: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape291, permute_dims292, out_dtype="void")
            add144: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul292, add143)
            rms_norm147: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add144, model_layers_8_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv368 = R.call_tir(cls.dequantize3, (model_layers_8_mlp_gate_up_proj_q_weight4, model_layers_8_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims293: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv368, axes=None)
            matmul293: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm147, permute_dims293, out_dtype="void")
            split72: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul293, indices_or_sections=2, axis=-1)
            split_072: R.Tensor((1, seq_len, 14336), dtype="float16") = split72[0]
            split_172: R.Tensor((1, seq_len, 14336), dtype="float16") = split72[1]
            silu72: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_072)
            mul72: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu72, split_172)
            lv369 = R.call_tir(cls.dequantize4, (model_layers_8_mlp_down_proj_q_weight4, model_layers_8_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims294: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv369, axes=None)
            matmul294: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul72, permute_dims294, out_dtype="void")
            add145: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul294, add144)
            rms_norm148: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add145, model_layers_9_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv370 = R.call_tir(cls.dequantize1, (model_layers_9_self_attn_qkv_proj_q_weight4, model_layers_9_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims295: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv370, axes=None)
            matmul295: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm148, permute_dims295, out_dtype="void")
            reshape292: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul295, R.shape([1, seq_len, 48, 128]))
            reshape293: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape292, R.shape([seq_len, 48, 128]))
            lv371 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(9), R.prim_value(T.float32(1.0)), reshape293), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape294: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv371, R.shape([1, seq_len, 32, 128]))
            reshape295: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape294, R.shape([1, seq_len, 4096]))
            lv372 = R.call_tir(cls.dequantize2, (model_layers_9_self_attn_o_proj_q_weight4, model_layers_9_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims296: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv372, axes=None)
            matmul296: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape295, permute_dims296, out_dtype="void")
            add146: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul296, add145)
            rms_norm149: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add146, model_layers_9_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv373 = R.call_tir(cls.dequantize3, (model_layers_9_mlp_gate_up_proj_q_weight4, model_layers_9_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims297: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv373, axes=None)
            matmul297: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm149, permute_dims297, out_dtype="void")
            split73: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul297, indices_or_sections=2, axis=-1)
            split_073: R.Tensor((1, seq_len, 14336), dtype="float16") = split73[0]
            split_173: R.Tensor((1, seq_len, 14336), dtype="float16") = split73[1]
            silu73: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_073)
            mul73: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu73, split_173)
            lv374 = R.call_tir(cls.dequantize4, (model_layers_9_mlp_down_proj_q_weight4, model_layers_9_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims298: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv374, axes=None)
            matmul298: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul73, permute_dims298, out_dtype="void")
            add147: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul298, add146)
            rms_norm150: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add147, model_layers_10_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv375 = R.call_tir(cls.dequantize1, (model_layers_10_self_attn_qkv_proj_q_weight4, model_layers_10_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims299: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv375, axes=None)
            matmul299: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm150, permute_dims299, out_dtype="void")
            reshape296: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul299, R.shape([1, seq_len, 48, 128]))
            reshape297: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape296, R.shape([seq_len, 48, 128]))
            lv376 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(10), R.prim_value(T.float32(1.0)), reshape297), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape298: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv376, R.shape([1, seq_len, 32, 128]))
            reshape299: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape298, R.shape([1, seq_len, 4096]))
            lv377 = R.call_tir(cls.dequantize2, (model_layers_10_self_attn_o_proj_q_weight4, model_layers_10_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims300: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv377, axes=None)
            matmul300: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape299, permute_dims300, out_dtype="void")
            add148: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul300, add147)
            rms_norm151: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add148, model_layers_10_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv378 = R.call_tir(cls.dequantize3, (model_layers_10_mlp_gate_up_proj_q_weight4, model_layers_10_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims301: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv378, axes=None)
            matmul301: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm151, permute_dims301, out_dtype="void")
            split74: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul301, indices_or_sections=2, axis=-1)
            split_074: R.Tensor((1, seq_len, 14336), dtype="float16") = split74[0]
            split_174: R.Tensor((1, seq_len, 14336), dtype="float16") = split74[1]
            silu74: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_074)
            mul74: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu74, split_174)
            lv379 = R.call_tir(cls.dequantize4, (model_layers_10_mlp_down_proj_q_weight4, model_layers_10_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims302: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv379, axes=None)
            matmul302: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul74, permute_dims302, out_dtype="void")
            add149: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul302, add148)
            rms_norm152: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add149, model_layers_11_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv380 = R.call_tir(cls.dequantize1, (model_layers_11_self_attn_qkv_proj_q_weight4, model_layers_11_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims303: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv380, axes=None)
            matmul303: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm152, permute_dims303, out_dtype="void")
            reshape300: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul303, R.shape([1, seq_len, 48, 128]))
            reshape301: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape300, R.shape([seq_len, 48, 128]))
            lv381 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(11), R.prim_value(T.float32(1.0)), reshape301), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape302: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv381, R.shape([1, seq_len, 32, 128]))
            reshape303: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape302, R.shape([1, seq_len, 4096]))
            lv382 = R.call_tir(cls.dequantize2, (model_layers_11_self_attn_o_proj_q_weight4, model_layers_11_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims304: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv382, axes=None)
            matmul304: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape303, permute_dims304, out_dtype="void")
            add150: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul304, add149)
            rms_norm153: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add150, model_layers_11_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv383 = R.call_tir(cls.dequantize3, (model_layers_11_mlp_gate_up_proj_q_weight4, model_layers_11_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims305: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv383, axes=None)
            matmul305: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm153, permute_dims305, out_dtype="void")
            split75: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul305, indices_or_sections=2, axis=-1)
            split_075: R.Tensor((1, seq_len, 14336), dtype="float16") = split75[0]
            split_175: R.Tensor((1, seq_len, 14336), dtype="float16") = split75[1]
            silu75: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_075)
            mul75: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu75, split_175)
            lv384 = R.call_tir(cls.dequantize4, (model_layers_11_mlp_down_proj_q_weight4, model_layers_11_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims306: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv384, axes=None)
            matmul306: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul75, permute_dims306, out_dtype="void")
            add151: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul306, add150)
            rms_norm154: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add151, model_layers_12_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv385 = R.call_tir(cls.dequantize1, (model_layers_12_self_attn_qkv_proj_q_weight4, model_layers_12_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims307: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv385, axes=None)
            matmul307: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm154, permute_dims307, out_dtype="void")
            reshape304: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul307, R.shape([1, seq_len, 48, 128]))
            reshape305: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape304, R.shape([seq_len, 48, 128]))
            lv386 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(12), R.prim_value(T.float32(1.0)), reshape305), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape306: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv386, R.shape([1, seq_len, 32, 128]))
            reshape307: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape306, R.shape([1, seq_len, 4096]))
            lv387 = R.call_tir(cls.dequantize2, (model_layers_12_self_attn_o_proj_q_weight4, model_layers_12_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims308: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv387, axes=None)
            matmul308: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape307, permute_dims308, out_dtype="void")
            add152: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul308, add151)
            rms_norm155: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add152, model_layers_12_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv388 = R.call_tir(cls.dequantize3, (model_layers_12_mlp_gate_up_proj_q_weight4, model_layers_12_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims309: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv388, axes=None)
            matmul309: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm155, permute_dims309, out_dtype="void")
            split76: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul309, indices_or_sections=2, axis=-1)
            split_076: R.Tensor((1, seq_len, 14336), dtype="float16") = split76[0]
            split_176: R.Tensor((1, seq_len, 14336), dtype="float16") = split76[1]
            silu76: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_076)
            mul76: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu76, split_176)
            lv389 = R.call_tir(cls.dequantize4, (model_layers_12_mlp_down_proj_q_weight4, model_layers_12_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims310: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv389, axes=None)
            matmul310: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul76, permute_dims310, out_dtype="void")
            add153: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul310, add152)
            rms_norm156: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add153, model_layers_13_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv390 = R.call_tir(cls.dequantize1, (model_layers_13_self_attn_qkv_proj_q_weight4, model_layers_13_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims311: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv390, axes=None)
            matmul311: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm156, permute_dims311, out_dtype="void")
            reshape308: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul311, R.shape([1, seq_len, 48, 128]))
            reshape309: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape308, R.shape([seq_len, 48, 128]))
            lv391 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(13), R.prim_value(T.float32(1.0)), reshape309), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape310: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv391, R.shape([1, seq_len, 32, 128]))
            reshape311: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape310, R.shape([1, seq_len, 4096]))
            lv392 = R.call_tir(cls.dequantize2, (model_layers_13_self_attn_o_proj_q_weight4, model_layers_13_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims312: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv392, axes=None)
            matmul312: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape311, permute_dims312, out_dtype="void")
            add154: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul312, add153)
            rms_norm157: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add154, model_layers_13_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv393 = R.call_tir(cls.dequantize3, (model_layers_13_mlp_gate_up_proj_q_weight4, model_layers_13_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims313: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv393, axes=None)
            matmul313: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm157, permute_dims313, out_dtype="void")
            split77: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul313, indices_or_sections=2, axis=-1)
            split_077: R.Tensor((1, seq_len, 14336), dtype="float16") = split77[0]
            split_177: R.Tensor((1, seq_len, 14336), dtype="float16") = split77[1]
            silu77: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_077)
            mul77: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu77, split_177)
            lv394 = R.call_tir(cls.dequantize4, (model_layers_13_mlp_down_proj_q_weight4, model_layers_13_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims314: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv394, axes=None)
            matmul314: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul77, permute_dims314, out_dtype="void")
            add155: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul314, add154)
            rms_norm158: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add155, model_layers_14_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv395 = R.call_tir(cls.dequantize1, (model_layers_14_self_attn_qkv_proj_q_weight4, model_layers_14_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims315: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv395, axes=None)
            matmul315: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm158, permute_dims315, out_dtype="void")
            reshape312: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul315, R.shape([1, seq_len, 48, 128]))
            reshape313: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape312, R.shape([seq_len, 48, 128]))
            lv396 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(14), R.prim_value(T.float32(1.0)), reshape313), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape314: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv396, R.shape([1, seq_len, 32, 128]))
            reshape315: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape314, R.shape([1, seq_len, 4096]))
            lv397 = R.call_tir(cls.dequantize2, (model_layers_14_self_attn_o_proj_q_weight4, model_layers_14_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims316: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv397, axes=None)
            matmul316: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape315, permute_dims316, out_dtype="void")
            add156: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul316, add155)
            rms_norm159: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add156, model_layers_14_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv398 = R.call_tir(cls.dequantize3, (model_layers_14_mlp_gate_up_proj_q_weight4, model_layers_14_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims317: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv398, axes=None)
            matmul317: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm159, permute_dims317, out_dtype="void")
            split78: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul317, indices_or_sections=2, axis=-1)
            split_078: R.Tensor((1, seq_len, 14336), dtype="float16") = split78[0]
            split_178: R.Tensor((1, seq_len, 14336), dtype="float16") = split78[1]
            silu78: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_078)
            mul78: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu78, split_178)
            lv399 = R.call_tir(cls.dequantize4, (model_layers_14_mlp_down_proj_q_weight4, model_layers_14_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims318: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv399, axes=None)
            matmul318: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul78, permute_dims318, out_dtype="void")
            add157: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul318, add156)
            rms_norm160: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add157, model_layers_15_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv400 = R.call_tir(cls.dequantize1, (model_layers_15_self_attn_qkv_proj_q_weight4, model_layers_15_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims319: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv400, axes=None)
            matmul319: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm160, permute_dims319, out_dtype="void")
            reshape316: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul319, R.shape([1, seq_len, 48, 128]))
            reshape317: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape316, R.shape([seq_len, 48, 128]))
            lv401 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(15), R.prim_value(T.float32(1.0)), reshape317), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape318: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv401, R.shape([1, seq_len, 32, 128]))
            reshape319: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape318, R.shape([1, seq_len, 4096]))
            lv402 = R.call_tir(cls.dequantize2, (model_layers_15_self_attn_o_proj_q_weight4, model_layers_15_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims320: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv402, axes=None)
            matmul320: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape319, permute_dims320, out_dtype="void")
            add158: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul320, add157)
            rms_norm161: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add158, model_layers_15_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv403 = R.call_tir(cls.dequantize3, (model_layers_15_mlp_gate_up_proj_q_weight4, model_layers_15_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims321: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv403, axes=None)
            matmul321: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm161, permute_dims321, out_dtype="void")
            split79: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul321, indices_or_sections=2, axis=-1)
            split_079: R.Tensor((1, seq_len, 14336), dtype="float16") = split79[0]
            split_179: R.Tensor((1, seq_len, 14336), dtype="float16") = split79[1]
            silu79: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_079)
            mul79: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu79, split_179)
            lv404 = R.call_tir(cls.dequantize4, (model_layers_15_mlp_down_proj_q_weight4, model_layers_15_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims322: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv404, axes=None)
            matmul322: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul79, permute_dims322, out_dtype="void")
            add159: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul322, add158)
            rms_norm162: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add159, model_layers_16_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv405 = R.call_tir(cls.dequantize1, (model_layers_16_self_attn_qkv_proj_q_weight4, model_layers_16_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims323: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv405, axes=None)
            matmul323: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm162, permute_dims323, out_dtype="void")
            reshape320: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul323, R.shape([1, seq_len, 48, 128]))
            reshape321: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape320, R.shape([seq_len, 48, 128]))
            lv406 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(16), R.prim_value(T.float32(1.0)), reshape321), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape322: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv406, R.shape([1, seq_len, 32, 128]))
            reshape323: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape322, R.shape([1, seq_len, 4096]))
            lv407 = R.call_tir(cls.dequantize2, (model_layers_16_self_attn_o_proj_q_weight4, model_layers_16_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims324: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv407, axes=None)
            matmul324: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape323, permute_dims324, out_dtype="void")
            add160: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul324, add159)
            rms_norm163: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add160, model_layers_16_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv408 = R.call_tir(cls.dequantize3, (model_layers_16_mlp_gate_up_proj_q_weight4, model_layers_16_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims325: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv408, axes=None)
            matmul325: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm163, permute_dims325, out_dtype="void")
            split80: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul325, indices_or_sections=2, axis=-1)
            split_080: R.Tensor((1, seq_len, 14336), dtype="float16") = split80[0]
            split_180: R.Tensor((1, seq_len, 14336), dtype="float16") = split80[1]
            silu80: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_080)
            mul80: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu80, split_180)
            lv409 = R.call_tir(cls.dequantize4, (model_layers_16_mlp_down_proj_q_weight4, model_layers_16_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims326: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv409, axes=None)
            matmul326: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul80, permute_dims326, out_dtype="void")
            add161: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul326, add160)
            rms_norm164: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add161, model_layers_17_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv410 = R.call_tir(cls.dequantize1, (model_layers_17_self_attn_qkv_proj_q_weight4, model_layers_17_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims327: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv410, axes=None)
            matmul327: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm164, permute_dims327, out_dtype="void")
            reshape324: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul327, R.shape([1, seq_len, 48, 128]))
            reshape325: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape324, R.shape([seq_len, 48, 128]))
            lv411 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(17), R.prim_value(T.float32(1.0)), reshape325), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape326: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv411, R.shape([1, seq_len, 32, 128]))
            reshape327: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape326, R.shape([1, seq_len, 4096]))
            lv412 = R.call_tir(cls.dequantize2, (model_layers_17_self_attn_o_proj_q_weight4, model_layers_17_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims328: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv412, axes=None)
            matmul328: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape327, permute_dims328, out_dtype="void")
            add162: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul328, add161)
            rms_norm165: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add162, model_layers_17_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv413 = R.call_tir(cls.dequantize3, (model_layers_17_mlp_gate_up_proj_q_weight4, model_layers_17_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims329: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv413, axes=None)
            matmul329: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm165, permute_dims329, out_dtype="void")
            split81: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul329, indices_or_sections=2, axis=-1)
            split_081: R.Tensor((1, seq_len, 14336), dtype="float16") = split81[0]
            split_181: R.Tensor((1, seq_len, 14336), dtype="float16") = split81[1]
            silu81: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_081)
            mul81: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu81, split_181)
            lv414 = R.call_tir(cls.dequantize4, (model_layers_17_mlp_down_proj_q_weight4, model_layers_17_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims330: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv414, axes=None)
            matmul330: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul81, permute_dims330, out_dtype="void")
            add163: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul330, add162)
            rms_norm166: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add163, model_layers_18_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv415 = R.call_tir(cls.dequantize1, (model_layers_18_self_attn_qkv_proj_q_weight4, model_layers_18_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims331: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv415, axes=None)
            matmul331: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm166, permute_dims331, out_dtype="void")
            reshape328: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul331, R.shape([1, seq_len, 48, 128]))
            reshape329: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape328, R.shape([seq_len, 48, 128]))
            lv416 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(18), R.prim_value(T.float32(1.0)), reshape329), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape330: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv416, R.shape([1, seq_len, 32, 128]))
            reshape331: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape330, R.shape([1, seq_len, 4096]))
            lv417 = R.call_tir(cls.dequantize2, (model_layers_18_self_attn_o_proj_q_weight4, model_layers_18_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims332: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv417, axes=None)
            matmul332: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape331, permute_dims332, out_dtype="void")
            add164: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul332, add163)
            rms_norm167: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add164, model_layers_18_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv418 = R.call_tir(cls.dequantize3, (model_layers_18_mlp_gate_up_proj_q_weight4, model_layers_18_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims333: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv418, axes=None)
            matmul333: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm167, permute_dims333, out_dtype="void")
            split82: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul333, indices_or_sections=2, axis=-1)
            split_082: R.Tensor((1, seq_len, 14336), dtype="float16") = split82[0]
            split_182: R.Tensor((1, seq_len, 14336), dtype="float16") = split82[1]
            silu82: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_082)
            mul82: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu82, split_182)
            lv419 = R.call_tir(cls.dequantize4, (model_layers_18_mlp_down_proj_q_weight4, model_layers_18_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims334: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv419, axes=None)
            matmul334: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul82, permute_dims334, out_dtype="void")
            add165: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul334, add164)
            rms_norm168: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add165, model_layers_19_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv420 = R.call_tir(cls.dequantize1, (model_layers_19_self_attn_qkv_proj_q_weight4, model_layers_19_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims335: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv420, axes=None)
            matmul335: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm168, permute_dims335, out_dtype="void")
            reshape332: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul335, R.shape([1, seq_len, 48, 128]))
            reshape333: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape332, R.shape([seq_len, 48, 128]))
            lv421 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(19), R.prim_value(T.float32(1.0)), reshape333), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape334: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv421, R.shape([1, seq_len, 32, 128]))
            reshape335: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape334, R.shape([1, seq_len, 4096]))
            lv422 = R.call_tir(cls.dequantize2, (model_layers_19_self_attn_o_proj_q_weight4, model_layers_19_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims336: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv422, axes=None)
            matmul336: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape335, permute_dims336, out_dtype="void")
            add166: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul336, add165)
            rms_norm169: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add166, model_layers_19_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv423 = R.call_tir(cls.dequantize3, (model_layers_19_mlp_gate_up_proj_q_weight4, model_layers_19_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims337: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv423, axes=None)
            matmul337: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm169, permute_dims337, out_dtype="void")
            split83: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul337, indices_or_sections=2, axis=-1)
            split_083: R.Tensor((1, seq_len, 14336), dtype="float16") = split83[0]
            split_183: R.Tensor((1, seq_len, 14336), dtype="float16") = split83[1]
            silu83: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_083)
            mul83: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu83, split_183)
            lv424 = R.call_tir(cls.dequantize4, (model_layers_19_mlp_down_proj_q_weight4, model_layers_19_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims338: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv424, axes=None)
            matmul338: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul83, permute_dims338, out_dtype="void")
            add167: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul338, add166)
            rms_norm170: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add167, model_layers_20_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv425 = R.call_tir(cls.dequantize1, (model_layers_20_self_attn_qkv_proj_q_weight4, model_layers_20_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims339: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv425, axes=None)
            matmul339: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm170, permute_dims339, out_dtype="void")
            reshape336: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul339, R.shape([1, seq_len, 48, 128]))
            reshape337: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape336, R.shape([seq_len, 48, 128]))
            lv426 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(20), R.prim_value(T.float32(1.0)), reshape337), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape338: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv426, R.shape([1, seq_len, 32, 128]))
            reshape339: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape338, R.shape([1, seq_len, 4096]))
            lv427 = R.call_tir(cls.dequantize2, (model_layers_20_self_attn_o_proj_q_weight4, model_layers_20_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims340: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv427, axes=None)
            matmul340: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape339, permute_dims340, out_dtype="void")
            add168: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul340, add167)
            rms_norm171: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add168, model_layers_20_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv428 = R.call_tir(cls.dequantize3, (model_layers_20_mlp_gate_up_proj_q_weight4, model_layers_20_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims341: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv428, axes=None)
            matmul341: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm171, permute_dims341, out_dtype="void")
            split84: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul341, indices_or_sections=2, axis=-1)
            split_084: R.Tensor((1, seq_len, 14336), dtype="float16") = split84[0]
            split_184: R.Tensor((1, seq_len, 14336), dtype="float16") = split84[1]
            silu84: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_084)
            mul84: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu84, split_184)
            lv429 = R.call_tir(cls.dequantize4, (model_layers_20_mlp_down_proj_q_weight4, model_layers_20_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims342: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv429, axes=None)
            matmul342: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul84, permute_dims342, out_dtype="void")
            add169: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul342, add168)
            rms_norm172: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add169, model_layers_21_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv430 = R.call_tir(cls.dequantize1, (model_layers_21_self_attn_qkv_proj_q_weight4, model_layers_21_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims343: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv430, axes=None)
            matmul343: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm172, permute_dims343, out_dtype="void")
            reshape340: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul343, R.shape([1, seq_len, 48, 128]))
            reshape341: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape340, R.shape([seq_len, 48, 128]))
            lv431 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(21), R.prim_value(T.float32(1.0)), reshape341), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape342: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv431, R.shape([1, seq_len, 32, 128]))
            reshape343: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape342, R.shape([1, seq_len, 4096]))
            lv432 = R.call_tir(cls.dequantize2, (model_layers_21_self_attn_o_proj_q_weight4, model_layers_21_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims344: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv432, axes=None)
            matmul344: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape343, permute_dims344, out_dtype="void")
            add170: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul344, add169)
            rms_norm173: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add170, model_layers_21_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv433 = R.call_tir(cls.dequantize3, (model_layers_21_mlp_gate_up_proj_q_weight4, model_layers_21_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims345: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv433, axes=None)
            matmul345: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm173, permute_dims345, out_dtype="void")
            split85: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul345, indices_or_sections=2, axis=-1)
            split_085: R.Tensor((1, seq_len, 14336), dtype="float16") = split85[0]
            split_185: R.Tensor((1, seq_len, 14336), dtype="float16") = split85[1]
            silu85: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_085)
            mul85: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu85, split_185)
            lv434 = R.call_tir(cls.dequantize4, (model_layers_21_mlp_down_proj_q_weight4, model_layers_21_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims346: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv434, axes=None)
            matmul346: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul85, permute_dims346, out_dtype="void")
            add171: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul346, add170)
            rms_norm174: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add171, model_layers_22_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv435 = R.call_tir(cls.dequantize1, (model_layers_22_self_attn_qkv_proj_q_weight4, model_layers_22_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims347: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv435, axes=None)
            matmul347: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm174, permute_dims347, out_dtype="void")
            reshape344: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul347, R.shape([1, seq_len, 48, 128]))
            reshape345: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape344, R.shape([seq_len, 48, 128]))
            lv436 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(22), R.prim_value(T.float32(1.0)), reshape345), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape346: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv436, R.shape([1, seq_len, 32, 128]))
            reshape347: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape346, R.shape([1, seq_len, 4096]))
            lv437 = R.call_tir(cls.dequantize2, (model_layers_22_self_attn_o_proj_q_weight4, model_layers_22_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims348: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv437, axes=None)
            matmul348: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape347, permute_dims348, out_dtype="void")
            add172: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul348, add171)
            rms_norm175: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add172, model_layers_22_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv438 = R.call_tir(cls.dequantize3, (model_layers_22_mlp_gate_up_proj_q_weight4, model_layers_22_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims349: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv438, axes=None)
            matmul349: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm175, permute_dims349, out_dtype="void")
            split86: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul349, indices_or_sections=2, axis=-1)
            split_086: R.Tensor((1, seq_len, 14336), dtype="float16") = split86[0]
            split_186: R.Tensor((1, seq_len, 14336), dtype="float16") = split86[1]
            silu86: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_086)
            mul86: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu86, split_186)
            lv439 = R.call_tir(cls.dequantize4, (model_layers_22_mlp_down_proj_q_weight4, model_layers_22_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims350: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv439, axes=None)
            matmul350: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul86, permute_dims350, out_dtype="void")
            add173: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul350, add172)
            rms_norm176: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add173, model_layers_23_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv440 = R.call_tir(cls.dequantize1, (model_layers_23_self_attn_qkv_proj_q_weight4, model_layers_23_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims351: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv440, axes=None)
            matmul351: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm176, permute_dims351, out_dtype="void")
            reshape348: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul351, R.shape([1, seq_len, 48, 128]))
            reshape349: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape348, R.shape([seq_len, 48, 128]))
            lv441 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(23), R.prim_value(T.float32(1.0)), reshape349), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape350: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv441, R.shape([1, seq_len, 32, 128]))
            reshape351: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape350, R.shape([1, seq_len, 4096]))
            lv442 = R.call_tir(cls.dequantize2, (model_layers_23_self_attn_o_proj_q_weight4, model_layers_23_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims352: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv442, axes=None)
            matmul352: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape351, permute_dims352, out_dtype="void")
            add174: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul352, add173)
            rms_norm177: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add174, model_layers_23_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv443 = R.call_tir(cls.dequantize3, (model_layers_23_mlp_gate_up_proj_q_weight4, model_layers_23_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims353: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv443, axes=None)
            matmul353: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm177, permute_dims353, out_dtype="void")
            split87: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul353, indices_or_sections=2, axis=-1)
            split_087: R.Tensor((1, seq_len, 14336), dtype="float16") = split87[0]
            split_187: R.Tensor((1, seq_len, 14336), dtype="float16") = split87[1]
            silu87: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_087)
            mul87: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu87, split_187)
            lv444 = R.call_tir(cls.dequantize4, (model_layers_23_mlp_down_proj_q_weight4, model_layers_23_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims354: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv444, axes=None)
            matmul354: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul87, permute_dims354, out_dtype="void")
            add175: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul354, add174)
            rms_norm178: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add175, model_layers_24_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv445 = R.call_tir(cls.dequantize1, (model_layers_24_self_attn_qkv_proj_q_weight4, model_layers_24_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims355: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv445, axes=None)
            matmul355: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm178, permute_dims355, out_dtype="void")
            reshape352: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul355, R.shape([1, seq_len, 48, 128]))
            reshape353: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape352, R.shape([seq_len, 48, 128]))
            lv446 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(24), R.prim_value(T.float32(1.0)), reshape353), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape354: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv446, R.shape([1, seq_len, 32, 128]))
            reshape355: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape354, R.shape([1, seq_len, 4096]))
            lv447 = R.call_tir(cls.dequantize2, (model_layers_24_self_attn_o_proj_q_weight4, model_layers_24_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims356: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv447, axes=None)
            matmul356: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape355, permute_dims356, out_dtype="void")
            add176: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul356, add175)
            rms_norm179: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add176, model_layers_24_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv448 = R.call_tir(cls.dequantize3, (model_layers_24_mlp_gate_up_proj_q_weight4, model_layers_24_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims357: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv448, axes=None)
            matmul357: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm179, permute_dims357, out_dtype="void")
            split88: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul357, indices_or_sections=2, axis=-1)
            split_088: R.Tensor((1, seq_len, 14336), dtype="float16") = split88[0]
            split_188: R.Tensor((1, seq_len, 14336), dtype="float16") = split88[1]
            silu88: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_088)
            mul88: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu88, split_188)
            lv449 = R.call_tir(cls.dequantize4, (model_layers_24_mlp_down_proj_q_weight4, model_layers_24_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims358: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv449, axes=None)
            matmul358: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul88, permute_dims358, out_dtype="void")
            add177: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul358, add176)
            rms_norm180: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add177, model_layers_25_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv450 = R.call_tir(cls.dequantize1, (model_layers_25_self_attn_qkv_proj_q_weight4, model_layers_25_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims359: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv450, axes=None)
            matmul359: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm180, permute_dims359, out_dtype="void")
            reshape356: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul359, R.shape([1, seq_len, 48, 128]))
            reshape357: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape356, R.shape([seq_len, 48, 128]))
            lv451 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(25), R.prim_value(T.float32(1.0)), reshape357), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape358: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv451, R.shape([1, seq_len, 32, 128]))
            reshape359: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape358, R.shape([1, seq_len, 4096]))
            lv452 = R.call_tir(cls.dequantize2, (model_layers_25_self_attn_o_proj_q_weight4, model_layers_25_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims360: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv452, axes=None)
            matmul360: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape359, permute_dims360, out_dtype="void")
            add178: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul360, add177)
            rms_norm181: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add178, model_layers_25_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv453 = R.call_tir(cls.dequantize3, (model_layers_25_mlp_gate_up_proj_q_weight4, model_layers_25_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims361: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv453, axes=None)
            matmul361: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm181, permute_dims361, out_dtype="void")
            split89: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul361, indices_or_sections=2, axis=-1)
            split_089: R.Tensor((1, seq_len, 14336), dtype="float16") = split89[0]
            split_189: R.Tensor((1, seq_len, 14336), dtype="float16") = split89[1]
            silu89: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_089)
            mul89: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu89, split_189)
            lv454 = R.call_tir(cls.dequantize4, (model_layers_25_mlp_down_proj_q_weight4, model_layers_25_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims362: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv454, axes=None)
            matmul362: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul89, permute_dims362, out_dtype="void")
            add179: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul362, add178)
            rms_norm182: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add179, model_layers_26_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv455 = R.call_tir(cls.dequantize1, (model_layers_26_self_attn_qkv_proj_q_weight4, model_layers_26_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims363: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv455, axes=None)
            matmul363: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm182, permute_dims363, out_dtype="void")
            reshape360: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul363, R.shape([1, seq_len, 48, 128]))
            reshape361: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape360, R.shape([seq_len, 48, 128]))
            lv456 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(26), R.prim_value(T.float32(1.0)), reshape361), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape362: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv456, R.shape([1, seq_len, 32, 128]))
            reshape363: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape362, R.shape([1, seq_len, 4096]))
            lv457 = R.call_tir(cls.dequantize2, (model_layers_26_self_attn_o_proj_q_weight4, model_layers_26_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims364: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv457, axes=None)
            matmul364: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape363, permute_dims364, out_dtype="void")
            add180: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul364, add179)
            rms_norm183: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add180, model_layers_26_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv458 = R.call_tir(cls.dequantize3, (model_layers_26_mlp_gate_up_proj_q_weight4, model_layers_26_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims365: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv458, axes=None)
            matmul365: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm183, permute_dims365, out_dtype="void")
            split90: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul365, indices_or_sections=2, axis=-1)
            split_090: R.Tensor((1, seq_len, 14336), dtype="float16") = split90[0]
            split_190: R.Tensor((1, seq_len, 14336), dtype="float16") = split90[1]
            silu90: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_090)
            mul90: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu90, split_190)
            lv459 = R.call_tir(cls.dequantize4, (model_layers_26_mlp_down_proj_q_weight4, model_layers_26_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims366: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv459, axes=None)
            matmul366: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul90, permute_dims366, out_dtype="void")
            add181: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul366, add180)
            rms_norm184: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add181, model_layers_27_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv460 = R.call_tir(cls.dequantize1, (model_layers_27_self_attn_qkv_proj_q_weight4, model_layers_27_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims367: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv460, axes=None)
            matmul367: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm184, permute_dims367, out_dtype="void")
            reshape364: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul367, R.shape([1, seq_len, 48, 128]))
            reshape365: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape364, R.shape([seq_len, 48, 128]))
            lv461 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(27), R.prim_value(T.float32(1.0)), reshape365), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape366: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv461, R.shape([1, seq_len, 32, 128]))
            reshape367: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape366, R.shape([1, seq_len, 4096]))
            lv462 = R.call_tir(cls.dequantize2, (model_layers_27_self_attn_o_proj_q_weight4, model_layers_27_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims368: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv462, axes=None)
            matmul368: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape367, permute_dims368, out_dtype="void")
            add182: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul368, add181)
            rms_norm185: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add182, model_layers_27_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv463 = R.call_tir(cls.dequantize3, (model_layers_27_mlp_gate_up_proj_q_weight4, model_layers_27_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims369: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv463, axes=None)
            matmul369: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm185, permute_dims369, out_dtype="void")
            split91: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul369, indices_or_sections=2, axis=-1)
            split_091: R.Tensor((1, seq_len, 14336), dtype="float16") = split91[0]
            split_191: R.Tensor((1, seq_len, 14336), dtype="float16") = split91[1]
            silu91: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_091)
            mul91: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu91, split_191)
            lv464 = R.call_tir(cls.dequantize4, (model_layers_27_mlp_down_proj_q_weight4, model_layers_27_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims370: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv464, axes=None)
            matmul370: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul91, permute_dims370, out_dtype="void")
            add183: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul370, add182)
            rms_norm186: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add183, model_layers_28_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv465 = R.call_tir(cls.dequantize1, (model_layers_28_self_attn_qkv_proj_q_weight4, model_layers_28_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims371: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv465, axes=None)
            matmul371: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm186, permute_dims371, out_dtype="void")
            reshape368: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul371, R.shape([1, seq_len, 48, 128]))
            reshape369: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape368, R.shape([seq_len, 48, 128]))
            lv466 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(28), R.prim_value(T.float32(1.0)), reshape369), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape370: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv466, R.shape([1, seq_len, 32, 128]))
            reshape371: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape370, R.shape([1, seq_len, 4096]))
            lv467 = R.call_tir(cls.dequantize2, (model_layers_28_self_attn_o_proj_q_weight4, model_layers_28_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims372: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv467, axes=None)
            matmul372: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape371, permute_dims372, out_dtype="void")
            add184: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul372, add183)
            rms_norm187: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add184, model_layers_28_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv468 = R.call_tir(cls.dequantize3, (model_layers_28_mlp_gate_up_proj_q_weight4, model_layers_28_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims373: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv468, axes=None)
            matmul373: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm187, permute_dims373, out_dtype="void")
            split92: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul373, indices_or_sections=2, axis=-1)
            split_092: R.Tensor((1, seq_len, 14336), dtype="float16") = split92[0]
            split_192: R.Tensor((1, seq_len, 14336), dtype="float16") = split92[1]
            silu92: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_092)
            mul92: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu92, split_192)
            lv469 = R.call_tir(cls.dequantize4, (model_layers_28_mlp_down_proj_q_weight4, model_layers_28_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims374: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv469, axes=None)
            matmul374: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul92, permute_dims374, out_dtype="void")
            add185: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul374, add184)
            rms_norm188: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add185, model_layers_29_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv470 = R.call_tir(cls.dequantize1, (model_layers_29_self_attn_qkv_proj_q_weight4, model_layers_29_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims375: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv470, axes=None)
            matmul375: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm188, permute_dims375, out_dtype="void")
            reshape372: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul375, R.shape([1, seq_len, 48, 128]))
            reshape373: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape372, R.shape([seq_len, 48, 128]))
            lv471 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(29), R.prim_value(T.float32(1.0)), reshape373), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape374: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv471, R.shape([1, seq_len, 32, 128]))
            reshape375: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape374, R.shape([1, seq_len, 4096]))
            lv472 = R.call_tir(cls.dequantize2, (model_layers_29_self_attn_o_proj_q_weight4, model_layers_29_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims376: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv472, axes=None)
            matmul376: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape375, permute_dims376, out_dtype="void")
            add186: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul376, add185)
            rms_norm189: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add186, model_layers_29_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv473 = R.call_tir(cls.dequantize3, (model_layers_29_mlp_gate_up_proj_q_weight4, model_layers_29_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims377: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv473, axes=None)
            matmul377: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm189, permute_dims377, out_dtype="void")
            split93: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul377, indices_or_sections=2, axis=-1)
            split_093: R.Tensor((1, seq_len, 14336), dtype="float16") = split93[0]
            split_193: R.Tensor((1, seq_len, 14336), dtype="float16") = split93[1]
            silu93: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_093)
            mul93: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu93, split_193)
            lv474 = R.call_tir(cls.dequantize4, (model_layers_29_mlp_down_proj_q_weight4, model_layers_29_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims378: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv474, axes=None)
            matmul378: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul93, permute_dims378, out_dtype="void")
            add187: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul378, add186)
            rms_norm190: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add187, model_layers_30_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv475 = R.call_tir(cls.dequantize1, (model_layers_30_self_attn_qkv_proj_q_weight4, model_layers_30_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims379: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv475, axes=None)
            matmul379: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm190, permute_dims379, out_dtype="void")
            reshape376: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul379, R.shape([1, seq_len, 48, 128]))
            reshape377: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape376, R.shape([seq_len, 48, 128]))
            lv476 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(30), R.prim_value(T.float32(1.0)), reshape377), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape378: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv476, R.shape([1, seq_len, 32, 128]))
            reshape379: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape378, R.shape([1, seq_len, 4096]))
            lv477 = R.call_tir(cls.dequantize2, (model_layers_30_self_attn_o_proj_q_weight4, model_layers_30_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims380: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv477, axes=None)
            matmul380: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape379, permute_dims380, out_dtype="void")
            add188: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul380, add187)
            rms_norm191: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add188, model_layers_30_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv478 = R.call_tir(cls.dequantize3, (model_layers_30_mlp_gate_up_proj_q_weight4, model_layers_30_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims381: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv478, axes=None)
            matmul381: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm191, permute_dims381, out_dtype="void")
            split94: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul381, indices_or_sections=2, axis=-1)
            split_094: R.Tensor((1, seq_len, 14336), dtype="float16") = split94[0]
            split_194: R.Tensor((1, seq_len, 14336), dtype="float16") = split94[1]
            silu94: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_094)
            mul94: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu94, split_194)
            lv479 = R.call_tir(cls.dequantize4, (model_layers_30_mlp_down_proj_q_weight4, model_layers_30_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims382: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv479, axes=None)
            matmul382: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul94, permute_dims382, out_dtype="void")
            add189: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul382, add188)
            rms_norm192: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add189, model_layers_31_input_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv480 = R.call_tir(cls.dequantize1, (model_layers_31_self_attn_qkv_proj_q_weight4, model_layers_31_self_attn_qkv_proj_q_scale4), out_sinfo=R.Tensor((6144, 4096), dtype="float16"))
            permute_dims383: R.Tensor((4096, 6144), dtype="float16") = R.permute_dims(lv480, axes=None)
            matmul383: R.Tensor((1, seq_len, 6144), dtype="float16") = R.matmul(rms_norm192, permute_dims383, out_dtype="void")
            reshape380: R.Tensor((1, seq_len, 48, 128), dtype="float16") = R.reshape(matmul383, R.shape([1, seq_len, 48, 128]))
            reshape381: R.Tensor((seq_len, 48, 128), dtype="float16") = R.reshape(reshape380, R.shape([seq_len, 48, 128]))
            lv481 = R.call_dps_packed("vm.builtin.attention_kv_cache_attention_with_fused_qkv", (paged_kv_cache, R.prim_value(31), R.prim_value(T.float32(1.0)), reshape381), out_sinfo=R.Tensor((seq_len, 32, 128), dtype="float16"))
            reshape382: R.Tensor((1, seq_len, 32, 128), dtype="float16") = R.reshape(lv481, R.shape([1, seq_len, 32, 128]))
            reshape383: R.Tensor((1, seq_len, 4096), dtype="float16") = R.reshape(reshape382, R.shape([1, seq_len, 4096]))
            lv482 = R.call_tir(cls.dequantize2, (model_layers_31_self_attn_o_proj_q_weight4, model_layers_31_self_attn_o_proj_q_scale4), out_sinfo=R.Tensor((4096, 4096), dtype="float16"))
            permute_dims384: R.Tensor((4096, 4096), dtype="float16") = R.permute_dims(lv482, axes=None)
            matmul384: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(reshape383, permute_dims384, out_dtype="void")
            add190: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul384, add189)
            rms_norm193: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add190, model_layers_31_post_attention_layernorm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            lv483 = R.call_tir(cls.dequantize3, (model_layers_31_mlp_gate_up_proj_q_weight4, model_layers_31_mlp_gate_up_proj_q_scale4), out_sinfo=R.Tensor((28672, 4096), dtype="float16"))
            permute_dims385: R.Tensor((4096, 28672), dtype="float16") = R.permute_dims(lv483, axes=None)
            matmul385: R.Tensor((1, seq_len, 28672), dtype="float16") = R.matmul(rms_norm193, permute_dims385, out_dtype="void")
            split95: R.Tuple(R.Tensor((1, seq_len, 14336), dtype="float16"), R.Tensor((1, seq_len, 14336), dtype="float16")) = R.split(matmul385, indices_or_sections=2, axis=-1)
            split_095: R.Tensor((1, seq_len, 14336), dtype="float16") = split95[0]
            split_195: R.Tensor((1, seq_len, 14336), dtype="float16") = split95[1]
            silu95: R.Tensor((1, seq_len, 14336), dtype="float16") = R.nn.silu(split_095)
            mul95: R.Tensor((1, seq_len, 14336), dtype="float16") = R.multiply(silu95, split_195)
            lv484 = R.call_tir(cls.dequantize4, (model_layers_31_mlp_down_proj_q_weight4, model_layers_31_mlp_down_proj_q_scale4), out_sinfo=R.Tensor((4096, 14336), dtype="float16"))
            permute_dims386: R.Tensor((14336, 4096), dtype="float16") = R.permute_dims(lv484, axes=None)
            matmul386: R.Tensor((1, seq_len, 4096), dtype="float16") = R.matmul(mul95, permute_dims386, out_dtype="void")
            add191: R.Tensor((1, seq_len, 4096), dtype="float16") = R.add(matmul386, add190)
            rms_norm194: R.Tensor((1, seq_len, 4096), dtype="float16") = R.nn.rms_norm(add191, model_norm_weight4, axes=[-1], epsilon=1.0000000000000001e-05)
            gv5: R.Tuple(R.Tensor((1, seq_len, 4096), dtype="float16"), R.Object) = rms_norm194, paged_kv_cache
            R.output(gv5)
        return gv5

    @R.function
    def renormalize_by_top_p(probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), top_p: R.Tensor(("batch_size",), dtype="float32"), init_pivots: R.Tensor(("batch_size", 3), dtype="float32")) -> R.Tensor(("batch_size", "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            lv6 = R.call_tir(cls.top_p_pivot_cutoff, (probs, top_p, init_pivots), out_sinfo=[R.Tensor((batch_size,), dtype="float32"), R.Tensor((batch_size,), dtype="float32")])
            lv7: R.Tensor((batch_size,), dtype="float32") = lv6[0]
            lv8: R.Tensor((batch_size,), dtype="float32") = lv6[1]
            gv5 = R.call_tir(cls.top_p_renorm_after_cutoff, (probs, lv7, lv8), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            R.output(gv5)
        return gv5

    @R.function
    def sample_with_top_p(sorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), uniform_samples: R.Tensor(("num_samples",), dtype="float32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), top_p: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("num_samples",), dtype="int32"):
        num_samples = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            sorted_probs_1: R.Tensor((batch_size, vocab_size), dtype="float32") = sorted_probs
            sorted_indices_1: R.Tensor((batch_size, vocab_size), dtype="int32") = sorted_indices
            uniform_samples1: R.Tensor((num_samples, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", uniform_samples, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="float32"),))
            sample_indices1: R.Tensor((num_samples, 1), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", sample_indices, R.shape([num_samples, 1]), sinfo_args=(R.Tensor((num_samples, 1), dtype="int32"),))
            sample_indices2: R.Tensor((batch_size, 1), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", top_p, R.shape([batch_size, 1]), sinfo_args=(R.Tensor((batch_size, 1), dtype="float32"),))
            lv3 = R.call_tir(cls.full, R.tuple(), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"), tir_vars=R.shape([vocab_size]))
            cumsum: R.Tensor((batch_size, vocab_size), dtype="float32") = R.cumsum(sorted_probs_1, axis=1, dtype="void", exclusive=False)
            lv4 = R.call_tir(cls.get_renorm_prob, (cumsum, sample_indices2, lv3), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv5 = R.call_tir(cls.get_index_from_sorted, (cumsum, sorted_indices_1, lv4, uniform_samples1, sample_indices1), out_sinfo=R.Tensor((num_samples, 1), dtype="int32"))
            gv2: R.Tensor((num_samples,), dtype="int32") = R.call_pure_packed("vm.builtin.reshape", lv5, R.shape([num_samples]), sinfo_args=(R.Tensor((num_samples,), dtype="int32"),))
            R.output(gv2)
        return gv2

    @R.function
    def sampler_take_probs(unsorted_probs: R.Tensor(("batch_size", "vocab_size"), dtype="float32"), sorted_indices: R.Tensor(("batch_size", "vocab_size"), dtype="int32"), sample_indices: R.Tensor(("num_samples",), dtype="int32"), sampling_result: R.Tensor(("num_samples",), dtype="int32"), lobprob_offsets: R.Tensor(("num_positions",), dtype="int32")) -> R.Tuple(R.Tensor(("num_samples",), dtype="float32"), R.Tensor(("num_positions",), dtype="float32"), R.Tensor(("num_positions",), dtype="int32")):
        num_samples = T.int64(is_size_var=True)
        num_positions = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv3 = R.call_tir(cls.sampler_take_probs_tir, (unsorted_probs, sorted_indices, sample_indices, sampling_result, lobprob_offsets), out_sinfo=[R.Tensor((num_samples,), dtype="float32"), R.Tensor((num_positions,), dtype="float32"), R.Tensor((num_positions,), dtype="int32")])
            R.output(gv3)
        return gv3

    @R.function
    def sampler_verify_draft_tokens(draft_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), draft_tokens: R.Tensor(("num_nodes",), dtype="int32"), model_probs: R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), token_tree_first_child: R.Tensor(("num_nodes",), dtype="int32"), token_tree_next_sibling: R.Tensor(("num_nodes",), dtype="int32"), uniform_samples: R.Tensor(("num_nodes",), dtype="float32"), token_tree_parent_ptr: R.Tensor(("nbatch",), dtype="int32")) -> R.Tuple(R.Tensor(("num_nodes", "vocab_size"), dtype="float32"), R.Tensor(("nbatch",), dtype="int32")):
        num_nodes = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        nbatch = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "num_positions": 768, "num_samples": 128}})
        cls = Module
        with R.dataflow():
            gv4: R.Tuple(R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")) = R.call_tir_inplace(cls.batch_verify_on_gpu_single_kernel, (draft_probs, draft_tokens, model_probs, token_tree_first_child, token_tree_next_sibling, uniform_samples, token_tree_parent_ptr), out_sinfo=[R.Tensor((num_nodes, vocab_size), dtype="float32"), R.Tensor((nbatch,), dtype="int32")], inplace_indices=[2, 6])
            R.output(gv4)
        return gv4

    @R.function
    def scatter_hidden_states(src: R.Tensor(("batch_size", "n"), dtype="float16"), indices: R.Tensor(("batch_size",), dtype="int32"), dst: R.Tensor(("m", "n"), dtype="float16")) -> R.Tensor(("m", "n"), dtype="float16"):
        m = T.int64(is_size_var=True)
        n = T.int64(is_size_var=True)
        batch_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True})
        cls = Module
        with R.dataflow():
            gv1: R.Tensor((m, n), dtype="float16") = R.call_tir_inplace(cls._scatter_hidden_states, (src, indices, dst), out_sinfo=R.Tensor((m, n), dtype="float16"), inplace_indices=[2])
            R.output(gv1)
        return gv1

    @R.function
    def softmax_with_temperature(logits: R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"), temperature: R.Tensor(("batch_size",), dtype="float32")) -> R.Tensor(("batch_size", 1, "vocab_size"), dtype="float32"):
        batch_size = T.int64(is_size_var=True)
        vocab_size = T.int64(is_size_var=True)
        R.func_attr({"relax.memory_plan_dynamic_func_output": True, "tir_non_negative_var": ["vocab_size"], "tir_var_upper_bound": {"batch_size": 128, "seq_len": 8192, "total_seq_len": 8192}})
        cls = Module
        with R.dataflow():
            lv: R.Tensor((batch_size, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", logits, R.shape([batch_size, vocab_size]), sinfo_args=(R.Tensor((batch_size, vocab_size), dtype="float32"),))
            lv1 = R.call_tir(cls.chunk_lse, (lv, temperature), out_sinfo=[R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32"), R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32")])
            lv2: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[0]
            lv3: R.Tensor((batch_size, (vocab_size + 4096 - 1) // 4096), dtype="float32") = lv1[1]
            lv4 = R.call_tir(cls.softmax_with_chunked_sum, (lv, temperature, lv2, lv3), out_sinfo=R.Tensor((batch_size, vocab_size), dtype="float32"))
            gv: R.Tensor((batch_size, 1, vocab_size), dtype="float32") = R.call_pure_packed("vm.builtin.reshape", lv4, R.shape([batch_size, 1, vocab_size]), sinfo_args=(R.Tensor((batch_size, 1, vocab_size), dtype="float32"),))
            R.output(gv)
        return gv