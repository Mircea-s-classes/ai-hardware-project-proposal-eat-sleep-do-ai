# MLC LLM å¿«é€Ÿå¯åŠ¨æŒ‡å—

## âœ… ç¯å¢ƒé…ç½®æ€»ç»“

### å·²å®Œæˆé…ç½®
1. **Conda ç¯å¢ƒ**: `mlc-llm-env` (Python 3.11)
2. **ä¾èµ–å®‰è£…**: PyTorch, Transformers, MLC LLM (CUDA 12.1ç‰ˆæœ¬)
3. **Git LFS**: å·²å®‰è£…å¹¶åˆå§‹åŒ–
4. **ä»“åº“å…‹éš†**: `/home/liangtaodai/dailt_workplace/mlc_llm/`
5. **CUDA å·¥å…·é“¾**: é€šè¿‡ module load åŠ è½½ CUDA 12.4

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ¯æ¬¡ä½¿ç”¨æ—¶å¿…é¡»æ‰§è¡Œçš„å‘½ä»¤

```bash
# 1. åŠ è½½ CUDA æ¨¡å—ï¼ˆé‡è¦ï¼ï¼‰
module load cuda

# 2. æ¿€æ´» conda ç¯å¢ƒ
conda activate mlc-llm-env

# 3. è¿è¡Œ MLC LLM
mlc_llm chat HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC
```

### å¿«æ·æ–¹å¼

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æˆ‘åˆ›å»ºçš„å¯åŠ¨è„šæœ¬ï¼š
```bash
cd /home/liangtaodai/dailt_workplace/mlc_llm
chmod +x run_mlc.sh
./run_mlc.sh
```

## ğŸ“ è¿è¡Œç¤ºä¾‹

### å‘½ä»¤è¡ŒèŠå¤©
```bash
mlc_llm chat HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC
```

èŠå¤©ç•Œé¢ç‰¹æ®Šå‘½ä»¤ï¼š
- `/help` - æŸ¥çœ‹å¸®åŠ©
- `/exit` - é€€å‡º
- `/reset` - é‡ç½®å¯¹è¯
- `/stats` - æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
- `/metrics` - æ˜¾ç¤ºå¼•æ“æŒ‡æ ‡
- `/set temperature=0.5;top_p=0.8` - è®¾ç½®ç”Ÿæˆå‚æ•°

### Python API ä½¿ç”¨

```python
from mlc_llm import MLCEngine

model = "HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC"
engine = MLCEngine(model)

response = engine.chat.completions.create(
    messages=[{"role": "user", "content": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}],
    model=model,
    stream=True
)

for r in response:
    for choice in r.choices:
        print(choice.delta.content, end="", flush=True)
print("\n")

engine.terminate()
```

### REST Server

```bash
# å¯åŠ¨æœåŠ¡å™¨
mlc_llm serve HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯å‘é€è¯·æ±‚
curl -X POST \
  -H "Content-Type: application/json" \
  -d '{
        "model": "HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC",
        "messages": [{"role": "user", "content": "ä½ å¥½ï¼"}]
  }' \
  http://127.0.0.1:8000/v1/chat/completions
```

## ğŸ”§ å¸¸è§é—®é¢˜

### é—®é¢˜1: nvcc æœªæ‰¾åˆ°
**è§£å†³æ–¹æ³•**: 
```bash
module load cuda
```

### é—®é¢˜2: æ¨¡å‹ä¸‹è½½å¤±è´¥
**è§£å†³æ–¹æ³•**: é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆçº¦5GBï¼‰ï¼Œç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ã€‚

### é—®é¢˜3: GPU æ˜¾å­˜ä¸è¶³
**è§£å†³æ–¹æ³•**: 
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- å‡å°‘ `context_window_size`
- ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬æ¨¡å‹

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

æ ¹æ®è¿è¡Œæ—¥å¿—ï¼š
- **CUDA æ¶æ„**: sm_89 (RTX 4090)
- **æ˜¾å­˜ä½¿ç”¨**: ~8GB
- **æ¨¡å‹å‚æ•°**: 4308 MB
- **KV Cache**: 1234 MB
- **ä¸´æ—¶ç¼“å†²åŒº**: 2626 MB

## ğŸ“š æ›´å¤šèµ„æº

- å®˜æ–¹æ–‡æ¡£: https://llm.mlc.ai/docs/
- ç¤ºä¾‹ä»£ç : `mlc_llm/examples/python/sample_mlc_engine.py`
- å¿«é€Ÿå¼€å§‹: `mlc_llm/docs/get_started/quick_start.rst`

## ğŸ‰ æ­å–œï¼

ç°åœ¨æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨ MLC LLM è¿›è¡Œ LLM æ¨ç†å’Œéƒ¨ç½²äº†ï¼

